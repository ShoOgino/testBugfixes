{"path":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest#matchedFieldsTestCase(boolean,boolean,String,String,Query...).mjava","commits":[{"id":"050e6201c3a7d4c351ebc06cbe4822e26e028117","date":1382375603,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest#matchedFieldsTestCase(boolean,boolean,String,String,Query...).mjava","pathOld":"/dev/null","sourceNew":"  private void matchedFieldsTestCase( boolean useMatchedFields, boolean fieldMatch, String fieldValue, String expected, Query... queryClauses ) throws IOException {\n    Document doc = new Document();\n    FieldType stored = new FieldType( TextField.TYPE_STORED );\n    stored.setStoreTermVectorOffsets( true );\n    stored.setStoreTermVectorPositions( true );\n    stored.setStoreTermVectors( true );\n    stored.freeze();\n    FieldType matched = new FieldType( TextField.TYPE_NOT_STORED );\n    matched.setStoreTermVectorOffsets( true );\n    matched.setStoreTermVectorPositions( true );\n    matched.setStoreTermVectors( true );\n    matched.freeze();\n    doc.add( new Field( \"field\", fieldValue, stored ) );               // Whitespace tokenized with English stop words\n    doc.add( new Field( \"field_exact\", fieldValue, matched ) );        // Whitespace tokenized without stop words\n    doc.add( new Field( \"field_super_exact\", fieldValue, matched ) );  // Whitespace tokenized without toLower\n    doc.add( new Field( \"field_characters\", fieldValue, matched ) );   // Each letter is a token\n    doc.add( new Field( \"field_tripples\", fieldValue, matched ) );     // Every three letters is a token\n    doc.add( new Field( \"field_sliced\", fieldValue.substring( 0,       // Sliced at 10 chars then analyzed just like field\n      Math.min( fieldValue.length() - 1 , 10 ) ), matched ) );\n    doc.add( new Field( \"field_der_red\", new CannedTokenStream(        // Hacky field containing \"der\" and \"red\" at pos = 0\n          token( \"der\", 1, 0, 3 ),\n          token( \"red\", 0, 0, 3 )\n        ), matched ) );\n\n    final Map<String, Analyzer> fieldAnalyzers = new TreeMap<String, Analyzer>();\n    fieldAnalyzers.put( \"field\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, true, MockTokenFilter.ENGLISH_STOPSET ) );\n    fieldAnalyzers.put( \"field_exact\", new MockAnalyzer( random() ) );\n    fieldAnalyzers.put( \"field_super_exact\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, false ) );\n    fieldAnalyzers.put( \"field_characters\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\".\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_tripples\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\"...\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_sliced\", fieldAnalyzers.get( \"field\" ) );\n    fieldAnalyzers.put( \"field_der_red\", fieldAnalyzers.get( \"field\" ) );  // This is required even though we provide a token stream\n    Analyzer analyzer = new AnalyzerWrapper() {\n      public Analyzer getWrappedAnalyzer(String fieldName) {\n        return fieldAnalyzers.get( fieldName );\n      }\n    };\n\n    Directory dir = newDirectory();\n    IndexWriter writer = new IndexWriter( dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer ) );\n    writer.addDocument( doc );\n\n    FastVectorHighlighter highlighter = new FastVectorHighlighter();\n    FragListBuilder fragListBuilder = new SimpleFragListBuilder();\n    FragmentsBuilder fragmentsBuilder = new ScoreOrderFragmentsBuilder();\n    IndexReader reader = DirectoryReader.open( writer, true );\n    String[] preTags = new String[] { \"<b>\" };\n    String[] postTags = new String[] { \"</b>\" };\n    Encoder encoder = new DefaultEncoder();\n    int docId = 0;\n    BooleanQuery query = new BooleanQuery();\n    for ( Query clause : queryClauses ) {\n      query.add( clause, Occur.MUST );\n    }\n    FieldQuery fieldQuery = new FieldQuery( query, reader, true, fieldMatch );\n    String[] bestFragments;\n    if ( useMatchedFields ) {\n      Set< String > matchedFields = new HashSet< String >();\n      matchedFields.add( \"field\" );\n      matchedFields.add( \"field_exact\" );\n      matchedFields.add( \"field_super_exact\" );\n      matchedFields.add( \"field_characters\" );\n      matchedFields.add( \"field_tripples\" );\n      matchedFields.add( \"field_sliced\" );\n      matchedFields.add( \"field_der_red\" );\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", matchedFields, 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    } else {\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    }\n    assertEquals( expected, bestFragments[ 0 ] );\n\n    reader.close();\n    writer.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest#matchedFieldsTestCase(boolean,boolean,String,String,Query...).mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest#matchedFieldsTestCase(boolean,boolean,String,String,Query...).mjava","sourceNew":"  private void matchedFieldsTestCase( boolean useMatchedFields, boolean fieldMatch, String fieldValue, String expected, Query... queryClauses ) throws IOException {\n    Document doc = new Document();\n    FieldType stored = new FieldType( TextField.TYPE_STORED );\n    stored.setStoreTermVectorOffsets( true );\n    stored.setStoreTermVectorPositions( true );\n    stored.setStoreTermVectors( true );\n    stored.freeze();\n    FieldType matched = new FieldType( TextField.TYPE_NOT_STORED );\n    matched.setStoreTermVectorOffsets( true );\n    matched.setStoreTermVectorPositions( true );\n    matched.setStoreTermVectors( true );\n    matched.freeze();\n    doc.add( new Field( \"field\", fieldValue, stored ) );               // Whitespace tokenized with English stop words\n    doc.add( new Field( \"field_exact\", fieldValue, matched ) );        // Whitespace tokenized without stop words\n    doc.add( new Field( \"field_super_exact\", fieldValue, matched ) );  // Whitespace tokenized without toLower\n    doc.add( new Field( \"field_characters\", fieldValue, matched ) );   // Each letter is a token\n    doc.add( new Field( \"field_tripples\", fieldValue, matched ) );     // Every three letters is a token\n    doc.add( new Field( \"field_sliced\", fieldValue.substring( 0,       // Sliced at 10 chars then analyzed just like field\n      Math.min( fieldValue.length() - 1 , 10 ) ), matched ) );\n    doc.add( new Field( \"field_der_red\", new CannedTokenStream(        // Hacky field containing \"der\" and \"red\" at pos = 0\n          token( \"der\", 1, 0, 3 ),\n          token( \"red\", 0, 0, 3 )\n        ), matched ) );\n\n    final Map<String, Analyzer> fieldAnalyzers = new TreeMap<>();\n    fieldAnalyzers.put( \"field\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, true, MockTokenFilter.ENGLISH_STOPSET ) );\n    fieldAnalyzers.put( \"field_exact\", new MockAnalyzer( random() ) );\n    fieldAnalyzers.put( \"field_super_exact\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, false ) );\n    fieldAnalyzers.put( \"field_characters\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\".\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_tripples\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\"...\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_sliced\", fieldAnalyzers.get( \"field\" ) );\n    fieldAnalyzers.put( \"field_der_red\", fieldAnalyzers.get( \"field\" ) );  // This is required even though we provide a token stream\n    Analyzer analyzer = new AnalyzerWrapper() {\n      public Analyzer getWrappedAnalyzer(String fieldName) {\n        return fieldAnalyzers.get( fieldName );\n      }\n    };\n\n    Directory dir = newDirectory();\n    IndexWriter writer = new IndexWriter( dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer ) );\n    writer.addDocument( doc );\n\n    FastVectorHighlighter highlighter = new FastVectorHighlighter();\n    FragListBuilder fragListBuilder = new SimpleFragListBuilder();\n    FragmentsBuilder fragmentsBuilder = new ScoreOrderFragmentsBuilder();\n    IndexReader reader = DirectoryReader.open( writer, true );\n    String[] preTags = new String[] { \"<b>\" };\n    String[] postTags = new String[] { \"</b>\" };\n    Encoder encoder = new DefaultEncoder();\n    int docId = 0;\n    BooleanQuery query = new BooleanQuery();\n    for ( Query clause : queryClauses ) {\n      query.add( clause, Occur.MUST );\n    }\n    FieldQuery fieldQuery = new FieldQuery( query, reader, true, fieldMatch );\n    String[] bestFragments;\n    if ( useMatchedFields ) {\n      Set< String > matchedFields = new HashSet<>();\n      matchedFields.add( \"field\" );\n      matchedFields.add( \"field_exact\" );\n      matchedFields.add( \"field_super_exact\" );\n      matchedFields.add( \"field_characters\" );\n      matchedFields.add( \"field_tripples\" );\n      matchedFields.add( \"field_sliced\" );\n      matchedFields.add( \"field_der_red\" );\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", matchedFields, 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    } else {\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    }\n    assertEquals( expected, bestFragments[ 0 ] );\n\n    reader.close();\n    writer.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void matchedFieldsTestCase( boolean useMatchedFields, boolean fieldMatch, String fieldValue, String expected, Query... queryClauses ) throws IOException {\n    Document doc = new Document();\n    FieldType stored = new FieldType( TextField.TYPE_STORED );\n    stored.setStoreTermVectorOffsets( true );\n    stored.setStoreTermVectorPositions( true );\n    stored.setStoreTermVectors( true );\n    stored.freeze();\n    FieldType matched = new FieldType( TextField.TYPE_NOT_STORED );\n    matched.setStoreTermVectorOffsets( true );\n    matched.setStoreTermVectorPositions( true );\n    matched.setStoreTermVectors( true );\n    matched.freeze();\n    doc.add( new Field( \"field\", fieldValue, stored ) );               // Whitespace tokenized with English stop words\n    doc.add( new Field( \"field_exact\", fieldValue, matched ) );        // Whitespace tokenized without stop words\n    doc.add( new Field( \"field_super_exact\", fieldValue, matched ) );  // Whitespace tokenized without toLower\n    doc.add( new Field( \"field_characters\", fieldValue, matched ) );   // Each letter is a token\n    doc.add( new Field( \"field_tripples\", fieldValue, matched ) );     // Every three letters is a token\n    doc.add( new Field( \"field_sliced\", fieldValue.substring( 0,       // Sliced at 10 chars then analyzed just like field\n      Math.min( fieldValue.length() - 1 , 10 ) ), matched ) );\n    doc.add( new Field( \"field_der_red\", new CannedTokenStream(        // Hacky field containing \"der\" and \"red\" at pos = 0\n          token( \"der\", 1, 0, 3 ),\n          token( \"red\", 0, 0, 3 )\n        ), matched ) );\n\n    final Map<String, Analyzer> fieldAnalyzers = new TreeMap<String, Analyzer>();\n    fieldAnalyzers.put( \"field\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, true, MockTokenFilter.ENGLISH_STOPSET ) );\n    fieldAnalyzers.put( \"field_exact\", new MockAnalyzer( random() ) );\n    fieldAnalyzers.put( \"field_super_exact\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, false ) );\n    fieldAnalyzers.put( \"field_characters\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\".\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_tripples\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\"...\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_sliced\", fieldAnalyzers.get( \"field\" ) );\n    fieldAnalyzers.put( \"field_der_red\", fieldAnalyzers.get( \"field\" ) );  // This is required even though we provide a token stream\n    Analyzer analyzer = new AnalyzerWrapper() {\n      public Analyzer getWrappedAnalyzer(String fieldName) {\n        return fieldAnalyzers.get( fieldName );\n      }\n    };\n\n    Directory dir = newDirectory();\n    IndexWriter writer = new IndexWriter( dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer ) );\n    writer.addDocument( doc );\n\n    FastVectorHighlighter highlighter = new FastVectorHighlighter();\n    FragListBuilder fragListBuilder = new SimpleFragListBuilder();\n    FragmentsBuilder fragmentsBuilder = new ScoreOrderFragmentsBuilder();\n    IndexReader reader = DirectoryReader.open( writer, true );\n    String[] preTags = new String[] { \"<b>\" };\n    String[] postTags = new String[] { \"</b>\" };\n    Encoder encoder = new DefaultEncoder();\n    int docId = 0;\n    BooleanQuery query = new BooleanQuery();\n    for ( Query clause : queryClauses ) {\n      query.add( clause, Occur.MUST );\n    }\n    FieldQuery fieldQuery = new FieldQuery( query, reader, true, fieldMatch );\n    String[] bestFragments;\n    if ( useMatchedFields ) {\n      Set< String > matchedFields = new HashSet< String >();\n      matchedFields.add( \"field\" );\n      matchedFields.add( \"field_exact\" );\n      matchedFields.add( \"field_super_exact\" );\n      matchedFields.add( \"field_characters\" );\n      matchedFields.add( \"field_tripples\" );\n      matchedFields.add( \"field_sliced\" );\n      matchedFields.add( \"field_der_red\" );\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", matchedFields, 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    } else {\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    }\n    assertEquals( expected, bestFragments[ 0 ] );\n\n    reader.close();\n    writer.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest#matchedFieldsTestCase(boolean,boolean,String,String,Query...).mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest#matchedFieldsTestCase(boolean,boolean,String,String,Query...).mjava","sourceNew":"  private void matchedFieldsTestCase( boolean useMatchedFields, boolean fieldMatch, String fieldValue, String expected, Query... queryClauses ) throws IOException {\n    Document doc = new Document();\n    FieldType stored = new FieldType( TextField.TYPE_STORED );\n    stored.setStoreTermVectorOffsets( true );\n    stored.setStoreTermVectorPositions( true );\n    stored.setStoreTermVectors( true );\n    stored.freeze();\n    FieldType matched = new FieldType( TextField.TYPE_NOT_STORED );\n    matched.setStoreTermVectorOffsets( true );\n    matched.setStoreTermVectorPositions( true );\n    matched.setStoreTermVectors( true );\n    matched.freeze();\n    doc.add( new Field( \"field\", fieldValue, stored ) );               // Whitespace tokenized with English stop words\n    doc.add( new Field( \"field_exact\", fieldValue, matched ) );        // Whitespace tokenized without stop words\n    doc.add( new Field( \"field_super_exact\", fieldValue, matched ) );  // Whitespace tokenized without toLower\n    doc.add( new Field( \"field_characters\", fieldValue, matched ) );   // Each letter is a token\n    doc.add( new Field( \"field_tripples\", fieldValue, matched ) );     // Every three letters is a token\n    doc.add( new Field( \"field_sliced\", fieldValue.substring( 0,       // Sliced at 10 chars then analyzed just like field\n      Math.min( fieldValue.length() - 1 , 10 ) ), matched ) );\n    doc.add( new Field( \"field_der_red\", new CannedTokenStream(        // Hacky field containing \"der\" and \"red\" at pos = 0\n          token( \"der\", 1, 0, 3 ),\n          token( \"red\", 0, 0, 3 )\n        ), matched ) );\n\n    final Map<String, Analyzer> fieldAnalyzers = new TreeMap<>();\n    fieldAnalyzers.put( \"field\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, true, MockTokenFilter.ENGLISH_STOPSET ) );\n    fieldAnalyzers.put( \"field_exact\", new MockAnalyzer( random() ) );\n    fieldAnalyzers.put( \"field_super_exact\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, false ) );\n    fieldAnalyzers.put( \"field_characters\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\".\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_tripples\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\"...\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_sliced\", fieldAnalyzers.get( \"field\" ) );\n    fieldAnalyzers.put( \"field_der_red\", fieldAnalyzers.get( \"field\" ) );  // This is required even though we provide a token stream\n    Analyzer analyzer = new AnalyzerWrapper() {\n      public Analyzer getWrappedAnalyzer(String fieldName) {\n        return fieldAnalyzers.get( fieldName );\n      }\n    };\n\n    Directory dir = newDirectory();\n    IndexWriter writer = new IndexWriter( dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer ) );\n    writer.addDocument( doc );\n\n    FastVectorHighlighter highlighter = new FastVectorHighlighter();\n    FragListBuilder fragListBuilder = new SimpleFragListBuilder();\n    FragmentsBuilder fragmentsBuilder = new ScoreOrderFragmentsBuilder();\n    IndexReader reader = DirectoryReader.open( writer, true );\n    String[] preTags = new String[] { \"<b>\" };\n    String[] postTags = new String[] { \"</b>\" };\n    Encoder encoder = new DefaultEncoder();\n    int docId = 0;\n    BooleanQuery query = new BooleanQuery();\n    for ( Query clause : queryClauses ) {\n      query.add( clause, Occur.MUST );\n    }\n    FieldQuery fieldQuery = new FieldQuery( query, reader, true, fieldMatch );\n    String[] bestFragments;\n    if ( useMatchedFields ) {\n      Set< String > matchedFields = new HashSet<>();\n      matchedFields.add( \"field\" );\n      matchedFields.add( \"field_exact\" );\n      matchedFields.add( \"field_super_exact\" );\n      matchedFields.add( \"field_characters\" );\n      matchedFields.add( \"field_tripples\" );\n      matchedFields.add( \"field_sliced\" );\n      matchedFields.add( \"field_der_red\" );\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", matchedFields, 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    } else {\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    }\n    assertEquals( expected, bestFragments[ 0 ] );\n\n    reader.close();\n    writer.shutdown();\n    dir.close();\n  }\n\n","sourceOld":"  private void matchedFieldsTestCase( boolean useMatchedFields, boolean fieldMatch, String fieldValue, String expected, Query... queryClauses ) throws IOException {\n    Document doc = new Document();\n    FieldType stored = new FieldType( TextField.TYPE_STORED );\n    stored.setStoreTermVectorOffsets( true );\n    stored.setStoreTermVectorPositions( true );\n    stored.setStoreTermVectors( true );\n    stored.freeze();\n    FieldType matched = new FieldType( TextField.TYPE_NOT_STORED );\n    matched.setStoreTermVectorOffsets( true );\n    matched.setStoreTermVectorPositions( true );\n    matched.setStoreTermVectors( true );\n    matched.freeze();\n    doc.add( new Field( \"field\", fieldValue, stored ) );               // Whitespace tokenized with English stop words\n    doc.add( new Field( \"field_exact\", fieldValue, matched ) );        // Whitespace tokenized without stop words\n    doc.add( new Field( \"field_super_exact\", fieldValue, matched ) );  // Whitespace tokenized without toLower\n    doc.add( new Field( \"field_characters\", fieldValue, matched ) );   // Each letter is a token\n    doc.add( new Field( \"field_tripples\", fieldValue, matched ) );     // Every three letters is a token\n    doc.add( new Field( \"field_sliced\", fieldValue.substring( 0,       // Sliced at 10 chars then analyzed just like field\n      Math.min( fieldValue.length() - 1 , 10 ) ), matched ) );\n    doc.add( new Field( \"field_der_red\", new CannedTokenStream(        // Hacky field containing \"der\" and \"red\" at pos = 0\n          token( \"der\", 1, 0, 3 ),\n          token( \"red\", 0, 0, 3 )\n        ), matched ) );\n\n    final Map<String, Analyzer> fieldAnalyzers = new TreeMap<>();\n    fieldAnalyzers.put( \"field\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, true, MockTokenFilter.ENGLISH_STOPSET ) );\n    fieldAnalyzers.put( \"field_exact\", new MockAnalyzer( random() ) );\n    fieldAnalyzers.put( \"field_super_exact\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, false ) );\n    fieldAnalyzers.put( \"field_characters\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\".\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_tripples\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\"...\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_sliced\", fieldAnalyzers.get( \"field\" ) );\n    fieldAnalyzers.put( \"field_der_red\", fieldAnalyzers.get( \"field\" ) );  // This is required even though we provide a token stream\n    Analyzer analyzer = new AnalyzerWrapper() {\n      public Analyzer getWrappedAnalyzer(String fieldName) {\n        return fieldAnalyzers.get( fieldName );\n      }\n    };\n\n    Directory dir = newDirectory();\n    IndexWriter writer = new IndexWriter( dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer ) );\n    writer.addDocument( doc );\n\n    FastVectorHighlighter highlighter = new FastVectorHighlighter();\n    FragListBuilder fragListBuilder = new SimpleFragListBuilder();\n    FragmentsBuilder fragmentsBuilder = new ScoreOrderFragmentsBuilder();\n    IndexReader reader = DirectoryReader.open( writer, true );\n    String[] preTags = new String[] { \"<b>\" };\n    String[] postTags = new String[] { \"</b>\" };\n    Encoder encoder = new DefaultEncoder();\n    int docId = 0;\n    BooleanQuery query = new BooleanQuery();\n    for ( Query clause : queryClauses ) {\n      query.add( clause, Occur.MUST );\n    }\n    FieldQuery fieldQuery = new FieldQuery( query, reader, true, fieldMatch );\n    String[] bestFragments;\n    if ( useMatchedFields ) {\n      Set< String > matchedFields = new HashSet<>();\n      matchedFields.add( \"field\" );\n      matchedFields.add( \"field_exact\" );\n      matchedFields.add( \"field_super_exact\" );\n      matchedFields.add( \"field_characters\" );\n      matchedFields.add( \"field_tripples\" );\n      matchedFields.add( \"field_sliced\" );\n      matchedFields.add( \"field_der_red\" );\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", matchedFields, 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    } else {\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    }\n    assertEquals( expected, bestFragments[ 0 ] );\n\n    reader.close();\n    writer.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ac34f0c5bb9274821fb0cb18075234e02002e9bf","date":1402508126,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest#matchedFieldsTestCase(boolean,boolean,String,String,Query...).mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest#matchedFieldsTestCase(boolean,boolean,String,String,Query...).mjava","sourceNew":"  private void matchedFieldsTestCase( boolean useMatchedFields, boolean fieldMatch, String fieldValue, String expected, Query... queryClauses ) throws IOException {\n    Document doc = new Document();\n    FieldType stored = new FieldType( TextField.TYPE_STORED );\n    stored.setStoreTermVectorOffsets( true );\n    stored.setStoreTermVectorPositions( true );\n    stored.setStoreTermVectors( true );\n    stored.freeze();\n    FieldType matched = new FieldType( TextField.TYPE_NOT_STORED );\n    matched.setStoreTermVectorOffsets( true );\n    matched.setStoreTermVectorPositions( true );\n    matched.setStoreTermVectors( true );\n    matched.freeze();\n    doc.add( new Field( \"field\", fieldValue, stored ) );               // Whitespace tokenized with English stop words\n    doc.add( new Field( \"field_exact\", fieldValue, matched ) );        // Whitespace tokenized without stop words\n    doc.add( new Field( \"field_super_exact\", fieldValue, matched ) );  // Whitespace tokenized without toLower\n    doc.add( new Field( \"field_characters\", fieldValue, matched ) );   // Each letter is a token\n    doc.add( new Field( \"field_tripples\", fieldValue, matched ) );     // Every three letters is a token\n    doc.add( new Field( \"field_sliced\", fieldValue.substring( 0,       // Sliced at 10 chars then analyzed just like field\n      Math.min( fieldValue.length() - 1 , 10 ) ), matched ) );\n    doc.add( new Field( \"field_der_red\", new CannedTokenStream(        // Hacky field containing \"der\" and \"red\" at pos = 0\n          token( \"der\", 1, 0, 3 ),\n          token( \"red\", 0, 0, 3 )\n        ), matched ) );\n\n    final Map<String, Analyzer> fieldAnalyzers = new TreeMap<>();\n    fieldAnalyzers.put( \"field\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, true, MockTokenFilter.ENGLISH_STOPSET ) );\n    fieldAnalyzers.put( \"field_exact\", new MockAnalyzer( random() ) );\n    fieldAnalyzers.put( \"field_super_exact\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, false ) );\n    fieldAnalyzers.put( \"field_characters\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\".\").toLightAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_tripples\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\"...\").toLightAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_sliced\", fieldAnalyzers.get( \"field\" ) );\n    fieldAnalyzers.put( \"field_der_red\", fieldAnalyzers.get( \"field\" ) );  // This is required even though we provide a token stream\n    Analyzer analyzer = new AnalyzerWrapper() {\n      public Analyzer getWrappedAnalyzer(String fieldName) {\n        return fieldAnalyzers.get( fieldName );\n      }\n    };\n\n    Directory dir = newDirectory();\n    IndexWriter writer = new IndexWriter( dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer ) );\n    writer.addDocument( doc );\n\n    FastVectorHighlighter highlighter = new FastVectorHighlighter();\n    FragListBuilder fragListBuilder = new SimpleFragListBuilder();\n    FragmentsBuilder fragmentsBuilder = new ScoreOrderFragmentsBuilder();\n    IndexReader reader = DirectoryReader.open( writer, true );\n    String[] preTags = new String[] { \"<b>\" };\n    String[] postTags = new String[] { \"</b>\" };\n    Encoder encoder = new DefaultEncoder();\n    int docId = 0;\n    BooleanQuery query = new BooleanQuery();\n    for ( Query clause : queryClauses ) {\n      query.add( clause, Occur.MUST );\n    }\n    FieldQuery fieldQuery = new FieldQuery( query, reader, true, fieldMatch );\n    String[] bestFragments;\n    if ( useMatchedFields ) {\n      Set< String > matchedFields = new HashSet<>();\n      matchedFields.add( \"field\" );\n      matchedFields.add( \"field_exact\" );\n      matchedFields.add( \"field_super_exact\" );\n      matchedFields.add( \"field_characters\" );\n      matchedFields.add( \"field_tripples\" );\n      matchedFields.add( \"field_sliced\" );\n      matchedFields.add( \"field_der_red\" );\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", matchedFields, 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    } else {\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    }\n    assertEquals( expected, bestFragments[ 0 ] );\n\n    reader.close();\n    writer.shutdown();\n    dir.close();\n  }\n\n","sourceOld":"  private void matchedFieldsTestCase( boolean useMatchedFields, boolean fieldMatch, String fieldValue, String expected, Query... queryClauses ) throws IOException {\n    Document doc = new Document();\n    FieldType stored = new FieldType( TextField.TYPE_STORED );\n    stored.setStoreTermVectorOffsets( true );\n    stored.setStoreTermVectorPositions( true );\n    stored.setStoreTermVectors( true );\n    stored.freeze();\n    FieldType matched = new FieldType( TextField.TYPE_NOT_STORED );\n    matched.setStoreTermVectorOffsets( true );\n    matched.setStoreTermVectorPositions( true );\n    matched.setStoreTermVectors( true );\n    matched.freeze();\n    doc.add( new Field( \"field\", fieldValue, stored ) );               // Whitespace tokenized with English stop words\n    doc.add( new Field( \"field_exact\", fieldValue, matched ) );        // Whitespace tokenized without stop words\n    doc.add( new Field( \"field_super_exact\", fieldValue, matched ) );  // Whitespace tokenized without toLower\n    doc.add( new Field( \"field_characters\", fieldValue, matched ) );   // Each letter is a token\n    doc.add( new Field( \"field_tripples\", fieldValue, matched ) );     // Every three letters is a token\n    doc.add( new Field( \"field_sliced\", fieldValue.substring( 0,       // Sliced at 10 chars then analyzed just like field\n      Math.min( fieldValue.length() - 1 , 10 ) ), matched ) );\n    doc.add( new Field( \"field_der_red\", new CannedTokenStream(        // Hacky field containing \"der\" and \"red\" at pos = 0\n          token( \"der\", 1, 0, 3 ),\n          token( \"red\", 0, 0, 3 )\n        ), matched ) );\n\n    final Map<String, Analyzer> fieldAnalyzers = new TreeMap<>();\n    fieldAnalyzers.put( \"field\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, true, MockTokenFilter.ENGLISH_STOPSET ) );\n    fieldAnalyzers.put( \"field_exact\", new MockAnalyzer( random() ) );\n    fieldAnalyzers.put( \"field_super_exact\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, false ) );\n    fieldAnalyzers.put( \"field_characters\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\".\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_tripples\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\"...\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_sliced\", fieldAnalyzers.get( \"field\" ) );\n    fieldAnalyzers.put( \"field_der_red\", fieldAnalyzers.get( \"field\" ) );  // This is required even though we provide a token stream\n    Analyzer analyzer = new AnalyzerWrapper() {\n      public Analyzer getWrappedAnalyzer(String fieldName) {\n        return fieldAnalyzers.get( fieldName );\n      }\n    };\n\n    Directory dir = newDirectory();\n    IndexWriter writer = new IndexWriter( dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer ) );\n    writer.addDocument( doc );\n\n    FastVectorHighlighter highlighter = new FastVectorHighlighter();\n    FragListBuilder fragListBuilder = new SimpleFragListBuilder();\n    FragmentsBuilder fragmentsBuilder = new ScoreOrderFragmentsBuilder();\n    IndexReader reader = DirectoryReader.open( writer, true );\n    String[] preTags = new String[] { \"<b>\" };\n    String[] postTags = new String[] { \"</b>\" };\n    Encoder encoder = new DefaultEncoder();\n    int docId = 0;\n    BooleanQuery query = new BooleanQuery();\n    for ( Query clause : queryClauses ) {\n      query.add( clause, Occur.MUST );\n    }\n    FieldQuery fieldQuery = new FieldQuery( query, reader, true, fieldMatch );\n    String[] bestFragments;\n    if ( useMatchedFields ) {\n      Set< String > matchedFields = new HashSet<>();\n      matchedFields.add( \"field\" );\n      matchedFields.add( \"field_exact\" );\n      matchedFields.add( \"field_super_exact\" );\n      matchedFields.add( \"field_characters\" );\n      matchedFields.add( \"field_tripples\" );\n      matchedFields.add( \"field_sliced\" );\n      matchedFields.add( \"field_der_red\" );\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", matchedFields, 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    } else {\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    }\n    assertEquals( expected, bestFragments[ 0 ] );\n\n    reader.close();\n    writer.shutdown();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4ca1c732df8923f5624f6c06b1dcca9e69d98c96","date":1402957391,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest#matchedFieldsTestCase(boolean,boolean,String,String,Query...).mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest#matchedFieldsTestCase(boolean,boolean,String,String,Query...).mjava","sourceNew":"  private void matchedFieldsTestCase( boolean useMatchedFields, boolean fieldMatch, String fieldValue, String expected, Query... queryClauses ) throws IOException {\n    Document doc = new Document();\n    FieldType stored = new FieldType( TextField.TYPE_STORED );\n    stored.setStoreTermVectorOffsets( true );\n    stored.setStoreTermVectorPositions( true );\n    stored.setStoreTermVectors( true );\n    stored.freeze();\n    FieldType matched = new FieldType( TextField.TYPE_NOT_STORED );\n    matched.setStoreTermVectorOffsets( true );\n    matched.setStoreTermVectorPositions( true );\n    matched.setStoreTermVectors( true );\n    matched.freeze();\n    doc.add( new Field( \"field\", fieldValue, stored ) );               // Whitespace tokenized with English stop words\n    doc.add( new Field( \"field_exact\", fieldValue, matched ) );        // Whitespace tokenized without stop words\n    doc.add( new Field( \"field_super_exact\", fieldValue, matched ) );  // Whitespace tokenized without toLower\n    doc.add( new Field( \"field_characters\", fieldValue, matched ) );   // Each letter is a token\n    doc.add( new Field( \"field_tripples\", fieldValue, matched ) );     // Every three letters is a token\n    doc.add( new Field( \"field_sliced\", fieldValue.substring( 0,       // Sliced at 10 chars then analyzed just like field\n      Math.min( fieldValue.length() - 1 , 10 ) ), matched ) );\n    doc.add( new Field( \"field_der_red\", new CannedTokenStream(        // Hacky field containing \"der\" and \"red\" at pos = 0\n          token( \"der\", 1, 0, 3 ),\n          token( \"red\", 0, 0, 3 )\n        ), matched ) );\n\n    final Map<String, Analyzer> fieldAnalyzers = new TreeMap<>();\n    fieldAnalyzers.put( \"field\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, true, MockTokenFilter.ENGLISH_STOPSET ) );\n    fieldAnalyzers.put( \"field_exact\", new MockAnalyzer( random() ) );\n    fieldAnalyzers.put( \"field_super_exact\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, false ) );\n    fieldAnalyzers.put( \"field_characters\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\".\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_tripples\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\"...\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_sliced\", fieldAnalyzers.get( \"field\" ) );\n    fieldAnalyzers.put( \"field_der_red\", fieldAnalyzers.get( \"field\" ) );  // This is required even though we provide a token stream\n    Analyzer analyzer = new AnalyzerWrapper() {\n      public Analyzer getWrappedAnalyzer(String fieldName) {\n        return fieldAnalyzers.get( fieldName );\n      }\n    };\n\n    Directory dir = newDirectory();\n    IndexWriter writer = new IndexWriter( dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer ) );\n    writer.addDocument( doc );\n\n    FastVectorHighlighter highlighter = new FastVectorHighlighter();\n    FragListBuilder fragListBuilder = new SimpleFragListBuilder();\n    FragmentsBuilder fragmentsBuilder = new ScoreOrderFragmentsBuilder();\n    IndexReader reader = DirectoryReader.open( writer, true );\n    String[] preTags = new String[] { \"<b>\" };\n    String[] postTags = new String[] { \"</b>\" };\n    Encoder encoder = new DefaultEncoder();\n    int docId = 0;\n    BooleanQuery query = new BooleanQuery();\n    for ( Query clause : queryClauses ) {\n      query.add( clause, Occur.MUST );\n    }\n    FieldQuery fieldQuery = new FieldQuery( query, reader, true, fieldMatch );\n    String[] bestFragments;\n    if ( useMatchedFields ) {\n      Set< String > matchedFields = new HashSet<>();\n      matchedFields.add( \"field\" );\n      matchedFields.add( \"field_exact\" );\n      matchedFields.add( \"field_super_exact\" );\n      matchedFields.add( \"field_characters\" );\n      matchedFields.add( \"field_tripples\" );\n      matchedFields.add( \"field_sliced\" );\n      matchedFields.add( \"field_der_red\" );\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", matchedFields, 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    } else {\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    }\n    assertEquals( expected, bestFragments[ 0 ] );\n\n    reader.close();\n    writer.shutdown();\n    dir.close();\n  }\n\n","sourceOld":"  private void matchedFieldsTestCase( boolean useMatchedFields, boolean fieldMatch, String fieldValue, String expected, Query... queryClauses ) throws IOException {\n    Document doc = new Document();\n    FieldType stored = new FieldType( TextField.TYPE_STORED );\n    stored.setStoreTermVectorOffsets( true );\n    stored.setStoreTermVectorPositions( true );\n    stored.setStoreTermVectors( true );\n    stored.freeze();\n    FieldType matched = new FieldType( TextField.TYPE_NOT_STORED );\n    matched.setStoreTermVectorOffsets( true );\n    matched.setStoreTermVectorPositions( true );\n    matched.setStoreTermVectors( true );\n    matched.freeze();\n    doc.add( new Field( \"field\", fieldValue, stored ) );               // Whitespace tokenized with English stop words\n    doc.add( new Field( \"field_exact\", fieldValue, matched ) );        // Whitespace tokenized without stop words\n    doc.add( new Field( \"field_super_exact\", fieldValue, matched ) );  // Whitespace tokenized without toLower\n    doc.add( new Field( \"field_characters\", fieldValue, matched ) );   // Each letter is a token\n    doc.add( new Field( \"field_tripples\", fieldValue, matched ) );     // Every three letters is a token\n    doc.add( new Field( \"field_sliced\", fieldValue.substring( 0,       // Sliced at 10 chars then analyzed just like field\n      Math.min( fieldValue.length() - 1 , 10 ) ), matched ) );\n    doc.add( new Field( \"field_der_red\", new CannedTokenStream(        // Hacky field containing \"der\" and \"red\" at pos = 0\n          token( \"der\", 1, 0, 3 ),\n          token( \"red\", 0, 0, 3 )\n        ), matched ) );\n\n    final Map<String, Analyzer> fieldAnalyzers = new TreeMap<>();\n    fieldAnalyzers.put( \"field\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, true, MockTokenFilter.ENGLISH_STOPSET ) );\n    fieldAnalyzers.put( \"field_exact\", new MockAnalyzer( random() ) );\n    fieldAnalyzers.put( \"field_super_exact\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, false ) );\n    fieldAnalyzers.put( \"field_characters\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\".\").toLightAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_tripples\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\"...\").toLightAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_sliced\", fieldAnalyzers.get( \"field\" ) );\n    fieldAnalyzers.put( \"field_der_red\", fieldAnalyzers.get( \"field\" ) );  // This is required even though we provide a token stream\n    Analyzer analyzer = new AnalyzerWrapper() {\n      public Analyzer getWrappedAnalyzer(String fieldName) {\n        return fieldAnalyzers.get( fieldName );\n      }\n    };\n\n    Directory dir = newDirectory();\n    IndexWriter writer = new IndexWriter( dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer ) );\n    writer.addDocument( doc );\n\n    FastVectorHighlighter highlighter = new FastVectorHighlighter();\n    FragListBuilder fragListBuilder = new SimpleFragListBuilder();\n    FragmentsBuilder fragmentsBuilder = new ScoreOrderFragmentsBuilder();\n    IndexReader reader = DirectoryReader.open( writer, true );\n    String[] preTags = new String[] { \"<b>\" };\n    String[] postTags = new String[] { \"</b>\" };\n    Encoder encoder = new DefaultEncoder();\n    int docId = 0;\n    BooleanQuery query = new BooleanQuery();\n    for ( Query clause : queryClauses ) {\n      query.add( clause, Occur.MUST );\n    }\n    FieldQuery fieldQuery = new FieldQuery( query, reader, true, fieldMatch );\n    String[] bestFragments;\n    if ( useMatchedFields ) {\n      Set< String > matchedFields = new HashSet<>();\n      matchedFields.add( \"field\" );\n      matchedFields.add( \"field_exact\" );\n      matchedFields.add( \"field_super_exact\" );\n      matchedFields.add( \"field_characters\" );\n      matchedFields.add( \"field_tripples\" );\n      matchedFields.add( \"field_sliced\" );\n      matchedFields.add( \"field_der_red\" );\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", matchedFields, 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    } else {\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    }\n    assertEquals( expected, bestFragments[ 0 ] );\n\n    reader.close();\n    writer.shutdown();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6e6076d5869e894e98558285d9c9be9179d93921","date":1404559951,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest#matchedFieldsTestCase(boolean,boolean,String,String,Query...).mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest#matchedFieldsTestCase(boolean,boolean,String,String,Query...).mjava","sourceNew":"  private void matchedFieldsTestCase( boolean useMatchedFields, boolean fieldMatch, String fieldValue, String expected, Query... queryClauses ) throws IOException {\n    Document doc = new Document();\n    FieldType stored = new FieldType( TextField.TYPE_STORED );\n    stored.setStoreTermVectorOffsets( true );\n    stored.setStoreTermVectorPositions( true );\n    stored.setStoreTermVectors( true );\n    stored.freeze();\n    FieldType matched = new FieldType( TextField.TYPE_NOT_STORED );\n    matched.setStoreTermVectorOffsets( true );\n    matched.setStoreTermVectorPositions( true );\n    matched.setStoreTermVectors( true );\n    matched.freeze();\n    doc.add( new Field( \"field\", fieldValue, stored ) );               // Whitespace tokenized with English stop words\n    doc.add( new Field( \"field_exact\", fieldValue, matched ) );        // Whitespace tokenized without stop words\n    doc.add( new Field( \"field_super_exact\", fieldValue, matched ) );  // Whitespace tokenized without toLower\n    doc.add( new Field( \"field_characters\", fieldValue, matched ) );   // Each letter is a token\n    doc.add( new Field( \"field_tripples\", fieldValue, matched ) );     // Every three letters is a token\n    doc.add( new Field( \"field_sliced\", fieldValue.substring( 0,       // Sliced at 10 chars then analyzed just like field\n      Math.min( fieldValue.length() - 1 , 10 ) ), matched ) );\n    doc.add( new Field( \"field_der_red\", new CannedTokenStream(        // Hacky field containing \"der\" and \"red\" at pos = 0\n          token( \"der\", 1, 0, 3 ),\n          token( \"red\", 0, 0, 3 )\n        ), matched ) );\n\n    final Map<String, Analyzer> fieldAnalyzers = new TreeMap<>();\n    fieldAnalyzers.put( \"field\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, true, MockTokenFilter.ENGLISH_STOPSET ) );\n    fieldAnalyzers.put( \"field_exact\", new MockAnalyzer( random() ) );\n    fieldAnalyzers.put( \"field_super_exact\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, false ) );\n    fieldAnalyzers.put( \"field_characters\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\".\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_tripples\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\"...\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_sliced\", fieldAnalyzers.get( \"field\" ) );\n    fieldAnalyzers.put( \"field_der_red\", fieldAnalyzers.get( \"field\" ) );  // This is required even though we provide a token stream\n    Analyzer analyzer = new DelegatingAnalyzerWrapper(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      public Analyzer getWrappedAnalyzer(String fieldName) {\n        return fieldAnalyzers.get( fieldName );\n      }\n    };\n\n    Directory dir = newDirectory();\n    IndexWriter writer = new IndexWriter( dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer ) );\n    writer.addDocument( doc );\n\n    FastVectorHighlighter highlighter = new FastVectorHighlighter();\n    FragListBuilder fragListBuilder = new SimpleFragListBuilder();\n    FragmentsBuilder fragmentsBuilder = new ScoreOrderFragmentsBuilder();\n    IndexReader reader = DirectoryReader.open( writer, true );\n    String[] preTags = new String[] { \"<b>\" };\n    String[] postTags = new String[] { \"</b>\" };\n    Encoder encoder = new DefaultEncoder();\n    int docId = 0;\n    BooleanQuery query = new BooleanQuery();\n    for ( Query clause : queryClauses ) {\n      query.add( clause, Occur.MUST );\n    }\n    FieldQuery fieldQuery = new FieldQuery( query, reader, true, fieldMatch );\n    String[] bestFragments;\n    if ( useMatchedFields ) {\n      Set< String > matchedFields = new HashSet<>();\n      matchedFields.add( \"field\" );\n      matchedFields.add( \"field_exact\" );\n      matchedFields.add( \"field_super_exact\" );\n      matchedFields.add( \"field_characters\" );\n      matchedFields.add( \"field_tripples\" );\n      matchedFields.add( \"field_sliced\" );\n      matchedFields.add( \"field_der_red\" );\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", matchedFields, 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    } else {\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    }\n    assertEquals( expected, bestFragments[ 0 ] );\n\n    reader.close();\n    writer.shutdown();\n    dir.close();\n  }\n\n","sourceOld":"  private void matchedFieldsTestCase( boolean useMatchedFields, boolean fieldMatch, String fieldValue, String expected, Query... queryClauses ) throws IOException {\n    Document doc = new Document();\n    FieldType stored = new FieldType( TextField.TYPE_STORED );\n    stored.setStoreTermVectorOffsets( true );\n    stored.setStoreTermVectorPositions( true );\n    stored.setStoreTermVectors( true );\n    stored.freeze();\n    FieldType matched = new FieldType( TextField.TYPE_NOT_STORED );\n    matched.setStoreTermVectorOffsets( true );\n    matched.setStoreTermVectorPositions( true );\n    matched.setStoreTermVectors( true );\n    matched.freeze();\n    doc.add( new Field( \"field\", fieldValue, stored ) );               // Whitespace tokenized with English stop words\n    doc.add( new Field( \"field_exact\", fieldValue, matched ) );        // Whitespace tokenized without stop words\n    doc.add( new Field( \"field_super_exact\", fieldValue, matched ) );  // Whitespace tokenized without toLower\n    doc.add( new Field( \"field_characters\", fieldValue, matched ) );   // Each letter is a token\n    doc.add( new Field( \"field_tripples\", fieldValue, matched ) );     // Every three letters is a token\n    doc.add( new Field( \"field_sliced\", fieldValue.substring( 0,       // Sliced at 10 chars then analyzed just like field\n      Math.min( fieldValue.length() - 1 , 10 ) ), matched ) );\n    doc.add( new Field( \"field_der_red\", new CannedTokenStream(        // Hacky field containing \"der\" and \"red\" at pos = 0\n          token( \"der\", 1, 0, 3 ),\n          token( \"red\", 0, 0, 3 )\n        ), matched ) );\n\n    final Map<String, Analyzer> fieldAnalyzers = new TreeMap<>();\n    fieldAnalyzers.put( \"field\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, true, MockTokenFilter.ENGLISH_STOPSET ) );\n    fieldAnalyzers.put( \"field_exact\", new MockAnalyzer( random() ) );\n    fieldAnalyzers.put( \"field_super_exact\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, false ) );\n    fieldAnalyzers.put( \"field_characters\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\".\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_tripples\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\"...\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_sliced\", fieldAnalyzers.get( \"field\" ) );\n    fieldAnalyzers.put( \"field_der_red\", fieldAnalyzers.get( \"field\" ) );  // This is required even though we provide a token stream\n    Analyzer analyzer = new AnalyzerWrapper() {\n      public Analyzer getWrappedAnalyzer(String fieldName) {\n        return fieldAnalyzers.get( fieldName );\n      }\n    };\n\n    Directory dir = newDirectory();\n    IndexWriter writer = new IndexWriter( dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer ) );\n    writer.addDocument( doc );\n\n    FastVectorHighlighter highlighter = new FastVectorHighlighter();\n    FragListBuilder fragListBuilder = new SimpleFragListBuilder();\n    FragmentsBuilder fragmentsBuilder = new ScoreOrderFragmentsBuilder();\n    IndexReader reader = DirectoryReader.open( writer, true );\n    String[] preTags = new String[] { \"<b>\" };\n    String[] postTags = new String[] { \"</b>\" };\n    Encoder encoder = new DefaultEncoder();\n    int docId = 0;\n    BooleanQuery query = new BooleanQuery();\n    for ( Query clause : queryClauses ) {\n      query.add( clause, Occur.MUST );\n    }\n    FieldQuery fieldQuery = new FieldQuery( query, reader, true, fieldMatch );\n    String[] bestFragments;\n    if ( useMatchedFields ) {\n      Set< String > matchedFields = new HashSet<>();\n      matchedFields.add( \"field\" );\n      matchedFields.add( \"field_exact\" );\n      matchedFields.add( \"field_super_exact\" );\n      matchedFields.add( \"field_characters\" );\n      matchedFields.add( \"field_tripples\" );\n      matchedFields.add( \"field_sliced\" );\n      matchedFields.add( \"field_der_red\" );\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", matchedFields, 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    } else {\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    }\n    assertEquals( expected, bestFragments[ 0 ] );\n\n    reader.close();\n    writer.shutdown();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest#matchedFieldsTestCase(boolean,boolean,String,String,Query...).mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest#matchedFieldsTestCase(boolean,boolean,String,String,Query...).mjava","sourceNew":"  private void matchedFieldsTestCase( boolean useMatchedFields, boolean fieldMatch, String fieldValue, String expected, Query... queryClauses ) throws IOException {\n    Document doc = new Document();\n    FieldType stored = new FieldType( TextField.TYPE_STORED );\n    stored.setStoreTermVectorOffsets( true );\n    stored.setStoreTermVectorPositions( true );\n    stored.setStoreTermVectors( true );\n    stored.freeze();\n    FieldType matched = new FieldType( TextField.TYPE_NOT_STORED );\n    matched.setStoreTermVectorOffsets( true );\n    matched.setStoreTermVectorPositions( true );\n    matched.setStoreTermVectors( true );\n    matched.freeze();\n    doc.add( new Field( \"field\", fieldValue, stored ) );               // Whitespace tokenized with English stop words\n    doc.add( new Field( \"field_exact\", fieldValue, matched ) );        // Whitespace tokenized without stop words\n    doc.add( new Field( \"field_super_exact\", fieldValue, matched ) );  // Whitespace tokenized without toLower\n    doc.add( new Field( \"field_characters\", fieldValue, matched ) );   // Each letter is a token\n    doc.add( new Field( \"field_tripples\", fieldValue, matched ) );     // Every three letters is a token\n    doc.add( new Field( \"field_sliced\", fieldValue.substring( 0,       // Sliced at 10 chars then analyzed just like field\n      Math.min( fieldValue.length() - 1 , 10 ) ), matched ) );\n    doc.add( new Field( \"field_der_red\", new CannedTokenStream(        // Hacky field containing \"der\" and \"red\" at pos = 0\n          token( \"der\", 1, 0, 3 ),\n          token( \"red\", 0, 0, 3 )\n        ), matched ) );\n\n    final Map<String, Analyzer> fieldAnalyzers = new TreeMap<>();\n    fieldAnalyzers.put( \"field\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, true, MockTokenFilter.ENGLISH_STOPSET ) );\n    fieldAnalyzers.put( \"field_exact\", new MockAnalyzer( random() ) );\n    fieldAnalyzers.put( \"field_super_exact\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, false ) );\n    fieldAnalyzers.put( \"field_characters\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\".\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_tripples\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\"...\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_sliced\", fieldAnalyzers.get( \"field\" ) );\n    fieldAnalyzers.put( \"field_der_red\", fieldAnalyzers.get( \"field\" ) );  // This is required even though we provide a token stream\n    Analyzer analyzer = new DelegatingAnalyzerWrapper(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      public Analyzer getWrappedAnalyzer(String fieldName) {\n        return fieldAnalyzers.get( fieldName );\n      }\n    };\n\n    Directory dir = newDirectory();\n    IndexWriter writer = new IndexWriter( dir, newIndexWriterConfig(analyzer));\n    writer.addDocument( doc );\n\n    FastVectorHighlighter highlighter = new FastVectorHighlighter();\n    FragListBuilder fragListBuilder = new SimpleFragListBuilder();\n    FragmentsBuilder fragmentsBuilder = new ScoreOrderFragmentsBuilder();\n    IndexReader reader = DirectoryReader.open( writer, true );\n    String[] preTags = new String[] { \"<b>\" };\n    String[] postTags = new String[] { \"</b>\" };\n    Encoder encoder = new DefaultEncoder();\n    int docId = 0;\n    BooleanQuery query = new BooleanQuery();\n    for ( Query clause : queryClauses ) {\n      query.add( clause, Occur.MUST );\n    }\n    FieldQuery fieldQuery = new FieldQuery( query, reader, true, fieldMatch );\n    String[] bestFragments;\n    if ( useMatchedFields ) {\n      Set< String > matchedFields = new HashSet<>();\n      matchedFields.add( \"field\" );\n      matchedFields.add( \"field_exact\" );\n      matchedFields.add( \"field_super_exact\" );\n      matchedFields.add( \"field_characters\" );\n      matchedFields.add( \"field_tripples\" );\n      matchedFields.add( \"field_sliced\" );\n      matchedFields.add( \"field_der_red\" );\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", matchedFields, 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    } else {\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    }\n    assertEquals( expected, bestFragments[ 0 ] );\n\n    reader.close();\n    writer.shutdown();\n    dir.close();\n  }\n\n","sourceOld":"  private void matchedFieldsTestCase( boolean useMatchedFields, boolean fieldMatch, String fieldValue, String expected, Query... queryClauses ) throws IOException {\n    Document doc = new Document();\n    FieldType stored = new FieldType( TextField.TYPE_STORED );\n    stored.setStoreTermVectorOffsets( true );\n    stored.setStoreTermVectorPositions( true );\n    stored.setStoreTermVectors( true );\n    stored.freeze();\n    FieldType matched = new FieldType( TextField.TYPE_NOT_STORED );\n    matched.setStoreTermVectorOffsets( true );\n    matched.setStoreTermVectorPositions( true );\n    matched.setStoreTermVectors( true );\n    matched.freeze();\n    doc.add( new Field( \"field\", fieldValue, stored ) );               // Whitespace tokenized with English stop words\n    doc.add( new Field( \"field_exact\", fieldValue, matched ) );        // Whitespace tokenized without stop words\n    doc.add( new Field( \"field_super_exact\", fieldValue, matched ) );  // Whitespace tokenized without toLower\n    doc.add( new Field( \"field_characters\", fieldValue, matched ) );   // Each letter is a token\n    doc.add( new Field( \"field_tripples\", fieldValue, matched ) );     // Every three letters is a token\n    doc.add( new Field( \"field_sliced\", fieldValue.substring( 0,       // Sliced at 10 chars then analyzed just like field\n      Math.min( fieldValue.length() - 1 , 10 ) ), matched ) );\n    doc.add( new Field( \"field_der_red\", new CannedTokenStream(        // Hacky field containing \"der\" and \"red\" at pos = 0\n          token( \"der\", 1, 0, 3 ),\n          token( \"red\", 0, 0, 3 )\n        ), matched ) );\n\n    final Map<String, Analyzer> fieldAnalyzers = new TreeMap<>();\n    fieldAnalyzers.put( \"field\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, true, MockTokenFilter.ENGLISH_STOPSET ) );\n    fieldAnalyzers.put( \"field_exact\", new MockAnalyzer( random() ) );\n    fieldAnalyzers.put( \"field_super_exact\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, false ) );\n    fieldAnalyzers.put( \"field_characters\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\".\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_tripples\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\"...\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_sliced\", fieldAnalyzers.get( \"field\" ) );\n    fieldAnalyzers.put( \"field_der_red\", fieldAnalyzers.get( \"field\" ) );  // This is required even though we provide a token stream\n    Analyzer analyzer = new DelegatingAnalyzerWrapper(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      public Analyzer getWrappedAnalyzer(String fieldName) {\n        return fieldAnalyzers.get( fieldName );\n      }\n    };\n\n    Directory dir = newDirectory();\n    IndexWriter writer = new IndexWriter( dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer ) );\n    writer.addDocument( doc );\n\n    FastVectorHighlighter highlighter = new FastVectorHighlighter();\n    FragListBuilder fragListBuilder = new SimpleFragListBuilder();\n    FragmentsBuilder fragmentsBuilder = new ScoreOrderFragmentsBuilder();\n    IndexReader reader = DirectoryReader.open( writer, true );\n    String[] preTags = new String[] { \"<b>\" };\n    String[] postTags = new String[] { \"</b>\" };\n    Encoder encoder = new DefaultEncoder();\n    int docId = 0;\n    BooleanQuery query = new BooleanQuery();\n    for ( Query clause : queryClauses ) {\n      query.add( clause, Occur.MUST );\n    }\n    FieldQuery fieldQuery = new FieldQuery( query, reader, true, fieldMatch );\n    String[] bestFragments;\n    if ( useMatchedFields ) {\n      Set< String > matchedFields = new HashSet<>();\n      matchedFields.add( \"field\" );\n      matchedFields.add( \"field_exact\" );\n      matchedFields.add( \"field_super_exact\" );\n      matchedFields.add( \"field_characters\" );\n      matchedFields.add( \"field_tripples\" );\n      matchedFields.add( \"field_sliced\" );\n      matchedFields.add( \"field_der_red\" );\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", matchedFields, 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    } else {\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    }\n    assertEquals( expected, bestFragments[ 0 ] );\n\n    reader.close();\n    writer.shutdown();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest#matchedFieldsTestCase(boolean,boolean,String,String,Query...).mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest#matchedFieldsTestCase(boolean,boolean,String,String,Query...).mjava","sourceNew":"  private void matchedFieldsTestCase( boolean useMatchedFields, boolean fieldMatch, String fieldValue, String expected, Query... queryClauses ) throws IOException {\n    Document doc = new Document();\n    FieldType stored = new FieldType( TextField.TYPE_STORED );\n    stored.setStoreTermVectorOffsets( true );\n    stored.setStoreTermVectorPositions( true );\n    stored.setStoreTermVectors( true );\n    stored.freeze();\n    FieldType matched = new FieldType( TextField.TYPE_NOT_STORED );\n    matched.setStoreTermVectorOffsets( true );\n    matched.setStoreTermVectorPositions( true );\n    matched.setStoreTermVectors( true );\n    matched.freeze();\n    doc.add( new Field( \"field\", fieldValue, stored ) );               // Whitespace tokenized with English stop words\n    doc.add( new Field( \"field_exact\", fieldValue, matched ) );        // Whitespace tokenized without stop words\n    doc.add( new Field( \"field_super_exact\", fieldValue, matched ) );  // Whitespace tokenized without toLower\n    doc.add( new Field( \"field_characters\", fieldValue, matched ) );   // Each letter is a token\n    doc.add( new Field( \"field_tripples\", fieldValue, matched ) );     // Every three letters is a token\n    doc.add( new Field( \"field_sliced\", fieldValue.substring( 0,       // Sliced at 10 chars then analyzed just like field\n      Math.min( fieldValue.length() - 1 , 10 ) ), matched ) );\n    doc.add( new Field( \"field_der_red\", new CannedTokenStream(        // Hacky field containing \"der\" and \"red\" at pos = 0\n          token( \"der\", 1, 0, 3 ),\n          token( \"red\", 0, 0, 3 )\n        ), matched ) );\n\n    final Map<String, Analyzer> fieldAnalyzers = new TreeMap<>();\n    fieldAnalyzers.put( \"field\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, true, MockTokenFilter.ENGLISH_STOPSET ) );\n    fieldAnalyzers.put( \"field_exact\", new MockAnalyzer( random() ) );\n    fieldAnalyzers.put( \"field_super_exact\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, false ) );\n    fieldAnalyzers.put( \"field_characters\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\".\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_tripples\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\"...\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_sliced\", fieldAnalyzers.get( \"field\" ) );\n    fieldAnalyzers.put( \"field_der_red\", fieldAnalyzers.get( \"field\" ) );  // This is required even though we provide a token stream\n    Analyzer analyzer = new DelegatingAnalyzerWrapper(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      public Analyzer getWrappedAnalyzer(String fieldName) {\n        return fieldAnalyzers.get( fieldName );\n      }\n    };\n\n    Directory dir = newDirectory();\n    IndexWriter writer = new IndexWriter( dir, newIndexWriterConfig(analyzer));\n    writer.addDocument( doc );\n\n    FastVectorHighlighter highlighter = new FastVectorHighlighter();\n    FragListBuilder fragListBuilder = new SimpleFragListBuilder();\n    FragmentsBuilder fragmentsBuilder = new ScoreOrderFragmentsBuilder();\n    IndexReader reader = DirectoryReader.open( writer, true );\n    String[] preTags = new String[] { \"<b>\" };\n    String[] postTags = new String[] { \"</b>\" };\n    Encoder encoder = new DefaultEncoder();\n    int docId = 0;\n    BooleanQuery query = new BooleanQuery();\n    for ( Query clause : queryClauses ) {\n      query.add( clause, Occur.MUST );\n    }\n    FieldQuery fieldQuery = new FieldQuery( query, reader, true, fieldMatch );\n    String[] bestFragments;\n    if ( useMatchedFields ) {\n      Set< String > matchedFields = new HashSet<>();\n      matchedFields.add( \"field\" );\n      matchedFields.add( \"field_exact\" );\n      matchedFields.add( \"field_super_exact\" );\n      matchedFields.add( \"field_characters\" );\n      matchedFields.add( \"field_tripples\" );\n      matchedFields.add( \"field_sliced\" );\n      matchedFields.add( \"field_der_red\" );\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", matchedFields, 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    } else {\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    }\n    assertEquals( expected, bestFragments[ 0 ] );\n\n    reader.close();\n    writer.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void matchedFieldsTestCase( boolean useMatchedFields, boolean fieldMatch, String fieldValue, String expected, Query... queryClauses ) throws IOException {\n    Document doc = new Document();\n    FieldType stored = new FieldType( TextField.TYPE_STORED );\n    stored.setStoreTermVectorOffsets( true );\n    stored.setStoreTermVectorPositions( true );\n    stored.setStoreTermVectors( true );\n    stored.freeze();\n    FieldType matched = new FieldType( TextField.TYPE_NOT_STORED );\n    matched.setStoreTermVectorOffsets( true );\n    matched.setStoreTermVectorPositions( true );\n    matched.setStoreTermVectors( true );\n    matched.freeze();\n    doc.add( new Field( \"field\", fieldValue, stored ) );               // Whitespace tokenized with English stop words\n    doc.add( new Field( \"field_exact\", fieldValue, matched ) );        // Whitespace tokenized without stop words\n    doc.add( new Field( \"field_super_exact\", fieldValue, matched ) );  // Whitespace tokenized without toLower\n    doc.add( new Field( \"field_characters\", fieldValue, matched ) );   // Each letter is a token\n    doc.add( new Field( \"field_tripples\", fieldValue, matched ) );     // Every three letters is a token\n    doc.add( new Field( \"field_sliced\", fieldValue.substring( 0,       // Sliced at 10 chars then analyzed just like field\n      Math.min( fieldValue.length() - 1 , 10 ) ), matched ) );\n    doc.add( new Field( \"field_der_red\", new CannedTokenStream(        // Hacky field containing \"der\" and \"red\" at pos = 0\n          token( \"der\", 1, 0, 3 ),\n          token( \"red\", 0, 0, 3 )\n        ), matched ) );\n\n    final Map<String, Analyzer> fieldAnalyzers = new TreeMap<>();\n    fieldAnalyzers.put( \"field\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, true, MockTokenFilter.ENGLISH_STOPSET ) );\n    fieldAnalyzers.put( \"field_exact\", new MockAnalyzer( random() ) );\n    fieldAnalyzers.put( \"field_super_exact\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, false ) );\n    fieldAnalyzers.put( \"field_characters\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\".\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_tripples\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\"...\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_sliced\", fieldAnalyzers.get( \"field\" ) );\n    fieldAnalyzers.put( \"field_der_red\", fieldAnalyzers.get( \"field\" ) );  // This is required even though we provide a token stream\n    Analyzer analyzer = new DelegatingAnalyzerWrapper(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      public Analyzer getWrappedAnalyzer(String fieldName) {\n        return fieldAnalyzers.get( fieldName );\n      }\n    };\n\n    Directory dir = newDirectory();\n    IndexWriter writer = new IndexWriter( dir, newIndexWriterConfig(analyzer));\n    writer.addDocument( doc );\n\n    FastVectorHighlighter highlighter = new FastVectorHighlighter();\n    FragListBuilder fragListBuilder = new SimpleFragListBuilder();\n    FragmentsBuilder fragmentsBuilder = new ScoreOrderFragmentsBuilder();\n    IndexReader reader = DirectoryReader.open( writer, true );\n    String[] preTags = new String[] { \"<b>\" };\n    String[] postTags = new String[] { \"</b>\" };\n    Encoder encoder = new DefaultEncoder();\n    int docId = 0;\n    BooleanQuery query = new BooleanQuery();\n    for ( Query clause : queryClauses ) {\n      query.add( clause, Occur.MUST );\n    }\n    FieldQuery fieldQuery = new FieldQuery( query, reader, true, fieldMatch );\n    String[] bestFragments;\n    if ( useMatchedFields ) {\n      Set< String > matchedFields = new HashSet<>();\n      matchedFields.add( \"field\" );\n      matchedFields.add( \"field_exact\" );\n      matchedFields.add( \"field_super_exact\" );\n      matchedFields.add( \"field_characters\" );\n      matchedFields.add( \"field_tripples\" );\n      matchedFields.add( \"field_sliced\" );\n      matchedFields.add( \"field_der_red\" );\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", matchedFields, 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    } else {\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    }\n    assertEquals( expected, bestFragments[ 0 ] );\n\n    reader.close();\n    writer.shutdown();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3f68d01cf19df971dcdcb05e30247f4ad7ec9747","date":1434611645,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest#matchedFieldsTestCase(boolean,boolean,String,String,Query...).mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest#matchedFieldsTestCase(boolean,boolean,String,String,Query...).mjava","sourceNew":"  private void matchedFieldsTestCase( boolean useMatchedFields, boolean fieldMatch, String fieldValue, String expected, Query... queryClauses ) throws IOException {\n    Document doc = new Document();\n    FieldType stored = new FieldType( TextField.TYPE_STORED );\n    stored.setStoreTermVectorOffsets( true );\n    stored.setStoreTermVectorPositions( true );\n    stored.setStoreTermVectors( true );\n    stored.freeze();\n    FieldType matched = new FieldType( TextField.TYPE_NOT_STORED );\n    matched.setStoreTermVectorOffsets( true );\n    matched.setStoreTermVectorPositions( true );\n    matched.setStoreTermVectors( true );\n    matched.freeze();\n    doc.add( new Field( \"field\", fieldValue, stored ) );               // Whitespace tokenized with English stop words\n    doc.add( new Field( \"field_exact\", fieldValue, matched ) );        // Whitespace tokenized without stop words\n    doc.add( new Field( \"field_super_exact\", fieldValue, matched ) );  // Whitespace tokenized without toLower\n    doc.add( new Field( \"field_characters\", fieldValue, matched ) );   // Each letter is a token\n    doc.add( new Field( \"field_tripples\", fieldValue, matched ) );     // Every three letters is a token\n    doc.add( new Field( \"field_sliced\", fieldValue.substring( 0,       // Sliced at 10 chars then analyzed just like field\n      Math.min( fieldValue.length() - 1 , 10 ) ), matched ) );\n    doc.add( new Field( \"field_der_red\", new CannedTokenStream(        // Hacky field containing \"der\" and \"red\" at pos = 0\n          token( \"der\", 1, 0, 3 ),\n          token( \"red\", 0, 0, 3 )\n        ), matched ) );\n\n    final Map<String, Analyzer> fieldAnalyzers = new TreeMap<>();\n    fieldAnalyzers.put( \"field\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, true, MockTokenFilter.ENGLISH_STOPSET ) );\n    fieldAnalyzers.put( \"field_exact\", new MockAnalyzer( random() ) );\n    fieldAnalyzers.put( \"field_super_exact\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, false ) );\n    fieldAnalyzers.put( \"field_characters\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\".\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_tripples\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\"...\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_sliced\", fieldAnalyzers.get( \"field\" ) );\n    fieldAnalyzers.put( \"field_der_red\", fieldAnalyzers.get( \"field\" ) );  // This is required even though we provide a token stream\n    Analyzer analyzer = new DelegatingAnalyzerWrapper(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      public Analyzer getWrappedAnalyzer(String fieldName) {\n        return fieldAnalyzers.get( fieldName );\n      }\n    };\n\n    Directory dir = newDirectory();\n    IndexWriter writer = new IndexWriter( dir, newIndexWriterConfig(analyzer));\n    writer.addDocument( doc );\n\n    FastVectorHighlighter highlighter = new FastVectorHighlighter();\n    FragListBuilder fragListBuilder = new SimpleFragListBuilder();\n    FragmentsBuilder fragmentsBuilder = new ScoreOrderFragmentsBuilder();\n    IndexReader reader = DirectoryReader.open( writer, true );\n    String[] preTags = new String[] { \"<b>\" };\n    String[] postTags = new String[] { \"</b>\" };\n    Encoder encoder = new DefaultEncoder();\n    int docId = 0;\n    BooleanQuery.Builder query = new BooleanQuery.Builder();\n    for ( Query clause : queryClauses ) {\n      query.add( clause, Occur.MUST );\n    }\n    FieldQuery fieldQuery = new FieldQuery( query.build(), reader, true, fieldMatch );\n    String[] bestFragments;\n    if ( useMatchedFields ) {\n      Set< String > matchedFields = new HashSet<>();\n      matchedFields.add( \"field\" );\n      matchedFields.add( \"field_exact\" );\n      matchedFields.add( \"field_super_exact\" );\n      matchedFields.add( \"field_characters\" );\n      matchedFields.add( \"field_tripples\" );\n      matchedFields.add( \"field_sliced\" );\n      matchedFields.add( \"field_der_red\" );\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", matchedFields, 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    } else {\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    }\n    assertEquals( expected, bestFragments[ 0 ] );\n\n    reader.close();\n    writer.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void matchedFieldsTestCase( boolean useMatchedFields, boolean fieldMatch, String fieldValue, String expected, Query... queryClauses ) throws IOException {\n    Document doc = new Document();\n    FieldType stored = new FieldType( TextField.TYPE_STORED );\n    stored.setStoreTermVectorOffsets( true );\n    stored.setStoreTermVectorPositions( true );\n    stored.setStoreTermVectors( true );\n    stored.freeze();\n    FieldType matched = new FieldType( TextField.TYPE_NOT_STORED );\n    matched.setStoreTermVectorOffsets( true );\n    matched.setStoreTermVectorPositions( true );\n    matched.setStoreTermVectors( true );\n    matched.freeze();\n    doc.add( new Field( \"field\", fieldValue, stored ) );               // Whitespace tokenized with English stop words\n    doc.add( new Field( \"field_exact\", fieldValue, matched ) );        // Whitespace tokenized without stop words\n    doc.add( new Field( \"field_super_exact\", fieldValue, matched ) );  // Whitespace tokenized without toLower\n    doc.add( new Field( \"field_characters\", fieldValue, matched ) );   // Each letter is a token\n    doc.add( new Field( \"field_tripples\", fieldValue, matched ) );     // Every three letters is a token\n    doc.add( new Field( \"field_sliced\", fieldValue.substring( 0,       // Sliced at 10 chars then analyzed just like field\n      Math.min( fieldValue.length() - 1 , 10 ) ), matched ) );\n    doc.add( new Field( \"field_der_red\", new CannedTokenStream(        // Hacky field containing \"der\" and \"red\" at pos = 0\n          token( \"der\", 1, 0, 3 ),\n          token( \"red\", 0, 0, 3 )\n        ), matched ) );\n\n    final Map<String, Analyzer> fieldAnalyzers = new TreeMap<>();\n    fieldAnalyzers.put( \"field\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, true, MockTokenFilter.ENGLISH_STOPSET ) );\n    fieldAnalyzers.put( \"field_exact\", new MockAnalyzer( random() ) );\n    fieldAnalyzers.put( \"field_super_exact\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, false ) );\n    fieldAnalyzers.put( \"field_characters\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\".\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_tripples\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\"...\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_sliced\", fieldAnalyzers.get( \"field\" ) );\n    fieldAnalyzers.put( \"field_der_red\", fieldAnalyzers.get( \"field\" ) );  // This is required even though we provide a token stream\n    Analyzer analyzer = new DelegatingAnalyzerWrapper(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      public Analyzer getWrappedAnalyzer(String fieldName) {\n        return fieldAnalyzers.get( fieldName );\n      }\n    };\n\n    Directory dir = newDirectory();\n    IndexWriter writer = new IndexWriter( dir, newIndexWriterConfig(analyzer));\n    writer.addDocument( doc );\n\n    FastVectorHighlighter highlighter = new FastVectorHighlighter();\n    FragListBuilder fragListBuilder = new SimpleFragListBuilder();\n    FragmentsBuilder fragmentsBuilder = new ScoreOrderFragmentsBuilder();\n    IndexReader reader = DirectoryReader.open( writer, true );\n    String[] preTags = new String[] { \"<b>\" };\n    String[] postTags = new String[] { \"</b>\" };\n    Encoder encoder = new DefaultEncoder();\n    int docId = 0;\n    BooleanQuery query = new BooleanQuery();\n    for ( Query clause : queryClauses ) {\n      query.add( clause, Occur.MUST );\n    }\n    FieldQuery fieldQuery = new FieldQuery( query, reader, true, fieldMatch );\n    String[] bestFragments;\n    if ( useMatchedFields ) {\n      Set< String > matchedFields = new HashSet<>();\n      matchedFields.add( \"field\" );\n      matchedFields.add( \"field_exact\" );\n      matchedFields.add( \"field_super_exact\" );\n      matchedFields.add( \"field_characters\" );\n      matchedFields.add( \"field_tripples\" );\n      matchedFields.add( \"field_sliced\" );\n      matchedFields.add( \"field_der_red\" );\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", matchedFields, 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    } else {\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    }\n    assertEquals( expected, bestFragments[ 0 ] );\n\n    reader.close();\n    writer.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2a1862266772deb28cdcb7d996b64d2177022687","date":1453077824,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest#matchedFieldsTestCase(boolean,boolean,String,String,Query...).mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest#matchedFieldsTestCase(boolean,boolean,String,String,Query...).mjava","sourceNew":"  private void matchedFieldsTestCase( boolean useMatchedFields, boolean fieldMatch, String fieldValue, String expected, Query... queryClauses ) throws IOException {\n    Document doc = new Document();\n    FieldType stored = new FieldType( TextField.TYPE_STORED );\n    stored.setStoreTermVectorOffsets( true );\n    stored.setStoreTermVectorPositions( true );\n    stored.setStoreTermVectors( true );\n    stored.freeze();\n    FieldType matched = new FieldType( TextField.TYPE_NOT_STORED );\n    matched.setStoreTermVectorOffsets( true );\n    matched.setStoreTermVectorPositions( true );\n    matched.setStoreTermVectors( true );\n    matched.freeze();\n    doc.add( new Field( \"field\", fieldValue, stored ) );               // Whitespace tokenized with English stop words\n    doc.add( new Field( \"field_exact\", fieldValue, matched ) );        // Whitespace tokenized without stop words\n    doc.add( new Field( \"field_super_exact\", fieldValue, matched ) );  // Whitespace tokenized without toLower\n    doc.add( new Field( \"field_characters\", fieldValue, matched ) );   // Each letter is a token\n    doc.add( new Field( \"field_tripples\", fieldValue, matched ) );     // Every three letters is a token\n    doc.add( new Field( \"field_sliced\", fieldValue.substring( 0,       // Sliced at 10 chars then analyzed just like field\n      Math.min( fieldValue.length() - 1 , 10 ) ), matched ) );\n    doc.add( new Field( \"field_der_red\", new CannedTokenStream(        // Hacky field containing \"der\" and \"red\" at pos = 0\n          token( \"der\", 1, 0, 3 ),\n          token( \"red\", 0, 0, 3 )\n        ), matched ) );\n\n    final Map<String, Analyzer> fieldAnalyzers = new TreeMap<>();\n    fieldAnalyzers.put( \"field\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, true, MockTokenFilter.ENGLISH_STOPSET ) );\n    fieldAnalyzers.put( \"field_exact\", new MockAnalyzer( random() ) );\n    fieldAnalyzers.put( \"field_super_exact\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, false ) );\n    fieldAnalyzers.put( \"field_characters\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\".\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_tripples\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\"...\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_sliced\", fieldAnalyzers.get( \"field\" ) );\n    fieldAnalyzers.put( \"field_der_red\", fieldAnalyzers.get( \"field\" ) );  // This is required even though we provide a token stream\n    Analyzer analyzer = new DelegatingAnalyzerWrapper(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      public Analyzer getWrappedAnalyzer(String fieldName) {\n        return fieldAnalyzers.get( fieldName );\n      }\n    };\n\n    Directory dir = newDirectory();\n    IndexWriter writer = new IndexWriter( dir, newIndexWriterConfig(analyzer));\n    writer.addDocument( doc );\n\n    FastVectorHighlighter highlighter = new FastVectorHighlighter();\n    FragListBuilder fragListBuilder = new SimpleFragListBuilder();\n    FragmentsBuilder fragmentsBuilder = new ScoreOrderFragmentsBuilder();\n    IndexReader reader = DirectoryReader.open(writer);\n    String[] preTags = new String[] { \"<b>\" };\n    String[] postTags = new String[] { \"</b>\" };\n    Encoder encoder = new DefaultEncoder();\n    int docId = 0;\n    BooleanQuery.Builder query = new BooleanQuery.Builder();\n    for ( Query clause : queryClauses ) {\n      query.add( clause, Occur.MUST );\n    }\n    FieldQuery fieldQuery = new FieldQuery( query.build(), reader, true, fieldMatch );\n    String[] bestFragments;\n    if ( useMatchedFields ) {\n      Set< String > matchedFields = new HashSet<>();\n      matchedFields.add( \"field\" );\n      matchedFields.add( \"field_exact\" );\n      matchedFields.add( \"field_super_exact\" );\n      matchedFields.add( \"field_characters\" );\n      matchedFields.add( \"field_tripples\" );\n      matchedFields.add( \"field_sliced\" );\n      matchedFields.add( \"field_der_red\" );\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", matchedFields, 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    } else {\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    }\n    assertEquals( expected, bestFragments[ 0 ] );\n\n    reader.close();\n    writer.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void matchedFieldsTestCase( boolean useMatchedFields, boolean fieldMatch, String fieldValue, String expected, Query... queryClauses ) throws IOException {\n    Document doc = new Document();\n    FieldType stored = new FieldType( TextField.TYPE_STORED );\n    stored.setStoreTermVectorOffsets( true );\n    stored.setStoreTermVectorPositions( true );\n    stored.setStoreTermVectors( true );\n    stored.freeze();\n    FieldType matched = new FieldType( TextField.TYPE_NOT_STORED );\n    matched.setStoreTermVectorOffsets( true );\n    matched.setStoreTermVectorPositions( true );\n    matched.setStoreTermVectors( true );\n    matched.freeze();\n    doc.add( new Field( \"field\", fieldValue, stored ) );               // Whitespace tokenized with English stop words\n    doc.add( new Field( \"field_exact\", fieldValue, matched ) );        // Whitespace tokenized without stop words\n    doc.add( new Field( \"field_super_exact\", fieldValue, matched ) );  // Whitespace tokenized without toLower\n    doc.add( new Field( \"field_characters\", fieldValue, matched ) );   // Each letter is a token\n    doc.add( new Field( \"field_tripples\", fieldValue, matched ) );     // Every three letters is a token\n    doc.add( new Field( \"field_sliced\", fieldValue.substring( 0,       // Sliced at 10 chars then analyzed just like field\n      Math.min( fieldValue.length() - 1 , 10 ) ), matched ) );\n    doc.add( new Field( \"field_der_red\", new CannedTokenStream(        // Hacky field containing \"der\" and \"red\" at pos = 0\n          token( \"der\", 1, 0, 3 ),\n          token( \"red\", 0, 0, 3 )\n        ), matched ) );\n\n    final Map<String, Analyzer> fieldAnalyzers = new TreeMap<>();\n    fieldAnalyzers.put( \"field\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, true, MockTokenFilter.ENGLISH_STOPSET ) );\n    fieldAnalyzers.put( \"field_exact\", new MockAnalyzer( random() ) );\n    fieldAnalyzers.put( \"field_super_exact\", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, false ) );\n    fieldAnalyzers.put( \"field_characters\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\".\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_tripples\", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(\"...\").toAutomaton() ), true ) );\n    fieldAnalyzers.put( \"field_sliced\", fieldAnalyzers.get( \"field\" ) );\n    fieldAnalyzers.put( \"field_der_red\", fieldAnalyzers.get( \"field\" ) );  // This is required even though we provide a token stream\n    Analyzer analyzer = new DelegatingAnalyzerWrapper(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      public Analyzer getWrappedAnalyzer(String fieldName) {\n        return fieldAnalyzers.get( fieldName );\n      }\n    };\n\n    Directory dir = newDirectory();\n    IndexWriter writer = new IndexWriter( dir, newIndexWriterConfig(analyzer));\n    writer.addDocument( doc );\n\n    FastVectorHighlighter highlighter = new FastVectorHighlighter();\n    FragListBuilder fragListBuilder = new SimpleFragListBuilder();\n    FragmentsBuilder fragmentsBuilder = new ScoreOrderFragmentsBuilder();\n    IndexReader reader = DirectoryReader.open( writer, true );\n    String[] preTags = new String[] { \"<b>\" };\n    String[] postTags = new String[] { \"</b>\" };\n    Encoder encoder = new DefaultEncoder();\n    int docId = 0;\n    BooleanQuery.Builder query = new BooleanQuery.Builder();\n    for ( Query clause : queryClauses ) {\n      query.add( clause, Occur.MUST );\n    }\n    FieldQuery fieldQuery = new FieldQuery( query.build(), reader, true, fieldMatch );\n    String[] bestFragments;\n    if ( useMatchedFields ) {\n      Set< String > matchedFields = new HashSet<>();\n      matchedFields.add( \"field\" );\n      matchedFields.add( \"field_exact\" );\n      matchedFields.add( \"field_super_exact\" );\n      matchedFields.add( \"field_characters\" );\n      matchedFields.add( \"field_tripples\" );\n      matchedFields.add( \"field_sliced\" );\n      matchedFields.add( \"field_der_red\" );\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", matchedFields, 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    } else {\n      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, \"field\", 25, 1,\n        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );\n    }\n    assertEquals( expected, bestFragments[ 0 ] );\n\n    reader.close();\n    writer.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3f68d01cf19df971dcdcb05e30247f4ad7ec9747":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["050e6201c3a7d4c351ebc06cbe4822e26e028117"],"2a1862266772deb28cdcb7d996b64d2177022687":["3f68d01cf19df971dcdcb05e30247f4ad7ec9747"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["6e6076d5869e894e98558285d9c9be9179d93921"],"6e6076d5869e894e98558285d9c9be9179d93921":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"050e6201c3a7d4c351ebc06cbe4822e26e028117":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4ca1c732df8923f5624f6c06b1dcca9e69d98c96":["ac34f0c5bb9274821fb0cb18075234e02002e9bf"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"ac34f0c5bb9274821fb0cb18075234e02002e9bf":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["2a1862266772deb28cdcb7d996b64d2177022687"]},"commit2Childs":{"3f68d01cf19df971dcdcb05e30247f4ad7ec9747":["2a1862266772deb28cdcb7d996b64d2177022687"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"2a1862266772deb28cdcb7d996b64d2177022687":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"6e6076d5869e894e98558285d9c9be9179d93921":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"050e6201c3a7d4c351ebc06cbe4822e26e028117":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["050e6201c3a7d4c351ebc06cbe4822e26e028117"],"4ca1c732df8923f5624f6c06b1dcca9e69d98c96":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["3f68d01cf19df971dcdcb05e30247f4ad7ec9747"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["6e6076d5869e894e98558285d9c9be9179d93921","ac34f0c5bb9274821fb0cb18075234e02002e9bf"],"ac34f0c5bb9274821fb0cb18075234e02002e9bf":["4ca1c732df8923f5624f6c06b1dcca9e69d98c96"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4ca1c732df8923f5624f6c06b1dcca9e69d98c96","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}