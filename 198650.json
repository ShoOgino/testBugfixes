{"path":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, TextField.TYPE_UNSTORED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()),\n                                                    false);\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random.nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()),\n                                                                   false);\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, TextField.TYPE_UNSTORED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, TextField.TYPE_UNSTORED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()),\n                                                    false);\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random.nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()),\n                                                                   false);\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, TextField.TYPE_UNSTORED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, TextField.TYPE_UNSTORED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()),\n                                                    false);\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random().nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()),\n                                                                   false);\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, TextField.TYPE_UNSTORED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, TextField.TYPE_UNSTORED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()),\n                                                    false);\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random.nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()),\n                                                                   false);\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, TextField.TYPE_UNSTORED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","date":1338430031,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, TextField.TYPE_UNSTORED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = DirectoryReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()),\n                                                    false);\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random().nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()),\n                                                                   false);\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, TextField.TYPE_UNSTORED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = DirectoryReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, TextField.TYPE_UNSTORED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()),\n                                                    false);\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random().nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()),\n                                                                   false);\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, TextField.TYPE_UNSTORED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newTextField(fieldName, content, Field.Store.NO));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = DirectoryReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()),\n                                                    false);\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random().nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()),\n                                                                   false);\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = DirectoryReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newField(fieldName, content, TextField.TYPE_UNSTORED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = DirectoryReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()),\n                                                    false);\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random().nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()),\n                                                                   false);\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newField(fieldName, singleTerm, TextField.TYPE_UNSTORED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = DirectoryReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"322360ac5185a8446d3e0b530b2068bef67cd3d5","date":1343669494,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newTextField(fieldName, content, Field.Store.NO));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = DirectoryReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random().nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = DirectoryReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newTextField(fieldName, content, Field.Store.NO));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = DirectoryReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()),\n                                                    false);\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random().nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()),\n                                                                   false);\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = DirectoryReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","date":1343768312,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newTextField(fieldName, content, Field.Store.NO));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = DirectoryReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random().nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = DirectoryReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newTextField(fieldName, content, Field.Store.NO));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = DirectoryReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()),\n                                                    false);\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random().nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()),\n                                                                   false);\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = DirectoryReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newTextField(fieldName, content, Field.Store.NO));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = DirectoryReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random().nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = DirectoryReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newTextField(fieldName, content, Field.Store.NO));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = DirectoryReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()),\n                                                    false);\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random().nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()),\n                                                                   false);\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = DirectoryReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm),\n                                              false);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552","date":1344797146,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newTextField(fieldName, content, Field.Store.NO));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = DirectoryReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    if (br != null) {\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = DirectoryReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newTextField(fieldName, content, Field.Store.NO));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = DirectoryReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random().nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = DirectoryReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c7869f64c874ebf7f317d22c00baf2b6857797a6","date":1344856617,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newTextField(fieldName, content, Field.Store.NO));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = DirectoryReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    if (br != null) {\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = DirectoryReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newTextField(fieldName, content, Field.Store.NO));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = DirectoryReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random().nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = DirectoryReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9","date":1344867506,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newTextField(fieldName, content, Field.Store.NO));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = DirectoryReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    if (br != null) {\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = DirectoryReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newTextField(fieldName, content, Field.Store.NO));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = DirectoryReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    if (tps[j].hasPayload()) {\n                      BytesRef br = tps[j].getPayload();\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                      // Just to ensure all codecs can\n                      // handle a caller that mucks with the\n                      // returned payload:\n                      if (rarely()) {\n                        br.bytes = new byte[random().nextInt(5)];\n                      }\n                      br.length = 0;\n                      br.offset = 0;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        assertFalse(tp.hasPayload());\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = DirectoryReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newTextField(fieldName, content, Field.Store.NO));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.shutdown();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = DirectoryReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    if (br != null) {\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.shutdown();\n        \n        reader = DirectoryReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newTextField(fieldName, content, Field.Store.NO));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = DirectoryReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    if (br != null) {\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = DirectoryReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newTextField(fieldName, content, Field.Store.NO));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.shutdown();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = DirectoryReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    if (br != null) {\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                        .setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.shutdown();\n        \n        reader = DirectoryReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newTextField(fieldName, content, Field.Store.NO));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.shutdown();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = DirectoryReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    if (br != null) {\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.shutdown();\n        \n        reader = DirectoryReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newTextField(fieldName, content, Field.Store.NO));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = DirectoryReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    if (br != null) {\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                        .setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = DirectoryReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newTextField(fieldName, content, Field.Store.NO));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.shutdown();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = DirectoryReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    if (br != null) {\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                        .setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.shutdown();\n        \n        reader = DirectoryReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5c1c5aa8e88aa52c9e1cbfc696b611d3a56223c0","date":1422781929,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"  // builds an index with payloads in the given Directory and performs\n  // different tests to verify the payload encoding\n  private void performTest(Directory dir) throws Exception {\n    PayloadAnalyzer analyzer = new PayloadAnalyzer();\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                         .setOpenMode(OpenMode.CREATE)\n                                         .setMergePolicy(newLogMergePolicy()));\n        \n    // should be in sync with value in TermInfosWriter\n    final int skipInterval = 16;\n        \n    final int numTerms = 5;\n    final String fieldName = \"f1\";\n        \n    int numDocs = skipInterval + 1; \n    // create content for the test documents with just a few terms\n    Term[] terms = generateTerms(fieldName, numTerms);\n    StringBuilder sb = new StringBuilder();\n    for (int i = 0; i < terms.length; i++) {\n      sb.append(terms[i].text());\n      sb.append(\" \");\n    }\n    String content = sb.toString();\n        \n        \n    int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n    byte[] payloadData = generateRandomData(payloadDataLength);\n        \n    Document d = new Document();\n    d.add(newTextField(fieldName, content, Field.Store.NO));\n    // add the same document multiple times to have the same payload lengths for all\n    // occurrences within two consecutive skip intervals\n    int offset = 0;\n    for (int i = 0; i < 2 * numDocs; i++) {\n      analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n      offset += numTerms;\n      writer.addDocument(d);\n    }\n        \n    // make sure we create more than one segment to test merging\n    writer.commit();\n        \n    // now we make sure to have different payload lengths next at the next skip point        \n    for (int i = 0; i < numDocs; i++) {\n      analyzer.setPayloadData(fieldName, payloadData, offset, i);\n      offset += i * numTerms;\n      writer.addDocument(d);\n    }\n        \n    writer.forceMerge(1);\n    // flush\n    writer.close();\n        \n        \n    /*\n     * Verify the index\n     * first we test if all payloads are stored correctly\n     */        \n    IndexReader reader = DirectoryReader.open(dir);\n\n    byte[] verifyPayloadData = new byte[payloadDataLength];\n    offset = 0;\n    DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n    for (int i = 0; i < numTerms; i++) {\n      tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                MultiFields.getLiveDocs(reader),\n                                                terms[i].field(),\n                                                new BytesRef(terms[i].text()));\n    }\n        \n    while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      for (int i = 1; i < numTerms; i++) {\n        tps[i].nextDoc();\n      }\n      int freq = tps[0].freq();\n\n      for (int i = 0; i < freq; i++) {\n        for (int j = 0; j < numTerms; j++) {\n          tps[j].nextPosition();\n          BytesRef br = tps[j].getPayload();\n          if (br != null) {\n            System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n            offset += br.length;\n          }\n        }\n      }\n    }\n        \n    assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n    /*\n     *  test lazy skipping\n     */        \n    DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                               MultiFields.getLiveDocs(reader),\n                                                               terms[0].field(),\n                                                               new BytesRef(terms[0].text()));\n    tp.nextDoc();\n    tp.nextPosition();\n    // NOTE: prior rev of this test was failing to first\n    // call next here:\n    tp.nextDoc();\n    // now we don't read this payload\n    tp.nextPosition();\n    BytesRef payload = tp.getPayload();\n    assertEquals(\"Wrong payload length.\", 1, payload.length);\n    assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n    tp.nextDoc();\n    tp.nextPosition();\n        \n    // we don't read this payload and skip to a different document\n    tp.advance(5);\n    tp.nextPosition();\n    payload = tp.getPayload();\n    assertEquals(\"Wrong payload length.\", 1, payload.length);\n    assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n    /*\n     * Test different lengths at skip points\n     */\n    tp = MultiFields.getTermPositionsEnum(reader,\n                                          MultiFields.getLiveDocs(reader),\n                                          terms[1].field(),\n                                          new BytesRef(terms[1].text()));\n    tp.nextDoc();\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n    tp.advance(skipInterval - 1);\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n    tp.advance(2 * skipInterval - 1);\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n    tp.advance(3 * skipInterval - 1);\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n    reader.close();\n        \n    // test long payload\n    analyzer = new PayloadAnalyzer();\n    writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                             .setOpenMode(OpenMode.CREATE));\n    String singleTerm = \"lucene\";\n        \n    d = new Document();\n    d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n    // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n    payloadData = generateRandomData(2000);\n    analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n    writer.addDocument(d);\n\n        \n    writer.forceMerge(1);\n    // flush\n    writer.close();\n        \n    reader = DirectoryReader.open(dir);\n    tp = MultiFields.getTermPositionsEnum(reader,\n                                          MultiFields.getLiveDocs(reader),\n                                          fieldName,\n                                          new BytesRef(singleTerm));\n    tp.nextDoc();\n    tp.nextPosition();\n        \n    BytesRef br = tp.getPayload();\n    verifyPayloadData = new byte[br.length];\n    byte[] portion = new byte[1500];\n    System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n    assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n    reader.close();\n        \n  }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n            .setOpenMode(OpenMode.CREATE)\n            .setMergePolicy(newLogMergePolicy()));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text());\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(newTextField(fieldName, content, Field.Store.NO));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer = new PayloadAnalyzer(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d, analyzer);\n        }\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = DirectoryReader.open(dir);\n\n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n          tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                    MultiFields.getLiveDocs(reader),\n                                                    terms[i].field(),\n                                                    new BytesRef(terms[i].text()));\n        }\n        \n        while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].nextDoc();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    BytesRef br = tps[j].getPayload();\n                    if (br != null) {\n                      System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n                      offset += br.length;\n                    }\n                }\n            }\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                                   MultiFields.getLiveDocs(reader),\n                                                                   terms[0].field(),\n                                                                   new BytesRef(terms[0].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        // NOTE: prior rev of this test was failing to first\n        // call next here:\n        tp.nextDoc();\n        // now we don't read this payload\n        tp.nextPosition();\n        BytesRef payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.advance(5);\n        tp.nextPosition();\n        payload = tp.getPayload();\n        assertEquals(\"Wrong payload length.\", 1, payload.length);\n        assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              terms[1].field(),\n                                              new BytesRef(terms[1].text()));\n        tp.nextDoc();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n        tp.advance(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                        .setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.forceMerge(1);\n        // flush\n        writer.close();\n        \n        reader = DirectoryReader.open(dir);\n        tp = MultiFields.getTermPositionsEnum(reader,\n                                              MultiFields.getLiveDocs(reader),\n                                              fieldName,\n                                              new BytesRef(singleTerm));\n        tp.nextDoc();\n        tp.nextPosition();\n        \n        BytesRef br = tp.getPayload();\n        verifyPayloadData = new byte[br.length];\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"  // builds an index with payloads in the given Directory and performs\n  // different tests to verify the payload encoding\n  private void performTest(Directory dir) throws Exception {\n    PayloadAnalyzer analyzer = new PayloadAnalyzer();\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                         .setOpenMode(OpenMode.CREATE)\n                                         .setMergePolicy(newLogMergePolicy()));\n        \n    // should be in sync with value in TermInfosWriter\n    final int skipInterval = 16;\n        \n    final int numTerms = 5;\n    final String fieldName = \"f1\";\n        \n    int numDocs = skipInterval + 1; \n    // create content for the test documents with just a few terms\n    Term[] terms = generateTerms(fieldName, numTerms);\n    StringBuilder sb = new StringBuilder();\n    for (int i = 0; i < terms.length; i++) {\n      sb.append(terms[i].text());\n      sb.append(\" \");\n    }\n    String content = sb.toString();\n        \n        \n    int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n    byte[] payloadData = generateRandomData(payloadDataLength);\n        \n    Document d = new Document();\n    d.add(newTextField(fieldName, content, Field.Store.NO));\n    // add the same document multiple times to have the same payload lengths for all\n    // occurrences within two consecutive skip intervals\n    int offset = 0;\n    for (int i = 0; i < 2 * numDocs; i++) {\n      analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n      offset += numTerms;\n      writer.addDocument(d);\n    }\n        \n    // make sure we create more than one segment to test merging\n    writer.commit();\n        \n    // now we make sure to have different payload lengths next at the next skip point        \n    for (int i = 0; i < numDocs; i++) {\n      analyzer.setPayloadData(fieldName, payloadData, offset, i);\n      offset += i * numTerms;\n      writer.addDocument(d);\n    }\n        \n    writer.forceMerge(1);\n    // flush\n    writer.close();\n        \n        \n    /*\n     * Verify the index\n     * first we test if all payloads are stored correctly\n     */        \n    IndexReader reader = DirectoryReader.open(dir);\n\n    byte[] verifyPayloadData = new byte[payloadDataLength];\n    offset = 0;\n    PostingsEnum[] tps = new PostingsEnum[numTerms];\n    for (int i = 0; i < numTerms; i++) {\n      tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                MultiFields.getLiveDocs(reader),\n                                                terms[i].field(),\n                                                new BytesRef(terms[i].text()));\n    }\n\n    while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      for (int i = 1; i < numTerms; i++) {\n        tps[i].nextDoc();\n      }\n      int freq = tps[0].freq();\n\n      for (int i = 0; i < freq; i++) {\n        for (int j = 0; j < numTerms; j++) {\n          tps[j].nextPosition();\n          BytesRef br = tps[j].getPayload();\n          if (br != null) {\n            System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n            offset += br.length;\n          }\n        }\n      }\n    }\n\n    assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n    /*\n     *  test lazy skipping\n     */        \n    PostingsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                               MultiFields.getLiveDocs(reader),\n                                                               terms[0].field(),\n                                                               new BytesRef(terms[0].text()));\n    tp.nextDoc();\n    tp.nextPosition();\n    // NOTE: prior rev of this test was failing to first\n    // call next here:\n    tp.nextDoc();\n    // now we don't read this payload\n    tp.nextPosition();\n    BytesRef payload = tp.getPayload();\n    assertEquals(\"Wrong payload length.\", 1, payload.length);\n    assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n    tp.nextDoc();\n    tp.nextPosition();\n        \n    // we don't read this payload and skip to a different document\n    tp.advance(5);\n    tp.nextPosition();\n    payload = tp.getPayload();\n    assertEquals(\"Wrong payload length.\", 1, payload.length);\n    assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n        \n    /*\n     * Test different lengths at skip points\n     */\n    tp = MultiFields.getTermPositionsEnum(reader,\n                                          MultiFields.getLiveDocs(reader),\n                                          terms[1].field(),\n                                          new BytesRef(terms[1].text()));\n    tp.nextDoc();\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n    tp.advance(skipInterval - 1);\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n    tp.advance(2 * skipInterval - 1);\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n    tp.advance(3 * skipInterval - 1);\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n    reader.close();\n        \n    // test long payload\n    analyzer = new PayloadAnalyzer();\n    writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                             .setOpenMode(OpenMode.CREATE));\n    String singleTerm = \"lucene\";\n        \n    d = new Document();\n    d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n    // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n    payloadData = generateRandomData(2000);\n    analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n    writer.addDocument(d);\n\n        \n    writer.forceMerge(1);\n    // flush\n    writer.close();\n        \n    reader = DirectoryReader.open(dir);\n    tp = MultiFields.getTermPositionsEnum(reader,\n                                          MultiFields.getLiveDocs(reader),\n                                          fieldName,\n                                          new BytesRef(singleTerm));\n    tp.nextDoc();\n    tp.nextPosition();\n        \n    BytesRef br = tp.getPayload();\n    verifyPayloadData = new byte[br.length];\n    byte[] portion = new byte[1500];\n    System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n    assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n    reader.close();\n        \n  }\n\n","sourceOld":"  // builds an index with payloads in the given Directory and performs\n  // different tests to verify the payload encoding\n  private void performTest(Directory dir) throws Exception {\n    PayloadAnalyzer analyzer = new PayloadAnalyzer();\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                         .setOpenMode(OpenMode.CREATE)\n                                         .setMergePolicy(newLogMergePolicy()));\n        \n    // should be in sync with value in TermInfosWriter\n    final int skipInterval = 16;\n        \n    final int numTerms = 5;\n    final String fieldName = \"f1\";\n        \n    int numDocs = skipInterval + 1; \n    // create content for the test documents with just a few terms\n    Term[] terms = generateTerms(fieldName, numTerms);\n    StringBuilder sb = new StringBuilder();\n    for (int i = 0; i < terms.length; i++) {\n      sb.append(terms[i].text());\n      sb.append(\" \");\n    }\n    String content = sb.toString();\n        \n        \n    int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n    byte[] payloadData = generateRandomData(payloadDataLength);\n        \n    Document d = new Document();\n    d.add(newTextField(fieldName, content, Field.Store.NO));\n    // add the same document multiple times to have the same payload lengths for all\n    // occurrences within two consecutive skip intervals\n    int offset = 0;\n    for (int i = 0; i < 2 * numDocs; i++) {\n      analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n      offset += numTerms;\n      writer.addDocument(d);\n    }\n        \n    // make sure we create more than one segment to test merging\n    writer.commit();\n        \n    // now we make sure to have different payload lengths next at the next skip point        \n    for (int i = 0; i < numDocs; i++) {\n      analyzer.setPayloadData(fieldName, payloadData, offset, i);\n      offset += i * numTerms;\n      writer.addDocument(d);\n    }\n        \n    writer.forceMerge(1);\n    // flush\n    writer.close();\n        \n        \n    /*\n     * Verify the index\n     * first we test if all payloads are stored correctly\n     */        \n    IndexReader reader = DirectoryReader.open(dir);\n\n    byte[] verifyPayloadData = new byte[payloadDataLength];\n    offset = 0;\n    DocsAndPositionsEnum[] tps = new DocsAndPositionsEnum[numTerms];\n    for (int i = 0; i < numTerms; i++) {\n      tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                MultiFields.getLiveDocs(reader),\n                                                terms[i].field(),\n                                                new BytesRef(terms[i].text()));\n    }\n        \n    while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      for (int i = 1; i < numTerms; i++) {\n        tps[i].nextDoc();\n      }\n      int freq = tps[0].freq();\n\n      for (int i = 0; i < freq; i++) {\n        for (int j = 0; j < numTerms; j++) {\n          tps[j].nextPosition();\n          BytesRef br = tps[j].getPayload();\n          if (br != null) {\n            System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n            offset += br.length;\n          }\n        }\n      }\n    }\n        \n    assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n    /*\n     *  test lazy skipping\n     */        \n    DocsAndPositionsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                               MultiFields.getLiveDocs(reader),\n                                                               terms[0].field(),\n                                                               new BytesRef(terms[0].text()));\n    tp.nextDoc();\n    tp.nextPosition();\n    // NOTE: prior rev of this test was failing to first\n    // call next here:\n    tp.nextDoc();\n    // now we don't read this payload\n    tp.nextPosition();\n    BytesRef payload = tp.getPayload();\n    assertEquals(\"Wrong payload length.\", 1, payload.length);\n    assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n    tp.nextDoc();\n    tp.nextPosition();\n        \n    // we don't read this payload and skip to a different document\n    tp.advance(5);\n    tp.nextPosition();\n    payload = tp.getPayload();\n    assertEquals(\"Wrong payload length.\", 1, payload.length);\n    assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n                \n        \n    /*\n     * Test different lengths at skip points\n     */\n    tp = MultiFields.getTermPositionsEnum(reader,\n                                          MultiFields.getLiveDocs(reader),\n                                          terms[1].field(),\n                                          new BytesRef(terms[1].text()));\n    tp.nextDoc();\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n    tp.advance(skipInterval - 1);\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n    tp.advance(2 * skipInterval - 1);\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n    tp.advance(3 * skipInterval - 1);\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n    reader.close();\n        \n    // test long payload\n    analyzer = new PayloadAnalyzer();\n    writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                             .setOpenMode(OpenMode.CREATE));\n    String singleTerm = \"lucene\";\n        \n    d = new Document();\n    d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n    // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n    payloadData = generateRandomData(2000);\n    analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n    writer.addDocument(d);\n\n        \n    writer.forceMerge(1);\n    // flush\n    writer.close();\n        \n    reader = DirectoryReader.open(dir);\n    tp = MultiFields.getTermPositionsEnum(reader,\n                                          MultiFields.getLiveDocs(reader),\n                                          fieldName,\n                                          new BytesRef(singleTerm));\n    tp.nextDoc();\n    tp.nextPosition();\n        \n    BytesRef br = tp.getPayload();\n    verifyPayloadData = new byte[br.length];\n    byte[] portion = new byte[1500];\n    System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n    assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n    reader.close();\n        \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"  // builds an index with payloads in the given Directory and performs\n  // different tests to verify the payload encoding\n  private void performTest(Directory dir) throws Exception {\n    PayloadAnalyzer analyzer = new PayloadAnalyzer();\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                         .setOpenMode(OpenMode.CREATE)\n                                         .setMergePolicy(newLogMergePolicy()));\n        \n    // should be in sync with value in TermInfosWriter\n    final int skipInterval = 16;\n        \n    final int numTerms = 5;\n    final String fieldName = \"f1\";\n        \n    int numDocs = skipInterval + 1; \n    // create content for the test documents with just a few terms\n    Term[] terms = generateTerms(fieldName, numTerms);\n    StringBuilder sb = new StringBuilder();\n    for (int i = 0; i < terms.length; i++) {\n      sb.append(terms[i].text());\n      sb.append(\" \");\n    }\n    String content = sb.toString();\n        \n        \n    int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n    byte[] payloadData = generateRandomData(payloadDataLength);\n        \n    Document d = new Document();\n    d.add(newTextField(fieldName, content, Field.Store.NO));\n    // add the same document multiple times to have the same payload lengths for all\n    // occurrences within two consecutive skip intervals\n    int offset = 0;\n    for (int i = 0; i < 2 * numDocs; i++) {\n      analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n      offset += numTerms;\n      writer.addDocument(d);\n    }\n        \n    // make sure we create more than one segment to test merging\n    writer.commit();\n        \n    // now we make sure to have different payload lengths next at the next skip point        \n    for (int i = 0; i < numDocs; i++) {\n      analyzer.setPayloadData(fieldName, payloadData, offset, i);\n      offset += i * numTerms;\n      writer.addDocument(d);\n    }\n        \n    writer.forceMerge(1);\n    // flush\n    writer.close();\n        \n        \n    /*\n     * Verify the index\n     * first we test if all payloads are stored correctly\n     */        \n    IndexReader reader = DirectoryReader.open(dir);\n\n    byte[] verifyPayloadData = new byte[payloadDataLength];\n    offset = 0;\n    PostingsEnum[] tps = new PostingsEnum[numTerms];\n    for (int i = 0; i < numTerms; i++) {\n      tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                terms[i].field(),\n                                                new BytesRef(terms[i].text()));\n    }\n\n    while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      for (int i = 1; i < numTerms; i++) {\n        tps[i].nextDoc();\n      }\n      int freq = tps[0].freq();\n\n      for (int i = 0; i < freq; i++) {\n        for (int j = 0; j < numTerms; j++) {\n          tps[j].nextPosition();\n          BytesRef br = tps[j].getPayload();\n          if (br != null) {\n            System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n            offset += br.length;\n          }\n        }\n      }\n    }\n\n    assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n    /*\n     *  test lazy skipping\n     */        \n    PostingsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                               terms[0].field(),\n                                                               new BytesRef(terms[0].text()));\n    tp.nextDoc();\n    tp.nextPosition();\n    // NOTE: prior rev of this test was failing to first\n    // call next here:\n    tp.nextDoc();\n    // now we don't read this payload\n    tp.nextPosition();\n    BytesRef payload = tp.getPayload();\n    assertEquals(\"Wrong payload length.\", 1, payload.length);\n    assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n    tp.nextDoc();\n    tp.nextPosition();\n        \n    // we don't read this payload and skip to a different document\n    tp.advance(5);\n    tp.nextPosition();\n    payload = tp.getPayload();\n    assertEquals(\"Wrong payload length.\", 1, payload.length);\n    assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n        \n    /*\n     * Test different lengths at skip points\n     */\n    tp = MultiFields.getTermPositionsEnum(reader,\n                                          terms[1].field(),\n                                          new BytesRef(terms[1].text()));\n    tp.nextDoc();\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n    tp.advance(skipInterval - 1);\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n    tp.advance(2 * skipInterval - 1);\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n    tp.advance(3 * skipInterval - 1);\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n    reader.close();\n        \n    // test long payload\n    analyzer = new PayloadAnalyzer();\n    writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                             .setOpenMode(OpenMode.CREATE));\n    String singleTerm = \"lucene\";\n        \n    d = new Document();\n    d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n    // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n    payloadData = generateRandomData(2000);\n    analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n    writer.addDocument(d);\n\n        \n    writer.forceMerge(1);\n    // flush\n    writer.close();\n        \n    reader = DirectoryReader.open(dir);\n    tp = MultiFields.getTermPositionsEnum(reader,\n                                          fieldName,\n                                          new BytesRef(singleTerm));\n    tp.nextDoc();\n    tp.nextPosition();\n        \n    BytesRef br = tp.getPayload();\n    verifyPayloadData = new byte[br.length];\n    byte[] portion = new byte[1500];\n    System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n    assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n    reader.close();\n        \n  }\n\n","sourceOld":"  // builds an index with payloads in the given Directory and performs\n  // different tests to verify the payload encoding\n  private void performTest(Directory dir) throws Exception {\n    PayloadAnalyzer analyzer = new PayloadAnalyzer();\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                         .setOpenMode(OpenMode.CREATE)\n                                         .setMergePolicy(newLogMergePolicy()));\n        \n    // should be in sync with value in TermInfosWriter\n    final int skipInterval = 16;\n        \n    final int numTerms = 5;\n    final String fieldName = \"f1\";\n        \n    int numDocs = skipInterval + 1; \n    // create content for the test documents with just a few terms\n    Term[] terms = generateTerms(fieldName, numTerms);\n    StringBuilder sb = new StringBuilder();\n    for (int i = 0; i < terms.length; i++) {\n      sb.append(terms[i].text());\n      sb.append(\" \");\n    }\n    String content = sb.toString();\n        \n        \n    int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n    byte[] payloadData = generateRandomData(payloadDataLength);\n        \n    Document d = new Document();\n    d.add(newTextField(fieldName, content, Field.Store.NO));\n    // add the same document multiple times to have the same payload lengths for all\n    // occurrences within two consecutive skip intervals\n    int offset = 0;\n    for (int i = 0; i < 2 * numDocs; i++) {\n      analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n      offset += numTerms;\n      writer.addDocument(d);\n    }\n        \n    // make sure we create more than one segment to test merging\n    writer.commit();\n        \n    // now we make sure to have different payload lengths next at the next skip point        \n    for (int i = 0; i < numDocs; i++) {\n      analyzer.setPayloadData(fieldName, payloadData, offset, i);\n      offset += i * numTerms;\n      writer.addDocument(d);\n    }\n        \n    writer.forceMerge(1);\n    // flush\n    writer.close();\n        \n        \n    /*\n     * Verify the index\n     * first we test if all payloads are stored correctly\n     */        \n    IndexReader reader = DirectoryReader.open(dir);\n\n    byte[] verifyPayloadData = new byte[payloadDataLength];\n    offset = 0;\n    PostingsEnum[] tps = new PostingsEnum[numTerms];\n    for (int i = 0; i < numTerms; i++) {\n      tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                MultiFields.getLiveDocs(reader),\n                                                terms[i].field(),\n                                                new BytesRef(terms[i].text()));\n    }\n\n    while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      for (int i = 1; i < numTerms; i++) {\n        tps[i].nextDoc();\n      }\n      int freq = tps[0].freq();\n\n      for (int i = 0; i < freq; i++) {\n        for (int j = 0; j < numTerms; j++) {\n          tps[j].nextPosition();\n          BytesRef br = tps[j].getPayload();\n          if (br != null) {\n            System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n            offset += br.length;\n          }\n        }\n      }\n    }\n\n    assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n    /*\n     *  test lazy skipping\n     */        \n    PostingsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                               MultiFields.getLiveDocs(reader),\n                                                               terms[0].field(),\n                                                               new BytesRef(terms[0].text()));\n    tp.nextDoc();\n    tp.nextPosition();\n    // NOTE: prior rev of this test was failing to first\n    // call next here:\n    tp.nextDoc();\n    // now we don't read this payload\n    tp.nextPosition();\n    BytesRef payload = tp.getPayload();\n    assertEquals(\"Wrong payload length.\", 1, payload.length);\n    assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n    tp.nextDoc();\n    tp.nextPosition();\n        \n    // we don't read this payload and skip to a different document\n    tp.advance(5);\n    tp.nextPosition();\n    payload = tp.getPayload();\n    assertEquals(\"Wrong payload length.\", 1, payload.length);\n    assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n        \n    /*\n     * Test different lengths at skip points\n     */\n    tp = MultiFields.getTermPositionsEnum(reader,\n                                          MultiFields.getLiveDocs(reader),\n                                          terms[1].field(),\n                                          new BytesRef(terms[1].text()));\n    tp.nextDoc();\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n    tp.advance(skipInterval - 1);\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n    tp.advance(2 * skipInterval - 1);\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n    tp.advance(3 * skipInterval - 1);\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n    reader.close();\n        \n    // test long payload\n    analyzer = new PayloadAnalyzer();\n    writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                             .setOpenMode(OpenMode.CREATE));\n    String singleTerm = \"lucene\";\n        \n    d = new Document();\n    d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n    // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n    payloadData = generateRandomData(2000);\n    analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n    writer.addDocument(d);\n\n        \n    writer.forceMerge(1);\n    // flush\n    writer.close();\n        \n    reader = DirectoryReader.open(dir);\n    tp = MultiFields.getTermPositionsEnum(reader,\n                                          MultiFields.getLiveDocs(reader),\n                                          fieldName,\n                                          new BytesRef(singleTerm));\n    tp.nextDoc();\n    tp.nextPosition();\n        \n    BytesRef br = tp.getPayload();\n    verifyPayloadData = new byte[br.length];\n    byte[] portion = new byte[1500];\n    System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n    assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n    reader.close();\n        \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"04e775de416dd2d8067b10db1c8af975a1d5017e","date":1539906554,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"  // builds an index with payloads in the given Directory and performs\n  // different tests to verify the payload encoding\n  private void performTest(Directory dir) throws Exception {\n    PayloadAnalyzer analyzer = new PayloadAnalyzer();\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                         .setOpenMode(OpenMode.CREATE)\n                                         .setMergePolicy(newLogMergePolicy()));\n        \n    // should be in sync with value in TermInfosWriter\n    final int skipInterval = 16;\n        \n    final int numTerms = 5;\n    final String fieldName = \"f1\";\n        \n    int numDocs = skipInterval + 1; \n    // create content for the test documents with just a few terms\n    Term[] terms = generateTerms(fieldName, numTerms);\n    StringBuilder sb = new StringBuilder();\n    for (int i = 0; i < terms.length; i++) {\n      sb.append(terms[i].text());\n      sb.append(\" \");\n    }\n    String content = sb.toString();\n        \n        \n    int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n    byte[] payloadData = generateRandomData(payloadDataLength);\n        \n    Document d = new Document();\n    d.add(newTextField(fieldName, content, Field.Store.NO));\n    // add the same document multiple times to have the same payload lengths for all\n    // occurrences within two consecutive skip intervals\n    int offset = 0;\n    for (int i = 0; i < 2 * numDocs; i++) {\n      analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n      offset += numTerms;\n      writer.addDocument(d);\n    }\n        \n    // make sure we create more than one segment to test merging\n    writer.commit();\n        \n    // now we make sure to have different payload lengths next at the next skip point        \n    for (int i = 0; i < numDocs; i++) {\n      analyzer.setPayloadData(fieldName, payloadData, offset, i);\n      offset += i * numTerms;\n      writer.addDocument(d);\n    }\n        \n    writer.forceMerge(1);\n    // flush\n    writer.close();\n        \n        \n    /*\n     * Verify the index\n     * first we test if all payloads are stored correctly\n     */        \n    IndexReader reader = DirectoryReader.open(dir);\n\n    byte[] verifyPayloadData = new byte[payloadDataLength];\n    offset = 0;\n    PostingsEnum[] tps = new PostingsEnum[numTerms];\n    for (int i = 0; i < numTerms; i++) {\n      tps[i] = MultiTerms.getTermPostingsEnum(reader,\n                                                terms[i].field(),\n                                                new BytesRef(terms[i].text()));\n    }\n\n    while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      for (int i = 1; i < numTerms; i++) {\n        tps[i].nextDoc();\n      }\n      int freq = tps[0].freq();\n\n      for (int i = 0; i < freq; i++) {\n        for (int j = 0; j < numTerms; j++) {\n          tps[j].nextPosition();\n          BytesRef br = tps[j].getPayload();\n          if (br != null) {\n            System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n            offset += br.length;\n          }\n        }\n      }\n    }\n\n    assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n    /*\n     *  test lazy skipping\n     */        \n    PostingsEnum tp = MultiTerms.getTermPostingsEnum(reader,\n                                                               terms[0].field(),\n                                                               new BytesRef(terms[0].text()));\n    tp.nextDoc();\n    tp.nextPosition();\n    // NOTE: prior rev of this test was failing to first\n    // call next here:\n    tp.nextDoc();\n    // now we don't read this payload\n    tp.nextPosition();\n    BytesRef payload = tp.getPayload();\n    assertEquals(\"Wrong payload length.\", 1, payload.length);\n    assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n    tp.nextDoc();\n    tp.nextPosition();\n        \n    // we don't read this payload and skip to a different document\n    tp.advance(5);\n    tp.nextPosition();\n    payload = tp.getPayload();\n    assertEquals(\"Wrong payload length.\", 1, payload.length);\n    assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n        \n    /*\n     * Test different lengths at skip points\n     */\n    tp = MultiTerms.getTermPostingsEnum(reader,\n                                          terms[1].field(),\n                                          new BytesRef(terms[1].text()));\n    tp.nextDoc();\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n    tp.advance(skipInterval - 1);\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n    tp.advance(2 * skipInterval - 1);\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n    tp.advance(3 * skipInterval - 1);\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n    reader.close();\n        \n    // test long payload\n    analyzer = new PayloadAnalyzer();\n    writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                             .setOpenMode(OpenMode.CREATE));\n    String singleTerm = \"lucene\";\n        \n    d = new Document();\n    d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n    // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n    payloadData = generateRandomData(2000);\n    analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n    writer.addDocument(d);\n\n        \n    writer.forceMerge(1);\n    // flush\n    writer.close();\n        \n    reader = DirectoryReader.open(dir);\n    tp = MultiTerms.getTermPostingsEnum(reader,\n                                          fieldName,\n                                          new BytesRef(singleTerm));\n    tp.nextDoc();\n    tp.nextPosition();\n        \n    BytesRef br = tp.getPayload();\n    verifyPayloadData = new byte[br.length];\n    byte[] portion = new byte[1500];\n    System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n    assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n    reader.close();\n        \n  }\n\n","sourceOld":"  // builds an index with payloads in the given Directory and performs\n  // different tests to verify the payload encoding\n  private void performTest(Directory dir) throws Exception {\n    PayloadAnalyzer analyzer = new PayloadAnalyzer();\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                         .setOpenMode(OpenMode.CREATE)\n                                         .setMergePolicy(newLogMergePolicy()));\n        \n    // should be in sync with value in TermInfosWriter\n    final int skipInterval = 16;\n        \n    final int numTerms = 5;\n    final String fieldName = \"f1\";\n        \n    int numDocs = skipInterval + 1; \n    // create content for the test documents with just a few terms\n    Term[] terms = generateTerms(fieldName, numTerms);\n    StringBuilder sb = new StringBuilder();\n    for (int i = 0; i < terms.length; i++) {\n      sb.append(terms[i].text());\n      sb.append(\" \");\n    }\n    String content = sb.toString();\n        \n        \n    int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n    byte[] payloadData = generateRandomData(payloadDataLength);\n        \n    Document d = new Document();\n    d.add(newTextField(fieldName, content, Field.Store.NO));\n    // add the same document multiple times to have the same payload lengths for all\n    // occurrences within two consecutive skip intervals\n    int offset = 0;\n    for (int i = 0; i < 2 * numDocs; i++) {\n      analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n      offset += numTerms;\n      writer.addDocument(d);\n    }\n        \n    // make sure we create more than one segment to test merging\n    writer.commit();\n        \n    // now we make sure to have different payload lengths next at the next skip point        \n    for (int i = 0; i < numDocs; i++) {\n      analyzer.setPayloadData(fieldName, payloadData, offset, i);\n      offset += i * numTerms;\n      writer.addDocument(d);\n    }\n        \n    writer.forceMerge(1);\n    // flush\n    writer.close();\n        \n        \n    /*\n     * Verify the index\n     * first we test if all payloads are stored correctly\n     */        \n    IndexReader reader = DirectoryReader.open(dir);\n\n    byte[] verifyPayloadData = new byte[payloadDataLength];\n    offset = 0;\n    PostingsEnum[] tps = new PostingsEnum[numTerms];\n    for (int i = 0; i < numTerms; i++) {\n      tps[i] = MultiFields.getTermPositionsEnum(reader,\n                                                terms[i].field(),\n                                                new BytesRef(terms[i].text()));\n    }\n\n    while (tps[0].nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      for (int i = 1; i < numTerms; i++) {\n        tps[i].nextDoc();\n      }\n      int freq = tps[0].freq();\n\n      for (int i = 0; i < freq; i++) {\n        for (int j = 0; j < numTerms; j++) {\n          tps[j].nextPosition();\n          BytesRef br = tps[j].getPayload();\n          if (br != null) {\n            System.arraycopy(br.bytes, br.offset, verifyPayloadData, offset, br.length);\n            offset += br.length;\n          }\n        }\n      }\n    }\n\n    assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n    /*\n     *  test lazy skipping\n     */        \n    PostingsEnum tp = MultiFields.getTermPositionsEnum(reader,\n                                                               terms[0].field(),\n                                                               new BytesRef(terms[0].text()));\n    tp.nextDoc();\n    tp.nextPosition();\n    // NOTE: prior rev of this test was failing to first\n    // call next here:\n    tp.nextDoc();\n    // now we don't read this payload\n    tp.nextPosition();\n    BytesRef payload = tp.getPayload();\n    assertEquals(\"Wrong payload length.\", 1, payload.length);\n    assertEquals(payload.bytes[payload.offset], payloadData[numTerms]);\n    tp.nextDoc();\n    tp.nextPosition();\n        \n    // we don't read this payload and skip to a different document\n    tp.advance(5);\n    tp.nextPosition();\n    payload = tp.getPayload();\n    assertEquals(\"Wrong payload length.\", 1, payload.length);\n    assertEquals(payload.bytes[payload.offset], payloadData[5 * numTerms]);\n        \n    /*\n     * Test different lengths at skip points\n     */\n    tp = MultiFields.getTermPositionsEnum(reader,\n                                          terms[1].field(),\n                                          new BytesRef(terms[1].text()));\n    tp.nextDoc();\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n    tp.advance(skipInterval - 1);\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n    tp.advance(2 * skipInterval - 1);\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 1, tp.getPayload().length);\n    tp.advance(3 * skipInterval - 1);\n    tp.nextPosition();\n    assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayload().length);\n        \n    reader.close();\n        \n    // test long payload\n    analyzer = new PayloadAnalyzer();\n    writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                             .setOpenMode(OpenMode.CREATE));\n    String singleTerm = \"lucene\";\n        \n    d = new Document();\n    d.add(newTextField(fieldName, singleTerm, Field.Store.NO));\n    // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n    payloadData = generateRandomData(2000);\n    analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n    writer.addDocument(d);\n\n        \n    writer.forceMerge(1);\n    // flush\n    writer.close();\n        \n    reader = DirectoryReader.open(dir);\n    tp = MultiFields.getTermPositionsEnum(reader,\n                                          fieldName,\n                                          new BytesRef(singleTerm));\n    tp.nextDoc();\n    tp.nextPosition();\n        \n    BytesRef br = tp.getPayload();\n    verifyPayloadData = new byte[br.length];\n    byte[] portion = new byte[1500];\n    System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n    assertByteArrayEquals(portion, br.bytes, br.offset, br.length);\n    reader.close();\n        \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"51f5280f31484820499077f41fcdfe92d527d9dc":["5c1c5aa8e88aa52c9e1cbfc696b611d3a56223c0"],"5c1c5aa8e88aa52c9e1cbfc696b611d3a56223c0":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["51f5280f31484820499077f41fcdfe92d527d9dc"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552"],"2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552":["322360ac5185a8446d3e0b530b2068bef67cd3d5"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["04f07771a2a7dd3a395700665ed839c3dae2def2","322360ac5185a8446d3e0b530b2068bef67cd3d5"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["04f07771a2a7dd3a395700665ed839c3dae2def2","322360ac5185a8446d3e0b530b2068bef67cd3d5"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"322360ac5185a8446d3e0b530b2068bef67cd3d5":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9":["d6f074e73200c07d54f242d3880a8da5a35ff97b","2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["04e775de416dd2d8067b10db1c8af975a1d5017e"]},"commit2Childs":{"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"51f5280f31484820499077f41fcdfe92d527d9dc":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"5c1c5aa8e88aa52c9e1cbfc696b611d3a56223c0":["51f5280f31484820499077f41fcdfe92d527d9dc"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b","322360ac5185a8446d3e0b530b2068bef67cd3d5"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["04e775de416dd2d8067b10db1c8af975a1d5017e"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":[],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552":["c7869f64c874ebf7f317d22c00baf2b6857797a6","ae14298f4eec6d5faee6a149f88ba57d14a6f21a","d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["c7869f64c874ebf7f317d22c00baf2b6857797a6"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["5c1c5aa8e88aa52c9e1cbfc696b611d3a56223c0"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"322360ac5185a8446d3e0b530b2068bef67cd3d5":["2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c7869f64c874ebf7f317d22c00baf2b6857797a6","d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}