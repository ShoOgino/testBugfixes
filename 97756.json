{"path":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","sourceNew":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n    int termID = bytesHash.addByPoolOffset(textStart);\n    if (termID >= 0) {      // New posting\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","sourceOld":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n    int termID = bytesHash.addByPoolOffset(textStart);\n    if (termID >= 0) {      // New posting\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"30fd30bfbfa6b9e036bcd99c8339712e965d4a63","date":1351859294,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","sourceNew":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n    int termID = bytesHash.addByPoolOffset(textStart);\n    if (termID >= 0) {      // New posting\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","sourceOld":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n    int termID = bytesHash.addByPoolOffset(textStart);\n    if (termID >= 0) {      // New posting\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5","date":1379624229,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","sourceNew":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart.  term vectors use\n  // this API.\n  public void add(int textStart) throws IOException {\n    int termID = bytesHash.addByPoolOffset(textStart);\n    if (termID >= 0) {      // New posting\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","sourceOld":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n    int termID = bytesHash.addByPoolOffset(textStart);\n    if (termID >= 0) {      // New posting\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"52c7e49be259508735752fba88085255014a6ecf","date":1398706273,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","sourceNew":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart.  term vectors use\n  // this API.\n  public void add(int textStart) throws IOException {\n    int termID = bytesHash.addByPoolOffset(textStart);\n    if (termID >= 0) {      // New posting\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      addTerm(termID);\n    }\n  }\n\n","sourceOld":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart.  term vectors use\n  // this API.\n  public void add(int textStart) throws IOException {\n    int termID = bytesHash.addByPoolOffset(textStart);\n    if (termID >= 0) {      // New posting\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3394716f52b34ab259ad5247e7595d9f9db6e935","date":1398791921,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","sourceNew":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart.  term vectors use\n  // this API.\n  public void add(int textStart) throws IOException {\n    int termID = bytesHash.addByPoolOffset(textStart);\n    if (termID >= 0) {      // New posting\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      addTerm(termID);\n    }\n  }\n\n","sourceOld":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart.  term vectors use\n  // this API.\n  public void add(int textStart) throws IOException {\n    int termID = bytesHash.addByPoolOffset(textStart);\n    if (termID >= 0) {      // New posting\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","date":1398844771,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","sourceNew":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart.  term vectors use\n  // this API.\n  public void add(int textStart) throws IOException {\n    int termID = bytesHash.addByPoolOffset(textStart);\n    if (termID >= 0) {      // New posting\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      addTerm(termID);\n    }\n  }\n\n","sourceOld":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart.  term vectors use\n  // this API.\n  public void add(int textStart) throws IOException {\n    int termID = bytesHash.addByPoolOffset(textStart);\n    if (termID >= 0) {      // New posting\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3cc3fa1ecad75b99ec55169e44628808f9866ad","date":1592311545,"type":4,"author":"Simon Willnauer","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","sourceNew":null,"sourceOld":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart.  term vectors use\n  // this API.\n  public void add(int textStart) throws IOException {\n    int termID = bytesHash.addByPoolOffset(textStart);\n    if (termID >= 0) {      // New posting\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      addTerm(termID);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3394716f52b34ab259ad5247e7595d9f9db6e935":["519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5","52c7e49be259508735752fba88085255014a6ecf"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":["519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5","3394716f52b34ab259ad5247e7595d9f9db6e935"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5":["30fd30bfbfa6b9e036bcd99c8339712e965d4a63"],"30fd30bfbfa6b9e036bcd99c8339712e965d4a63":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"52c7e49be259508735752fba88085255014a6ecf":["519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5"],"d3cc3fa1ecad75b99ec55169e44628808f9866ad":["3394716f52b34ab259ad5247e7595d9f9db6e935"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d3cc3fa1ecad75b99ec55169e44628808f9866ad"]},"commit2Childs":{"3394716f52b34ab259ad5247e7595d9f9db6e935":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","d3cc3fa1ecad75b99ec55169e44628808f9866ad"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["30fd30bfbfa6b9e036bcd99c8339712e965d4a63"],"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5":["3394716f52b34ab259ad5247e7595d9f9db6e935","c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","52c7e49be259508735752fba88085255014a6ecf"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"30fd30bfbfa6b9e036bcd99c8339712e965d4a63":["519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5"],"52c7e49be259508735752fba88085255014a6ecf":["3394716f52b34ab259ad5247e7595d9f9db6e935"],"d3cc3fa1ecad75b99ec55169e44628808f9866ad":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}