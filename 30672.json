{"path":"lucene/core/src/test/org/apache/lucene/index/TestSoftDeletesRetentionMergePolicy#testSoftDeleteWithRetention().mjava","commits":[{"id":"5ee0394b8176abd7c90a4be8c05465be1879db79","date":1522842314,"type":0,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSoftDeletesRetentionMergePolicy#testSoftDeleteWithRetention().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * tests soft deletes that carry over deleted documents on merge for history rentention.\n   */\n  public void testSoftDeleteWithRetention() throws IOException, InterruptedException {\n    AtomicInteger seqIds = new AtomicInteger(0);\n    Directory dir = newDirectory();\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\n    indexWriterConfig.setMergePolicy(new SoftDeletesRetentionMergePolicy(\"soft_delete\",\n        () -> IntPoint.newRangeQuery(\"seq_id\", seqIds.intValue() - 50, Integer.MAX_VALUE),\n        indexWriterConfig.getMergePolicy()));\n    indexWriterConfig.setSoftDeletesField(\"soft_delete\");\n    IndexWriter writer = new IndexWriter(dir, indexWriterConfig);\n    Thread[] threads = new Thread[2 + random().nextInt(3)];\n    CountDownLatch startLatch = new CountDownLatch(1);\n    CountDownLatch started = new CountDownLatch(threads.length);\n    boolean updateSeveralDocs = random().nextBoolean();\n    Set<String> ids = Collections.synchronizedSet(new HashSet<>());\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new Thread(() -> {\n        try {\n          started.countDown();\n          startLatch.await();\n          for (int d = 0;  d < 100; d++) {\n            String id = String.valueOf(random().nextInt(10));\n            int seqId = seqIds.incrementAndGet();\n            if (updateSeveralDocs) {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              doc.add(new IntPoint(\"seq_id\", seqId));\n              writer.softUpdateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc),\n                  new NumericDocValuesField(\"soft_delete\", 1));\n            } else {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              doc.add(new IntPoint(\"seq_id\", seqId));\n              writer.softUpdateDocument(new Term(\"id\", id), doc,\n                  new NumericDocValuesField(\"soft_delete\", 1));\n            }\n            ids.add(id);\n          }\n        } catch (IOException | InterruptedException e) {\n          throw new AssertionError(e);\n        }\n      });\n      threads[i].start();\n    }\n    started.await();\n    startLatch.countDown();\n\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].join();\n    }\n    DirectoryReader reader = DirectoryReader.open(writer);\n    IndexSearcher searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      TopDocs topDocs = searcher.search(new TermQuery(new Term(\"id\", id)), 10);\n      if (updateSeveralDocs) {\n        assertEquals(2, topDocs.totalHits);\n        assertEquals(Math.abs(topDocs.scoreDocs[0].doc - topDocs.scoreDocs[1].doc), 1);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n      }\n    }\n    writer.addDocument(new Document()); // add a dummy doc to trigger a segment here\n    writer.flush();\n    writer.forceMerge(1);\n    DirectoryReader oldReader = reader;\n    reader = DirectoryReader.openIfChanged(reader, writer);\n    if (reader != null) {\n      oldReader.close();\n      assertNotSame(oldReader, reader);\n    } else {\n      reader = oldReader;\n    }\n    assertEquals(1, reader.leaves().size());\n    LeafReaderContext leafReaderContext = reader.leaves().get(0);\n    LeafReader leafReader = leafReaderContext.reader();\n    searcher = new IndexSearcher(new FilterLeafReader(leafReader) {\n      @Override\n      public CacheHelper getCoreCacheHelper() {\n        return leafReader.getCoreCacheHelper();\n      }\n\n      @Override\n      public CacheHelper getReaderCacheHelper() {\n        return leafReader.getReaderCacheHelper();\n      }\n\n      @Override\n      public Bits getLiveDocs() {\n        return null;\n      }\n\n      @Override\n      public int numDocs() {\n        return maxDoc();\n      }\n    });\n    TopDocs seq_id = searcher.search(IntPoint.newRangeQuery(\"seq_id\", seqIds.intValue() - 50, Integer.MAX_VALUE), 10);\n    assertTrue(seq_id.totalHits + \" hits\", seq_id.totalHits >= 50);\n    searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      if (updateSeveralDocs) {\n        assertEquals(2, searcher.search(new TermQuery(new Term(\"id\", id)), 10).totalHits);\n      } else {\n        assertEquals(1, searcher.search(new TermQuery(new Term(\"id\", id)), 10).totalHits);\n      }\n    }\n    IOUtils.close(reader, writer, dir);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f372764a5bd3ebacde5b99ee3303153eb5ec0d2f","date":1525347515,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSoftDeletesRetentionMergePolicy#testSoftDeleteWithRetention().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSoftDeletesRetentionMergePolicy#testSoftDeleteWithRetention().mjava","sourceNew":"  /**\n   * tests soft deletes that carry over deleted documents on merge for history rentention.\n   */\n  public void testSoftDeleteWithRetention() throws IOException, InterruptedException {\n    AtomicInteger seqIds = new AtomicInteger(0);\n    Directory dir = newDirectory();\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\n    indexWriterConfig.setMergePolicy(new SoftDeletesRetentionMergePolicy(\"soft_delete\",\n        () -> IntPoint.newRangeQuery(\"seq_id\", seqIds.intValue() - 50, Integer.MAX_VALUE),\n        indexWriterConfig.getMergePolicy()));\n    indexWriterConfig.setSoftDeletesField(\"soft_delete\");\n    IndexWriter writer = new IndexWriter(dir, indexWriterConfig);\n    Thread[] threads = new Thread[2 + random().nextInt(3)];\n    CountDownLatch startLatch = new CountDownLatch(1);\n    CountDownLatch started = new CountDownLatch(threads.length);\n    boolean updateSeveralDocs = random().nextBoolean();\n    Set<String> ids = Collections.synchronizedSet(new HashSet<>());\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new Thread(() -> {\n        try {\n          started.countDown();\n          startLatch.await();\n          for (int d = 0;  d < 100; d++) {\n            String id = String.valueOf(random().nextInt(10));\n            int seqId = seqIds.incrementAndGet();\n            if (updateSeveralDocs) {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              doc.add(new IntPoint(\"seq_id\", seqId));\n              writer.softUpdateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc),\n                  new NumericDocValuesField(\"soft_delete\", 1));\n            } else {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              doc.add(new IntPoint(\"seq_id\", seqId));\n              writer.softUpdateDocument(new Term(\"id\", id), doc,\n                  new NumericDocValuesField(\"soft_delete\", 1));\n            }\n            if (rarely()) {\n              writer.flush();\n            }\n            ids.add(id);\n          }\n        } catch (IOException | InterruptedException e) {\n          throw new AssertionError(e);\n        }\n      });\n      threads[i].start();\n    }\n    started.await();\n    startLatch.countDown();\n\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].join();\n    }\n    DirectoryReader reader = DirectoryReader.open(writer);\n    IndexSearcher searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      TopDocs topDocs = searcher.search(new TermQuery(new Term(\"id\", id)), 10);\n      if (updateSeveralDocs) {\n        assertEquals(2, topDocs.totalHits);\n        assertEquals(Math.abs(topDocs.scoreDocs[0].doc - topDocs.scoreDocs[1].doc), 1);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n      }\n    }\n    writer.addDocument(new Document()); // add a dummy doc to trigger a segment here\n    writer.flush();\n    writer.forceMerge(1);\n    DirectoryReader oldReader = reader;\n    reader = DirectoryReader.openIfChanged(reader, writer);\n    if (reader != null) {\n      oldReader.close();\n      assertNotSame(oldReader, reader);\n    } else {\n      reader = oldReader;\n    }\n    assertEquals(1, reader.leaves().size());\n    LeafReaderContext leafReaderContext = reader.leaves().get(0);\n    LeafReader leafReader = leafReaderContext.reader();\n    searcher = new IndexSearcher(new FilterLeafReader(leafReader) {\n      @Override\n      public CacheHelper getCoreCacheHelper() {\n        return leafReader.getCoreCacheHelper();\n      }\n\n      @Override\n      public CacheHelper getReaderCacheHelper() {\n        return leafReader.getReaderCacheHelper();\n      }\n\n      @Override\n      public Bits getLiveDocs() {\n        return null;\n      }\n\n      @Override\n      public int numDocs() {\n        return maxDoc();\n      }\n    });\n    TopDocs seq_id = searcher.search(IntPoint.newRangeQuery(\"seq_id\", seqIds.intValue() - 50, Integer.MAX_VALUE), 10);\n    assertTrue(seq_id.totalHits + \" hits\", seq_id.totalHits >= 50);\n    searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      if (updateSeveralDocs) {\n        assertEquals(2, searcher.search(new TermQuery(new Term(\"id\", id)), 10).totalHits);\n      } else {\n        assertEquals(1, searcher.search(new TermQuery(new Term(\"id\", id)), 10).totalHits);\n      }\n    }\n    IOUtils.close(reader, writer, dir);\n  }\n\n","sourceOld":"  /**\n   * tests soft deletes that carry over deleted documents on merge for history rentention.\n   */\n  public void testSoftDeleteWithRetention() throws IOException, InterruptedException {\n    AtomicInteger seqIds = new AtomicInteger(0);\n    Directory dir = newDirectory();\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\n    indexWriterConfig.setMergePolicy(new SoftDeletesRetentionMergePolicy(\"soft_delete\",\n        () -> IntPoint.newRangeQuery(\"seq_id\", seqIds.intValue() - 50, Integer.MAX_VALUE),\n        indexWriterConfig.getMergePolicy()));\n    indexWriterConfig.setSoftDeletesField(\"soft_delete\");\n    IndexWriter writer = new IndexWriter(dir, indexWriterConfig);\n    Thread[] threads = new Thread[2 + random().nextInt(3)];\n    CountDownLatch startLatch = new CountDownLatch(1);\n    CountDownLatch started = new CountDownLatch(threads.length);\n    boolean updateSeveralDocs = random().nextBoolean();\n    Set<String> ids = Collections.synchronizedSet(new HashSet<>());\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new Thread(() -> {\n        try {\n          started.countDown();\n          startLatch.await();\n          for (int d = 0;  d < 100; d++) {\n            String id = String.valueOf(random().nextInt(10));\n            int seqId = seqIds.incrementAndGet();\n            if (updateSeveralDocs) {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              doc.add(new IntPoint(\"seq_id\", seqId));\n              writer.softUpdateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc),\n                  new NumericDocValuesField(\"soft_delete\", 1));\n            } else {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              doc.add(new IntPoint(\"seq_id\", seqId));\n              writer.softUpdateDocument(new Term(\"id\", id), doc,\n                  new NumericDocValuesField(\"soft_delete\", 1));\n            }\n            ids.add(id);\n          }\n        } catch (IOException | InterruptedException e) {\n          throw new AssertionError(e);\n        }\n      });\n      threads[i].start();\n    }\n    started.await();\n    startLatch.countDown();\n\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].join();\n    }\n    DirectoryReader reader = DirectoryReader.open(writer);\n    IndexSearcher searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      TopDocs topDocs = searcher.search(new TermQuery(new Term(\"id\", id)), 10);\n      if (updateSeveralDocs) {\n        assertEquals(2, topDocs.totalHits);\n        assertEquals(Math.abs(topDocs.scoreDocs[0].doc - topDocs.scoreDocs[1].doc), 1);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n      }\n    }\n    writer.addDocument(new Document()); // add a dummy doc to trigger a segment here\n    writer.flush();\n    writer.forceMerge(1);\n    DirectoryReader oldReader = reader;\n    reader = DirectoryReader.openIfChanged(reader, writer);\n    if (reader != null) {\n      oldReader.close();\n      assertNotSame(oldReader, reader);\n    } else {\n      reader = oldReader;\n    }\n    assertEquals(1, reader.leaves().size());\n    LeafReaderContext leafReaderContext = reader.leaves().get(0);\n    LeafReader leafReader = leafReaderContext.reader();\n    searcher = new IndexSearcher(new FilterLeafReader(leafReader) {\n      @Override\n      public CacheHelper getCoreCacheHelper() {\n        return leafReader.getCoreCacheHelper();\n      }\n\n      @Override\n      public CacheHelper getReaderCacheHelper() {\n        return leafReader.getReaderCacheHelper();\n      }\n\n      @Override\n      public Bits getLiveDocs() {\n        return null;\n      }\n\n      @Override\n      public int numDocs() {\n        return maxDoc();\n      }\n    });\n    TopDocs seq_id = searcher.search(IntPoint.newRangeQuery(\"seq_id\", seqIds.intValue() - 50, Integer.MAX_VALUE), 10);\n    assertTrue(seq_id.totalHits + \" hits\", seq_id.totalHits >= 50);\n    searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      if (updateSeveralDocs) {\n        assertEquals(2, searcher.search(new TermQuery(new Term(\"id\", id)), 10).totalHits);\n      } else {\n        assertEquals(1, searcher.search(new TermQuery(new Term(\"id\", id)), 10).totalHits);\n      }\n    }\n    IOUtils.close(reader, writer, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"83788ad129a5154d5c6562c4e8ce3db48793aada","date":1532961485,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSoftDeletesRetentionMergePolicy#testSoftDeleteWithRetention().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSoftDeletesRetentionMergePolicy#testSoftDeleteWithRetention().mjava","sourceNew":"  /**\n   * tests soft deletes that carry over deleted documents on merge for history rentention.\n   */\n  public void testSoftDeleteWithRetention() throws IOException, InterruptedException {\n    AtomicInteger seqIds = new AtomicInteger(0);\n    Directory dir = newDirectory();\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\n    indexWriterConfig.setMergePolicy(new SoftDeletesRetentionMergePolicy(\"soft_delete\",\n        () -> IntPoint.newRangeQuery(\"seq_id\", seqIds.intValue() - 50, Integer.MAX_VALUE),\n        indexWriterConfig.getMergePolicy()));\n    indexWriterConfig.setSoftDeletesField(\"soft_delete\");\n    IndexWriter writer = new IndexWriter(dir, indexWriterConfig);\n    Thread[] threads = new Thread[2 + random().nextInt(3)];\n    CountDownLatch startLatch = new CountDownLatch(1);\n    CountDownLatch started = new CountDownLatch(threads.length);\n    boolean updateSeveralDocs = random().nextBoolean();\n    Set<String> ids = Collections.synchronizedSet(new HashSet<>());\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new Thread(() -> {\n        try {\n          started.countDown();\n          startLatch.await();\n          for (int d = 0;  d < 100; d++) {\n            String id = String.valueOf(random().nextInt(10));\n            int seqId = seqIds.incrementAndGet();\n            if (updateSeveralDocs) {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              doc.add(new IntPoint(\"seq_id\", seqId));\n              writer.softUpdateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc),\n                  new NumericDocValuesField(\"soft_delete\", 1));\n            } else {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              doc.add(new IntPoint(\"seq_id\", seqId));\n              writer.softUpdateDocument(new Term(\"id\", id), doc,\n                  new NumericDocValuesField(\"soft_delete\", 1));\n            }\n            if (rarely()) {\n              writer.flush();\n            }\n            ids.add(id);\n          }\n        } catch (IOException | InterruptedException e) {\n          throw new AssertionError(e);\n        }\n      });\n      threads[i].start();\n    }\n    started.await();\n    startLatch.countDown();\n\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].join();\n    }\n    DirectoryReader reader = DirectoryReader.open(writer);\n    IndexSearcher searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      TopDocs topDocs = searcher.search(new TermQuery(new Term(\"id\", id)), 10);\n      if (updateSeveralDocs) {\n        assertEquals(2, topDocs.totalHits.value);\n        assertEquals(Math.abs(topDocs.scoreDocs[0].doc - topDocs.scoreDocs[1].doc), 1);\n      } else {\n        assertEquals(1, topDocs.totalHits.value);\n      }\n    }\n    writer.addDocument(new Document()); // add a dummy doc to trigger a segment here\n    writer.flush();\n    writer.forceMerge(1);\n    DirectoryReader oldReader = reader;\n    reader = DirectoryReader.openIfChanged(reader, writer);\n    if (reader != null) {\n      oldReader.close();\n      assertNotSame(oldReader, reader);\n    } else {\n      reader = oldReader;\n    }\n    assertEquals(1, reader.leaves().size());\n    LeafReaderContext leafReaderContext = reader.leaves().get(0);\n    LeafReader leafReader = leafReaderContext.reader();\n    searcher = new IndexSearcher(new FilterLeafReader(leafReader) {\n      @Override\n      public CacheHelper getCoreCacheHelper() {\n        return leafReader.getCoreCacheHelper();\n      }\n\n      @Override\n      public CacheHelper getReaderCacheHelper() {\n        return leafReader.getReaderCacheHelper();\n      }\n\n      @Override\n      public Bits getLiveDocs() {\n        return null;\n      }\n\n      @Override\n      public int numDocs() {\n        return maxDoc();\n      }\n    });\n    TopDocs seq_id = searcher.search(IntPoint.newRangeQuery(\"seq_id\", seqIds.intValue() - 50, Integer.MAX_VALUE), 10);\n    assertTrue(seq_id.totalHits.value + \" hits\", seq_id.totalHits.value >= 50);\n    searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      if (updateSeveralDocs) {\n        assertEquals(2, searcher.search(new TermQuery(new Term(\"id\", id)), 10).totalHits.value);\n      } else {\n        assertEquals(1, searcher.search(new TermQuery(new Term(\"id\", id)), 10).totalHits.value);\n      }\n    }\n    IOUtils.close(reader, writer, dir);\n  }\n\n","sourceOld":"  /**\n   * tests soft deletes that carry over deleted documents on merge for history rentention.\n   */\n  public void testSoftDeleteWithRetention() throws IOException, InterruptedException {\n    AtomicInteger seqIds = new AtomicInteger(0);\n    Directory dir = newDirectory();\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\n    indexWriterConfig.setMergePolicy(new SoftDeletesRetentionMergePolicy(\"soft_delete\",\n        () -> IntPoint.newRangeQuery(\"seq_id\", seqIds.intValue() - 50, Integer.MAX_VALUE),\n        indexWriterConfig.getMergePolicy()));\n    indexWriterConfig.setSoftDeletesField(\"soft_delete\");\n    IndexWriter writer = new IndexWriter(dir, indexWriterConfig);\n    Thread[] threads = new Thread[2 + random().nextInt(3)];\n    CountDownLatch startLatch = new CountDownLatch(1);\n    CountDownLatch started = new CountDownLatch(threads.length);\n    boolean updateSeveralDocs = random().nextBoolean();\n    Set<String> ids = Collections.synchronizedSet(new HashSet<>());\n    for (int i = 0; i < threads.length; i++) {\n      threads[i] = new Thread(() -> {\n        try {\n          started.countDown();\n          startLatch.await();\n          for (int d = 0;  d < 100; d++) {\n            String id = String.valueOf(random().nextInt(10));\n            int seqId = seqIds.incrementAndGet();\n            if (updateSeveralDocs) {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              doc.add(new IntPoint(\"seq_id\", seqId));\n              writer.softUpdateDocuments(new Term(\"id\", id), Arrays.asList(doc, doc),\n                  new NumericDocValuesField(\"soft_delete\", 1));\n            } else {\n              Document doc = new Document();\n              doc.add(new StringField(\"id\", id, Field.Store.YES));\n              doc.add(new IntPoint(\"seq_id\", seqId));\n              writer.softUpdateDocument(new Term(\"id\", id), doc,\n                  new NumericDocValuesField(\"soft_delete\", 1));\n            }\n            if (rarely()) {\n              writer.flush();\n            }\n            ids.add(id);\n          }\n        } catch (IOException | InterruptedException e) {\n          throw new AssertionError(e);\n        }\n      });\n      threads[i].start();\n    }\n    started.await();\n    startLatch.countDown();\n\n    for (int i = 0; i < threads.length; i++) {\n      threads[i].join();\n    }\n    DirectoryReader reader = DirectoryReader.open(writer);\n    IndexSearcher searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      TopDocs topDocs = searcher.search(new TermQuery(new Term(\"id\", id)), 10);\n      if (updateSeveralDocs) {\n        assertEquals(2, topDocs.totalHits);\n        assertEquals(Math.abs(topDocs.scoreDocs[0].doc - topDocs.scoreDocs[1].doc), 1);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n      }\n    }\n    writer.addDocument(new Document()); // add a dummy doc to trigger a segment here\n    writer.flush();\n    writer.forceMerge(1);\n    DirectoryReader oldReader = reader;\n    reader = DirectoryReader.openIfChanged(reader, writer);\n    if (reader != null) {\n      oldReader.close();\n      assertNotSame(oldReader, reader);\n    } else {\n      reader = oldReader;\n    }\n    assertEquals(1, reader.leaves().size());\n    LeafReaderContext leafReaderContext = reader.leaves().get(0);\n    LeafReader leafReader = leafReaderContext.reader();\n    searcher = new IndexSearcher(new FilterLeafReader(leafReader) {\n      @Override\n      public CacheHelper getCoreCacheHelper() {\n        return leafReader.getCoreCacheHelper();\n      }\n\n      @Override\n      public CacheHelper getReaderCacheHelper() {\n        return leafReader.getReaderCacheHelper();\n      }\n\n      @Override\n      public Bits getLiveDocs() {\n        return null;\n      }\n\n      @Override\n      public int numDocs() {\n        return maxDoc();\n      }\n    });\n    TopDocs seq_id = searcher.search(IntPoint.newRangeQuery(\"seq_id\", seqIds.intValue() - 50, Integer.MAX_VALUE), 10);\n    assertTrue(seq_id.totalHits + \" hits\", seq_id.totalHits >= 50);\n    searcher = new IndexSearcher(reader);\n    for (String id : ids) {\n      if (updateSeveralDocs) {\n        assertEquals(2, searcher.search(new TermQuery(new Term(\"id\", id)), 10).totalHits);\n      } else {\n        assertEquals(1, searcher.search(new TermQuery(new Term(\"id\", id)), 10).totalHits);\n      }\n    }\n    IOUtils.close(reader, writer, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"f372764a5bd3ebacde5b99ee3303153eb5ec0d2f":["5ee0394b8176abd7c90a4be8c05465be1879db79"],"5ee0394b8176abd7c90a4be8c05465be1879db79":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"83788ad129a5154d5c6562c4e8ce3db48793aada":["f372764a5bd3ebacde5b99ee3303153eb5ec0d2f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["83788ad129a5154d5c6562c4e8ce3db48793aada"]},"commit2Childs":{"f372764a5bd3ebacde5b99ee3303153eb5ec0d2f":["83788ad129a5154d5c6562c4e8ce3db48793aada"],"5ee0394b8176abd7c90a4be8c05465be1879db79":["f372764a5bd3ebacde5b99ee3303153eb5ec0d2f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["5ee0394b8176abd7c90a4be8c05465be1879db79"],"83788ad129a5154d5c6562c4e8ce3db48793aada":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}