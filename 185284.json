{"path":"lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SolrSynonymParser#addInternal(BufferedReader).mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SolrSynonymParser#addInternal(BufferedReader).mjava","pathOld":"modules/analysis/common/src/java/org/apache/lucene/analysis/synonym/SolrSynonymParser#addInternal(BufferedReader).mjava","sourceNew":"  private void addInternal(BufferedReader in) throws IOException {\n    String line = null;\n    while ((line = in.readLine()) != null) {\n      if (line.length() == 0 || line.charAt(0) == '#') {\n        continue; // ignore empty lines and comments\n      }\n      \n      CharsRef inputs[];\n      CharsRef outputs[];\n      \n      // TODO: we could process this more efficiently.\n      String sides[] = split(line, \"=>\");\n      if (sides.length > 1) { // explicit mapping\n        if (sides.length != 2) {\n          throw new IllegalArgumentException(\"more than one explicit mapping specified on the same line\");\n        }\n        String inputStrings[] = split(sides[0], \",\");\n        inputs = new CharsRef[inputStrings.length];\n        for (int i = 0; i < inputs.length; i++) {\n          inputs[i] = analyze(analyzer, unescape(inputStrings[i]).trim(), new CharsRef());\n        }\n        \n        String outputStrings[] = split(sides[1], \",\");\n        outputs = new CharsRef[outputStrings.length];\n        for (int i = 0; i < outputs.length; i++) {\n          outputs[i] = analyze(analyzer, unescape(outputStrings[i]).trim(), new CharsRef());\n        }\n      } else {\n        String inputStrings[] = split(line, \",\");\n        inputs = new CharsRef[inputStrings.length];\n        for (int i = 0; i < inputs.length; i++) {\n          inputs[i] = analyze(analyzer, unescape(inputStrings[i]).trim(), new CharsRef());\n        }\n        if (expand) {\n          outputs = inputs;\n        } else {\n          outputs = new CharsRef[1];\n          outputs[0] = inputs[0];\n        }\n      }\n      \n      // currently we include the term itself in the map,\n      // and use includeOrig = false always.\n      // this is how the existing filter does it, but its actually a bug,\n      // especially if combined with ignoreCase = true\n      for (int i = 0; i < inputs.length; i++) {\n        for (int j = 0; j < outputs.length; j++) {\n          add(inputs[i], outputs[j], false);\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void addInternal(BufferedReader in) throws IOException {\n    String line = null;\n    while ((line = in.readLine()) != null) {\n      if (line.length() == 0 || line.charAt(0) == '#') {\n        continue; // ignore empty lines and comments\n      }\n      \n      CharsRef inputs[];\n      CharsRef outputs[];\n      \n      // TODO: we could process this more efficiently.\n      String sides[] = split(line, \"=>\");\n      if (sides.length > 1) { // explicit mapping\n        if (sides.length != 2) {\n          throw new IllegalArgumentException(\"more than one explicit mapping specified on the same line\");\n        }\n        String inputStrings[] = split(sides[0], \",\");\n        inputs = new CharsRef[inputStrings.length];\n        for (int i = 0; i < inputs.length; i++) {\n          inputs[i] = analyze(analyzer, unescape(inputStrings[i]).trim(), new CharsRef());\n        }\n        \n        String outputStrings[] = split(sides[1], \",\");\n        outputs = new CharsRef[outputStrings.length];\n        for (int i = 0; i < outputs.length; i++) {\n          outputs[i] = analyze(analyzer, unescape(outputStrings[i]).trim(), new CharsRef());\n        }\n      } else {\n        String inputStrings[] = split(line, \",\");\n        inputs = new CharsRef[inputStrings.length];\n        for (int i = 0; i < inputs.length; i++) {\n          inputs[i] = analyze(analyzer, unescape(inputStrings[i]).trim(), new CharsRef());\n        }\n        if (expand) {\n          outputs = inputs;\n        } else {\n          outputs = new CharsRef[1];\n          outputs[0] = inputs[0];\n        }\n      }\n      \n      // currently we include the term itself in the map,\n      // and use includeOrig = false always.\n      // this is how the existing filter does it, but its actually a bug,\n      // especially if combined with ignoreCase = true\n      for (int i = 0; i < inputs.length; i++) {\n        for (int j = 0; j < outputs.length; j++) {\n          add(inputs[i], outputs[j], false);\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"85f3a2d749715373feb8529516e92d3538103525","date":1379624134,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SolrSynonymParser#addInternal(BufferedReader).mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SolrSynonymParser#addInternal(BufferedReader).mjava","sourceNew":"  private void addInternal(BufferedReader in) throws IOException {\n    String line = null;\n    while ((line = in.readLine()) != null) {\n      if (line.length() == 0 || line.charAt(0) == '#') {\n        continue; // ignore empty lines and comments\n      }\n      \n      CharsRef inputs[];\n      CharsRef outputs[];\n      \n      // TODO: we could process this more efficiently.\n      String sides[] = split(line, \"=>\");\n      if (sides.length > 1) { // explicit mapping\n        if (sides.length != 2) {\n          throw new IllegalArgumentException(\"more than one explicit mapping specified on the same line\");\n        }\n        String inputStrings[] = split(sides[0], \",\");\n        inputs = new CharsRef[inputStrings.length];\n        for (int i = 0; i < inputs.length; i++) {\n          inputs[i] = analyze(unescape(inputStrings[i]).trim(), new CharsRef());\n        }\n        \n        String outputStrings[] = split(sides[1], \",\");\n        outputs = new CharsRef[outputStrings.length];\n        for (int i = 0; i < outputs.length; i++) {\n          outputs[i] = analyze(unescape(outputStrings[i]).trim(), new CharsRef());\n        }\n      } else {\n        String inputStrings[] = split(line, \",\");\n        inputs = new CharsRef[inputStrings.length];\n        for (int i = 0; i < inputs.length; i++) {\n          inputs[i] = analyze(unescape(inputStrings[i]).trim(), new CharsRef());\n        }\n        if (expand) {\n          outputs = inputs;\n        } else {\n          outputs = new CharsRef[1];\n          outputs[0] = inputs[0];\n        }\n      }\n      \n      // currently we include the term itself in the map,\n      // and use includeOrig = false always.\n      // this is how the existing filter does it, but its actually a bug,\n      // especially if combined with ignoreCase = true\n      for (int i = 0; i < inputs.length; i++) {\n        for (int j = 0; j < outputs.length; j++) {\n          add(inputs[i], outputs[j], false);\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void addInternal(BufferedReader in) throws IOException {\n    String line = null;\n    while ((line = in.readLine()) != null) {\n      if (line.length() == 0 || line.charAt(0) == '#') {\n        continue; // ignore empty lines and comments\n      }\n      \n      CharsRef inputs[];\n      CharsRef outputs[];\n      \n      // TODO: we could process this more efficiently.\n      String sides[] = split(line, \"=>\");\n      if (sides.length > 1) { // explicit mapping\n        if (sides.length != 2) {\n          throw new IllegalArgumentException(\"more than one explicit mapping specified on the same line\");\n        }\n        String inputStrings[] = split(sides[0], \",\");\n        inputs = new CharsRef[inputStrings.length];\n        for (int i = 0; i < inputs.length; i++) {\n          inputs[i] = analyze(analyzer, unescape(inputStrings[i]).trim(), new CharsRef());\n        }\n        \n        String outputStrings[] = split(sides[1], \",\");\n        outputs = new CharsRef[outputStrings.length];\n        for (int i = 0; i < outputs.length; i++) {\n          outputs[i] = analyze(analyzer, unescape(outputStrings[i]).trim(), new CharsRef());\n        }\n      } else {\n        String inputStrings[] = split(line, \",\");\n        inputs = new CharsRef[inputStrings.length];\n        for (int i = 0; i < inputs.length; i++) {\n          inputs[i] = analyze(analyzer, unescape(inputStrings[i]).trim(), new CharsRef());\n        }\n        if (expand) {\n          outputs = inputs;\n        } else {\n          outputs = new CharsRef[1];\n          outputs[0] = inputs[0];\n        }\n      }\n      \n      // currently we include the term itself in the map,\n      // and use includeOrig = false always.\n      // this is how the existing filter does it, but its actually a bug,\n      // especially if combined with ignoreCase = true\n      for (int i = 0; i < inputs.length; i++) {\n        for (int j = 0; j < outputs.length; j++) {\n          add(inputs[i], outputs[j], false);\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50","date":1407854805,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SolrSynonymParser#addInternal(BufferedReader).mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SolrSynonymParser#addInternal(BufferedReader).mjava","sourceNew":"  private void addInternal(BufferedReader in) throws IOException {\n    String line = null;\n    while ((line = in.readLine()) != null) {\n      if (line.length() == 0 || line.charAt(0) == '#') {\n        continue; // ignore empty lines and comments\n      }\n      \n      CharsRef inputs[];\n      CharsRef outputs[];\n      \n      // TODO: we could process this more efficiently.\n      String sides[] = split(line, \"=>\");\n      if (sides.length > 1) { // explicit mapping\n        if (sides.length != 2) {\n          throw new IllegalArgumentException(\"more than one explicit mapping specified on the same line\");\n        }\n        String inputStrings[] = split(sides[0], \",\");\n        inputs = new CharsRef[inputStrings.length];\n        for (int i = 0; i < inputs.length; i++) {\n          inputs[i] = analyze(unescape(inputStrings[i]).trim(), new CharsRefBuilder());\n        }\n        \n        String outputStrings[] = split(sides[1], \",\");\n        outputs = new CharsRef[outputStrings.length];\n        for (int i = 0; i < outputs.length; i++) {\n          outputs[i] = analyze(unescape(outputStrings[i]).trim(), new CharsRefBuilder());\n        }\n      } else {\n        String inputStrings[] = split(line, \",\");\n        inputs = new CharsRef[inputStrings.length];\n        for (int i = 0; i < inputs.length; i++) {\n          inputs[i] = analyze(unescape(inputStrings[i]).trim(), new CharsRefBuilder());\n        }\n        if (expand) {\n          outputs = inputs;\n        } else {\n          outputs = new CharsRef[1];\n          outputs[0] = inputs[0];\n        }\n      }\n      \n      // currently we include the term itself in the map,\n      // and use includeOrig = false always.\n      // this is how the existing filter does it, but its actually a bug,\n      // especially if combined with ignoreCase = true\n      for (int i = 0; i < inputs.length; i++) {\n        for (int j = 0; j < outputs.length; j++) {\n          add(inputs[i], outputs[j], false);\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void addInternal(BufferedReader in) throws IOException {\n    String line = null;\n    while ((line = in.readLine()) != null) {\n      if (line.length() == 0 || line.charAt(0) == '#') {\n        continue; // ignore empty lines and comments\n      }\n      \n      CharsRef inputs[];\n      CharsRef outputs[];\n      \n      // TODO: we could process this more efficiently.\n      String sides[] = split(line, \"=>\");\n      if (sides.length > 1) { // explicit mapping\n        if (sides.length != 2) {\n          throw new IllegalArgumentException(\"more than one explicit mapping specified on the same line\");\n        }\n        String inputStrings[] = split(sides[0], \",\");\n        inputs = new CharsRef[inputStrings.length];\n        for (int i = 0; i < inputs.length; i++) {\n          inputs[i] = analyze(unescape(inputStrings[i]).trim(), new CharsRef());\n        }\n        \n        String outputStrings[] = split(sides[1], \",\");\n        outputs = new CharsRef[outputStrings.length];\n        for (int i = 0; i < outputs.length; i++) {\n          outputs[i] = analyze(unescape(outputStrings[i]).trim(), new CharsRef());\n        }\n      } else {\n        String inputStrings[] = split(line, \",\");\n        inputs = new CharsRef[inputStrings.length];\n        for (int i = 0; i < inputs.length; i++) {\n          inputs[i] = analyze(unescape(inputStrings[i]).trim(), new CharsRef());\n        }\n        if (expand) {\n          outputs = inputs;\n        } else {\n          outputs = new CharsRef[1];\n          outputs[0] = inputs[0];\n        }\n      }\n      \n      // currently we include the term itself in the map,\n      // and use includeOrig = false always.\n      // this is how the existing filter does it, but its actually a bug,\n      // especially if combined with ignoreCase = true\n      for (int i = 0; i < inputs.length; i++) {\n        for (int j = 0; j < outputs.length; j++) {\n          add(inputs[i], outputs[j], false);\n        }\n      }\n    }\n  }\n\n","bugFix":["85f3a2d749715373feb8529516e92d3538103525"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1","date":1419400138,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SolrSynonymParser#addInternal(BufferedReader).mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SolrSynonymParser#addInternal(BufferedReader).mjava","sourceNew":"  private void addInternal(BufferedReader in) throws IOException {\n    String line = null;\n    while ((line = in.readLine()) != null) {\n      if (line.length() == 0 || line.charAt(0) == '#') {\n        continue; // ignore empty lines and comments\n      }\n      \n      CharsRef inputs[];\n      CharsRef outputs[];\n      \n      // TODO: we could process this more efficiently.\n      String sides[] = split(line, \"=>\");\n      if (sides.length > 1) { // explicit mapping\n        if (sides.length != 2) {\n          throw new IllegalArgumentException(\"more than one explicit mapping specified on the same line\");\n        }\n        String inputStrings[] = split(sides[0], \",\");\n        inputs = new CharsRef[inputStrings.length];\n        for (int i = 0; i < inputs.length; i++) {\n          inputs[i] = analyze(unescape(inputStrings[i]).trim(), new CharsRefBuilder());\n        }\n        \n        String outputStrings[] = split(sides[1], \",\");\n        outputs = new CharsRef[outputStrings.length];\n        for (int i = 0; i < outputs.length; i++) {\n          outputs[i] = analyze(unescape(outputStrings[i]).trim(), new CharsRefBuilder());\n        }\n      } else {\n        String inputStrings[] = split(line, \",\");\n        inputs = new CharsRef[inputStrings.length];\n        for (int i = 0; i < inputs.length; i++) {\n          inputs[i] = analyze(unescape(inputStrings[i]).trim(), new CharsRefBuilder());\n        }\n        if (expand) {\n          outputs = inputs;\n        } else {\n          outputs = new CharsRef[1];\n          outputs[0] = inputs[0];\n        }\n      }\n      \n      // currently we include the term itself in the map,\n      // and use includeOrig = false always.\n      // this is how the existing filter does it, but it's actually a bug,\n      // especially if combined with ignoreCase = true\n      for (int i = 0; i < inputs.length; i++) {\n        for (int j = 0; j < outputs.length; j++) {\n          add(inputs[i], outputs[j], false);\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void addInternal(BufferedReader in) throws IOException {\n    String line = null;\n    while ((line = in.readLine()) != null) {\n      if (line.length() == 0 || line.charAt(0) == '#') {\n        continue; // ignore empty lines and comments\n      }\n      \n      CharsRef inputs[];\n      CharsRef outputs[];\n      \n      // TODO: we could process this more efficiently.\n      String sides[] = split(line, \"=>\");\n      if (sides.length > 1) { // explicit mapping\n        if (sides.length != 2) {\n          throw new IllegalArgumentException(\"more than one explicit mapping specified on the same line\");\n        }\n        String inputStrings[] = split(sides[0], \",\");\n        inputs = new CharsRef[inputStrings.length];\n        for (int i = 0; i < inputs.length; i++) {\n          inputs[i] = analyze(unescape(inputStrings[i]).trim(), new CharsRefBuilder());\n        }\n        \n        String outputStrings[] = split(sides[1], \",\");\n        outputs = new CharsRef[outputStrings.length];\n        for (int i = 0; i < outputs.length; i++) {\n          outputs[i] = analyze(unescape(outputStrings[i]).trim(), new CharsRefBuilder());\n        }\n      } else {\n        String inputStrings[] = split(line, \",\");\n        inputs = new CharsRef[inputStrings.length];\n        for (int i = 0; i < inputs.length; i++) {\n          inputs[i] = analyze(unescape(inputStrings[i]).trim(), new CharsRefBuilder());\n        }\n        if (expand) {\n          outputs = inputs;\n        } else {\n          outputs = new CharsRef[1];\n          outputs[0] = inputs[0];\n        }\n      }\n      \n      // currently we include the term itself in the map,\n      // and use includeOrig = false always.\n      // this is how the existing filter does it, but its actually a bug,\n      // especially if combined with ignoreCase = true\n      for (int i = 0; i < inputs.length; i++) {\n        for (int j = 0; j < outputs.length; j++) {\n          add(inputs[i], outputs[j], false);\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["927d09add12b0fa3c10f6f9ae564d85bef5dc12c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"927d09add12b0fa3c10f6f9ae564d85bef5dc12c","date":1429219595,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SolrSynonymParser#addInternal(BufferedReader).mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SolrSynonymParser#addInternal(BufferedReader).mjava","sourceNew":"  private void addInternal(BufferedReader in) throws IOException {\n    String line = null;\n    while ((line = in.readLine()) != null) {\n      if (line.length() == 0 || line.charAt(0) == '#') {\n        continue; // ignore empty lines and comments\n      }\n      \n      // TODO: we could process this more efficiently.\n      String sides[] = split(line, \"=>\");\n      if (sides.length > 1) { // explicit mapping\n        if (sides.length != 2) {\n          throw new IllegalArgumentException(\"more than one explicit mapping specified on the same line\");\n        }\n        String inputStrings[] = split(sides[0], \",\");\n        CharsRef[] inputs = new CharsRef[inputStrings.length];\n        for (int i = 0; i < inputs.length; i++) {\n          inputs[i] = analyze(unescape(inputStrings[i]).trim(), new CharsRefBuilder());\n        }\n        \n        String outputStrings[] = split(sides[1], \",\");\n        CharsRef[] outputs = new CharsRef[outputStrings.length];\n        for (int i = 0; i < outputs.length; i++) {\n          outputs[i] = analyze(unescape(outputStrings[i]).trim(), new CharsRefBuilder());\n        }\n        // these mappings are explicit and never preserve original\n        for (int i = 0; i < inputs.length; i++) {\n          for (int j = 0; j < outputs.length; j++) {\n            add(inputs[i], outputs[j], false);\n          }\n        }\n      } else {\n        String inputStrings[] = split(line, \",\");\n        CharsRef[] inputs = new CharsRef[inputStrings.length];\n        for (int i = 0; i < inputs.length; i++) {\n          inputs[i] = analyze(unescape(inputStrings[i]).trim(), new CharsRefBuilder());\n        }\n        if (expand) {\n          // all pairs\n          for (int i = 0; i < inputs.length; i++) {\n            for (int j = 0; j < inputs.length; j++) {\n              if (i != j) {\n                add(inputs[i], inputs[j], true);\n              }\n            }\n          }\n        } else {\n          // all subsequent inputs map to first one; we also add inputs[0] here\n          // so that we \"effectively\" (because we remove the original input and\n          // add back a synonym with the same text) change that token's type to\n          // SYNONYM (matching legacy behavior):\n          for (int i = 0; i < inputs.length; i++) {\n            add(inputs[i], inputs[0], false);\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void addInternal(BufferedReader in) throws IOException {\n    String line = null;\n    while ((line = in.readLine()) != null) {\n      if (line.length() == 0 || line.charAt(0) == '#') {\n        continue; // ignore empty lines and comments\n      }\n      \n      CharsRef inputs[];\n      CharsRef outputs[];\n      \n      // TODO: we could process this more efficiently.\n      String sides[] = split(line, \"=>\");\n      if (sides.length > 1) { // explicit mapping\n        if (sides.length != 2) {\n          throw new IllegalArgumentException(\"more than one explicit mapping specified on the same line\");\n        }\n        String inputStrings[] = split(sides[0], \",\");\n        inputs = new CharsRef[inputStrings.length];\n        for (int i = 0; i < inputs.length; i++) {\n          inputs[i] = analyze(unescape(inputStrings[i]).trim(), new CharsRefBuilder());\n        }\n        \n        String outputStrings[] = split(sides[1], \",\");\n        outputs = new CharsRef[outputStrings.length];\n        for (int i = 0; i < outputs.length; i++) {\n          outputs[i] = analyze(unescape(outputStrings[i]).trim(), new CharsRefBuilder());\n        }\n      } else {\n        String inputStrings[] = split(line, \",\");\n        inputs = new CharsRef[inputStrings.length];\n        for (int i = 0; i < inputs.length; i++) {\n          inputs[i] = analyze(unescape(inputStrings[i]).trim(), new CharsRefBuilder());\n        }\n        if (expand) {\n          outputs = inputs;\n        } else {\n          outputs = new CharsRef[1];\n          outputs[0] = inputs[0];\n        }\n      }\n      \n      // currently we include the term itself in the map,\n      // and use includeOrig = false always.\n      // this is how the existing filter does it, but it's actually a bug,\n      // especially if combined with ignoreCase = true\n      for (int i = 0; i < inputs.length; i++) {\n        for (int j = 0; j < outputs.length; j++) {\n          add(inputs[i], outputs[j], false);\n        }\n      }\n    }\n  }\n\n","bugFix":["44d6f0ab53c1962856b9f48dedb7a2a6cc18905c","8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"927d09add12b0fa3c10f6f9ae564d85bef5dc12c":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1"],"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50":["85f3a2d749715373feb8529516e92d3538103525"],"85f3a2d749715373feb8529516e92d3538103525":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["927d09add12b0fa3c10f6f9ae564d85bef5dc12c"]},"commit2Childs":{"927d09add12b0fa3c10f6f9ae564d85bef5dc12c":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1":["927d09add12b0fa3c10f6f9ae564d85bef5dc12c"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["85f3a2d749715373feb8529516e92d3538103525"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1"],"85f3a2d749715373feb8529516e92d3538103525":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}