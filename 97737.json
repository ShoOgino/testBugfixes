{"path":"src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","commits":[{"id":"8522ae207a56c6db28ca06fe6cc33e70911c3600","date":1173935743,"type":0,"author":"Michael Busch","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"/dev/null","sourceNew":"    // builds an index with payloads in the given Directory and performs\r\n    // different tests to verify the payload encoding\r\n    private void performTest(Directory dir) throws Exception {\r\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\r\n        IndexWriter writer = new IndexWriter(dir, analyzer, true);\r\n        \r\n        // should be in sync with value in TermInfosWriter\r\n        final int skipInterval = 16;\r\n        \r\n        final int numTerms = 5;\r\n        final String fieldName = \"f1\";\r\n        \r\n        int numDocs = skipInterval + 1; \r\n        // create content for the test documents with just a few terms\r\n        Term[] terms = generateTerms(fieldName, numTerms);\r\n        StringBuffer sb = new StringBuffer();\r\n        for (int i = 0; i < terms.length; i++) {\r\n            sb.append(terms[i].text);\r\n            sb.append(\" \");\r\n        }\r\n        String content = sb.toString();\r\n        \r\n        \r\n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\r\n        byte[] payloadData = generateRandomData(payloadDataLength);\r\n        \r\n        Document d = new Document();\r\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.TOKENIZED));\r\n        // add the same document multiple times to have the same payload lengths for all\r\n        // occurrences within two consecutive skip intervals\r\n        int offset = 0;\r\n        for (int i = 0; i < 2 * numDocs; i++) {\r\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\r\n            offset += numTerms;\r\n            writer.addDocument(d);\r\n        }\r\n        \r\n        // now we make sure to have different payload lengths next at the next skip point        \r\n        for (int i = 0; i < numDocs; i++) {\r\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\r\n            offset += i * numTerms;\r\n            writer.addDocument(d);\r\n        }\r\n        \r\n        writer.optimize();\r\n        // flush\r\n        writer.close();\r\n        \r\n        \r\n        /*\r\n         * Verify the index\r\n         * first we test if all payloads are stored correctly\r\n         */        \r\n        IndexReader reader = IndexReader.open(dir);\r\n        \r\n        byte[] verifyPayloadData = new byte[payloadDataLength];\r\n        offset = 0;\r\n        TermPositions[] tps = new TermPositions[numTerms];\r\n        for (int i = 0; i < numTerms; i++) {\r\n            tps[i] = reader.termPositions(terms[i]);\r\n        }\r\n        \r\n        while (tps[0].next()) {\r\n            for (int i = 1; i < numTerms; i++) {\r\n                tps[i].next();\r\n            }\r\n            int freq = tps[0].freq();\r\n\r\n            for (int i = 0; i < freq; i++) {\r\n                for (int j = 0; j < numTerms; j++) {\r\n                    tps[j].nextPosition();\r\n                    tps[j].getPayload(verifyPayloadData, offset);\r\n                    offset += tps[j].getPayloadLength();\r\n                }\r\n            }\r\n        }\r\n        \r\n        for (int i = 0; i < numTerms; i++) {\r\n            tps[i].close();\r\n        }\r\n        \r\n        assertByteArrayEquals(payloadData, verifyPayloadData);\r\n        \r\n        /*\r\n         *  test lazy skipping\r\n         */        \r\n        TermPositions tp = reader.termPositions(terms[0]);\r\n        tp.next();\r\n        tp.nextPosition();\r\n        // now we don't read this payload\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        byte[] payload = tp.getPayload(null, 0);\r\n        assertEquals(payload[0], payloadData[numTerms]);\r\n        tp.nextPosition();\r\n        \r\n        // we don't read this payload and skip to a different document\r\n        tp.skipTo(5);\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        payload = tp.getPayload(null, 0);\r\n        assertEquals(payload[0], payloadData[5 * numTerms]);\r\n                \r\n        \r\n        /*\r\n         * Test different lengths at skip points\r\n         */\r\n        tp.seek(terms[1]);\r\n        tp.next();\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        tp.skipTo(skipInterval - 1);\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        tp.skipTo(2 * skipInterval - 1);\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        tp.skipTo(3 * skipInterval - 1);\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\r\n        \r\n        /*\r\n         * Test multiple call of getPayload()\r\n         */\r\n        tp.getPayload(null, 0);\r\n        try {\r\n            // it is forbidden to call getPayload() more than once\r\n            // without calling nextPosition()\r\n            tp.getPayload(null, 0);\r\n            fail(\"Expected exception not thrown\");\r\n        } catch (Exception expected) {\r\n            // expected exception\r\n        }\r\n        \r\n        reader.close();\r\n        \r\n        // test long payload\r\n        analyzer = new PayloadAnalyzer();\r\n        writer = new IndexWriter(dir, analyzer, true);\r\n        String singleTerm = \"lucene\";\r\n        \r\n        d = new Document();\r\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.TOKENIZED));\r\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\r\n        payloadData = generateRandomData(2000);\r\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\r\n        writer.addDocument(d);\r\n\r\n        \r\n        writer.optimize();\r\n        // flush\r\n        writer.close();\r\n        \r\n        reader = IndexReader.open(dir);\r\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\r\n        tp.next();\r\n        tp.nextPosition();\r\n\r\n        verifyPayloadData = new byte[tp.getPayloadLength()];\r\n        tp.getPayload(verifyPayloadData, 0);\r\n        byte[] portion = new byte[1500];\r\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\r\n        \r\n        assertByteArrayEquals(portion, verifyPayloadData);\r\n        reader.close();\r\n        \r\n    }\r\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f271ff507bf58806fc089e76e38057c95e391dcd","date":1196311383,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\r\n    // different tests to verify the payload encoding\r\n    private void performTest(Directory dir) throws Exception {\r\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\r\n        IndexWriter writer = new IndexWriter(dir, analyzer, true);\r\n        \r\n        // should be in sync with value in TermInfosWriter\r\n        final int skipInterval = 16;\r\n        \r\n        final int numTerms = 5;\r\n        final String fieldName = \"f1\";\r\n        \r\n        int numDocs = skipInterval + 1; \r\n        // create content for the test documents with just a few terms\r\n        Term[] terms = generateTerms(fieldName, numTerms);\r\n        StringBuffer sb = new StringBuffer();\r\n        for (int i = 0; i < terms.length; i++) {\r\n            sb.append(terms[i].text);\r\n            sb.append(\" \");\r\n        }\r\n        String content = sb.toString();\r\n        \r\n        \r\n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\r\n        byte[] payloadData = generateRandomData(payloadDataLength);\r\n        \r\n        Document d = new Document();\r\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.TOKENIZED));\r\n        // add the same document multiple times to have the same payload lengths for all\r\n        // occurrences within two consecutive skip intervals\r\n        int offset = 0;\r\n        for (int i = 0; i < 2 * numDocs; i++) {\r\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\r\n            offset += numTerms;\r\n            writer.addDocument(d);\r\n        }\r\n        \r\n        // make sure we create more than one segment to test merging\r\n        writer.flush();\r\n        \r\n        // now we make sure to have different payload lengths next at the next skip point        \r\n        for (int i = 0; i < numDocs; i++) {\r\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\r\n            offset += i * numTerms;\r\n            writer.addDocument(d);\r\n        }\r\n        \r\n        writer.optimize();\r\n        // flush\r\n        writer.close();\r\n        \r\n        \r\n        /*\r\n         * Verify the index\r\n         * first we test if all payloads are stored correctly\r\n         */        \r\n        IndexReader reader = IndexReader.open(dir);\r\n        \r\n        byte[] verifyPayloadData = new byte[payloadDataLength];\r\n        offset = 0;\r\n        TermPositions[] tps = new TermPositions[numTerms];\r\n        for (int i = 0; i < numTerms; i++) {\r\n            tps[i] = reader.termPositions(terms[i]);\r\n        }\r\n        \r\n        while (tps[0].next()) {\r\n            for (int i = 1; i < numTerms; i++) {\r\n                tps[i].next();\r\n            }\r\n            int freq = tps[0].freq();\r\n\r\n            for (int i = 0; i < freq; i++) {\r\n                for (int j = 0; j < numTerms; j++) {\r\n                    tps[j].nextPosition();\r\n                    tps[j].getPayload(verifyPayloadData, offset);\r\n                    offset += tps[j].getPayloadLength();\r\n                }\r\n            }\r\n        }\r\n        \r\n        for (int i = 0; i < numTerms; i++) {\r\n            tps[i].close();\r\n        }\r\n        \r\n        assertByteArrayEquals(payloadData, verifyPayloadData);\r\n        \r\n        /*\r\n         *  test lazy skipping\r\n         */        \r\n        TermPositions tp = reader.termPositions(terms[0]);\r\n        tp.next();\r\n        tp.nextPosition();\r\n        // now we don't read this payload\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        byte[] payload = tp.getPayload(null, 0);\r\n        assertEquals(payload[0], payloadData[numTerms]);\r\n        tp.nextPosition();\r\n        \r\n        // we don't read this payload and skip to a different document\r\n        tp.skipTo(5);\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        payload = tp.getPayload(null, 0);\r\n        assertEquals(payload[0], payloadData[5 * numTerms]);\r\n                \r\n        \r\n        /*\r\n         * Test different lengths at skip points\r\n         */\r\n        tp.seek(terms[1]);\r\n        tp.next();\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        tp.skipTo(skipInterval - 1);\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        tp.skipTo(2 * skipInterval - 1);\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        tp.skipTo(3 * skipInterval - 1);\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\r\n        \r\n        /*\r\n         * Test multiple call of getPayload()\r\n         */\r\n        tp.getPayload(null, 0);\r\n        try {\r\n            // it is forbidden to call getPayload() more than once\r\n            // without calling nextPosition()\r\n            tp.getPayload(null, 0);\r\n            fail(\"Expected exception not thrown\");\r\n        } catch (Exception expected) {\r\n            // expected exception\r\n        }\r\n        \r\n        reader.close();\r\n        \r\n        // test long payload\r\n        analyzer = new PayloadAnalyzer();\r\n        writer = new IndexWriter(dir, analyzer, true);\r\n        String singleTerm = \"lucene\";\r\n        \r\n        d = new Document();\r\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.TOKENIZED));\r\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\r\n        payloadData = generateRandomData(2000);\r\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\r\n        writer.addDocument(d);\r\n\r\n        \r\n        writer.optimize();\r\n        // flush\r\n        writer.close();\r\n        \r\n        reader = IndexReader.open(dir);\r\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\r\n        tp.next();\r\n        tp.nextPosition();\r\n\r\n        verifyPayloadData = new byte[tp.getPayloadLength()];\r\n        tp.getPayload(verifyPayloadData, 0);\r\n        byte[] portion = new byte[1500];\r\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\r\n        \r\n        assertByteArrayEquals(portion, verifyPayloadData);\r\n        reader.close();\r\n        \r\n    }\r\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\r\n    // different tests to verify the payload encoding\r\n    private void performTest(Directory dir) throws Exception {\r\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\r\n        IndexWriter writer = new IndexWriter(dir, analyzer, true);\r\n        \r\n        // should be in sync with value in TermInfosWriter\r\n        final int skipInterval = 16;\r\n        \r\n        final int numTerms = 5;\r\n        final String fieldName = \"f1\";\r\n        \r\n        int numDocs = skipInterval + 1; \r\n        // create content for the test documents with just a few terms\r\n        Term[] terms = generateTerms(fieldName, numTerms);\r\n        StringBuffer sb = new StringBuffer();\r\n        for (int i = 0; i < terms.length; i++) {\r\n            sb.append(terms[i].text);\r\n            sb.append(\" \");\r\n        }\r\n        String content = sb.toString();\r\n        \r\n        \r\n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\r\n        byte[] payloadData = generateRandomData(payloadDataLength);\r\n        \r\n        Document d = new Document();\r\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.TOKENIZED));\r\n        // add the same document multiple times to have the same payload lengths for all\r\n        // occurrences within two consecutive skip intervals\r\n        int offset = 0;\r\n        for (int i = 0; i < 2 * numDocs; i++) {\r\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\r\n            offset += numTerms;\r\n            writer.addDocument(d);\r\n        }\r\n        \r\n        // now we make sure to have different payload lengths next at the next skip point        \r\n        for (int i = 0; i < numDocs; i++) {\r\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\r\n            offset += i * numTerms;\r\n            writer.addDocument(d);\r\n        }\r\n        \r\n        writer.optimize();\r\n        // flush\r\n        writer.close();\r\n        \r\n        \r\n        /*\r\n         * Verify the index\r\n         * first we test if all payloads are stored correctly\r\n         */        \r\n        IndexReader reader = IndexReader.open(dir);\r\n        \r\n        byte[] verifyPayloadData = new byte[payloadDataLength];\r\n        offset = 0;\r\n        TermPositions[] tps = new TermPositions[numTerms];\r\n        for (int i = 0; i < numTerms; i++) {\r\n            tps[i] = reader.termPositions(terms[i]);\r\n        }\r\n        \r\n        while (tps[0].next()) {\r\n            for (int i = 1; i < numTerms; i++) {\r\n                tps[i].next();\r\n            }\r\n            int freq = tps[0].freq();\r\n\r\n            for (int i = 0; i < freq; i++) {\r\n                for (int j = 0; j < numTerms; j++) {\r\n                    tps[j].nextPosition();\r\n                    tps[j].getPayload(verifyPayloadData, offset);\r\n                    offset += tps[j].getPayloadLength();\r\n                }\r\n            }\r\n        }\r\n        \r\n        for (int i = 0; i < numTerms; i++) {\r\n            tps[i].close();\r\n        }\r\n        \r\n        assertByteArrayEquals(payloadData, verifyPayloadData);\r\n        \r\n        /*\r\n         *  test lazy skipping\r\n         */        \r\n        TermPositions tp = reader.termPositions(terms[0]);\r\n        tp.next();\r\n        tp.nextPosition();\r\n        // now we don't read this payload\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        byte[] payload = tp.getPayload(null, 0);\r\n        assertEquals(payload[0], payloadData[numTerms]);\r\n        tp.nextPosition();\r\n        \r\n        // we don't read this payload and skip to a different document\r\n        tp.skipTo(5);\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        payload = tp.getPayload(null, 0);\r\n        assertEquals(payload[0], payloadData[5 * numTerms]);\r\n                \r\n        \r\n        /*\r\n         * Test different lengths at skip points\r\n         */\r\n        tp.seek(terms[1]);\r\n        tp.next();\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        tp.skipTo(skipInterval - 1);\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        tp.skipTo(2 * skipInterval - 1);\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        tp.skipTo(3 * skipInterval - 1);\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\r\n        \r\n        /*\r\n         * Test multiple call of getPayload()\r\n         */\r\n        tp.getPayload(null, 0);\r\n        try {\r\n            // it is forbidden to call getPayload() more than once\r\n            // without calling nextPosition()\r\n            tp.getPayload(null, 0);\r\n            fail(\"Expected exception not thrown\");\r\n        } catch (Exception expected) {\r\n            // expected exception\r\n        }\r\n        \r\n        reader.close();\r\n        \r\n        // test long payload\r\n        analyzer = new PayloadAnalyzer();\r\n        writer = new IndexWriter(dir, analyzer, true);\r\n        String singleTerm = \"lucene\";\r\n        \r\n        d = new Document();\r\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.TOKENIZED));\r\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\r\n        payloadData = generateRandomData(2000);\r\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\r\n        writer.addDocument(d);\r\n\r\n        \r\n        writer.optimize();\r\n        // flush\r\n        writer.close();\r\n        \r\n        reader = IndexReader.open(dir);\r\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\r\n        tp.next();\r\n        tp.nextPosition();\r\n\r\n        verifyPayloadData = new byte[tp.getPayloadLength()];\r\n        tp.getPayload(verifyPayloadData, 0);\r\n        byte[] portion = new byte[1500];\r\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\r\n        \r\n        assertByteArrayEquals(portion, verifyPayloadData);\r\n        reader.close();\r\n        \r\n    }\r\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0018e7a0579df5d3de71d0bd878322a7abef04d9","date":1202242049,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\r\n    // different tests to verify the payload encoding\r\n    private void performTest(Directory dir) throws Exception {\r\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\r\n        IndexWriter writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\r\n        \r\n        // should be in sync with value in TermInfosWriter\r\n        final int skipInterval = 16;\r\n        \r\n        final int numTerms = 5;\r\n        final String fieldName = \"f1\";\r\n        \r\n        int numDocs = skipInterval + 1; \r\n        // create content for the test documents with just a few terms\r\n        Term[] terms = generateTerms(fieldName, numTerms);\r\n        StringBuffer sb = new StringBuffer();\r\n        for (int i = 0; i < terms.length; i++) {\r\n            sb.append(terms[i].text);\r\n            sb.append(\" \");\r\n        }\r\n        String content = sb.toString();\r\n        \r\n        \r\n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\r\n        byte[] payloadData = generateRandomData(payloadDataLength);\r\n        \r\n        Document d = new Document();\r\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.TOKENIZED));\r\n        // add the same document multiple times to have the same payload lengths for all\r\n        // occurrences within two consecutive skip intervals\r\n        int offset = 0;\r\n        for (int i = 0; i < 2 * numDocs; i++) {\r\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\r\n            offset += numTerms;\r\n            writer.addDocument(d);\r\n        }\r\n        \r\n        // make sure we create more than one segment to test merging\r\n        writer.flush();\r\n        \r\n        // now we make sure to have different payload lengths next at the next skip point        \r\n        for (int i = 0; i < numDocs; i++) {\r\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\r\n            offset += i * numTerms;\r\n            writer.addDocument(d);\r\n        }\r\n        \r\n        writer.optimize();\r\n        // flush\r\n        writer.close();\r\n        \r\n        \r\n        /*\r\n         * Verify the index\r\n         * first we test if all payloads are stored correctly\r\n         */        \r\n        IndexReader reader = IndexReader.open(dir);\r\n        \r\n        byte[] verifyPayloadData = new byte[payloadDataLength];\r\n        offset = 0;\r\n        TermPositions[] tps = new TermPositions[numTerms];\r\n        for (int i = 0; i < numTerms; i++) {\r\n            tps[i] = reader.termPositions(terms[i]);\r\n        }\r\n        \r\n        while (tps[0].next()) {\r\n            for (int i = 1; i < numTerms; i++) {\r\n                tps[i].next();\r\n            }\r\n            int freq = tps[0].freq();\r\n\r\n            for (int i = 0; i < freq; i++) {\r\n                for (int j = 0; j < numTerms; j++) {\r\n                    tps[j].nextPosition();\r\n                    tps[j].getPayload(verifyPayloadData, offset);\r\n                    offset += tps[j].getPayloadLength();\r\n                }\r\n            }\r\n        }\r\n        \r\n        for (int i = 0; i < numTerms; i++) {\r\n            tps[i].close();\r\n        }\r\n        \r\n        assertByteArrayEquals(payloadData, verifyPayloadData);\r\n        \r\n        /*\r\n         *  test lazy skipping\r\n         */        \r\n        TermPositions tp = reader.termPositions(terms[0]);\r\n        tp.next();\r\n        tp.nextPosition();\r\n        // now we don't read this payload\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        byte[] payload = tp.getPayload(null, 0);\r\n        assertEquals(payload[0], payloadData[numTerms]);\r\n        tp.nextPosition();\r\n        \r\n        // we don't read this payload and skip to a different document\r\n        tp.skipTo(5);\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        payload = tp.getPayload(null, 0);\r\n        assertEquals(payload[0], payloadData[5 * numTerms]);\r\n                \r\n        \r\n        /*\r\n         * Test different lengths at skip points\r\n         */\r\n        tp.seek(terms[1]);\r\n        tp.next();\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        tp.skipTo(skipInterval - 1);\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        tp.skipTo(2 * skipInterval - 1);\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        tp.skipTo(3 * skipInterval - 1);\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\r\n        \r\n        /*\r\n         * Test multiple call of getPayload()\r\n         */\r\n        tp.getPayload(null, 0);\r\n        try {\r\n            // it is forbidden to call getPayload() more than once\r\n            // without calling nextPosition()\r\n            tp.getPayload(null, 0);\r\n            fail(\"Expected exception not thrown\");\r\n        } catch (Exception expected) {\r\n            // expected exception\r\n        }\r\n        \r\n        reader.close();\r\n        \r\n        // test long payload\r\n        analyzer = new PayloadAnalyzer();\r\n        writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\r\n        String singleTerm = \"lucene\";\r\n        \r\n        d = new Document();\r\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.TOKENIZED));\r\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\r\n        payloadData = generateRandomData(2000);\r\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\r\n        writer.addDocument(d);\r\n\r\n        \r\n        writer.optimize();\r\n        // flush\r\n        writer.close();\r\n        \r\n        reader = IndexReader.open(dir);\r\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\r\n        tp.next();\r\n        tp.nextPosition();\r\n\r\n        verifyPayloadData = new byte[tp.getPayloadLength()];\r\n        tp.getPayload(verifyPayloadData, 0);\r\n        byte[] portion = new byte[1500];\r\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\r\n        \r\n        assertByteArrayEquals(portion, verifyPayloadData);\r\n        reader.close();\r\n        \r\n    }\r\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\r\n    // different tests to verify the payload encoding\r\n    private void performTest(Directory dir) throws Exception {\r\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\r\n        IndexWriter writer = new IndexWriter(dir, analyzer, true);\r\n        \r\n        // should be in sync with value in TermInfosWriter\r\n        final int skipInterval = 16;\r\n        \r\n        final int numTerms = 5;\r\n        final String fieldName = \"f1\";\r\n        \r\n        int numDocs = skipInterval + 1; \r\n        // create content for the test documents with just a few terms\r\n        Term[] terms = generateTerms(fieldName, numTerms);\r\n        StringBuffer sb = new StringBuffer();\r\n        for (int i = 0; i < terms.length; i++) {\r\n            sb.append(terms[i].text);\r\n            sb.append(\" \");\r\n        }\r\n        String content = sb.toString();\r\n        \r\n        \r\n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\r\n        byte[] payloadData = generateRandomData(payloadDataLength);\r\n        \r\n        Document d = new Document();\r\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.TOKENIZED));\r\n        // add the same document multiple times to have the same payload lengths for all\r\n        // occurrences within two consecutive skip intervals\r\n        int offset = 0;\r\n        for (int i = 0; i < 2 * numDocs; i++) {\r\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\r\n            offset += numTerms;\r\n            writer.addDocument(d);\r\n        }\r\n        \r\n        // make sure we create more than one segment to test merging\r\n        writer.flush();\r\n        \r\n        // now we make sure to have different payload lengths next at the next skip point        \r\n        for (int i = 0; i < numDocs; i++) {\r\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\r\n            offset += i * numTerms;\r\n            writer.addDocument(d);\r\n        }\r\n        \r\n        writer.optimize();\r\n        // flush\r\n        writer.close();\r\n        \r\n        \r\n        /*\r\n         * Verify the index\r\n         * first we test if all payloads are stored correctly\r\n         */        \r\n        IndexReader reader = IndexReader.open(dir);\r\n        \r\n        byte[] verifyPayloadData = new byte[payloadDataLength];\r\n        offset = 0;\r\n        TermPositions[] tps = new TermPositions[numTerms];\r\n        for (int i = 0; i < numTerms; i++) {\r\n            tps[i] = reader.termPositions(terms[i]);\r\n        }\r\n        \r\n        while (tps[0].next()) {\r\n            for (int i = 1; i < numTerms; i++) {\r\n                tps[i].next();\r\n            }\r\n            int freq = tps[0].freq();\r\n\r\n            for (int i = 0; i < freq; i++) {\r\n                for (int j = 0; j < numTerms; j++) {\r\n                    tps[j].nextPosition();\r\n                    tps[j].getPayload(verifyPayloadData, offset);\r\n                    offset += tps[j].getPayloadLength();\r\n                }\r\n            }\r\n        }\r\n        \r\n        for (int i = 0; i < numTerms; i++) {\r\n            tps[i].close();\r\n        }\r\n        \r\n        assertByteArrayEquals(payloadData, verifyPayloadData);\r\n        \r\n        /*\r\n         *  test lazy skipping\r\n         */        \r\n        TermPositions tp = reader.termPositions(terms[0]);\r\n        tp.next();\r\n        tp.nextPosition();\r\n        // now we don't read this payload\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        byte[] payload = tp.getPayload(null, 0);\r\n        assertEquals(payload[0], payloadData[numTerms]);\r\n        tp.nextPosition();\r\n        \r\n        // we don't read this payload and skip to a different document\r\n        tp.skipTo(5);\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        payload = tp.getPayload(null, 0);\r\n        assertEquals(payload[0], payloadData[5 * numTerms]);\r\n                \r\n        \r\n        /*\r\n         * Test different lengths at skip points\r\n         */\r\n        tp.seek(terms[1]);\r\n        tp.next();\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        tp.skipTo(skipInterval - 1);\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        tp.skipTo(2 * skipInterval - 1);\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        tp.skipTo(3 * skipInterval - 1);\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\r\n        \r\n        /*\r\n         * Test multiple call of getPayload()\r\n         */\r\n        tp.getPayload(null, 0);\r\n        try {\r\n            // it is forbidden to call getPayload() more than once\r\n            // without calling nextPosition()\r\n            tp.getPayload(null, 0);\r\n            fail(\"Expected exception not thrown\");\r\n        } catch (Exception expected) {\r\n            // expected exception\r\n        }\r\n        \r\n        reader.close();\r\n        \r\n        // test long payload\r\n        analyzer = new PayloadAnalyzer();\r\n        writer = new IndexWriter(dir, analyzer, true);\r\n        String singleTerm = \"lucene\";\r\n        \r\n        d = new Document();\r\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.TOKENIZED));\r\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\r\n        payloadData = generateRandomData(2000);\r\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\r\n        writer.addDocument(d);\r\n\r\n        \r\n        writer.optimize();\r\n        // flush\r\n        writer.close();\r\n        \r\n        reader = IndexReader.open(dir);\r\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\r\n        tp.next();\r\n        tp.nextPosition();\r\n\r\n        verifyPayloadData = new byte[tp.getPayloadLength()];\r\n        tp.getPayload(verifyPayloadData, 0);\r\n        byte[] portion = new byte[1500];\r\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\r\n        \r\n        assertByteArrayEquals(portion, verifyPayloadData);\r\n        reader.close();\r\n        \r\n    }\r\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2123bddbd65dea198cac380540636ce43a880403","date":1211269254,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuffer sb = new StringBuffer();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.TOKENIZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.flush();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.TOKENIZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\r\n    // different tests to verify the payload encoding\r\n    private void performTest(Directory dir) throws Exception {\r\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\r\n        IndexWriter writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\r\n        \r\n        // should be in sync with value in TermInfosWriter\r\n        final int skipInterval = 16;\r\n        \r\n        final int numTerms = 5;\r\n        final String fieldName = \"f1\";\r\n        \r\n        int numDocs = skipInterval + 1; \r\n        // create content for the test documents with just a few terms\r\n        Term[] terms = generateTerms(fieldName, numTerms);\r\n        StringBuffer sb = new StringBuffer();\r\n        for (int i = 0; i < terms.length; i++) {\r\n            sb.append(terms[i].text);\r\n            sb.append(\" \");\r\n        }\r\n        String content = sb.toString();\r\n        \r\n        \r\n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\r\n        byte[] payloadData = generateRandomData(payloadDataLength);\r\n        \r\n        Document d = new Document();\r\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.TOKENIZED));\r\n        // add the same document multiple times to have the same payload lengths for all\r\n        // occurrences within two consecutive skip intervals\r\n        int offset = 0;\r\n        for (int i = 0; i < 2 * numDocs; i++) {\r\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\r\n            offset += numTerms;\r\n            writer.addDocument(d);\r\n        }\r\n        \r\n        // make sure we create more than one segment to test merging\r\n        writer.flush();\r\n        \r\n        // now we make sure to have different payload lengths next at the next skip point        \r\n        for (int i = 0; i < numDocs; i++) {\r\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\r\n            offset += i * numTerms;\r\n            writer.addDocument(d);\r\n        }\r\n        \r\n        writer.optimize();\r\n        // flush\r\n        writer.close();\r\n        \r\n        \r\n        /*\r\n         * Verify the index\r\n         * first we test if all payloads are stored correctly\r\n         */        \r\n        IndexReader reader = IndexReader.open(dir);\r\n        \r\n        byte[] verifyPayloadData = new byte[payloadDataLength];\r\n        offset = 0;\r\n        TermPositions[] tps = new TermPositions[numTerms];\r\n        for (int i = 0; i < numTerms; i++) {\r\n            tps[i] = reader.termPositions(terms[i]);\r\n        }\r\n        \r\n        while (tps[0].next()) {\r\n            for (int i = 1; i < numTerms; i++) {\r\n                tps[i].next();\r\n            }\r\n            int freq = tps[0].freq();\r\n\r\n            for (int i = 0; i < freq; i++) {\r\n                for (int j = 0; j < numTerms; j++) {\r\n                    tps[j].nextPosition();\r\n                    tps[j].getPayload(verifyPayloadData, offset);\r\n                    offset += tps[j].getPayloadLength();\r\n                }\r\n            }\r\n        }\r\n        \r\n        for (int i = 0; i < numTerms; i++) {\r\n            tps[i].close();\r\n        }\r\n        \r\n        assertByteArrayEquals(payloadData, verifyPayloadData);\r\n        \r\n        /*\r\n         *  test lazy skipping\r\n         */        \r\n        TermPositions tp = reader.termPositions(terms[0]);\r\n        tp.next();\r\n        tp.nextPosition();\r\n        // now we don't read this payload\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        byte[] payload = tp.getPayload(null, 0);\r\n        assertEquals(payload[0], payloadData[numTerms]);\r\n        tp.nextPosition();\r\n        \r\n        // we don't read this payload and skip to a different document\r\n        tp.skipTo(5);\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        payload = tp.getPayload(null, 0);\r\n        assertEquals(payload[0], payloadData[5 * numTerms]);\r\n                \r\n        \r\n        /*\r\n         * Test different lengths at skip points\r\n         */\r\n        tp.seek(terms[1]);\r\n        tp.next();\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        tp.skipTo(skipInterval - 1);\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        tp.skipTo(2 * skipInterval - 1);\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\r\n        tp.skipTo(3 * skipInterval - 1);\r\n        tp.nextPosition();\r\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\r\n        \r\n        /*\r\n         * Test multiple call of getPayload()\r\n         */\r\n        tp.getPayload(null, 0);\r\n        try {\r\n            // it is forbidden to call getPayload() more than once\r\n            // without calling nextPosition()\r\n            tp.getPayload(null, 0);\r\n            fail(\"Expected exception not thrown\");\r\n        } catch (Exception expected) {\r\n            // expected exception\r\n        }\r\n        \r\n        reader.close();\r\n        \r\n        // test long payload\r\n        analyzer = new PayloadAnalyzer();\r\n        writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\r\n        String singleTerm = \"lucene\";\r\n        \r\n        d = new Document();\r\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.TOKENIZED));\r\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\r\n        payloadData = generateRandomData(2000);\r\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\r\n        writer.addDocument(d);\r\n\r\n        \r\n        writer.optimize();\r\n        // flush\r\n        writer.close();\r\n        \r\n        reader = IndexReader.open(dir);\r\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\r\n        tp.next();\r\n        tp.nextPosition();\r\n\r\n        verifyPayloadData = new byte[tp.getPayloadLength()];\r\n        tp.getPayload(verifyPayloadData, 0);\r\n        byte[] portion = new byte[1500];\r\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\r\n        \r\n        assertByteArrayEquals(portion, verifyPayloadData);\r\n        reader.close();\r\n        \r\n    }\r\n\n","bugFix":null,"bugIntro":["5c4531fadbecf73a7716fdf5cd742463e866e84e","5c4531fadbecf73a7716fdf5cd742463e866e84e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a","date":1221082732,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuffer sb = new StringBuffer();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.flush();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuffer sb = new StringBuffer();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.TOKENIZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.flush();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.TOKENIZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4625cb7ffd7c9caaf2d62b206ba9a382d68da82c","date":1254521470,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.flush();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuffer sb = new StringBuffer();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.flush();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e8d1458a2543cbd30cbfe7929be4dcb5c5251659","date":1254582241,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuffer sb = new StringBuffer();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.flush();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuffer sb = new StringBuffer();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.flush();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0a046c0c310bc77931fc8441bd920053b607dd14","date":1254584734,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.flush();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.flush();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"be2419774ad2eb3c65ca1cb035c3a2ccc6ae7da1","date":1255502337,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.flush();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1cedb00d2dd44640194401179358a2e3ba6051bf","date":1268243626,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n            TEST_VERSION_CURRENT).setAnalyzer(analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT)\n        .setAnalyzer(analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e52fea2c4081a1e552b98506691990be59503168","date":1268250331,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n            TEST_VERSION_CURRENT).setAnalyzer(analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT)\n        .setAnalyzer(analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8","date":1268494368,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","pathOld":"src/test/org/apache/lucene/index/TestPayloads#performTest(Directory).mjava","sourceNew":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","sourceOld":"    // builds an index with payloads in the given Directory and performs\n    // different tests to verify the payload encoding\n    private void performTest(Directory dir) throws Exception {\n        PayloadAnalyzer analyzer = new PayloadAnalyzer();\n        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.CREATE));\n        \n        // should be in sync with value in TermInfosWriter\n        final int skipInterval = 16;\n        \n        final int numTerms = 5;\n        final String fieldName = \"f1\";\n        \n        int numDocs = skipInterval + 1; \n        // create content for the test documents with just a few terms\n        Term[] terms = generateTerms(fieldName, numTerms);\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < terms.length; i++) {\n            sb.append(terms[i].text);\n            sb.append(\" \");\n        }\n        String content = sb.toString();\n        \n        \n        int payloadDataLength = numTerms * numDocs * 2 + numTerms * numDocs * (numDocs - 1) / 2;\n        byte[] payloadData = generateRandomData(payloadDataLength);\n        \n        Document d = new Document();\n        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));\n        // add the same document multiple times to have the same payload lengths for all\n        // occurrences within two consecutive skip intervals\n        int offset = 0;\n        for (int i = 0; i < 2 * numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, 1);\n            offset += numTerms;\n            writer.addDocument(d);\n        }\n        \n        // make sure we create more than one segment to test merging\n        writer.commit();\n        \n        // now we make sure to have different payload lengths next at the next skip point        \n        for (int i = 0; i < numDocs; i++) {\n            analyzer.setPayloadData(fieldName, payloadData, offset, i);\n            offset += i * numTerms;\n            writer.addDocument(d);\n        }\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        \n        /*\n         * Verify the index\n         * first we test if all payloads are stored correctly\n         */        \n        IndexReader reader = IndexReader.open(dir, true);\n        \n        byte[] verifyPayloadData = new byte[payloadDataLength];\n        offset = 0;\n        TermPositions[] tps = new TermPositions[numTerms];\n        for (int i = 0; i < numTerms; i++) {\n            tps[i] = reader.termPositions(terms[i]);\n        }\n        \n        while (tps[0].next()) {\n            for (int i = 1; i < numTerms; i++) {\n                tps[i].next();\n            }\n            int freq = tps[0].freq();\n\n            for (int i = 0; i < freq; i++) {\n                for (int j = 0; j < numTerms; j++) {\n                    tps[j].nextPosition();\n                    tps[j].getPayload(verifyPayloadData, offset);\n                    offset += tps[j].getPayloadLength();\n                }\n            }\n        }\n        \n        for (int i = 0; i < numTerms; i++) {\n            tps[i].close();\n        }\n        \n        assertByteArrayEquals(payloadData, verifyPayloadData);\n        \n        /*\n         *  test lazy skipping\n         */        \n        TermPositions tp = reader.termPositions(terms[0]);\n        tp.next();\n        tp.nextPosition();\n        // now we don't read this payload\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        byte[] payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[numTerms]);\n        tp.nextPosition();\n        \n        // we don't read this payload and skip to a different document\n        tp.skipTo(5);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        payload = tp.getPayload(null, 0);\n        assertEquals(payload[0], payloadData[5 * numTerms]);\n                \n        \n        /*\n         * Test different lengths at skip points\n         */\n        tp.seek(terms[1]);\n        tp.next();\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(2 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 1, tp.getPayloadLength());\n        tp.skipTo(3 * skipInterval - 1);\n        tp.nextPosition();\n        assertEquals(\"Wrong payload length.\", 3 * skipInterval - 2 * numDocs - 1, tp.getPayloadLength());\n        \n        /*\n         * Test multiple call of getPayload()\n         */\n        tp.getPayload(null, 0);\n        try {\n            // it is forbidden to call getPayload() more than once\n            // without calling nextPosition()\n            tp.getPayload(null, 0);\n            fail(\"Expected exception not thrown\");\n        } catch (Exception expected) {\n            // expected exception\n        }\n        \n        reader.close();\n        \n        // test long payload\n        analyzer = new PayloadAnalyzer();\n        writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n            analyzer).setOpenMode(OpenMode.CREATE));\n        String singleTerm = \"lucene\";\n        \n        d = new Document();\n        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));\n        // add a payload whose length is greater than the buffer size of BufferedIndexOutput\n        payloadData = generateRandomData(2000);\n        analyzer.setPayloadData(fieldName, payloadData, 100, 1500);\n        writer.addDocument(d);\n\n        \n        writer.optimize();\n        // flush\n        writer.close();\n        \n        reader = IndexReader.open(dir, true);\n        tp = reader.termPositions(new Term(fieldName, singleTerm));\n        tp.next();\n        tp.nextPosition();\n\n        verifyPayloadData = new byte[tp.getPayloadLength()];\n        tp.getPayload(verifyPayloadData, 0);\n        byte[] portion = new byte[1500];\n        System.arraycopy(payloadData, 100, portion, 0, 1500);\n        \n        assertByteArrayEquals(portion, verifyPayloadData);\n        reader.close();\n        \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"8522ae207a56c6db28ca06fe6cc33e70911c3600":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"0a046c0c310bc77931fc8441bd920053b607dd14":["4625cb7ffd7c9caaf2d62b206ba9a382d68da82c","e8d1458a2543cbd30cbfe7929be4dcb5c5251659"],"2123bddbd65dea198cac380540636ce43a880403":["0018e7a0579df5d3de71d0bd878322a7abef04d9"],"1cedb00d2dd44640194401179358a2e3ba6051bf":["be2419774ad2eb3c65ca1cb035c3a2ccc6ae7da1"],"be2419774ad2eb3c65ca1cb035c3a2ccc6ae7da1":["0a046c0c310bc77931fc8441bd920053b607dd14"],"b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a":["2123bddbd65dea198cac380540636ce43a880403"],"e52fea2c4081a1e552b98506691990be59503168":["1cedb00d2dd44640194401179358a2e3ba6051bf"],"0018e7a0579df5d3de71d0bd878322a7abef04d9":["f271ff507bf58806fc089e76e38057c95e391dcd"],"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8":["e52fea2c4081a1e552b98506691990be59503168"],"e8d1458a2543cbd30cbfe7929be4dcb5c5251659":["b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4625cb7ffd7c9caaf2d62b206ba9a382d68da82c":["b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a"],"f271ff507bf58806fc089e76e38057c95e391dcd":["8522ae207a56c6db28ca06fe6cc33e70911c3600"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8"]},"commit2Childs":{"8522ae207a56c6db28ca06fe6cc33e70911c3600":["f271ff507bf58806fc089e76e38057c95e391dcd"],"0a046c0c310bc77931fc8441bd920053b607dd14":["be2419774ad2eb3c65ca1cb035c3a2ccc6ae7da1"],"2123bddbd65dea198cac380540636ce43a880403":["b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a"],"1cedb00d2dd44640194401179358a2e3ba6051bf":["e52fea2c4081a1e552b98506691990be59503168"],"be2419774ad2eb3c65ca1cb035c3a2ccc6ae7da1":["1cedb00d2dd44640194401179358a2e3ba6051bf"],"b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a":["e8d1458a2543cbd30cbfe7929be4dcb5c5251659","4625cb7ffd7c9caaf2d62b206ba9a382d68da82c"],"0018e7a0579df5d3de71d0bd878322a7abef04d9":["2123bddbd65dea198cac380540636ce43a880403"],"e52fea2c4081a1e552b98506691990be59503168":["84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8"],"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"e8d1458a2543cbd30cbfe7929be4dcb5c5251659":["0a046c0c310bc77931fc8441bd920053b607dd14"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["8522ae207a56c6db28ca06fe6cc33e70911c3600"],"4625cb7ffd7c9caaf2d62b206ba9a382d68da82c":["0a046c0c310bc77931fc8441bd920053b607dd14"],"f271ff507bf58806fc089e76e38057c95e391dcd":["0018e7a0579df5d3de71d0bd878322a7abef04d9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}