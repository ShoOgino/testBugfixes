{"path":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[String,TermsHashConsumerPerField],SegmentWriteState).mjava","commits":[{"id":"cd9165e54429bb5c99e75d5cb1c926cc98772456","date":1337362687,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[String,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(Map<String,TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n        final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n        if (perField.termsHashPerField.bytesHash.size() > 0) {\n          allFields.add(perField);\n        }\n    }\n\n    final int numAllFields = allFields.size();\n\n    // Sort by field name\n    CollectionUtil.quickSort(allFields);\n\n    final FieldsConsumer consumer = state.codec.postingsFormat().fieldsConsumer(state);\n\n    boolean success = false;\n\n    try {\n      TermsHash termsHash = null;\n      \n      /*\n    Current writer chain:\n      FieldsConsumer\n        -> IMPL: FormatPostingsTermsDictWriter\n          -> TermsConsumer\n            -> IMPL: FormatPostingsTermsDictWriter.TermsWriter\n              -> DocsConsumer\n                -> IMPL: FormatPostingsDocsWriter\n                  -> PositionsConsumer\n                    -> IMPL: FormatPostingsPositionsWriter\n       */\n      \n      for (int fieldNumber = 0; fieldNumber < numAllFields; fieldNumber++) {\n        final FieldInfo fieldInfo = allFields.get(fieldNumber).fieldInfo;\n        \n        final FreqProxTermsWriterPerField fieldWriter = allFields.get(fieldNumber);\n\n        // If this field has postings then add them to the\n        // segment\n        fieldWriter.flush(fieldInfo.name, consumer, state);\n        \n        TermsHashPerField perField = fieldWriter.termsHashPerField;\n        assert termsHash == null || termsHash == perField.termsHash;\n        termsHash = perField.termsHash;\n        int numPostings = perField.bytesHash.size();\n        perField.reset();\n        perField.shrinkHash(numPostings);\n        fieldWriter.reset();\n      }\n      \n      if (termsHash != null) {\n        termsHash.reset();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n        final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n        if (perField.termsHashPerField.bytesHash.size() > 0) {\n          allFields.add(perField);\n        }\n    }\n\n    final int numAllFields = allFields.size();\n\n    // Sort by field name\n    CollectionUtil.quickSort(allFields);\n\n    final FieldsConsumer consumer = state.codec.postingsFormat().fieldsConsumer(state);\n\n    boolean success = false;\n\n    try {\n      TermsHash termsHash = null;\n      \n      /*\n    Current writer chain:\n      FieldsConsumer\n        -> IMPL: FormatPostingsTermsDictWriter\n          -> TermsConsumer\n            -> IMPL: FormatPostingsTermsDictWriter.TermsWriter\n              -> DocsConsumer\n                -> IMPL: FormatPostingsDocsWriter\n                  -> PositionsConsumer\n                    -> IMPL: FormatPostingsPositionsWriter\n       */\n      \n      for (int fieldNumber = 0; fieldNumber < numAllFields; fieldNumber++) {\n        final FieldInfo fieldInfo = allFields.get(fieldNumber).fieldInfo;\n        \n        final FreqProxTermsWriterPerField fieldWriter = allFields.get(fieldNumber);\n        \n        // Aggregate the storePayload as seen by the same\n        // field across multiple threads\n        if (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {\n          if (fieldWriter.hasPayloads) {\n            fieldInfo.setStorePayloads();\n          }\n        }\n        \n        // If this field has postings then add them to the\n        // segment\n        fieldWriter.flush(fieldInfo.name, consumer, state);\n        \n        TermsHashPerField perField = fieldWriter.termsHashPerField;\n        assert termsHash == null || termsHash == perField.termsHash;\n        termsHash = perField.termsHash;\n        int numPostings = perField.bytesHash.size();\n        perField.reset();\n        perField.shrinkHash(numPostings);\n        fieldWriter.reset();\n      }\n      \n      if (termsHash != null) {\n        termsHash.reset();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0cadbbc3b8df99c8c7acd19da62f6b35eb126c85","date":1337798576,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[String,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[String,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(Map<String,TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n        final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n        if (perField.termsHashPerField.bytesHash.size() > 0) {\n          allFields.add(perField);\n        }\n    }\n\n    final int numAllFields = allFields.size();\n\n    // Sort by field name\n    CollectionUtil.quickSort(allFields);\n\n    final FieldsConsumer consumer = state.segmentInfo.getCodec().postingsFormat().fieldsConsumer(state);\n\n    boolean success = false;\n\n    try {\n      TermsHash termsHash = null;\n      \n      /*\n    Current writer chain:\n      FieldsConsumer\n        -> IMPL: FormatPostingsTermsDictWriter\n          -> TermsConsumer\n            -> IMPL: FormatPostingsTermsDictWriter.TermsWriter\n              -> DocsConsumer\n                -> IMPL: FormatPostingsDocsWriter\n                  -> PositionsConsumer\n                    -> IMPL: FormatPostingsPositionsWriter\n       */\n      \n      for (int fieldNumber = 0; fieldNumber < numAllFields; fieldNumber++) {\n        final FieldInfo fieldInfo = allFields.get(fieldNumber).fieldInfo;\n        \n        final FreqProxTermsWriterPerField fieldWriter = allFields.get(fieldNumber);\n\n        // If this field has postings then add them to the\n        // segment\n        fieldWriter.flush(fieldInfo.name, consumer, state);\n        \n        TermsHashPerField perField = fieldWriter.termsHashPerField;\n        assert termsHash == null || termsHash == perField.termsHash;\n        termsHash = perField.termsHash;\n        int numPostings = perField.bytesHash.size();\n        perField.reset();\n        perField.shrinkHash(numPostings);\n        fieldWriter.reset();\n      }\n      \n      if (termsHash != null) {\n        termsHash.reset();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void flush(Map<String,TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n        final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n        if (perField.termsHashPerField.bytesHash.size() > 0) {\n          allFields.add(perField);\n        }\n    }\n\n    final int numAllFields = allFields.size();\n\n    // Sort by field name\n    CollectionUtil.quickSort(allFields);\n\n    final FieldsConsumer consumer = state.codec.postingsFormat().fieldsConsumer(state);\n\n    boolean success = false;\n\n    try {\n      TermsHash termsHash = null;\n      \n      /*\n    Current writer chain:\n      FieldsConsumer\n        -> IMPL: FormatPostingsTermsDictWriter\n          -> TermsConsumer\n            -> IMPL: FormatPostingsTermsDictWriter.TermsWriter\n              -> DocsConsumer\n                -> IMPL: FormatPostingsDocsWriter\n                  -> PositionsConsumer\n                    -> IMPL: FormatPostingsPositionsWriter\n       */\n      \n      for (int fieldNumber = 0; fieldNumber < numAllFields; fieldNumber++) {\n        final FieldInfo fieldInfo = allFields.get(fieldNumber).fieldInfo;\n        \n        final FreqProxTermsWriterPerField fieldWriter = allFields.get(fieldNumber);\n\n        // If this field has postings then add them to the\n        // segment\n        fieldWriter.flush(fieldInfo.name, consumer, state);\n        \n        TermsHashPerField perField = fieldWriter.termsHashPerField;\n        assert termsHash == null || termsHash == perField.termsHash;\n        termsHash = perField.termsHash;\n        int numPostings = perField.bytesHash.size();\n        perField.reset();\n        perField.shrinkHash(numPostings);\n        fieldWriter.reset();\n      }\n      \n      if (termsHash != null) {\n        termsHash.reset();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"615ddbd81799980d0fdd95e0238e1c498b6f47b0","date":1338233290,"type":1,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[String,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(Map<String,TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n        final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n        if (perField.termsHashPerField.bytesHash.size() > 0) {\n          allFields.add(perField);\n        }\n    }\n\n    final int numAllFields = allFields.size();\n\n    // Sort by field name\n    CollectionUtil.quickSort(allFields);\n\n    final FieldsConsumer consumer = state.segmentInfo.getCodec().postingsFormat().fieldsConsumer(state);\n\n    boolean success = false;\n\n    try {\n      TermsHash termsHash = null;\n      \n      /*\n    Current writer chain:\n      FieldsConsumer\n        -> IMPL: FormatPostingsTermsDictWriter\n          -> TermsConsumer\n            -> IMPL: FormatPostingsTermsDictWriter.TermsWriter\n              -> DocsConsumer\n                -> IMPL: FormatPostingsDocsWriter\n                  -> PositionsConsumer\n                    -> IMPL: FormatPostingsPositionsWriter\n       */\n      \n      for (int fieldNumber = 0; fieldNumber < numAllFields; fieldNumber++) {\n        final FieldInfo fieldInfo = allFields.get(fieldNumber).fieldInfo;\n        \n        final FreqProxTermsWriterPerField fieldWriter = allFields.get(fieldNumber);\n\n        // If this field has postings then add them to the\n        // segment\n        fieldWriter.flush(fieldInfo.name, consumer, state);\n        \n        TermsHashPerField perField = fieldWriter.termsHashPerField;\n        assert termsHash == null || termsHash == perField.termsHash;\n        termsHash = perField.termsHash;\n        int numPostings = perField.bytesHash.size();\n        perField.reset();\n        perField.shrinkHash(numPostings);\n        fieldWriter.reset();\n      }\n      \n      if (termsHash != null) {\n        termsHash.reset();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n        final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n        if (perField.termsHashPerField.bytesHash.size() > 0) {\n          allFields.add(perField);\n        }\n    }\n\n    final int numAllFields = allFields.size();\n\n    // Sort by field name\n    CollectionUtil.quickSort(allFields);\n\n    final FieldsConsumer consumer = state.codec.postingsFormat().fieldsConsumer(state);\n\n    boolean success = false;\n\n    try {\n      TermsHash termsHash = null;\n      \n      /*\n    Current writer chain:\n      FieldsConsumer\n        -> IMPL: FormatPostingsTermsDictWriter\n          -> TermsConsumer\n            -> IMPL: FormatPostingsTermsDictWriter.TermsWriter\n              -> DocsConsumer\n                -> IMPL: FormatPostingsDocsWriter\n                  -> PositionsConsumer\n                    -> IMPL: FormatPostingsPositionsWriter\n       */\n      \n      for (int fieldNumber = 0; fieldNumber < numAllFields; fieldNumber++) {\n        final FieldInfo fieldInfo = allFields.get(fieldNumber).fieldInfo;\n        \n        final FreqProxTermsWriterPerField fieldWriter = allFields.get(fieldNumber);\n        \n        // Aggregate the storePayload as seen by the same\n        // field across multiple threads\n        if (fieldInfo.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {\n          fieldInfo.storePayloads |= fieldWriter.hasPayloads;\n        }\n        \n        // If this field has postings then add them to the\n        // segment\n        fieldWriter.flush(fieldInfo.name, consumer, state);\n        \n        TermsHashPerField perField = fieldWriter.termsHashPerField;\n        assert termsHash == null || termsHash == perField.termsHash;\n        termsHash = perField.termsHash;\n        int numPostings = perField.bytesHash.size();\n        perField.reset();\n        perField.shrinkHash(numPostings);\n        fieldWriter.reset();\n      }\n      \n      if (termsHash != null) {\n        termsHash.reset();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6647091125f681395cbde9bb2b7b947cc4ef9bb3","date":1352400554,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[String,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[String,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(Map<String,TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n      final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n      if (perField.termsHashPerField.bytesHash.size() > 0) {\n        allFields.add(perField);\n      }\n    }\n\n    final int numAllFields = allFields.size();\n\n    // Sort by field name\n    CollectionUtil.quickSort(allFields);\n\n    final FieldsConsumer consumer = state.segmentInfo.getCodec().postingsFormat().fieldsConsumer(state);\n\n    boolean success = false;\n\n    try {\n      TermsHash termsHash = null;\n      \n      /*\n    Current writer chain:\n      FieldsConsumer\n        -> IMPL: FormatPostingsTermsDictWriter\n          -> TermsConsumer\n            -> IMPL: FormatPostingsTermsDictWriter.TermsWriter\n              -> DocsConsumer\n                -> IMPL: FormatPostingsDocsWriter\n                  -> PositionsConsumer\n                    -> IMPL: FormatPostingsPositionsWriter\n       */\n      \n      for (int fieldNumber = 0; fieldNumber < numAllFields; fieldNumber++) {\n        final FieldInfo fieldInfo = allFields.get(fieldNumber).fieldInfo;\n        \n        final FreqProxTermsWriterPerField fieldWriter = allFields.get(fieldNumber);\n\n        // If this field has postings then add them to the\n        // segment\n        fieldWriter.flush(fieldInfo.name, consumer, state);\n        \n        TermsHashPerField perField = fieldWriter.termsHashPerField;\n        assert termsHash == null || termsHash == perField.termsHash;\n        termsHash = perField.termsHash;\n        int numPostings = perField.bytesHash.size();\n        perField.reset();\n        perField.shrinkHash(numPostings);\n        fieldWriter.reset();\n      }\n      \n      if (termsHash != null) {\n        termsHash.reset();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void flush(Map<String,TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n        final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n        if (perField.termsHashPerField.bytesHash.size() > 0) {\n          allFields.add(perField);\n        }\n    }\n\n    final int numAllFields = allFields.size();\n\n    // Sort by field name\n    CollectionUtil.quickSort(allFields);\n\n    final FieldsConsumer consumer = state.segmentInfo.getCodec().postingsFormat().fieldsConsumer(state);\n\n    boolean success = false;\n\n    try {\n      TermsHash termsHash = null;\n      \n      /*\n    Current writer chain:\n      FieldsConsumer\n        -> IMPL: FormatPostingsTermsDictWriter\n          -> TermsConsumer\n            -> IMPL: FormatPostingsTermsDictWriter.TermsWriter\n              -> DocsConsumer\n                -> IMPL: FormatPostingsDocsWriter\n                  -> PositionsConsumer\n                    -> IMPL: FormatPostingsPositionsWriter\n       */\n      \n      for (int fieldNumber = 0; fieldNumber < numAllFields; fieldNumber++) {\n        final FieldInfo fieldInfo = allFields.get(fieldNumber).fieldInfo;\n        \n        final FreqProxTermsWriterPerField fieldWriter = allFields.get(fieldNumber);\n\n        // If this field has postings then add them to the\n        // segment\n        fieldWriter.flush(fieldInfo.name, consumer, state);\n        \n        TermsHashPerField perField = fieldWriter.termsHashPerField;\n        assert termsHash == null || termsHash == perField.termsHash;\n        termsHash = perField.termsHash;\n        int numPostings = perField.bytesHash.size();\n        perField.reset();\n        perField.shrinkHash(numPostings);\n        fieldWriter.reset();\n      }\n      \n      if (termsHash != null) {\n        termsHash.reset();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d4d69c535930b5cce125cff868d40f6373dc27d4","date":1360270101,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[String,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[String,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(Map<String,TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n      final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n      if (perField.termsHashPerField.bytesHash.size() > 0) {\n        allFields.add(perField);\n      }\n    }\n\n    final int numAllFields = allFields.size();\n\n    // Sort by field name\n    CollectionUtil.quickSort(allFields);\n\n    final FieldsConsumer consumer = state.segmentInfo.getCodec().postingsFormat().fieldsConsumer(state);\n\n    boolean success = false;\n\n    try {\n      TermsHash termsHash = null;\n      \n      /*\n    Current writer chain:\n      FieldsConsumer\n        -> IMPL: FormatPostingsTermsDictWriter\n          -> TermsConsumer\n            -> IMPL: FormatPostingsTermsDictWriter.TermsWriter\n              -> DocsConsumer\n                -> IMPL: FormatPostingsDocsWriter\n                  -> PositionsConsumer\n                    -> IMPL: FormatPostingsPositionsWriter\n       */\n      \n      for (int fieldNumber = 0; fieldNumber < numAllFields; fieldNumber++) {\n        final FieldInfo fieldInfo = allFields.get(fieldNumber).fieldInfo;\n        \n        final FreqProxTermsWriterPerField fieldWriter = allFields.get(fieldNumber);\n\n        // If this field has postings then add them to the\n        // segment\n        fieldWriter.flush(fieldInfo.name, consumer, state);\n        \n        TermsHashPerField perField = fieldWriter.termsHashPerField;\n        assert termsHash == null || termsHash == perField.termsHash;\n        termsHash = perField.termsHash;\n        int numPostings = perField.bytesHash.size();\n        perField.reset();\n        perField.shrinkHash(numPostings);\n        fieldWriter.reset();\n      }\n      \n      if (termsHash != null) {\n        termsHash.reset();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void flush(Map<String,TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n        final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n        if (perField.termsHashPerField.bytesHash.size() > 0) {\n          allFields.add(perField);\n        }\n    }\n\n    final int numAllFields = allFields.size();\n\n    // Sort by field name\n    CollectionUtil.quickSort(allFields);\n\n    final FieldsConsumer consumer = state.segmentInfo.getCodec().postingsFormat().fieldsConsumer(state);\n\n    boolean success = false;\n\n    try {\n      TermsHash termsHash = null;\n      \n      /*\n    Current writer chain:\n      FieldsConsumer\n        -> IMPL: FormatPostingsTermsDictWriter\n          -> TermsConsumer\n            -> IMPL: FormatPostingsTermsDictWriter.TermsWriter\n              -> DocsConsumer\n                -> IMPL: FormatPostingsDocsWriter\n                  -> PositionsConsumer\n                    -> IMPL: FormatPostingsPositionsWriter\n       */\n      \n      for (int fieldNumber = 0; fieldNumber < numAllFields; fieldNumber++) {\n        final FieldInfo fieldInfo = allFields.get(fieldNumber).fieldInfo;\n        \n        final FreqProxTermsWriterPerField fieldWriter = allFields.get(fieldNumber);\n\n        // If this field has postings then add them to the\n        // segment\n        fieldWriter.flush(fieldInfo.name, consumer, state);\n        \n        TermsHashPerField perField = fieldWriter.termsHashPerField;\n        assert termsHash == null || termsHash == perField.termsHash;\n        termsHash = perField.termsHash;\n        int numPostings = perField.bytesHash.size();\n        perField.reset();\n        perField.shrinkHash(numPostings);\n        fieldWriter.reset();\n      }\n      \n      if (termsHash != null) {\n        termsHash.reset();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","bugFix":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","6c18273ea5b3974d2f30117f46f1ae416c28f727"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dcc555744b1a581a4beccd0b75f8d3fe49735a2f","date":1367588265,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[String,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[String,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(Map<String,TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n      final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n      if (perField.termsHashPerField.bytesHash.size() > 0) {\n        allFields.add(perField);\n      }\n    }\n\n    final int numAllFields = allFields.size();\n\n    // Sort by field name\n    CollectionUtil.introSort(allFields);\n\n    final FieldsConsumer consumer = state.segmentInfo.getCodec().postingsFormat().fieldsConsumer(state);\n\n    boolean success = false;\n\n    try {\n      TermsHash termsHash = null;\n      \n      /*\n    Current writer chain:\n      FieldsConsumer\n        -> IMPL: FormatPostingsTermsDictWriter\n          -> TermsConsumer\n            -> IMPL: FormatPostingsTermsDictWriter.TermsWriter\n              -> DocsConsumer\n                -> IMPL: FormatPostingsDocsWriter\n                  -> PositionsConsumer\n                    -> IMPL: FormatPostingsPositionsWriter\n       */\n      \n      for (int fieldNumber = 0; fieldNumber < numAllFields; fieldNumber++) {\n        final FieldInfo fieldInfo = allFields.get(fieldNumber).fieldInfo;\n        \n        final FreqProxTermsWriterPerField fieldWriter = allFields.get(fieldNumber);\n\n        // If this field has postings then add them to the\n        // segment\n        fieldWriter.flush(fieldInfo.name, consumer, state);\n        \n        TermsHashPerField perField = fieldWriter.termsHashPerField;\n        assert termsHash == null || termsHash == perField.termsHash;\n        termsHash = perField.termsHash;\n        int numPostings = perField.bytesHash.size();\n        perField.reset();\n        perField.shrinkHash(numPostings);\n        fieldWriter.reset();\n      }\n      \n      if (termsHash != null) {\n        termsHash.reset();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void flush(Map<String,TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n      final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n      if (perField.termsHashPerField.bytesHash.size() > 0) {\n        allFields.add(perField);\n      }\n    }\n\n    final int numAllFields = allFields.size();\n\n    // Sort by field name\n    CollectionUtil.quickSort(allFields);\n\n    final FieldsConsumer consumer = state.segmentInfo.getCodec().postingsFormat().fieldsConsumer(state);\n\n    boolean success = false;\n\n    try {\n      TermsHash termsHash = null;\n      \n      /*\n    Current writer chain:\n      FieldsConsumer\n        -> IMPL: FormatPostingsTermsDictWriter\n          -> TermsConsumer\n            -> IMPL: FormatPostingsTermsDictWriter.TermsWriter\n              -> DocsConsumer\n                -> IMPL: FormatPostingsDocsWriter\n                  -> PositionsConsumer\n                    -> IMPL: FormatPostingsPositionsWriter\n       */\n      \n      for (int fieldNumber = 0; fieldNumber < numAllFields; fieldNumber++) {\n        final FieldInfo fieldInfo = allFields.get(fieldNumber).fieldInfo;\n        \n        final FreqProxTermsWriterPerField fieldWriter = allFields.get(fieldNumber);\n\n        // If this field has postings then add them to the\n        // segment\n        fieldWriter.flush(fieldInfo.name, consumer, state);\n        \n        TermsHashPerField perField = fieldWriter.termsHashPerField;\n        assert termsHash == null || termsHash == perField.termsHash;\n        termsHash = perField.termsHash;\n        int numPostings = perField.bytesHash.size();\n        perField.reset();\n        perField.shrinkHash(numPostings);\n        fieldWriter.reset();\n      }\n      \n      if (termsHash != null) {\n        termsHash.reset();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5","date":1379624229,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[String,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[String,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(Map<String,TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n      final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n      if (perField.termsHashPerField.bytesHash.size() > 0) {\n        perField.sortPostings();\n        assert perField.fieldInfo.isIndexed();\n        allFields.add(perField);\n      }\n    }\n\n    // Sort by field name\n    CollectionUtil.introSort(allFields);\n\n    Fields fields = new FreqProxFields(allFields);\n\n    applyDeletes(state, fields);\n\n    state.segmentInfo.getCodec().postingsFormat().fieldsConsumer(state).write(fields);\n  }\n\n","sourceOld":"  @Override\n  public void flush(Map<String,TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n      final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n      if (perField.termsHashPerField.bytesHash.size() > 0) {\n        allFields.add(perField);\n      }\n    }\n\n    final int numAllFields = allFields.size();\n\n    // Sort by field name\n    CollectionUtil.introSort(allFields);\n\n    final FieldsConsumer consumer = state.segmentInfo.getCodec().postingsFormat().fieldsConsumer(state);\n\n    boolean success = false;\n\n    try {\n      TermsHash termsHash = null;\n      \n      /*\n    Current writer chain:\n      FieldsConsumer\n        -> IMPL: FormatPostingsTermsDictWriter\n          -> TermsConsumer\n            -> IMPL: FormatPostingsTermsDictWriter.TermsWriter\n              -> DocsConsumer\n                -> IMPL: FormatPostingsDocsWriter\n                  -> PositionsConsumer\n                    -> IMPL: FormatPostingsPositionsWriter\n       */\n      \n      for (int fieldNumber = 0; fieldNumber < numAllFields; fieldNumber++) {\n        final FieldInfo fieldInfo = allFields.get(fieldNumber).fieldInfo;\n        \n        final FreqProxTermsWriterPerField fieldWriter = allFields.get(fieldNumber);\n\n        // If this field has postings then add them to the\n        // segment\n        fieldWriter.flush(fieldInfo.name, consumer, state);\n        \n        TermsHashPerField perField = fieldWriter.termsHashPerField;\n        assert termsHash == null || termsHash == perField.termsHash;\n        termsHash = perField.termsHash;\n        int numPostings = perField.bytesHash.size();\n        perField.reset();\n        perField.shrinkHash(numPostings);\n        fieldWriter.reset();\n      }\n      \n      if (termsHash != null) {\n        termsHash.reset();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[String,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[String,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(Map<String,TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n      final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n      if (perField.termsHashPerField.bytesHash.size() > 0) {\n        perField.sortPostings();\n        assert perField.fieldInfo.isIndexed();\n        allFields.add(perField);\n      }\n    }\n\n    // Sort by field name\n    CollectionUtil.introSort(allFields);\n\n    Fields fields = new FreqProxFields(allFields);\n\n    applyDeletes(state, fields);\n\n    state.segmentInfo.getCodec().postingsFormat().fieldsConsumer(state).write(fields);\n  }\n\n","sourceOld":"  @Override\n  public void flush(Map<String,TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n      final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n      if (perField.termsHashPerField.bytesHash.size() > 0) {\n        perField.sortPostings();\n        assert perField.fieldInfo.isIndexed();\n        allFields.add(perField);\n      }\n    }\n\n    // Sort by field name\n    CollectionUtil.introSort(allFields);\n\n    Fields fields = new FreqProxFields(allFields);\n\n    applyDeletes(state, fields);\n\n    state.segmentInfo.getCodec().postingsFormat().fieldsConsumer(state).write(fields);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"52c7e49be259508735752fba88085255014a6ecf","date":1398706273,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[String,TermsHashPerField],SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[String,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(Map<String,TermsHashPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    super.flush(fieldsToFlush, state);\n\n    // Gather all fields that saw any postings:\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<>();\n\n    for (TermsHashPerField f : fieldsToFlush.values()) {\n      final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n      if (perField.bytesHash.size() > 0) {\n        perField.sortPostings();\n        assert perField.fieldInfo.isIndexed();\n        allFields.add(perField);\n      }\n    }\n\n    // Sort by field name\n    CollectionUtil.introSort(allFields);\n\n    Fields fields = new FreqProxFields(allFields);\n\n    applyDeletes(state, fields);\n\n    state.segmentInfo.getCodec().postingsFormat().fieldsConsumer(state).write(fields);\n  }\n\n","sourceOld":"  @Override\n  public void flush(Map<String,TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n      final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n      if (perField.termsHashPerField.bytesHash.size() > 0) {\n        perField.sortPostings();\n        assert perField.fieldInfo.isIndexed();\n        allFields.add(perField);\n      }\n    }\n\n    // Sort by field name\n    CollectionUtil.introSort(allFields);\n\n    Fields fields = new FreqProxFields(allFields);\n\n    applyDeletes(state, fields);\n\n    state.segmentInfo.getCodec().postingsFormat().fieldsConsumer(state).write(fields);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3394716f52b34ab259ad5247e7595d9f9db6e935","date":1398791921,"type":5,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[String,TermsHashPerField],SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[String,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(Map<String,TermsHashPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    super.flush(fieldsToFlush, state);\n\n    // Gather all fields that saw any postings:\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<>();\n\n    for (TermsHashPerField f : fieldsToFlush.values()) {\n      final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n      if (perField.bytesHash.size() > 0) {\n        perField.sortPostings();\n        assert perField.fieldInfo.isIndexed();\n        allFields.add(perField);\n      }\n    }\n\n    // Sort by field name\n    CollectionUtil.introSort(allFields);\n\n    Fields fields = new FreqProxFields(allFields);\n\n    applyDeletes(state, fields);\n\n    state.segmentInfo.getCodec().postingsFormat().fieldsConsumer(state).write(fields);\n  }\n\n","sourceOld":"  @Override\n  public void flush(Map<String,TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n      final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n      if (perField.termsHashPerField.bytesHash.size() > 0) {\n        perField.sortPostings();\n        assert perField.fieldInfo.isIndexed();\n        allFields.add(perField);\n      }\n    }\n\n    // Sort by field name\n    CollectionUtil.introSort(allFields);\n\n    Fields fields = new FreqProxFields(allFields);\n\n    applyDeletes(state, fields);\n\n    state.segmentInfo.getCodec().postingsFormat().fieldsConsumer(state).write(fields);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","date":1398844771,"type":5,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[String,TermsHashPerField],SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter#flush(Map[String,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(Map<String,TermsHashPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    super.flush(fieldsToFlush, state);\n\n    // Gather all fields that saw any postings:\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<>();\n\n    for (TermsHashPerField f : fieldsToFlush.values()) {\n      final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n      if (perField.bytesHash.size() > 0) {\n        perField.sortPostings();\n        assert perField.fieldInfo.isIndexed();\n        allFields.add(perField);\n      }\n    }\n\n    // Sort by field name\n    CollectionUtil.introSort(allFields);\n\n    Fields fields = new FreqProxFields(allFields);\n\n    applyDeletes(state, fields);\n\n    state.segmentInfo.getCodec().postingsFormat().fieldsConsumer(state).write(fields);\n  }\n\n","sourceOld":"  @Override\n  public void flush(Map<String,TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    List<FreqProxTermsWriterPerField> allFields = new ArrayList<>();\n\n    for (TermsHashConsumerPerField f : fieldsToFlush.values()) {\n      final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;\n      if (perField.termsHashPerField.bytesHash.size() > 0) {\n        perField.sortPostings();\n        assert perField.fieldInfo.isIndexed();\n        allFields.add(perField);\n      }\n    }\n\n    // Sort by field name\n    CollectionUtil.introSort(allFields);\n\n    Fields fields = new FreqProxFields(allFields);\n\n    applyDeletes(state, fields);\n\n    state.segmentInfo.getCodec().postingsFormat().fieldsConsumer(state).write(fields);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","3394716f52b34ab259ad5247e7595d9f9db6e935"],"6647091125f681395cbde9bb2b7b947cc4ef9bb3":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"cd9165e54429bb5c99e75d5cb1c926cc98772456":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3394716f52b34ab259ad5247e7595d9f9db6e935":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","52c7e49be259508735752fba88085255014a6ecf"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","0cadbbc3b8df99c8c7acd19da62f6b35eb126c85"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["615ddbd81799980d0fdd95e0238e1c498b6f47b0","6647091125f681395cbde9bb2b7b947cc4ef9bb3"],"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5":["dcc555744b1a581a4beccd0b75f8d3fe49735a2f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"dcc555744b1a581a4beccd0b75f8d3fe49735a2f":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"52c7e49be259508735752fba88085255014a6ecf":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"0cadbbc3b8df99c8c7acd19da62f6b35eb126c85":["cd9165e54429bb5c99e75d5cb1c926cc98772456"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3394716f52b34ab259ad5247e7595d9f9db6e935"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","3394716f52b34ab259ad5247e7595d9f9db6e935","52c7e49be259508735752fba88085255014a6ecf"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":[],"6647091125f681395cbde9bb2b7b947cc4ef9bb3":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"cd9165e54429bb5c99e75d5cb1c926cc98772456":["0cadbbc3b8df99c8c7acd19da62f6b35eb126c85"],"3394716f52b34ab259ad5247e7595d9f9db6e935":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["6647091125f681395cbde9bb2b7b947cc4ef9bb3","d4d69c535930b5cce125cff868d40f6373dc27d4"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["dcc555744b1a581a4beccd0b75f8d3fe49735a2f"],"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cd9165e54429bb5c99e75d5cb1c926cc98772456","615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"dcc555744b1a581a4beccd0b75f8d3fe49735a2f":["519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5"],"52c7e49be259508735752fba88085255014a6ecf":["3394716f52b34ab259ad5247e7595d9f9db6e935"],"0cadbbc3b8df99c8c7acd19da62f6b35eb126c85":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}