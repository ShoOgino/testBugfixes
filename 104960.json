{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUnicodeWhitespaceTokenizer#testParamsFactory().mjava","commits":[{"id":"367e74558f41dfa2d24b323440dcb2d653ad1a29","date":1496009928,"type":0,"author":"Erick Erickson","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUnicodeWhitespaceTokenizer#testParamsFactory().mjava","pathOld":"/dev/null","sourceNew":"  public void testParamsFactory() throws IOException {\n    \n\n    // negative maxTokenLen\n    IllegalArgumentException iae = expectThrows(IllegalArgumentException.class, () ->\n        new WhitespaceTokenizerFactory(makeArgs(\"rule\", \"unicode\", \"maxTokenLen\", \"-1\")));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: -1\", iae.getMessage());\n\n    // zero maxTokenLen\n    iae = expectThrows(IllegalArgumentException.class, () ->\n        new WhitespaceTokenizerFactory(makeArgs(\"rule\", \"unicode\", \"maxTokenLen\", \"0\")));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 0\", iae.getMessage());\n\n    // Added random param, should throw illegal error\n    iae = expectThrows(IllegalArgumentException.class, () ->\n        new WhitespaceTokenizerFactory(makeArgs(\"rule\", \"unicode\", \"maxTokenLen\", \"255\", \"randomParam\", \"rValue\")));\n    assertEquals(\"Unknown parameters: {randomParam=rValue}\", iae.getMessage());\n\n    // tokeniser will split at 5, Token | izer, no matter what happens \n    WhitespaceTokenizerFactory factory = new WhitespaceTokenizerFactory(makeArgs(\"rule\", \"unicode\", \"maxTokenLen\", \"5\"));\n    AttributeFactory attributeFactory = newAttributeFactory();\n    Tokenizer tokenizer = factory.create(attributeFactory);\n    StringReader reader = new StringReader(\"Tokenizer \\ud801\\udc1ctest\");\n    tokenizer.setReader(reader);\n    assertTokenStreamContents(tokenizer, new String[]{\"Token\", \"izer\", \"\\ud801\\udc1ctes\", \"t\"});\n\n    // tokeniser will split at 2, To | ke | ni | ze | r, no matter what happens \n    factory = new WhitespaceTokenizerFactory(makeArgs(\"rule\", \"unicode\", \"maxTokenLen\", \"2\"));\n    attributeFactory = newAttributeFactory();\n    tokenizer = factory.create(attributeFactory);\n    reader = new StringReader(\"Tokenizer\\u00A0test\");\n    tokenizer.setReader(reader);\n    assertTokenStreamContents(tokenizer, new String[]{\"To\", \"ke\", \"ni\", \"ze\", \"r\", \"te\", \"st\"});\n\n    // tokeniser will split at 10, no matter what happens, \n    // but tokens' length are less than that\n    factory = new WhitespaceTokenizerFactory(makeArgs(\"rule\", \"unicode\", \"maxTokenLen\", \"10\"));\n    attributeFactory = newAttributeFactory();\n    tokenizer = factory.create(attributeFactory);\n    reader = new StringReader(\"Tokenizer\\u00A0test\");\n    tokenizer.setReader(reader);\n    assertTokenStreamContents(tokenizer, new String[]{\"Tokenizer\", \"test\"});\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d1f5728f32a4a256b36cfabd7a2636452f599bb9","date":1496231774,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUnicodeWhitespaceTokenizer#testParamsFactory().mjava","pathOld":"/dev/null","sourceNew":"  public void testParamsFactory() throws IOException {\n    \n\n    // negative maxTokenLen\n    IllegalArgumentException iae = expectThrows(IllegalArgumentException.class, () ->\n        new WhitespaceTokenizerFactory(makeArgs(\"rule\", \"unicode\", \"maxTokenLen\", \"-1\")));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: -1\", iae.getMessage());\n\n    // zero maxTokenLen\n    iae = expectThrows(IllegalArgumentException.class, () ->\n        new WhitespaceTokenizerFactory(makeArgs(\"rule\", \"unicode\", \"maxTokenLen\", \"0\")));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 0\", iae.getMessage());\n\n    // Added random param, should throw illegal error\n    iae = expectThrows(IllegalArgumentException.class, () ->\n        new WhitespaceTokenizerFactory(makeArgs(\"rule\", \"unicode\", \"maxTokenLen\", \"255\", \"randomParam\", \"rValue\")));\n    assertEquals(\"Unknown parameters: {randomParam=rValue}\", iae.getMessage());\n\n    // tokeniser will split at 5, Token | izer, no matter what happens \n    WhitespaceTokenizerFactory factory = new WhitespaceTokenizerFactory(makeArgs(\"rule\", \"unicode\", \"maxTokenLen\", \"5\"));\n    AttributeFactory attributeFactory = newAttributeFactory();\n    Tokenizer tokenizer = factory.create(attributeFactory);\n    StringReader reader = new StringReader(\"Tokenizer \\ud801\\udc1ctest\");\n    tokenizer.setReader(reader);\n    assertTokenStreamContents(tokenizer, new String[]{\"Token\", \"izer\", \"\\ud801\\udc1ctes\", \"t\"});\n\n    // tokeniser will split at 2, To | ke | ni | ze | r, no matter what happens \n    factory = new WhitespaceTokenizerFactory(makeArgs(\"rule\", \"unicode\", \"maxTokenLen\", \"2\"));\n    attributeFactory = newAttributeFactory();\n    tokenizer = factory.create(attributeFactory);\n    reader = new StringReader(\"Tokenizer\\u00A0test\");\n    tokenizer.setReader(reader);\n    assertTokenStreamContents(tokenizer, new String[]{\"To\", \"ke\", \"ni\", \"ze\", \"r\", \"te\", \"st\"});\n\n    // tokeniser will split at 10, no matter what happens, \n    // but tokens' length are less than that\n    factory = new WhitespaceTokenizerFactory(makeArgs(\"rule\", \"unicode\", \"maxTokenLen\", \"10\"));\n    attributeFactory = newAttributeFactory();\n    tokenizer = factory.create(attributeFactory);\n    reader = new StringReader(\"Tokenizer\\u00A0test\");\n    tokenizer.setReader(reader);\n    assertTokenStreamContents(tokenizer, new String[]{\"Tokenizer\", \"test\"});\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e9017cf144952056066919f1ebc7897ff9bd71b1","date":1496757600,"type":0,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUnicodeWhitespaceTokenizer#testParamsFactory().mjava","pathOld":"/dev/null","sourceNew":"  public void testParamsFactory() throws IOException {\n    \n\n    // negative maxTokenLen\n    IllegalArgumentException iae = expectThrows(IllegalArgumentException.class, () ->\n        new WhitespaceTokenizerFactory(makeArgs(\"rule\", \"unicode\", \"maxTokenLen\", \"-1\")));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: -1\", iae.getMessage());\n\n    // zero maxTokenLen\n    iae = expectThrows(IllegalArgumentException.class, () ->\n        new WhitespaceTokenizerFactory(makeArgs(\"rule\", \"unicode\", \"maxTokenLen\", \"0\")));\n    assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 0\", iae.getMessage());\n\n    // Added random param, should throw illegal error\n    iae = expectThrows(IllegalArgumentException.class, () ->\n        new WhitespaceTokenizerFactory(makeArgs(\"rule\", \"unicode\", \"maxTokenLen\", \"255\", \"randomParam\", \"rValue\")));\n    assertEquals(\"Unknown parameters: {randomParam=rValue}\", iae.getMessage());\n\n    // tokeniser will split at 5, Token | izer, no matter what happens \n    WhitespaceTokenizerFactory factory = new WhitespaceTokenizerFactory(makeArgs(\"rule\", \"unicode\", \"maxTokenLen\", \"5\"));\n    AttributeFactory attributeFactory = newAttributeFactory();\n    Tokenizer tokenizer = factory.create(attributeFactory);\n    StringReader reader = new StringReader(\"Tokenizer \\ud801\\udc1ctest\");\n    tokenizer.setReader(reader);\n    assertTokenStreamContents(tokenizer, new String[]{\"Token\", \"izer\", \"\\ud801\\udc1ctes\", \"t\"});\n\n    // tokeniser will split at 2, To | ke | ni | ze | r, no matter what happens \n    factory = new WhitespaceTokenizerFactory(makeArgs(\"rule\", \"unicode\", \"maxTokenLen\", \"2\"));\n    attributeFactory = newAttributeFactory();\n    tokenizer = factory.create(attributeFactory);\n    reader = new StringReader(\"Tokenizer\\u00A0test\");\n    tokenizer.setReader(reader);\n    assertTokenStreamContents(tokenizer, new String[]{\"To\", \"ke\", \"ni\", \"ze\", \"r\", \"te\", \"st\"});\n\n    // tokeniser will split at 10, no matter what happens, \n    // but tokens' length are less than that\n    factory = new WhitespaceTokenizerFactory(makeArgs(\"rule\", \"unicode\", \"maxTokenLen\", \"10\"));\n    attributeFactory = newAttributeFactory();\n    tokenizer = factory.create(attributeFactory);\n    reader = new StringReader(\"Tokenizer\\u00A0test\");\n    tokenizer.setReader(reader);\n    assertTokenStreamContents(tokenizer, new String[]{\"Tokenizer\", \"test\"});\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"e9017cf144952056066919f1ebc7897ff9bd71b1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","367e74558f41dfa2d24b323440dcb2d653ad1a29"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"367e74558f41dfa2d24b323440dcb2d653ad1a29":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"d1f5728f32a4a256b36cfabd7a2636452f599bb9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","367e74558f41dfa2d24b323440dcb2d653ad1a29"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d1f5728f32a4a256b36cfabd7a2636452f599bb9"]},"commit2Childs":{"e9017cf144952056066919f1ebc7897ff9bd71b1":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["e9017cf144952056066919f1ebc7897ff9bd71b1","367e74558f41dfa2d24b323440dcb2d653ad1a29","d1f5728f32a4a256b36cfabd7a2636452f599bb9"],"367e74558f41dfa2d24b323440dcb2d653ad1a29":["e9017cf144952056066919f1ebc7897ff9bd71b1","d1f5728f32a4a256b36cfabd7a2636452f599bb9"],"d1f5728f32a4a256b36cfabd7a2636452f599bb9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["e9017cf144952056066919f1ebc7897ff9bd71b1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}