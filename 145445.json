{"path":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","commits":[{"id":"3a2dfe27dcd813920f9df8dbd28079c6e6b9e93e","date":1159427430,"type":0,"author":"Wolfgang Hoschek","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    /*\n     * Note that this method signature avoids having a user call new\n     * o.a.l.d.Field(...) which would be much too expensive due to the\n     * String.intern() usage of that class.\n     * \n     * More often than not, String.intern() leads to serious performance\n     * degradations rather than improvements! If you're curious why, check\n     * out the JDK's native code, see how it oscillates multiple times back\n     * and forth between Java code and native code on each intern() call,\n     * only to end up using a plain vanilla java.util.HashMap on the Java\n     * heap for it's interned strings! String.equals() has a small cost\n     * compared to String.intern(), trust me. Application level interning\n     * (e.g. a HashMap per Directory/Index) typically leads to better\n     * solutions than frequent hidden low-level calls to String.intern().\n     * \n     * Perhaps with some luck, Lucene's Field.java (and Term.java) and\n     * cousins could be fixed to not use String.intern(). Sigh :-(\n     */\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap terms = new HashMap();\n      int numTokens = 0;\n      int pos = -1;\n      Token token;\n      \n      while ((token = stream.next()) != null) {\n        String term = token.termText();\n        if (term.length() == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        pos += token.getPositionIncrement();\n        \n        ArrayIntList positions = (ArrayIntList) terms.get(term);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(term, positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, token.startOffset(), token.endOffset());\n        }\n      }\n      \n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["171075f6e3fc0c1c7aced7429b945df3a436e343"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7e2cb543b41c145f33390f460ee743d6693c9c6c","date":1219243087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    /*\n     * Note that this method signature avoids having a user call new\n     * o.a.l.d.Field(...) which would be much too expensive due to the\n     * String.intern() usage of that class.\n     * \n     * More often than not, String.intern() leads to serious performance\n     * degradations rather than improvements! If you're curious why, check\n     * out the JDK's native code, see how it oscillates multiple times back\n     * and forth between Java code and native code on each intern() call,\n     * only to end up using a plain vanilla java.util.HashMap on the Java\n     * heap for it's interned strings! String.equals() has a small cost\n     * compared to String.intern(), trust me. Application level interning\n     * (e.g. a HashMap per Directory/Index) typically leads to better\n     * solutions than frequent hidden low-level calls to String.intern().\n     * \n     * Perhaps with some luck, Lucene's Field.java (and Term.java) and\n     * cousins could be fixed to not use String.intern(). Sigh :-(\n     */\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap terms = new HashMap();\n      int numTokens = 0;\n      int pos = -1;\n      final Token reusableToken = new Token();\n      for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {\n        String term = nextToken.term();\n        if (term.length() == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        pos += nextToken.getPositionIncrement();\n        \n        ArrayIntList positions = (ArrayIntList) terms.get(term);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(term, positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, nextToken.startOffset(), nextToken.endOffset());\n        }\n      }\n      \n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    /*\n     * Note that this method signature avoids having a user call new\n     * o.a.l.d.Field(...) which would be much too expensive due to the\n     * String.intern() usage of that class.\n     * \n     * More often than not, String.intern() leads to serious performance\n     * degradations rather than improvements! If you're curious why, check\n     * out the JDK's native code, see how it oscillates multiple times back\n     * and forth between Java code and native code on each intern() call,\n     * only to end up using a plain vanilla java.util.HashMap on the Java\n     * heap for it's interned strings! String.equals() has a small cost\n     * compared to String.intern(), trust me. Application level interning\n     * (e.g. a HashMap per Directory/Index) typically leads to better\n     * solutions than frequent hidden low-level calls to String.intern().\n     * \n     * Perhaps with some luck, Lucene's Field.java (and Term.java) and\n     * cousins could be fixed to not use String.intern(). Sigh :-(\n     */\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap terms = new HashMap();\n      int numTokens = 0;\n      int pos = -1;\n      Token token;\n      \n      while ((token = stream.next()) != null) {\n        String term = token.termText();\n        if (term.length() == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        pos += token.getPositionIncrement();\n        \n        ArrayIntList positions = (ArrayIntList) terms.get(term);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(term, positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, token.startOffset(), token.endOffset());\n        }\n      }\n      \n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d41ab32fc87982eab82a896cee390971b5c08fd2","date":1225735438,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    /*\n     * Note that this method signature avoids having a user call new\n     * o.a.l.d.Field(...) which would be much too expensive due to the\n     * String.intern() usage of that class.\n     * \n     * More often than not, String.intern() leads to serious performance\n     * degradations rather than improvements! If you're curious why, check\n     * out the JDK's native code, see how it oscillates multiple times back\n     * and forth between Java code and native code on each intern() call,\n     * only to end up using a plain vanilla java.util.HashMap on the Java\n     * heap for it's interned strings! String.equals() has a small cost\n     * compared to String.intern(), trust me. Application level interning\n     * (e.g. a HashMap per Directory/Index) typically leads to better\n     * solutions than frequent hidden low-level calls to String.intern().\n     * \n     * Perhaps with some luck, Lucene's Field.java (and Term.java) and\n     * cousins could be fixed to not use String.intern(). Sigh :-(\n     */\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap terms = new HashMap();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final Token reusableToken = new Token();\n      for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {\n        String term = nextToken.term();\n        if (term.length() == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = nextToken.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = (ArrayIntList) terms.get(term);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(term, positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, nextToken.startOffset(), nextToken.endOffset());\n        }\n      }\n      \n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    /*\n     * Note that this method signature avoids having a user call new\n     * o.a.l.d.Field(...) which would be much too expensive due to the\n     * String.intern() usage of that class.\n     * \n     * More often than not, String.intern() leads to serious performance\n     * degradations rather than improvements! If you're curious why, check\n     * out the JDK's native code, see how it oscillates multiple times back\n     * and forth between Java code and native code on each intern() call,\n     * only to end up using a plain vanilla java.util.HashMap on the Java\n     * heap for it's interned strings! String.equals() has a small cost\n     * compared to String.intern(), trust me. Application level interning\n     * (e.g. a HashMap per Directory/Index) typically leads to better\n     * solutions than frequent hidden low-level calls to String.intern().\n     * \n     * Perhaps with some luck, Lucene's Field.java (and Term.java) and\n     * cousins could be fixed to not use String.intern(). Sigh :-(\n     */\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap terms = new HashMap();\n      int numTokens = 0;\n      int pos = -1;\n      final Token reusableToken = new Token();\n      for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {\n        String term = nextToken.term();\n        if (term.length() == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        pos += nextToken.getPositionIncrement();\n        \n        ArrayIntList positions = (ArrayIntList) terms.get(term);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(term, positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, nextToken.startOffset(), nextToken.endOffset());\n        }\n      }\n      \n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9b5756469957918cac40a831acec9cf01c8c2bb3","date":1249167152,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    /*\n     * Note that this method signature avoids having a user call new\n     * o.a.l.d.Field(...) which would be much too expensive due to the\n     * String.intern() usage of that class.\n     * \n     * More often than not, String.intern() leads to serious performance\n     * degradations rather than improvements! If you're curious why, check\n     * out the JDK's native code, see how it oscillates multiple times back\n     * and forth between Java code and native code on each intern() call,\n     * only to end up using a plain vanilla java.util.HashMap on the Java\n     * heap for it's interned strings! String.equals() has a small cost\n     * compared to String.intern(), trust me. Application level interning\n     * (e.g. a HashMap per Directory/Index) typically leads to better\n     * solutions than frequent hidden low-level calls to String.intern().\n     * \n     * Perhaps with some luck, Lucene's Field.java (and Term.java) and\n     * cousins could be fixed to not use String.intern(). Sigh :-(\n     */\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap terms = new HashMap();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermAttribute termAtt = (TermAttribute) stream.addAttribute(TermAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = (PositionIncrementAttribute) stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = (OffsetAttribute) stream.addAttribute(OffsetAttribute.class);\n      \n      while (stream.incrementToken()) {\n        String term = termAtt.term();\n        if (term.length() == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = (ArrayIntList) terms.get(term);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(term, positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      \n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    /*\n     * Note that this method signature avoids having a user call new\n     * o.a.l.d.Field(...) which would be much too expensive due to the\n     * String.intern() usage of that class.\n     * \n     * More often than not, String.intern() leads to serious performance\n     * degradations rather than improvements! If you're curious why, check\n     * out the JDK's native code, see how it oscillates multiple times back\n     * and forth between Java code and native code on each intern() call,\n     * only to end up using a plain vanilla java.util.HashMap on the Java\n     * heap for it's interned strings! String.equals() has a small cost\n     * compared to String.intern(), trust me. Application level interning\n     * (e.g. a HashMap per Directory/Index) typically leads to better\n     * solutions than frequent hidden low-level calls to String.intern().\n     * \n     * Perhaps with some luck, Lucene's Field.java (and Term.java) and\n     * cousins could be fixed to not use String.intern(). Sigh :-(\n     */\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap terms = new HashMap();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final Token reusableToken = new Token();\n      for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {\n        String term = nextToken.term();\n        if (term.length() == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = nextToken.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = (ArrayIntList) terms.get(term);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(term, positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, nextToken.startOffset(), nextToken.endOffset());\n        }\n      }\n      \n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"99a7ff0fa358e0b2513ba76ee19b04663ff35be8","date":1249665593,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap terms = new HashMap();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermAttribute termAtt = (TermAttribute) stream.addAttribute(TermAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = (PositionIncrementAttribute) stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = (OffsetAttribute) stream.addAttribute(OffsetAttribute.class);\n      \n      while (stream.incrementToken()) {\n        String term = termAtt.term();\n        if (term.length() == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = (ArrayIntList) terms.get(term);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(term, positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      \n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    /*\n     * Note that this method signature avoids having a user call new\n     * o.a.l.d.Field(...) which would be much too expensive due to the\n     * String.intern() usage of that class.\n     * \n     * More often than not, String.intern() leads to serious performance\n     * degradations rather than improvements! If you're curious why, check\n     * out the JDK's native code, see how it oscillates multiple times back\n     * and forth between Java code and native code on each intern() call,\n     * only to end up using a plain vanilla java.util.HashMap on the Java\n     * heap for it's interned strings! String.equals() has a small cost\n     * compared to String.intern(), trust me. Application level interning\n     * (e.g. a HashMap per Directory/Index) typically leads to better\n     * solutions than frequent hidden low-level calls to String.intern().\n     * \n     * Perhaps with some luck, Lucene's Field.java (and Term.java) and\n     * cousins could be fixed to not use String.intern(). Sigh :-(\n     */\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap terms = new HashMap();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermAttribute termAtt = (TermAttribute) stream.addAttribute(TermAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = (PositionIncrementAttribute) stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = (OffsetAttribute) stream.addAttribute(OffsetAttribute.class);\n      \n      while (stream.incrementToken()) {\n        String term = termAtt.term();\n        if (term.length() == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = (ArrayIntList) terms.get(term);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(term, positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      \n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"171075f6e3fc0c1c7aced7429b945df3a436e343","date":1250841760,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap terms = new HashMap();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermAttribute termAtt = (TermAttribute) stream.addAttribute(TermAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = (PositionIncrementAttribute) stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = (OffsetAttribute) stream.addAttribute(OffsetAttribute.class);\n      \n      stream.reset();\n      while (stream.incrementToken()) {\n        String term = termAtt.term();\n        if (term.length() == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = (ArrayIntList) terms.get(term);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(term, positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap terms = new HashMap();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermAttribute termAtt = (TermAttribute) stream.addAttribute(TermAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = (PositionIncrementAttribute) stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = (OffsetAttribute) stream.addAttribute(OffsetAttribute.class);\n      \n      while (stream.incrementToken()) {\n        String term = termAtt.term();\n        if (term.length() == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = (ArrayIntList) terms.get(term);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(term, positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      \n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":["3a2dfe27dcd813920f9df8dbd28079c6e6b9e93e"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"24677ebe836c841fc7c198375595e77fcbc11468","date":1251328462,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap terms = new HashMap();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermAttribute termAtt = (TermAttribute) stream.addAttribute(TermAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = (PositionIncrementAttribute) stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = (OffsetAttribute) stream.addAttribute(OffsetAttribute.class);\n      \n      stream.reset();\n      while (stream.incrementToken()) {\n        String term = termAtt.term();\n        if (term.length() == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = (ArrayIntList) terms.get(term);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(term, positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap terms = new HashMap();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermAttribute termAtt = (TermAttribute) stream.addAttribute(TermAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = (PositionIncrementAttribute) stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = (OffsetAttribute) stream.addAttribute(OffsetAttribute.class);\n      \n      stream.reset();\n      while (stream.incrementToken()) {\n        String term = termAtt.term();\n        if (term.length() == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = (ArrayIntList) terms.get(term);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(term, positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8d78f014fded44fbde905f4f84cdc21907b371e8","date":1254383623,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap terms = new HashMap();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermAttribute termAtt = stream.addAttribute(TermAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      \n      stream.reset();\n      while (stream.incrementToken()) {\n        String term = termAtt.term();\n        if (term.length() == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = (ArrayIntList) terms.get(term);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(term, positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap terms = new HashMap();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermAttribute termAtt = (TermAttribute) stream.addAttribute(TermAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = (PositionIncrementAttribute) stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = (OffsetAttribute) stream.addAttribute(OffsetAttribute.class);\n      \n      stream.reset();\n      while (stream.incrementToken()) {\n        String term = termAtt.term();\n        if (term.length() == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = (ArrayIntList) terms.get(term);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(term, positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f11899016a0460a7ea2e4b008d002e1e75c7d867","date":1256772085,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<String,ArrayIntList> terms = new HashMap<String,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermAttribute termAtt = stream.addAttribute(TermAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      \n      stream.reset();\n      while (stream.incrementToken()) {\n        String term = termAtt.term();\n        if (term.length() == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(term);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(term, positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap terms = new HashMap();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermAttribute termAtt = stream.addAttribute(TermAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      \n      stream.reset();\n      while (stream.incrementToken()) {\n        String term = termAtt.term();\n        if (term.length() == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = (ArrayIntList) terms.get(term);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(term, positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<String,ArrayIntList> terms = new HashMap<String,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermAttribute termAtt = stream.addAttribute(TermAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      \n      stream.reset();\n      while (stream.incrementToken()) {\n        String term = termAtt.term();\n        if (term.length() == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(term);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(term, positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<String,ArrayIntList> terms = new HashMap<String,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermAttribute termAtt = stream.addAttribute(TermAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      \n      stream.reset();\n      while (stream.incrementToken()) {\n        String term = termAtt.term();\n        if (term.length() == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(term);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(term, positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"24677ebe836c841fc7c198375595e77fcbc11468":["171075f6e3fc0c1c7aced7429b945df3a436e343"],"7e2cb543b41c145f33390f460ee743d6693c9c6c":["3a2dfe27dcd813920f9df8dbd28079c6e6b9e93e"],"d41ab32fc87982eab82a896cee390971b5c08fd2":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"99a7ff0fa358e0b2513ba76ee19b04663ff35be8":["9b5756469957918cac40a831acec9cf01c8c2bb3"],"171075f6e3fc0c1c7aced7429b945df3a436e343":["99a7ff0fa358e0b2513ba76ee19b04663ff35be8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9b5756469957918cac40a831acec9cf01c8c2bb3":["d41ab32fc87982eab82a896cee390971b5c08fd2"],"8d78f014fded44fbde905f4f84cdc21907b371e8":["24677ebe836c841fc7c198375595e77fcbc11468"],"3a2dfe27dcd813920f9df8dbd28079c6e6b9e93e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["f11899016a0460a7ea2e4b008d002e1e75c7d867"],"f11899016a0460a7ea2e4b008d002e1e75c7d867":["8d78f014fded44fbde905f4f84cdc21907b371e8"]},"commit2Childs":{"24677ebe836c841fc7c198375595e77fcbc11468":["8d78f014fded44fbde905f4f84cdc21907b371e8"],"7e2cb543b41c145f33390f460ee743d6693c9c6c":["d41ab32fc87982eab82a896cee390971b5c08fd2"],"d41ab32fc87982eab82a896cee390971b5c08fd2":["9b5756469957918cac40a831acec9cf01c8c2bb3"],"99a7ff0fa358e0b2513ba76ee19b04663ff35be8":["171075f6e3fc0c1c7aced7429b945df3a436e343"],"171075f6e3fc0c1c7aced7429b945df3a436e343":["24677ebe836c841fc7c198375595e77fcbc11468"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a2dfe27dcd813920f9df8dbd28079c6e6b9e93e"],"9b5756469957918cac40a831acec9cf01c8c2bb3":["99a7ff0fa358e0b2513ba76ee19b04663ff35be8"],"8d78f014fded44fbde905f4f84cdc21907b371e8":["f11899016a0460a7ea2e4b008d002e1e75c7d867"],"3a2dfe27dcd813920f9df8dbd28079c6e6b9e93e":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"f11899016a0460a7ea2e4b008d002e1e75c7d867":["9454a6510e2db155fb01faa5c049b06ece95fab9"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}