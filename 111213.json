{"path":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","commits":[{"id":"eeefd99c477417e5c7c574228461ebafe92469d4","date":1166460329,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"/dev/null","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].list();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true);        \n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      Hits hits = searcher.search(new TermQuery(searchTerm));\n      assertEquals(\"first number of hits\", 57, hits.length());\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 1000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.list();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.list();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int method=0;method<3;method++) {\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        String testName = \"disk full test for method \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n\n        int cycleCount = 0;\n\n        while(!done) {\n\n          cycleCount++;\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false);\n          IOException err = null;\n\n          for(int x=0;x<2;x++) {\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug) {\n                System.out.println(\"\\ncycle: \" + methodName + \": \" + diskFree + \" bytes\");\n              }\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug) {\n                System.out.println(\"\\ncycle: \" + methodName + \", same writer: unlimited disk space\");\n              }\n            }\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i]);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n              }\n\n              if (1 == x) {\n                e.printStackTrace();\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Whether we succeeded or failed, check that all\n            // un-referenced files were in fact deleted (ie,\n            // we did not create garbage).  Just create a\n            // new IndexFileDeleter, have it delete\n            // unreferenced files, then verify that in fact\n            // no files were deleted:\n            String[] startFiles = dir.list();\n            SegmentInfos infos = new SegmentInfos();\n            infos.read(dir);\n            IndexFileDeleter d = new IndexFileDeleter(infos, dir);\n            d.findDeletableFiles();\n            d.deleteFiles();\n            String[] endFiles = dir.list();\n\n            Arrays.sort(startFiles);\n            Arrays.sort(endFiles);\n\n            /*\n              for(int i=0;i<startFiles.length;i++) {\n              System.out.println(\"  \" + i + \": \" + startFiles[i]);\n              }\n            */\n\n            if (!Arrays.equals(startFiles, endFiles)) {\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n                err.printStackTrace();\n              }\n              fail(methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes): before delete:\\n    \" + arrayToString(startFiles) + \"\\n  after delete:\\n    \" + arrayToString(endFiles));\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir);\n            } catch (IOException e) {\n              e.printStackTrace();\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace();\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm));\n            } catch (IOException e) {\n              e.printStackTrace();\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length();\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace();\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (result == END_COUNT) {\n              break;\n            }\n          }\n\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                     \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n\n          writer.close();\n          dir.close();\n\n          // Try again with 1000 more bytes of free space:\n          diskFree += 1000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["d54c7739bfe85d68f0352823cd70c10c619fad38","d54c7739bfe85d68f0352823cd70c10c619fad38","6456e4ba6cdce20f5615173eb208f063c8cb74b7","6456e4ba6cdce20f5615173eb208f063c8cb74b7"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8b6187898fc4413ccd18229711786550a280383c","date":1173776782,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].list();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true);        \n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      Hits hits = searcher.search(new TermQuery(searchTerm));\n      assertEquals(\"first number of hits\", 57, hits.length());\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.list();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.list();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<6;iter++) {\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean autoCommit = iter % 2 == 0;\n        int method = iter/2;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        String testName = \"disk full test for method \" + methodName + \" with disk full at \" + diskFree + \" bytes with autoCommit = \" + autoCommit;\n\n        int cycleCount = 0;\n\n        while(!done) {\n\n          cycleCount++;\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false);\n          IOException err = null;\n\n          for(int x=0;x<2;x++) {\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug) {\n                System.out.println(\"\\ncycle: \" + methodName + \": \" + diskFree + \" bytes\");\n              }\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug) {\n                System.out.println(\"\\ncycle: \" + methodName + \", same writer: unlimited disk space\");\n              }\n            }\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i]);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n              }\n\n              if (1 == x) {\n                e.printStackTrace();\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            if (autoCommit) {\n\n              // Whether we succeeded or failed, check that\n              // all un-referenced files were in fact\n              // deleted (ie, we did not create garbage).\n              // Only check this when autoCommit is true:\n              // when it's false, it's expected that there\n              // are unreferenced files (ie they won't be\n              // referenced until the \"commit on close\").\n              // Just create a new IndexFileDeleter, have it\n              // delete unreferenced files, then verify that\n              // in fact no files were deleted:\n\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n              }\n              String message = methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes)\";\n              assertNoUnreferencedFiles(dir, message);\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir);\n            } catch (IOException e) {\n              e.printStackTrace();\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (autoCommit && result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              } else if (!autoCommit && result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" [autoCommit = false]\");\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace();\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm));\n            } catch (IOException e) {\n              e.printStackTrace();\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length();\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace();\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          writer.close();\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].list();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true);        \n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      Hits hits = searcher.search(new TermQuery(searchTerm));\n      assertEquals(\"first number of hits\", 57, hits.length());\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 1000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.list();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.list();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int method=0;method<3;method++) {\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        String testName = \"disk full test for method \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n\n        int cycleCount = 0;\n\n        while(!done) {\n\n          cycleCount++;\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false);\n          IOException err = null;\n\n          for(int x=0;x<2;x++) {\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug) {\n                System.out.println(\"\\ncycle: \" + methodName + \": \" + diskFree + \" bytes\");\n              }\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug) {\n                System.out.println(\"\\ncycle: \" + methodName + \", same writer: unlimited disk space\");\n              }\n            }\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i]);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n              }\n\n              if (1 == x) {\n                e.printStackTrace();\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Whether we succeeded or failed, check that all\n            // un-referenced files were in fact deleted (ie,\n            // we did not create garbage).  Just create a\n            // new IndexFileDeleter, have it delete\n            // unreferenced files, then verify that in fact\n            // no files were deleted:\n            String[] startFiles = dir.list();\n            SegmentInfos infos = new SegmentInfos();\n            infos.read(dir);\n            IndexFileDeleter d = new IndexFileDeleter(infos, dir);\n            d.findDeletableFiles();\n            d.deleteFiles();\n            String[] endFiles = dir.list();\n\n            Arrays.sort(startFiles);\n            Arrays.sort(endFiles);\n\n            /*\n              for(int i=0;i<startFiles.length;i++) {\n              System.out.println(\"  \" + i + \": \" + startFiles[i]);\n              }\n            */\n\n            if (!Arrays.equals(startFiles, endFiles)) {\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n                err.printStackTrace();\n              }\n              fail(methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes): before delete:\\n    \" + arrayToString(startFiles) + \"\\n  after delete:\\n    \" + arrayToString(endFiles));\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir);\n            } catch (IOException e) {\n              e.printStackTrace();\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace();\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm));\n            } catch (IOException e) {\n              e.printStackTrace();\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length();\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace();\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (result == END_COUNT) {\n              break;\n            }\n          }\n\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                     \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n\n          writer.close();\n          dir.close();\n\n          // Try again with 1000 more bytes of free space:\n          diskFree += 1000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":["6456e4ba6cdce20f5615173eb208f063c8cb74b7","6456e4ba6cdce20f5615173eb208f063c8cb74b7"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6456e4ba6cdce20f5615173eb208f063c8cb74b7","date":1174828735,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].list();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true);        \n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      Hits hits = searcher.search(new TermQuery(searchTerm));\n      assertEquals(\"first number of hits\", 57, hits.length());\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.list();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.list();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<6;iter++) {\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean autoCommit = iter % 2 == 0;\n        int method = iter/2;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        int cycleCount = 0;\n\n        while(!done) {\n\n          cycleCount++;\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false);\n          IOException err = null;\n\n          for(int x=0;x<2;x++) {\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes autoCommit=\" + autoCommit;\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space autoCommit=\" + autoCommit;\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i]);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                // e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            if (autoCommit) {\n\n              // Whether we succeeded or failed, check that\n              // all un-referenced files were in fact\n              // deleted (ie, we did not create garbage).\n              // Only check this when autoCommit is true:\n              // when it's false, it's expected that there\n              // are unreferenced files (ie they won't be\n              // referenced until the \"commit on close\").\n              // Just create a new IndexFileDeleter, have it\n              // delete unreferenced files, then verify that\n              // in fact no files were deleted:\n\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n              }\n              String message = methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes)\";\n              assertNoUnreferencedFiles(dir, message);\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (autoCommit && result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              } else if (!autoCommit && result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" [autoCommit = false]\");\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm));\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length();\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          writer.close();\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].list();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true);        \n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      Hits hits = searcher.search(new TermQuery(searchTerm));\n      assertEquals(\"first number of hits\", 57, hits.length());\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.list();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.list();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<6;iter++) {\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean autoCommit = iter % 2 == 0;\n        int method = iter/2;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        String testName = \"disk full test for method \" + methodName + \" with disk full at \" + diskFree + \" bytes with autoCommit = \" + autoCommit;\n\n        int cycleCount = 0;\n\n        while(!done) {\n\n          cycleCount++;\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false);\n          IOException err = null;\n\n          for(int x=0;x<2;x++) {\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug) {\n                System.out.println(\"\\ncycle: \" + methodName + \": \" + diskFree + \" bytes\");\n              }\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug) {\n                System.out.println(\"\\ncycle: \" + methodName + \", same writer: unlimited disk space\");\n              }\n            }\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i]);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n              }\n\n              if (1 == x) {\n                e.printStackTrace();\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            if (autoCommit) {\n\n              // Whether we succeeded or failed, check that\n              // all un-referenced files were in fact\n              // deleted (ie, we did not create garbage).\n              // Only check this when autoCommit is true:\n              // when it's false, it's expected that there\n              // are unreferenced files (ie they won't be\n              // referenced until the \"commit on close\").\n              // Just create a new IndexFileDeleter, have it\n              // delete unreferenced files, then verify that\n              // in fact no files were deleted:\n\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n              }\n              String message = methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes)\";\n              assertNoUnreferencedFiles(dir, message);\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir);\n            } catch (IOException e) {\n              e.printStackTrace();\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (autoCommit && result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              } else if (!autoCommit && result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" [autoCommit = false]\");\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace();\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm));\n            } catch (IOException e) {\n              e.printStackTrace();\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length();\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace();\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          writer.close();\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":["eeefd99c477417e5c7c574228461ebafe92469d4","8b6187898fc4413ccd18229711786550a280383c"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b1405362241b561f5590ff4a87d5d6e173bcd9cf","date":1190107634,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].list();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true);        \n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      Hits hits = searcher.search(new TermQuery(searchTerm));\n      assertEquals(\"first number of hits\", 57, hits.length());\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.list();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.list();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<6;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean autoCommit = iter % 2 == 0;\n        int method = iter/2;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        int cycleCount = 0;\n\n        while(!done) {\n\n          cycleCount++;\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes autoCommit=\" + autoCommit;\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space autoCommit=\" + autoCommit;\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i]);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (autoCommit) {\n\n              // Whether we succeeded or failed, check that\n              // all un-referenced files were in fact\n              // deleted (ie, we did not create garbage).\n              // Only check this when autoCommit is true:\n              // when it's false, it's expected that there\n              // are unreferenced files (ie they won't be\n              // referenced until the \"commit on close\").\n              // Just create a new IndexFileDeleter, have it\n              // delete unreferenced files, then verify that\n              // in fact no files were deleted:\n\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n              }\n              String message = methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes)\";\n              assertNoUnreferencedFiles(dir, message);\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (autoCommit && result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              } else if (!autoCommit && result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" [autoCommit = false]\");\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm));\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length();\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].list();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true);        \n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      Hits hits = searcher.search(new TermQuery(searchTerm));\n      assertEquals(\"first number of hits\", 57, hits.length());\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.list();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.list();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<6;iter++) {\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean autoCommit = iter % 2 == 0;\n        int method = iter/2;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        int cycleCount = 0;\n\n        while(!done) {\n\n          cycleCount++;\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false);\n          IOException err = null;\n\n          for(int x=0;x<2;x++) {\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes autoCommit=\" + autoCommit;\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space autoCommit=\" + autoCommit;\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i]);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                // e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            if (autoCommit) {\n\n              // Whether we succeeded or failed, check that\n              // all un-referenced files were in fact\n              // deleted (ie, we did not create garbage).\n              // Only check this when autoCommit is true:\n              // when it's false, it's expected that there\n              // are unreferenced files (ie they won't be\n              // referenced until the \"commit on close\").\n              // Just create a new IndexFileDeleter, have it\n              // delete unreferenced files, then verify that\n              // in fact no files were deleted:\n\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n              }\n              String message = methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes)\";\n              assertNoUnreferencedFiles(dir, message);\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (autoCommit && result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              } else if (!autoCommit && result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" [autoCommit = false]\");\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm));\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length();\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          writer.close();\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"741a5cca05cabe1e7482410a29e563a08379251a","date":1196676550,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].list();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true);        \n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      Hits hits = searcher.search(new TermQuery(searchTerm));\n      assertEquals(\"first number of hits\", 57, hits.length());\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.list();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.list();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<6;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean autoCommit = iter % 2 == 0;\n        int method = iter/2;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes autoCommit=\" + autoCommit;\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space autoCommit=\" + autoCommit;\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i]);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (autoCommit) {\n\n              // Whether we succeeded or failed, check that\n              // all un-referenced files were in fact\n              // deleted (ie, we did not create garbage).\n              // Only check this when autoCommit is true:\n              // when it's false, it's expected that there\n              // are unreferenced files (ie they won't be\n              // referenced until the \"commit on close\").\n              // Just create a new IndexFileDeleter, have it\n              // delete unreferenced files, then verify that\n              // in fact no files were deleted:\n\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n              }\n              String message = methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes)\";\n              assertNoUnreferencedFiles(dir, message);\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (autoCommit && result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              } else if (!autoCommit && result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" [autoCommit = false]\");\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm));\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length();\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].list();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true);        \n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      Hits hits = searcher.search(new TermQuery(searchTerm));\n      assertEquals(\"first number of hits\", 57, hits.length());\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.list();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.list();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<6;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean autoCommit = iter % 2 == 0;\n        int method = iter/2;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        int cycleCount = 0;\n\n        while(!done) {\n\n          cycleCount++;\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes autoCommit=\" + autoCommit;\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space autoCommit=\" + autoCommit;\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i]);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (autoCommit) {\n\n              // Whether we succeeded or failed, check that\n              // all un-referenced files were in fact\n              // deleted (ie, we did not create garbage).\n              // Only check this when autoCommit is true:\n              // when it's false, it's expected that there\n              // are unreferenced files (ie they won't be\n              // referenced until the \"commit on close\").\n              // Just create a new IndexFileDeleter, have it\n              // delete unreferenced files, then verify that\n              // in fact no files were deleted:\n\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n              }\n              String message = methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes)\";\n              assertNoUnreferencedFiles(dir, message);\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (autoCommit && result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              } else if (!autoCommit && result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" [autoCommit = false]\");\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm));\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length();\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fd9aba6db0f2adde620bb61f591ed18dcfee36ac","date":1201778618,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].list();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      Hits hits = searcher.search(new TermQuery(searchTerm));\n      assertEquals(\"first number of hits\", 57, hits.length());\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.list();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.list();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<6;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean autoCommit = iter % 2 == 0;\n        int method = iter/2;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.LIMITED);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes autoCommit=\" + autoCommit;\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space autoCommit=\" + autoCommit;\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i]);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (autoCommit) {\n\n              // Whether we succeeded or failed, check that\n              // all un-referenced files were in fact\n              // deleted (ie, we did not create garbage).\n              // Only check this when autoCommit is true:\n              // when it's false, it's expected that there\n              // are unreferenced files (ie they won't be\n              // referenced until the \"commit on close\").\n              // Just create a new IndexFileDeleter, have it\n              // delete unreferenced files, then verify that\n              // in fact no files were deleted:\n\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n              }\n              String message = methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes)\";\n              assertNoUnreferencedFiles(dir, message);\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (autoCommit && result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              } else if (!autoCommit && result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" [autoCommit = false]\");\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm));\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length();\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].list();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true);        \n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      Hits hits = searcher.search(new TermQuery(searchTerm));\n      assertEquals(\"first number of hits\", 57, hits.length());\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.list();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.list();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<6;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean autoCommit = iter % 2 == 0;\n        int method = iter/2;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes autoCommit=\" + autoCommit;\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space autoCommit=\" + autoCommit;\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i]);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (autoCommit) {\n\n              // Whether we succeeded or failed, check that\n              // all un-referenced files were in fact\n              // deleted (ie, we did not create garbage).\n              // Only check this when autoCommit is true:\n              // when it's false, it's expected that there\n              // are unreferenced files (ie they won't be\n              // referenced until the \"commit on close\").\n              // Just create a new IndexFileDeleter, have it\n              // delete unreferenced files, then verify that\n              // in fact no files were deleted:\n\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n              }\n              String message = methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes)\";\n              assertNoUnreferencedFiles(dir, message);\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (autoCommit && result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              } else if (!autoCommit && result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" [autoCommit = false]\");\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm));\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length();\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5af07783dbc171e26a694c4f7d735e30c2769faa","date":1211569075,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].list();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.list();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.list();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<6;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean autoCommit = iter % 2 == 0;\n        int method = iter/2;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.LIMITED);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes autoCommit=\" + autoCommit;\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space autoCommit=\" + autoCommit;\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i]);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (autoCommit) {\n\n              // Whether we succeeded or failed, check that\n              // all un-referenced files were in fact\n              // deleted (ie, we did not create garbage).\n              // Only check this when autoCommit is true:\n              // when it's false, it's expected that there\n              // are unreferenced files (ie they won't be\n              // referenced until the \"commit on close\").\n              // Just create a new IndexFileDeleter, have it\n              // delete unreferenced files, then verify that\n              // in fact no files were deleted:\n\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n              }\n              String message = methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes)\";\n              assertNoUnreferencedFiles(dir, message);\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (autoCommit && result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              } else if (!autoCommit && result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" [autoCommit = false]\");\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].list();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      Hits hits = searcher.search(new TermQuery(searchTerm));\n      assertEquals(\"first number of hits\", 57, hits.length());\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.list();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.list();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<6;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean autoCommit = iter % 2 == 0;\n        int method = iter/2;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.LIMITED);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes autoCommit=\" + autoCommit;\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space autoCommit=\" + autoCommit;\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i]);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (autoCommit) {\n\n              // Whether we succeeded or failed, check that\n              // all un-referenced files were in fact\n              // deleted (ie, we did not create garbage).\n              // Only check this when autoCommit is true:\n              // when it's false, it's expected that there\n              // are unreferenced files (ie they won't be\n              // referenced until the \"commit on close\").\n              // Just create a new IndexFileDeleter, have it\n              // delete unreferenced files, then verify that\n              // in fact no files were deleted:\n\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n              }\n              String message = methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes)\";\n              assertNoUnreferencedFiles(dir, message);\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (autoCommit && result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              } else if (!autoCommit && result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" [autoCommit = false]\");\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm));\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length();\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2586f96f60332eb97ecd2934b0763791462568b2","date":1220116589,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].list();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.list();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.list();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<6;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean autoCommit = iter % 2 == 0;\n        int method = iter/2;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.LIMITED);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes autoCommit=\" + autoCommit;\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space autoCommit=\" + autoCommit;\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i]);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (autoCommit) {\n\n              // Whether we succeeded or failed, check that\n              // all un-referenced files were in fact\n              // deleted (ie, we did not create garbage).\n              // Only check this when autoCommit is true:\n              // when it's false, it's expected that there\n              // are unreferenced files (ie they won't be\n              // referenced until the \"commit on close\").\n              // Just create a new IndexFileDeleter, have it\n              // delete unreferenced files, then verify that\n              // in fact no files were deleted:\n\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n              }\n              String message = methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes)\";\n              assertNoUnreferencedFiles(dir, message);\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (autoCommit && result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              } else if (!autoCommit && result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" [autoCommit = false]\");\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].list();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.list();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.list();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<6;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean autoCommit = iter % 2 == 0;\n        int method = iter/2;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.LIMITED);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes autoCommit=\" + autoCommit;\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space autoCommit=\" + autoCommit;\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i]);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (autoCommit) {\n\n              // Whether we succeeded or failed, check that\n              // all un-referenced files were in fact\n              // deleted (ie, we did not create garbage).\n              // Only check this when autoCommit is true:\n              // when it's false, it's expected that there\n              // are unreferenced files (ie they won't be\n              // referenced until the \"commit on close\").\n              // Just create a new IndexFileDeleter, have it\n              // delete unreferenced files, then verify that\n              // in fact no files were deleted:\n\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n              }\n              String message = methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes)\";\n              assertNoUnreferencedFiles(dir, message);\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (autoCommit && result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              } else if (!autoCommit && result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" [autoCommit = false]\");\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c4ae99f08f69aa3acba7cd75134e8447eb747559","date":1222344278,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].list();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.list();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.list();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<6;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean autoCommit = iter % 2 == 0;\n        int method = iter/2;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes autoCommit=\" + autoCommit;\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space autoCommit=\" + autoCommit;\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i]);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (autoCommit) {\n\n              // Whether we succeeded or failed, check that\n              // all un-referenced files were in fact\n              // deleted (ie, we did not create garbage).\n              // Only check this when autoCommit is true:\n              // when it's false, it's expected that there\n              // are unreferenced files (ie they won't be\n              // referenced until the \"commit on close\").\n              // Just create a new IndexFileDeleter, have it\n              // delete unreferenced files, then verify that\n              // in fact no files were deleted:\n\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n              }\n              String message = methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes)\";\n              assertNoUnreferencedFiles(dir, message);\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (autoCommit && result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              } else if (!autoCommit && result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" [autoCommit = false]\");\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].list();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.list();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.list();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<6;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean autoCommit = iter % 2 == 0;\n        int method = iter/2;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.LIMITED);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes autoCommit=\" + autoCommit;\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space autoCommit=\" + autoCommit;\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i]);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (autoCommit) {\n\n              // Whether we succeeded or failed, check that\n              // all un-referenced files were in fact\n              // deleted (ie, we did not create garbage).\n              // Only check this when autoCommit is true:\n              // when it's false, it's expected that there\n              // are unreferenced files (ie they won't be\n              // referenced until the \"commit on close\").\n              // Just create a new IndexFileDeleter, have it\n              // delete unreferenced files, then verify that\n              // in fact no files were deleted:\n\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n              }\n              String message = methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes)\";\n              assertNoUnreferencedFiles(dir, message);\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (autoCommit && result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              } else if (!autoCommit && result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" [autoCommit = false]\");\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d54c7739bfe85d68f0352823cd70c10c619fad38","date":1228496593,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<6;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean autoCommit = iter % 2 == 0;\n        int method = iter/2;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes autoCommit=\" + autoCommit;\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space autoCommit=\" + autoCommit;\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i]);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (autoCommit) {\n\n              // Whether we succeeded or failed, check that\n              // all un-referenced files were in fact\n              // deleted (ie, we did not create garbage).\n              // Only check this when autoCommit is true:\n              // when it's false, it's expected that there\n              // are unreferenced files (ie they won't be\n              // referenced until the \"commit on close\").\n              // Just create a new IndexFileDeleter, have it\n              // delete unreferenced files, then verify that\n              // in fact no files were deleted:\n\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n              }\n              String message = methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes)\";\n              assertNoUnreferencedFiles(dir, message);\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (autoCommit && result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              } else if (!autoCommit && result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" [autoCommit = false]\");\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].list();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.list();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.list();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<6;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean autoCommit = iter % 2 == 0;\n        int method = iter/2;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes autoCommit=\" + autoCommit;\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space autoCommit=\" + autoCommit;\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i]);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (autoCommit) {\n\n              // Whether we succeeded or failed, check that\n              // all un-referenced files were in fact\n              // deleted (ie, we did not create garbage).\n              // Only check this when autoCommit is true:\n              // when it's false, it's expected that there\n              // are unreferenced files (ie they won't be\n              // referenced until the \"commit on close\").\n              // Just create a new IndexFileDeleter, have it\n              // delete unreferenced files, then verify that\n              // in fact no files were deleted:\n\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n              }\n              String message = methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes)\";\n              assertNoUnreferencedFiles(dir, message);\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (autoCommit && result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              } else if (!autoCommit && result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" [autoCommit = false]\");\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":["eeefd99c477417e5c7c574228461ebafe92469d4"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3215ae1377fc1ca1790921d75dd39cb764743b85","date":1237371771,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#xxxtestAddIndexOnDiskFull().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void xxxtestAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<6;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean autoCommit = iter % 2 == 0;\n        int method = iter/2;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes autoCommit=\" + autoCommit;\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space autoCommit=\" + autoCommit;\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i]);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (autoCommit) {\n\n              // Whether we succeeded or failed, check that\n              // all un-referenced files were in fact\n              // deleted (ie, we did not create garbage).\n              // Only check this when autoCommit is true:\n              // when it's false, it's expected that there\n              // are unreferenced files (ie they won't be\n              // referenced until the \"commit on close\").\n              // Just create a new IndexFileDeleter, have it\n              // delete unreferenced files, then verify that\n              // in fact no files were deleted:\n\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n              }\n              String message = methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes)\";\n              assertNoUnreferencedFiles(dir, message);\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (autoCommit && result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              } else if (!autoCommit && result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" [autoCommit = false]\");\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<6;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean autoCommit = iter % 2 == 0;\n        int method = iter/2;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes autoCommit=\" + autoCommit;\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space autoCommit=\" + autoCommit;\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i]);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (autoCommit) {\n\n              // Whether we succeeded or failed, check that\n              // all un-referenced files were in fact\n              // deleted (ie, we did not create garbage).\n              // Only check this when autoCommit is true:\n              // when it's false, it's expected that there\n              // are unreferenced files (ie they won't be\n              // referenced until the \"commit on close\").\n              // Just create a new IndexFileDeleter, have it\n              // delete unreferenced files, then verify that\n              // in fact no files were deleted:\n\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n              }\n              String message = methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes)\";\n              assertNoUnreferencedFiles(dir, message);\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (autoCommit && result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              } else if (!autoCommit && result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" [autoCommit = false]\");\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"09c482d1e63332617181729a225b215c452d8a79","date":1237396006,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#xxxtestAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<6;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean autoCommit = iter % 2 == 0;\n        int method = iter/2;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes autoCommit=\" + autoCommit;\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space autoCommit=\" + autoCommit;\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i]);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (autoCommit) {\n\n              // Whether we succeeded or failed, check that\n              // all un-referenced files were in fact\n              // deleted (ie, we did not create garbage).\n              // Only check this when autoCommit is true:\n              // when it's false, it's expected that there\n              // are unreferenced files (ie they won't be\n              // referenced until the \"commit on close\").\n              // Just create a new IndexFileDeleter, have it\n              // delete unreferenced files, then verify that\n              // in fact no files were deleted:\n\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n              }\n              String message = methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes)\";\n              assertNoUnreferencedFiles(dir, message);\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (autoCommit && result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              } else if (!autoCommit && result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" [autoCommit = false]\");\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void xxxtestAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<6;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean autoCommit = iter % 2 == 0;\n        int method = iter/2;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes autoCommit=\" + autoCommit;\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space autoCommit=\" + autoCommit;\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i]);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (autoCommit) {\n\n              // Whether we succeeded or failed, check that\n              // all un-referenced files were in fact\n              // deleted (ie, we did not create garbage).\n              // Only check this when autoCommit is true:\n              // when it's false, it's expected that there\n              // are unreferenced files (ie they won't be\n              // referenced until the \"commit on close\").\n              // Just create a new IndexFileDeleter, have it\n              // delete unreferenced files, then verify that\n              // in fact no files were deleted:\n\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n              }\n              String message = methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes)\";\n              assertNoUnreferencedFiles(dir, message);\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (autoCommit && result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              } else if (!autoCommit && result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" [autoCommit = false]\");\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e8d1458a2543cbd30cbfe7929be4dcb5c5251659","date":1254582241,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<6;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean autoCommit = iter % 2 == 0;\n        int method = iter/2;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes autoCommit=\" + autoCommit;\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space autoCommit=\" + autoCommit;\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (autoCommit) {\n\n              // Whether we succeeded or failed, check that\n              // all un-referenced files were in fact\n              // deleted (ie, we did not create garbage).\n              // Only check this when autoCommit is true:\n              // when it's false, it's expected that there\n              // are unreferenced files (ie they won't be\n              // referenced until the \"commit on close\").\n              // Just create a new IndexFileDeleter, have it\n              // delete unreferenced files, then verify that\n              // in fact no files were deleted:\n\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n              }\n              String message = methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes)\";\n              assertNoUnreferencedFiles(dir, message);\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (autoCommit && result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              } else if (!autoCommit && result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" [autoCommit = false]\");\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<6;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean autoCommit = iter % 2 == 0;\n        int method = iter/2;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes autoCommit=\" + autoCommit;\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space autoCommit=\" + autoCommit;\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i]);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (autoCommit) {\n\n              // Whether we succeeded or failed, check that\n              // all un-referenced files were in fact\n              // deleted (ie, we did not create garbage).\n              // Only check this when autoCommit is true:\n              // when it's false, it's expected that there\n              // are unreferenced files (ie they won't be\n              // referenced until the \"commit on close\").\n              // Just create a new IndexFileDeleter, have it\n              // delete unreferenced files, then verify that\n              // in fact no files were deleted:\n\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n              }\n              String message = methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes)\";\n              assertNoUnreferencedFiles(dir, message);\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (autoCommit && result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              } else if (!autoCommit && result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" [autoCommit = false]\");\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a046c0c310bc77931fc8441bd920053b607dd14","date":1254584734,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<6;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean autoCommit = iter % 2 == 0;\n        int method = iter/2;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes autoCommit=\" + autoCommit;\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space autoCommit=\" + autoCommit;\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (autoCommit) {\n\n              // Whether we succeeded or failed, check that\n              // all un-referenced files were in fact\n              // deleted (ie, we did not create garbage).\n              // Only check this when autoCommit is true:\n              // when it's false, it's expected that there\n              // are unreferenced files (ie they won't be\n              // referenced until the \"commit on close\").\n              // Just create a new IndexFileDeleter, have it\n              // delete unreferenced files, then verify that\n              // in fact no files were deleted:\n\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n              }\n              String message = methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes)\";\n              assertNoUnreferencedFiles(dir, message);\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (autoCommit && result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              } else if (!autoCommit && result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" [autoCommit = false]\");\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<6;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean autoCommit = iter % 2 == 0;\n        int method = iter/2;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes autoCommit=\" + autoCommit;\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space autoCommit=\" + autoCommit;\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i]);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (autoCommit) {\n\n              // Whether we succeeded or failed, check that\n              // all un-referenced files were in fact\n              // deleted (ie, we did not create garbage).\n              // Only check this when autoCommit is true:\n              // when it's false, it's expected that there\n              // are unreferenced files (ie they won't be\n              // referenced until the \"commit on close\").\n              // Just create a new IndexFileDeleter, have it\n              // delete unreferenced files, then verify that\n              // in fact no files were deleted:\n\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n              }\n              String message = methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes)\";\n              assertNoUnreferencedFiles(dir, message);\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (autoCommit && result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              } else if (!autoCommit && result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" [autoCommit = false]\");\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"87c966e9308847938a7c905c2e46a56d8df788b8","date":1255035452,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.UNLIMITED);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<6;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        boolean autoCommit = iter % 2 == 0;\n        int method = iter/2;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes autoCommit=\" + autoCommit;\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space autoCommit=\" + autoCommit;\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (autoCommit) {\n\n              // Whether we succeeded or failed, check that\n              // all un-referenced files were in fact\n              // deleted (ie, we did not create garbage).\n              // Only check this when autoCommit is true:\n              // when it's false, it's expected that there\n              // are unreferenced files (ie they won't be\n              // referenced until the \"commit on close\").\n              // Just create a new IndexFileDeleter, have it\n              // delete unreferenced files, then verify that\n              // in fact no files were deleted:\n\n              String successStr;\n              if (success) {\n                successStr = \"success\";\n              } else {\n                successStr = \"IOException\";\n              }\n              String message = methodName + \" failed to delete unreferenced files after \" + successStr + \" (\" + diskFree + \" bytes)\";\n              assertNoUnreferencedFiles(dir, message);\n            }\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (autoCommit && result != END_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + END_COUNT);\n              } else if (!autoCommit && result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" [autoCommit = false]\");\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7","date":1255555265,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexesNoOptimize(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexesNoOptimize(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexesNoOptimize will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexesNoOptimize should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.UNLIMITED);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexesNoOptimize(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexes into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[])\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.UNLIMITED);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"90cb6b3f4e5652555b614adc90204287fbebd27c","date":1259494272,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexesNoOptimize(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexesNoOptimize(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexesNoOptimize will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexesNoOptimize should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.UNLIMITED);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexesNoOptimize(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexesNoOptimize(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexesNoOptimize(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexesNoOptimize will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexesNoOptimize should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.UNLIMITED);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexesNoOptimize(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 2000 more bytes of free space:\n          diskFree += 2000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe0932c1d340f83fb0a611e5829b3046a1cc1152","date":1264946739,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexesNoOptimize(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexesNoOptimize(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexesNoOptimize will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexesNoOptimize should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new WhitespaceAnalyzer(Version.LUCENE_CURRENT), false, IndexWriter.MaxFieldLength.UNLIMITED);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexesNoOptimize(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexesNoOptimize(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexesNoOptimize(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexesNoOptimize will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexesNoOptimize should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.UNLIMITED);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexesNoOptimize(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"55f083e91bb056b57de136da1dfc3b9b6ecc4ef6","date":1265808957,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexesNoOptimize(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexesNoOptimize(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexesNoOptimize will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexesNoOptimize should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.UNLIMITED);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexesNoOptimize(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexesNoOptimize(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexesNoOptimize(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexesNoOptimize will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexesNoOptimize should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new WhitespaceAnalyzer(Version.LUCENE_CURRENT), false, IndexWriter.MaxFieldLength.UNLIMITED);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexesNoOptimize(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1cedb00d2dd44640194401179358a2e3ba6051bf","date":1268243626,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexesNoOptimize(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexesNoOptimize(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new IndexWriterConfig(TEST_VERSION_CURRENT));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexesNoOptimize will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexesNoOptimize should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexesNoOptimize(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexesNoOptimize(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexesNoOptimize(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexesNoOptimize will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexesNoOptimize should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.UNLIMITED);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexesNoOptimize(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e52fea2c4081a1e552b98506691990be59503168","date":1268250331,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexesNoOptimize(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexesNoOptimize(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexesNoOptimize will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexesNoOptimize should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.UNLIMITED);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexesNoOptimize(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexesNoOptimize(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexesNoOptimize(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new IndexWriterConfig(TEST_VERSION_CURRENT));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexesNoOptimize will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexesNoOptimize should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexesNoOptimize(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8","date":1268494368,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexesNoOptimize(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexesNoOptimize(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexesNoOptimize will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexesNoOptimize should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexesNoOptimize(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexesNoOptimize(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexesNoOptimize(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexesNoOptimize will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexesNoOptimize should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.UNLIMITED);\n          IOException err = null;\n\n          MergeScheduler ms = writer.getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexesNoOptimize(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"42607aa380c892dc1ec0ab26e86a575c28e13618","date":1268641604,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexesNoOptimize(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexesNoOptimize(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexesNoOptimize will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexesNoOptimize should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (VERBOSE)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (VERBOSE)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexesNoOptimize(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (VERBOSE) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (VERBOSE) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (VERBOSE) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (VERBOSE) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (VERBOSE) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexesNoOptimize(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      boolean debug = false;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexesNoOptimize(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexesNoOptimize will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexesNoOptimize should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (debug)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (debug)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (debug)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexesNoOptimize(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (debug) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (debug) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (debug) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (debug) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (debug) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexesNoOptimize(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexesNoOptimize(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexesNoOptimize will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexesNoOptimize should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (VERBOSE)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (VERBOSE)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexesNoOptimize(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (VERBOSE) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (VERBOSE) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (VERBOSE) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (VERBOSE) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (VERBOSE) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexesNoOptimize(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexesNoOptimize(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexesNoOptimize will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexesNoOptimize should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (VERBOSE)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (VERBOSE)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexesNoOptimize(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (VERBOSE) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (VERBOSE) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (VERBOSE) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (VERBOSE) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (VERBOSE) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0a046c0c310bc77931fc8441bd920053b607dd14":["09c482d1e63332617181729a225b215c452d8a79","e8d1458a2543cbd30cbfe7929be4dcb5c5251659"],"3215ae1377fc1ca1790921d75dd39cb764743b85":["d54c7739bfe85d68f0352823cd70c10c619fad38"],"1cedb00d2dd44640194401179358a2e3ba6051bf":["55f083e91bb056b57de136da1dfc3b9b6ecc4ef6"],"e52fea2c4081a1e552b98506691990be59503168":["1cedb00d2dd44640194401179358a2e3ba6051bf"],"fd9aba6db0f2adde620bb61f591ed18dcfee36ac":["741a5cca05cabe1e7482410a29e563a08379251a"],"90cb6b3f4e5652555b614adc90204287fbebd27c":["560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7"],"eeefd99c477417e5c7c574228461ebafe92469d4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5af07783dbc171e26a694c4f7d735e30c2769faa":["fd9aba6db0f2adde620bb61f591ed18dcfee36ac"],"8b6187898fc4413ccd18229711786550a280383c":["eeefd99c477417e5c7c574228461ebafe92469d4"],"42607aa380c892dc1ec0ab26e86a575c28e13618":["84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8"],"87c966e9308847938a7c905c2e46a56d8df788b8":["0a046c0c310bc77931fc8441bd920053b607dd14"],"2586f96f60332eb97ecd2934b0763791462568b2":["5af07783dbc171e26a694c4f7d735e30c2769faa"],"09c482d1e63332617181729a225b215c452d8a79":["3215ae1377fc1ca1790921d75dd39cb764743b85"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["42607aa380c892dc1ec0ab26e86a575c28e13618"],"b1405362241b561f5590ff4a87d5d6e173bcd9cf":["6456e4ba6cdce20f5615173eb208f063c8cb74b7"],"c4ae99f08f69aa3acba7cd75134e8447eb747559":["2586f96f60332eb97ecd2934b0763791462568b2"],"55f083e91bb056b57de136da1dfc3b9b6ecc4ef6":["fe0932c1d340f83fb0a611e5829b3046a1cc1152"],"560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7":["87c966e9308847938a7c905c2e46a56d8df788b8"],"741a5cca05cabe1e7482410a29e563a08379251a":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"d54c7739bfe85d68f0352823cd70c10c619fad38":["c4ae99f08f69aa3acba7cd75134e8447eb747559"],"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8":["e52fea2c4081a1e552b98506691990be59503168"],"6456e4ba6cdce20f5615173eb208f063c8cb74b7":["8b6187898fc4413ccd18229711786550a280383c"],"e8d1458a2543cbd30cbfe7929be4dcb5c5251659":["09c482d1e63332617181729a225b215c452d8a79"],"fe0932c1d340f83fb0a611e5829b3046a1cc1152":["90cb6b3f4e5652555b614adc90204287fbebd27c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"]},"commit2Childs":{"0a046c0c310bc77931fc8441bd920053b607dd14":["87c966e9308847938a7c905c2e46a56d8df788b8"],"3215ae1377fc1ca1790921d75dd39cb764743b85":["09c482d1e63332617181729a225b215c452d8a79"],"1cedb00d2dd44640194401179358a2e3ba6051bf":["e52fea2c4081a1e552b98506691990be59503168"],"e52fea2c4081a1e552b98506691990be59503168":["84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8"],"fd9aba6db0f2adde620bb61f591ed18dcfee36ac":["5af07783dbc171e26a694c4f7d735e30c2769faa"],"90cb6b3f4e5652555b614adc90204287fbebd27c":["fe0932c1d340f83fb0a611e5829b3046a1cc1152"],"eeefd99c477417e5c7c574228461ebafe92469d4":["8b6187898fc4413ccd18229711786550a280383c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["eeefd99c477417e5c7c574228461ebafe92469d4"],"5af07783dbc171e26a694c4f7d735e30c2769faa":["2586f96f60332eb97ecd2934b0763791462568b2"],"8b6187898fc4413ccd18229711786550a280383c":["6456e4ba6cdce20f5615173eb208f063c8cb74b7"],"42607aa380c892dc1ec0ab26e86a575c28e13618":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"87c966e9308847938a7c905c2e46a56d8df788b8":["560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7"],"2586f96f60332eb97ecd2934b0763791462568b2":["c4ae99f08f69aa3acba7cd75134e8447eb747559"],"09c482d1e63332617181729a225b215c452d8a79":["0a046c0c310bc77931fc8441bd920053b607dd14","e8d1458a2543cbd30cbfe7929be4dcb5c5251659"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"b1405362241b561f5590ff4a87d5d6e173bcd9cf":["741a5cca05cabe1e7482410a29e563a08379251a"],"c4ae99f08f69aa3acba7cd75134e8447eb747559":["d54c7739bfe85d68f0352823cd70c10c619fad38"],"55f083e91bb056b57de136da1dfc3b9b6ecc4ef6":["1cedb00d2dd44640194401179358a2e3ba6051bf"],"560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7":["90cb6b3f4e5652555b614adc90204287fbebd27c"],"741a5cca05cabe1e7482410a29e563a08379251a":["fd9aba6db0f2adde620bb61f591ed18dcfee36ac"],"d54c7739bfe85d68f0352823cd70c10c619fad38":["3215ae1377fc1ca1790921d75dd39cb764743b85"],"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8":["42607aa380c892dc1ec0ab26e86a575c28e13618"],"6456e4ba6cdce20f5615173eb208f063c8cb74b7":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"e8d1458a2543cbd30cbfe7929be4dcb5c5251659":["0a046c0c310bc77931fc8441bd920053b607dd14"],"fe0932c1d340f83fb0a611e5829b3046a1cc1152":["55f083e91bb056b57de136da1dfc3b9b6ecc4ef6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}