{"path":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        Set<String> stopWords = new HashSet<String>(1);\n        stopWords.add(\"stoppedtoken\");\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(stopWords.iterator().next());\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new StandardAnalyzer(TEST_VERSION_CURRENT, stopWords).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION_CURRENT, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION_CURRENT, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        Set<String> stopWords = new HashSet<String>(1);\n        stopWords.add(\"stoppedtoken\");\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(stopWords.iterator().next());\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new StandardAnalyzer(TEST_VERSION_CURRENT, stopWords).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION_CURRENT, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION_CURRENT, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c7f8e68717c68517265937c911e1ce9f25750247","date":1274071103,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        Set<String> stopWords = new HashSet<String>(1);\n        stopWords.add(\"stoppedtoken\");\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(stopWords.iterator().next());\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new StandardAnalyzer(TEST_VERSION_CURRENT, stopWords).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION_CURRENT, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION_CURRENT, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4e8cc373c801e54cec75daf9f52792cb4b17f536","date":1291116159,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3bb13258feba31ab676502787ab2e1779f129b7a","date":1291596436,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new MockAnalyzer(random, MockTokenizer.SIMPLE, true, stopWords, true).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new MockAnalyzer(random, MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new MockAnalyzer(random, MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new MockAnalyzer(random, MockTokenizer.SIMPLE, true, stopWords, true).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new MockAnalyzer(random, MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new MockAnalyzer(random, MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new MockAnalyzer(random, MockTokenizer.SIMPLE, true, stopWords, true).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new MockAnalyzer(random, MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new MockAnalyzer(random, MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4","date":1305207152,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n        // we disable MockTokenizer checks because we will forcefully limit the \n        // tokenstream and call end() before incrementToken() returns false.\n        final MockAnalyzer analyzer = new MockAnalyzer(random, MockTokenizer.SIMPLE, true, stopWords, true);\n        analyzer.setEnableChecks(false);\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", analyzer.tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new MockAnalyzer(random, MockTokenizer.SIMPLE, true, stopWords, true).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new MockAnalyzer(random, MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new MockAnalyzer(random, MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c700f8d0842d3e52bb2bdfbfdc046a137e836edb","date":1305285499,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n        // we disable MockTokenizer checks because we will forcefully limit the \n        // tokenstream and call end() before incrementToken() returns false.\n        final MockAnalyzer analyzer = new MockAnalyzer(random, MockTokenizer.SIMPLE, true, stopWords, true);\n        analyzer.setEnableChecks(false);\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", analyzer.tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new MockAnalyzer(random, MockTokenizer.SIMPLE, true, stopWords, true).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new MockAnalyzer(random, MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new MockAnalyzer(random, MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n        // we disable MockTokenizer checks because we will forcefully limit the \n        // tokenstream and call end() before incrementToken() returns false.\n        final MockAnalyzer analyzer = new MockAnalyzer(random, MockTokenizer.SIMPLE, true, stopWords, true);\n        analyzer.setEnableChecks(false);\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", analyzer.tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new MockAnalyzer(MockTokenizer.SIMPLE, true, stopWords, true), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cf7efd82433f3f64684711c16edfd149db6af111","date":1317013128,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n        // we disable MockTokenizer checks because we will forcefully limit the \n        // tokenstream and call end() before incrementToken() returns false.\n        final MockAnalyzer analyzer = new MockAnalyzer(random, MockTokenizer.SIMPLE, true, stopWords, true);\n        analyzer.setEnableChecks(false);\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n        // we disable MockTokenizer checks because we will forcefully limit the \n        // tokenstream and call end() before incrementToken() returns false.\n        final MockAnalyzer analyzer = new MockAnalyzer(random, MockTokenizer.SIMPLE, true, stopWords, true);\n        analyzer.setEnableChecks(false);\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", analyzer.tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n        // we disable MockTokenizer checks because we will forcefully limit the \n        // tokenstream and call end() before incrementToken() returns false.\n        final MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, stopWords, true);\n        analyzer.setEnableChecks(false);\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n        // we disable MockTokenizer checks because we will forcefully limit the \n        // tokenstream and call end() before incrementToken() returns false.\n        final MockAnalyzer analyzer = new MockAnalyzer(random, MockTokenizer.SIMPLE, true, stopWords, true);\n        analyzer.setEnableChecks(false);\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n        // we disable MockTokenizer checks because we will forcefully limit the \n        // tokenstream and call end() before incrementToken() returns false.\n        final MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, stopWords, true);\n        analyzer.setEnableChecks(false);\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        CharacterRunAutomaton stopWords = new CharacterRunAutomaton(BasicAutomata.makeString(\"stoppedtoken\"));\n        // we disable MockTokenizer checks because we will forcefully limit the \n        // tokenstream and call end() before incrementToken() returns false.\n        final MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, stopWords, true);\n        analyzer.setEnableChecks(false);\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(\"stoppedtoken\");\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(analyzer, \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"135621f3a0670a9394eb563224a3b76cc4dddc0f":["3bb13258feba31ab676502787ab2e1779f129b7a","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"c7f8e68717c68517265937c911e1ce9f25750247":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"962d04139994fce5193143ef35615499a9a96d78":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["c7f8e68717c68517265937c911e1ce9f25750247","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"cf7efd82433f3f64684711c16edfd149db6af111":["e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["4e8cc373c801e54cec75daf9f52792cb4b17f536"],"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4":["f2c5f0cb44df114db4228c8f77861714b5cabaea"],"a3776dccca01c11e7046323cfad46a3b4a471233":["4e8cc373c801e54cec75daf9f52792cb4b17f536","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["135621f3a0670a9394eb563224a3b76cc4dddc0f","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["cf7efd82433f3f64684711c16edfd149db6af111"],"3bb13258feba31ab676502787ab2e1779f129b7a":["c7f8e68717c68517265937c911e1ce9f25750247","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"4e8cc373c801e54cec75daf9f52792cb4b17f536":["c7f8e68717c68517265937c911e1ce9f25750247"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"135621f3a0670a9394eb563224a3b76cc4dddc0f":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"c7f8e68717c68517265937c911e1ce9f25750247":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","3bb13258feba31ab676502787ab2e1779f129b7a","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"962d04139994fce5193143ef35615499a9a96d78":[],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["962d04139994fce5193143ef35615499a9a96d78"],"cf7efd82433f3f64684711c16edfd149db6af111":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4":["cf7efd82433f3f64684711c16edfd149db6af111","a3776dccca01c11e7046323cfad46a3b4a471233","c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"a3776dccca01c11e7046323cfad46a3b4a471233":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":[],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"3bb13258feba31ab676502787ab2e1779f129b7a":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["c7f8e68717c68517265937c911e1ce9f25750247"],"4e8cc373c801e54cec75daf9f52792cb4b17f536":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","f2c5f0cb44df114db4228c8f77861714b5cabaea","a3776dccca01c11e7046323cfad46a3b4a471233","3bb13258feba31ab676502787ab2e1779f129b7a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["962d04139994fce5193143ef35615499a9a96d78","a3776dccca01c11e7046323cfad46a3b4a471233","c700f8d0842d3e52bb2bdfbfdc046a137e836edb","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}