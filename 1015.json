{"path":"lucene/test-framework/src/test/org/apache/lucene/codecs/compressing/TestCompressingTermVectorsFormat#testChunkCleanup().mjava","commits":[{"id":"b52491e71f0d5d0f0160d6ed0d39e0dd661be68a","date":1429550638,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/test/org/apache/lucene/codecs/compressing/TestCompressingTermVectorsFormat#testChunkCleanup().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/codecs/compressing/TestCompressingTermVectorsFormat#testChunkCleanup().mjava","sourceNew":"  /**\n   * writes some tiny segments with incomplete compressed blocks,\n   * and ensures merge recompresses them.\n   */\n  public void testChunkCleanup() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMergePolicy(NoMergePolicy.INSTANCE);\n    \n    // we have to enforce certain things like maxDocsPerChunk to cause dirty chunks to be created\n    // by this test.\n    iwConf.setCodec(CompressingCodec.randomInstance(random(), 4*1024, 100, false, 8));\n    IndexWriter iw = new IndexWriter(dir, iwConf);\n    DirectoryReader ir = DirectoryReader.open(iw, true);\n    for (int i = 0; i < 5; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n      ft.setStoreTermVectors(true);\n      doc.add(new Field(\"text\", \"not very long at all\", ft));\n      iw.addDocument(doc);\n      // force flush\n      DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n      assertNotNull(ir2);\n      ir.close();\n      ir = ir2;\n      // examine dirty counts:\n      for (LeafReaderContext leaf : ir2.leaves()) {\n        CodecReader sr = (CodecReader) leaf.reader();\n        CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n        assertEquals(1, reader.getNumChunks());\n        assertEquals(1, reader.getNumDirtyChunks());\n      }\n    }\n    iw.getConfig().setMergePolicy(newLogMergePolicy());\n    iw.forceMerge(1);\n    DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n    assertNotNull(ir2);\n    ir.close();\n    ir = ir2;\n    CodecReader sr = getOnlySegmentReader(ir);\n    CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n    // we could get lucky, and have zero, but typically one.\n    assertTrue(reader.getNumDirtyChunks() <= 1);\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * writes some tiny segments with incomplete compressed blocks,\n   * and ensures merge recompresses them.\n   */\n  public void testChunkCleanup() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMergePolicy(NoMergePolicy.INSTANCE);\n    \n    // we have to enforce certain things like maxDocsPerChunk to cause dirty chunks to be created\n    // by this test.\n    iwConf.setCodec(CompressingCodec.randomInstance(random(), 4*1024, 100, false, 8));\n    IndexWriter iw = new IndexWriter(dir, iwConf);\n    DirectoryReader ir = DirectoryReader.open(iw, true);\n    for (int i = 0; i < 5; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n      ft.setStoreTermVectors(true);\n      doc.add(new Field(\"text\", \"not very long at all\", ft));\n      iw.addDocument(doc);\n      // force flush\n      DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n      assertNotNull(ir2);\n      ir.close();\n      ir = ir2;\n      // examine dirty counts:\n      for (LeafReaderContext leaf : ir2.leaves()) {\n        CodecReader sr = (CodecReader) leaf.reader();\n        CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n        assertEquals(1, reader.getNumChunks());\n        assertEquals(1, reader.getNumDirtyChunks());\n      }\n    }\n    iw.getConfig().setMergePolicy(newLogMergePolicy());\n    iw.forceMerge(1);\n    DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n    assertNotNull(ir2);\n    ir.close();\n    ir = ir2;\n    CodecReader sr = getOnlySegmentReader(ir);\n    CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n    // we could get lucky, and have zero, but typically one.\n    assertTrue(reader.getNumDirtyChunks() <= 1);\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"46f77e32b3f394d5d5df9591ca222a03b142579d","date":1453475780,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/test/org/apache/lucene/codecs/compressing/TestCompressingTermVectorsFormat#testChunkCleanup().mjava","pathOld":"lucene/test-framework/src/test/org/apache/lucene/codecs/compressing/TestCompressingTermVectorsFormat#testChunkCleanup().mjava","sourceNew":"  /**\n   * writes some tiny segments with incomplete compressed blocks,\n   * and ensures merge recompresses them.\n   */\n  public void testChunkCleanup() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMergePolicy(NoMergePolicy.INSTANCE);\n    \n    // we have to enforce certain things like maxDocsPerChunk to cause dirty chunks to be created\n    // by this test.\n    iwConf.setCodec(CompressingCodec.randomInstance(random(), 4*1024, 100, false, 8));\n    IndexWriter iw = new IndexWriter(dir, iwConf);\n    DirectoryReader ir = DirectoryReader.open(iw);\n    for (int i = 0; i < 5; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n      ft.setStoreTermVectors(true);\n      doc.add(new Field(\"text\", \"not very long at all\", ft));\n      iw.addDocument(doc);\n      // force flush\n      DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n      assertNotNull(ir2);\n      ir.close();\n      ir = ir2;\n      // examine dirty counts:\n      for (LeafReaderContext leaf : ir2.leaves()) {\n        CodecReader sr = (CodecReader) leaf.reader();\n        CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n        assertEquals(1, reader.getNumChunks());\n        assertEquals(1, reader.getNumDirtyChunks());\n      }\n    }\n    iw.getConfig().setMergePolicy(newLogMergePolicy());\n    iw.forceMerge(1);\n    DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n    assertNotNull(ir2);\n    ir.close();\n    ir = ir2;\n    CodecReader sr = getOnlySegmentReader(ir);\n    CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n    // we could get lucky, and have zero, but typically one.\n    assertTrue(reader.getNumDirtyChunks() <= 1);\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * writes some tiny segments with incomplete compressed blocks,\n   * and ensures merge recompresses them.\n   */\n  public void testChunkCleanup() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMergePolicy(NoMergePolicy.INSTANCE);\n    \n    // we have to enforce certain things like maxDocsPerChunk to cause dirty chunks to be created\n    // by this test.\n    iwConf.setCodec(CompressingCodec.randomInstance(random(), 4*1024, 100, false, 8));\n    IndexWriter iw = new IndexWriter(dir, iwConf);\n    DirectoryReader ir = DirectoryReader.open(iw, true);\n    for (int i = 0; i < 5; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n      ft.setStoreTermVectors(true);\n      doc.add(new Field(\"text\", \"not very long at all\", ft));\n      iw.addDocument(doc);\n      // force flush\n      DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n      assertNotNull(ir2);\n      ir.close();\n      ir = ir2;\n      // examine dirty counts:\n      for (LeafReaderContext leaf : ir2.leaves()) {\n        CodecReader sr = (CodecReader) leaf.reader();\n        CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n        assertEquals(1, reader.getNumChunks());\n        assertEquals(1, reader.getNumDirtyChunks());\n      }\n    }\n    iw.getConfig().setMergePolicy(newLogMergePolicy());\n    iw.forceMerge(1);\n    DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n    assertNotNull(ir2);\n    ir.close();\n    ir = ir2;\n    CodecReader sr = getOnlySegmentReader(ir);\n    CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n    // we could get lucky, and have zero, but typically one.\n    assertTrue(reader.getNumDirtyChunks() <= 1);\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"92041add9368106252731c77e3528b6073bba8c0","date":1453475837,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/test/org/apache/lucene/codecs/compressing/TestCompressingTermVectorsFormat#testChunkCleanup().mjava","pathOld":"lucene/test-framework/src/test/org/apache/lucene/codecs/compressing/TestCompressingTermVectorsFormat#testChunkCleanup().mjava","sourceNew":"  /**\n   * writes some tiny segments with incomplete compressed blocks,\n   * and ensures merge recompresses them.\n   */\n  public void testChunkCleanup() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMergePolicy(NoMergePolicy.INSTANCE);\n    \n    // we have to enforce certain things like maxDocsPerChunk to cause dirty chunks to be created\n    // by this test.\n    iwConf.setCodec(CompressingCodec.randomInstance(random(), 4*1024, 100, false, 8));\n    IndexWriter iw = new IndexWriter(dir, iwConf);\n    DirectoryReader ir = DirectoryReader.open(iw, true);\n    for (int i = 0; i < 5; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n      ft.setStoreTermVectors(true);\n      doc.add(new Field(\"text\", \"not very long at all\", ft));\n      iw.addDocument(doc);\n      // force flush\n      DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n      assertNotNull(ir2);\n      ir.close();\n      ir = ir2;\n      // examine dirty counts:\n      for (LeafReaderContext leaf : ir2.leaves()) {\n        CodecReader sr = (CodecReader) leaf.reader();\n        CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n        assertEquals(1, reader.getNumChunks());\n        assertEquals(1, reader.getNumDirtyChunks());\n      }\n    }\n    iw.getConfig().setMergePolicy(newLogMergePolicy());\n    iw.forceMerge(1);\n    DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n    assertNotNull(ir2);\n    ir.close();\n    ir = ir2;\n    CodecReader sr = getOnlySegmentReader(ir);\n    CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n    // we could get lucky, and have zero, but typically one.\n    assertTrue(reader.getNumDirtyChunks() <= 1);\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * writes some tiny segments with incomplete compressed blocks,\n   * and ensures merge recompresses them.\n   */\n  public void testChunkCleanup() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMergePolicy(NoMergePolicy.INSTANCE);\n    \n    // we have to enforce certain things like maxDocsPerChunk to cause dirty chunks to be created\n    // by this test.\n    iwConf.setCodec(CompressingCodec.randomInstance(random(), 4*1024, 100, false, 8));\n    IndexWriter iw = new IndexWriter(dir, iwConf);\n    DirectoryReader ir = DirectoryReader.open(iw);\n    for (int i = 0; i < 5; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n      ft.setStoreTermVectors(true);\n      doc.add(new Field(\"text\", \"not very long at all\", ft));\n      iw.addDocument(doc);\n      // force flush\n      DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n      assertNotNull(ir2);\n      ir.close();\n      ir = ir2;\n      // examine dirty counts:\n      for (LeafReaderContext leaf : ir2.leaves()) {\n        CodecReader sr = (CodecReader) leaf.reader();\n        CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n        assertEquals(1, reader.getNumChunks());\n        assertEquals(1, reader.getNumDirtyChunks());\n      }\n    }\n    iw.getConfig().setMergePolicy(newLogMergePolicy());\n    iw.forceMerge(1);\n    DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n    assertNotNull(ir2);\n    ir.close();\n    ir = ir2;\n    CodecReader sr = getOnlySegmentReader(ir);\n    CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n    // we could get lucky, and have zero, but typically one.\n    assertTrue(reader.getNumDirtyChunks() <= 1);\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f90ee54b41e0e3187a4fedafcf2bd6d947befd31","date":1453479126,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/test/org/apache/lucene/codecs/compressing/TestCompressingTermVectorsFormat#testChunkCleanup().mjava","pathOld":"lucene/test-framework/src/test/org/apache/lucene/codecs/compressing/TestCompressingTermVectorsFormat#testChunkCleanup().mjava","sourceNew":"  /**\n   * writes some tiny segments with incomplete compressed blocks,\n   * and ensures merge recompresses them.\n   */\n  public void testChunkCleanup() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMergePolicy(NoMergePolicy.INSTANCE);\n    \n    // we have to enforce certain things like maxDocsPerChunk to cause dirty chunks to be created\n    // by this test.\n    iwConf.setCodec(CompressingCodec.randomInstance(random(), 4*1024, 100, false, 8));\n    IndexWriter iw = new IndexWriter(dir, iwConf);\n    DirectoryReader ir = DirectoryReader.open(iw);\n    for (int i = 0; i < 5; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n      ft.setStoreTermVectors(true);\n      doc.add(new Field(\"text\", \"not very long at all\", ft));\n      iw.addDocument(doc);\n      // force flush\n      DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n      assertNotNull(ir2);\n      ir.close();\n      ir = ir2;\n      // examine dirty counts:\n      for (LeafReaderContext leaf : ir2.leaves()) {\n        CodecReader sr = (CodecReader) leaf.reader();\n        CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n        assertEquals(1, reader.getNumChunks());\n        assertEquals(1, reader.getNumDirtyChunks());\n      }\n    }\n    iw.getConfig().setMergePolicy(newLogMergePolicy());\n    iw.forceMerge(1);\n    DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n    assertNotNull(ir2);\n    ir.close();\n    ir = ir2;\n    CodecReader sr = getOnlySegmentReader(ir);\n    CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n    // we could get lucky, and have zero, but typically one.\n    assertTrue(reader.getNumDirtyChunks() <= 1);\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * writes some tiny segments with incomplete compressed blocks,\n   * and ensures merge recompresses them.\n   */\n  public void testChunkCleanup() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMergePolicy(NoMergePolicy.INSTANCE);\n    \n    // we have to enforce certain things like maxDocsPerChunk to cause dirty chunks to be created\n    // by this test.\n    iwConf.setCodec(CompressingCodec.randomInstance(random(), 4*1024, 100, false, 8));\n    IndexWriter iw = new IndexWriter(dir, iwConf);\n    DirectoryReader ir = DirectoryReader.open(iw, true);\n    for (int i = 0; i < 5; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n      ft.setStoreTermVectors(true);\n      doc.add(new Field(\"text\", \"not very long at all\", ft));\n      iw.addDocument(doc);\n      // force flush\n      DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n      assertNotNull(ir2);\n      ir.close();\n      ir = ir2;\n      // examine dirty counts:\n      for (LeafReaderContext leaf : ir2.leaves()) {\n        CodecReader sr = (CodecReader) leaf.reader();\n        CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n        assertEquals(1, reader.getNumChunks());\n        assertEquals(1, reader.getNumDirtyChunks());\n      }\n    }\n    iw.getConfig().setMergePolicy(newLogMergePolicy());\n    iw.forceMerge(1);\n    DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n    assertNotNull(ir2);\n    ir.close();\n    ir = ir2;\n    CodecReader sr = getOnlySegmentReader(ir);\n    CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n    // we could get lucky, and have zero, but typically one.\n    assertTrue(reader.getNumDirtyChunks() <= 1);\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1","date":1457644139,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/test/org/apache/lucene/codecs/compressing/TestCompressingTermVectorsFormat#testChunkCleanup().mjava","pathOld":"lucene/test-framework/src/test/org/apache/lucene/codecs/compressing/TestCompressingTermVectorsFormat#testChunkCleanup().mjava","sourceNew":"  /**\n   * writes some tiny segments with incomplete compressed blocks,\n   * and ensures merge recompresses them.\n   */\n  public void testChunkCleanup() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMergePolicy(NoMergePolicy.INSTANCE);\n    \n    // we have to enforce certain things like maxDocsPerChunk to cause dirty chunks to be created\n    // by this test.\n    iwConf.setCodec(CompressingCodec.randomInstance(random(), 4*1024, 100, false, 8));\n    IndexWriter iw = new IndexWriter(dir, iwConf);\n    DirectoryReader ir = DirectoryReader.open(iw);\n    for (int i = 0; i < 5; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n      ft.setStoreTermVectors(true);\n      doc.add(new Field(\"text\", \"not very long at all\", ft));\n      iw.addDocument(doc);\n      // force flush\n      DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n      assertNotNull(ir2);\n      ir.close();\n      ir = ir2;\n      // examine dirty counts:\n      for (LeafReaderContext leaf : ir2.leaves()) {\n        CodecReader sr = (CodecReader) leaf.reader();\n        CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n        assertEquals(1, reader.getNumChunks());\n        assertEquals(1, reader.getNumDirtyChunks());\n      }\n    }\n    iw.getConfig().setMergePolicy(newLogMergePolicy());\n    iw.forceMerge(1);\n    DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n    assertNotNull(ir2);\n    ir.close();\n    ir = ir2;\n    CodecReader sr = (CodecReader) getOnlyLeafReader(ir);\n    CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n    // we could get lucky, and have zero, but typically one.\n    assertTrue(reader.getNumDirtyChunks() <= 1);\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * writes some tiny segments with incomplete compressed blocks,\n   * and ensures merge recompresses them.\n   */\n  public void testChunkCleanup() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMergePolicy(NoMergePolicy.INSTANCE);\n    \n    // we have to enforce certain things like maxDocsPerChunk to cause dirty chunks to be created\n    // by this test.\n    iwConf.setCodec(CompressingCodec.randomInstance(random(), 4*1024, 100, false, 8));\n    IndexWriter iw = new IndexWriter(dir, iwConf);\n    DirectoryReader ir = DirectoryReader.open(iw);\n    for (int i = 0; i < 5; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n      ft.setStoreTermVectors(true);\n      doc.add(new Field(\"text\", \"not very long at all\", ft));\n      iw.addDocument(doc);\n      // force flush\n      DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n      assertNotNull(ir2);\n      ir.close();\n      ir = ir2;\n      // examine dirty counts:\n      for (LeafReaderContext leaf : ir2.leaves()) {\n        CodecReader sr = (CodecReader) leaf.reader();\n        CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n        assertEquals(1, reader.getNumChunks());\n        assertEquals(1, reader.getNumDirtyChunks());\n      }\n    }\n    iw.getConfig().setMergePolicy(newLogMergePolicy());\n    iw.forceMerge(1);\n    DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n    assertNotNull(ir2);\n    ir.close();\n    ir = ir2;\n    CodecReader sr = getOnlySegmentReader(ir);\n    CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n    // we could get lucky, and have zero, but typically one.\n    assertTrue(reader.getNumDirtyChunks() <= 1);\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":["b88448324d3a96c5842455dabea63450b697b58f"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"45264aed0cfa8a8a55ae1292b0e336d29cd88401","date":1600361948,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/test-framework/src/test/org/apache/lucene/codecs/compressing/TestCompressingTermVectorsFormat#testChunkCleanup().mjava","pathOld":"lucene/test-framework/src/test/org/apache/lucene/codecs/compressing/TestCompressingTermVectorsFormat#testChunkCleanup().mjava","sourceNew":"  /**\n   * writes some tiny segments with incomplete compressed blocks,\n   * and ensures merge recompresses them.\n   */\n  public void testChunkCleanup() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMergePolicy(NoMergePolicy.INSTANCE);\n    \n    // we have to enforce certain things like maxDocsPerChunk to cause dirty chunks to be created\n    // by this test.\n    iwConf.setCodec(CompressingCodec.randomInstance(random(), 4*1024, 100, false, 8));\n    IndexWriter iw = new IndexWriter(dir, iwConf);\n    DirectoryReader ir = DirectoryReader.open(iw);\n    for (int i = 0; i < 5; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n      ft.setStoreTermVectors(true);\n      doc.add(new Field(\"text\", \"not very long at all\", ft));\n      iw.addDocument(doc);\n      // force flush\n      DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n      assertNotNull(ir2);\n      ir.close();\n      ir = ir2;\n      // examine dirty counts:\n      for (LeafReaderContext leaf : ir2.leaves()) {\n        CodecReader sr = (CodecReader) leaf.reader();\n        CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n        assertTrue(reader.getNumDirtyDocs() > 0);\n        assertEquals(1, reader.getNumDirtyChunks());\n      }\n    }\n    iw.getConfig().setMergePolicy(newLogMergePolicy());\n    iw.forceMerge(1);\n    DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n    assertNotNull(ir2);\n    ir.close();\n    ir = ir2;\n    CodecReader sr = (CodecReader) getOnlyLeafReader(ir);\n    CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n    // we could get lucky, and have zero, but typically one.\n    assertTrue(reader.getNumDirtyChunks() <= 1);\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * writes some tiny segments with incomplete compressed blocks,\n   * and ensures merge recompresses them.\n   */\n  public void testChunkCleanup() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMergePolicy(NoMergePolicy.INSTANCE);\n    \n    // we have to enforce certain things like maxDocsPerChunk to cause dirty chunks to be created\n    // by this test.\n    iwConf.setCodec(CompressingCodec.randomInstance(random(), 4*1024, 100, false, 8));\n    IndexWriter iw = new IndexWriter(dir, iwConf);\n    DirectoryReader ir = DirectoryReader.open(iw);\n    for (int i = 0; i < 5; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n      ft.setStoreTermVectors(true);\n      doc.add(new Field(\"text\", \"not very long at all\", ft));\n      iw.addDocument(doc);\n      // force flush\n      DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n      assertNotNull(ir2);\n      ir.close();\n      ir = ir2;\n      // examine dirty counts:\n      for (LeafReaderContext leaf : ir2.leaves()) {\n        CodecReader sr = (CodecReader) leaf.reader();\n        CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n        assertEquals(1, reader.getNumChunks());\n        assertEquals(1, reader.getNumDirtyChunks());\n      }\n    }\n    iw.getConfig().setMergePolicy(newLogMergePolicy());\n    iw.forceMerge(1);\n    DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n    assertNotNull(ir2);\n    ir.close();\n    ir = ir2;\n    CodecReader sr = (CodecReader) getOnlyLeafReader(ir);\n    CompressingTermVectorsReader reader = (CompressingTermVectorsReader)sr.getTermVectorsReader();\n    // we could get lucky, and have zero, but typically one.\n    assertTrue(reader.getNumDirtyChunks() <= 1);\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1":["f90ee54b41e0e3187a4fedafcf2bd6d947befd31"],"92041add9368106252731c77e3528b6073bba8c0":["46f77e32b3f394d5d5df9591ca222a03b142579d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"f90ee54b41e0e3187a4fedafcf2bd6d947befd31":["92041add9368106252731c77e3528b6073bba8c0"],"45264aed0cfa8a8a55ae1292b0e336d29cd88401":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1"],"b52491e71f0d5d0f0160d6ed0d39e0dd661be68a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["45264aed0cfa8a8a55ae1292b0e336d29cd88401"],"46f77e32b3f394d5d5df9591ca222a03b142579d":["b52491e71f0d5d0f0160d6ed0d39e0dd661be68a"]},"commit2Childs":{"0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1":["45264aed0cfa8a8a55ae1292b0e336d29cd88401"],"92041add9368106252731c77e3528b6073bba8c0":["f90ee54b41e0e3187a4fedafcf2bd6d947befd31"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b52491e71f0d5d0f0160d6ed0d39e0dd661be68a"],"f90ee54b41e0e3187a4fedafcf2bd6d947befd31":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1"],"45264aed0cfa8a8a55ae1292b0e336d29cd88401":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"b52491e71f0d5d0f0160d6ed0d39e0dd661be68a":["46f77e32b3f394d5d5df9591ca222a03b142579d"],"46f77e32b3f394d5d5df9591ca222a03b142579d":["92041add9368106252731c77e3528b6073bba8c0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}