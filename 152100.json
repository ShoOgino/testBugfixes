{"path":"contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizer#next(Token).mjava","commits":[{"id":"b305cb92ee47ddf7b15c6eeefe489c04d05b22ba","date":1199456955,"type":0,"author":"Grant Ingersoll","isMerge":false,"pathNew":"contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizer#next(Token).mjava","pathOld":"/dev/null","sourceNew":"  /*\n  * (non-Javadoc)\n  *\n  * @see org.apache.lucene.analysis.TokenStream#next()\n  */\n  public Token next(Token result) throws IOException {\n    int tokenType = scanner.getNextToken();\n\n    if (tokenType == WikipediaTokenizerImpl.YYEOF) {\n      return null;\n    }\n\n    scanner.getText(result, tokenType);\n    final int start = scanner.yychar();\n    result.setStartOffset(start);\n    result.setEndOffset(start + result.termLength());\n    result.setPositionIncrement(scanner.getPositionIncrement());\n    result.setType(WikipediaTokenizerImpl.TOKEN_TYPES[tokenType]);\n    return result;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"decc8a7344e9231708f9991fa09db2cafec7a2dd","date":1201187153,"type":3,"author":"Grant Ingersoll","isMerge":false,"pathNew":"contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizer#next(Token).mjava","pathOld":"contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizer#next(Token).mjava","sourceNew":"  /*\n  * (non-Javadoc)\n  *\n  * @see org.apache.lucene.analysis.TokenStream#next()\n  */\n  public Token next(Token result) throws IOException {\n    if (tokens != null && tokens.hasNext()){\n      return (Token)tokens.next();\n    }\n    int tokenType = scanner.getNextToken();\n\n    if (tokenType == WikipediaTokenizerImpl.YYEOF) {\n      return null;\n    }\n    String type = WikipediaTokenizerImpl.TOKEN_TYPES[tokenType];\n    if (tokenOutput == TOKENS_ONLY || untokenizedTypes.contains(type) == false){\n      setupToken(result);\n    } else if (tokenOutput == UNTOKENIZED_ONLY && untokenizedTypes.contains(type) == true){\n      collapseTokens(result, tokenType);\n\n    }\n    else if (tokenOutput == BOTH){\n      //collapse into a single token, add it to tokens AND output the individual tokens\n      //output the untokenized Token first\n      collapseAndSaveTokens(result, tokenType, type);\n    }\n    result.setPositionIncrement(scanner.getPositionIncrement());\n    result.setType(type);\n    return result;\n  }\n\n","sourceOld":"  /*\n  * (non-Javadoc)\n  *\n  * @see org.apache.lucene.analysis.TokenStream#next()\n  */\n  public Token next(Token result) throws IOException {\n    int tokenType = scanner.getNextToken();\n\n    if (tokenType == WikipediaTokenizerImpl.YYEOF) {\n      return null;\n    }\n\n    scanner.getText(result, tokenType);\n    final int start = scanner.yychar();\n    result.setStartOffset(start);\n    result.setEndOffset(start + result.termLength());\n    result.setPositionIncrement(scanner.getPositionIncrement());\n    result.setType(WikipediaTokenizerImpl.TOKEN_TYPES[tokenType]);\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e2cb543b41c145f33390f460ee743d6693c9c6c","date":1219243087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizer#next(Token).mjava","pathOld":"contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizer#next(Token).mjava","sourceNew":"  /*\n  * (non-Javadoc)\n  *\n  * @see org.apache.lucene.analysis.TokenStream#next()\n  */\n  public Token next(final Token reusableToken) throws IOException {\n    assert reusableToken != null;\n    if (tokens != null && tokens.hasNext()){\n      return (Token)tokens.next();\n    }\n    int tokenType = scanner.getNextToken();\n\n    if (tokenType == WikipediaTokenizerImpl.YYEOF) {\n      return null;\n    }\n    String type = WikipediaTokenizerImpl.TOKEN_TYPES[tokenType];\n    if (tokenOutput == TOKENS_ONLY || untokenizedTypes.contains(type) == false){\n      setupToken(reusableToken);\n    } else if (tokenOutput == UNTOKENIZED_ONLY && untokenizedTypes.contains(type) == true){\n      collapseTokens(reusableToken, tokenType);\n\n    }\n    else if (tokenOutput == BOTH){\n      //collapse into a single token, add it to tokens AND output the individual tokens\n      //output the untokenized Token first\n      collapseAndSaveTokens(reusableToken, tokenType, type);\n    }\n    reusableToken.setPositionIncrement(scanner.getPositionIncrement());\n    reusableToken.setType(type);\n    return reusableToken;\n  }\n\n","sourceOld":"  /*\n  * (non-Javadoc)\n  *\n  * @see org.apache.lucene.analysis.TokenStream#next()\n  */\n  public Token next(Token result) throws IOException {\n    if (tokens != null && tokens.hasNext()){\n      return (Token)tokens.next();\n    }\n    int tokenType = scanner.getNextToken();\n\n    if (tokenType == WikipediaTokenizerImpl.YYEOF) {\n      return null;\n    }\n    String type = WikipediaTokenizerImpl.TOKEN_TYPES[tokenType];\n    if (tokenOutput == TOKENS_ONLY || untokenizedTypes.contains(type) == false){\n      setupToken(result);\n    } else if (tokenOutput == UNTOKENIZED_ONLY && untokenizedTypes.contains(type) == true){\n      collapseTokens(result, tokenType);\n\n    }\n    else if (tokenOutput == BOTH){\n      //collapse into a single token, add it to tokens AND output the individual tokens\n      //output the untokenized Token first\n      collapseAndSaveTokens(result, tokenType, type);\n    }\n    result.setPositionIncrement(scanner.getPositionIncrement());\n    result.setType(type);\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9b5756469957918cac40a831acec9cf01c8c2bb3","date":1249167152,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizer#next(Token).mjava","pathOld":"contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizer#next(Token).mjava","sourceNew":"  /** @deprecated Will be removed in Lucene 3.0. This method is final, as it should\n   * not be overridden. Delegates to the backwards compatibility layer. */\n  public final Token next(final Token reusableToken) throws java.io.IOException {\n    return super.next(reusableToken);\n  }\n\n","sourceOld":"  /*\n  * (non-Javadoc)\n  *\n  * @see org.apache.lucene.analysis.TokenStream#next()\n  */\n  public Token next(final Token reusableToken) throws IOException {\n    assert reusableToken != null;\n    if (tokens != null && tokens.hasNext()){\n      return (Token)tokens.next();\n    }\n    int tokenType = scanner.getNextToken();\n\n    if (tokenType == WikipediaTokenizerImpl.YYEOF) {\n      return null;\n    }\n    String type = WikipediaTokenizerImpl.TOKEN_TYPES[tokenType];\n    if (tokenOutput == TOKENS_ONLY || untokenizedTypes.contains(type) == false){\n      setupToken(reusableToken);\n    } else if (tokenOutput == UNTOKENIZED_ONLY && untokenizedTypes.contains(type) == true){\n      collapseTokens(reusableToken, tokenType);\n\n    }\n    else if (tokenOutput == BOTH){\n      //collapse into a single token, add it to tokens AND output the individual tokens\n      //output the untokenized Token first\n      collapseAndSaveTokens(reusableToken, tokenType, type);\n    }\n    reusableToken.setPositionIncrement(scanner.getPositionIncrement());\n    reusableToken.setType(type);\n    return reusableToken;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"439b0fe2f799d1c722151e88e32bdefad8d34ebe","date":1255282509,"type":4,"author":"Uwe Schindler","isMerge":false,"pathNew":"/dev/null","pathOld":"contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizer#next(Token).mjava","sourceNew":null,"sourceOld":"  /** @deprecated Will be removed in Lucene 3.0. This method is final, as it should\n   * not be overridden. Delegates to the backwards compatibility layer. */\n  public final Token next(final Token reusableToken) throws java.io.IOException {\n    return super.next(reusableToken);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7e2cb543b41c145f33390f460ee743d6693c9c6c":["decc8a7344e9231708f9991fa09db2cafec7a2dd"],"decc8a7344e9231708f9991fa09db2cafec7a2dd":["b305cb92ee47ddf7b15c6eeefe489c04d05b22ba"],"b305cb92ee47ddf7b15c6eeefe489c04d05b22ba":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9b5756469957918cac40a831acec9cf01c8c2bb3":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"439b0fe2f799d1c722151e88e32bdefad8d34ebe":["9b5756469957918cac40a831acec9cf01c8c2bb3"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["439b0fe2f799d1c722151e88e32bdefad8d34ebe"]},"commit2Childs":{"7e2cb543b41c145f33390f460ee743d6693c9c6c":["9b5756469957918cac40a831acec9cf01c8c2bb3"],"decc8a7344e9231708f9991fa09db2cafec7a2dd":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"b305cb92ee47ddf7b15c6eeefe489c04d05b22ba":["decc8a7344e9231708f9991fa09db2cafec7a2dd"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b305cb92ee47ddf7b15c6eeefe489c04d05b22ba"],"9b5756469957918cac40a831acec9cf01c8c2bb3":["439b0fe2f799d1c722151e88e32bdefad8d34ebe"],"439b0fe2f799d1c722151e88e32bdefad8d34ebe":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}