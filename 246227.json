{"path":"solr/core/src/java/org/apache/solr/cloud/OverseerAutoReplicaFailoverThread#OverseerAutoReplicaFailoverThread(CloudConfig,ZkStateReader,UpdateShardHandler).mjava","commits":[{"id":"e15199583d3635cb940942caed05132dd6c4c7c6","date":1424875551,"type":1,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerAutoReplicaFailoverThread#OverseerAutoReplicaFailoverThread(CloudConfig,ZkStateReader,UpdateShardHandler).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerAutoReplicaFailoverThread#OverseerAutoReplicaFailoverThread(ConfigSolr,ZkStateReader,UpdateShardHandler).mjava","sourceNew":"  public OverseerAutoReplicaFailoverThread(CloudConfig config, ZkStateReader zkStateReader,\n      UpdateShardHandler updateShardHandler) {\n    this.zkStateReader = zkStateReader;\n    \n    this.workLoopDelay = config.getAutoReplicaFailoverWorkLoopDelay();\n    this.waitAfterExpiration = config.getAutoReplicaFailoverWaitAfterExpiration();\n    int badNodeExpiration = config.getAutoReplicaFailoverBadNodeExpiration();\n    \n    log.info(\n        \"Starting \"\n            + this.getClass().getSimpleName()\n            + \" autoReplicaFailoverWorkLoopDelay={} autoReplicaFailoverWaitAfterExpiration={} autoReplicaFailoverBadNodeExpiration={}\",\n        workLoopDelay, waitAfterExpiration, badNodeExpiration);\n\n    baseUrlForBadNodes = CacheBuilder.newBuilder()\n        .concurrencyLevel(1).expireAfterWrite(badNodeExpiration, TimeUnit.MILLISECONDS).build();\n    \n    // TODO: Speed up our work loop when live_nodes changes??\n\n    updateExecutor = updateShardHandler.getUpdateExecutor();\n\n    \n    // TODO: perhaps do a health ping periodically to each node (scaryish)\n    // And/OR work on JIRA issue around self health checks (SOLR-5805)\n  }\n\n","sourceOld":"  public OverseerAutoReplicaFailoverThread(ConfigSolr config, ZkStateReader zkStateReader,\n      UpdateShardHandler updateShardHandler) {\n    this.zkStateReader = zkStateReader;\n    \n    this.workLoopDelay = config.getAutoReplicaFailoverWorkLoopDelay();\n    this.waitAfterExpiration = config.getAutoReplicaFailoverWaitAfterExpiration();\n    int badNodeExpiration = config.getAutoReplicaFailoverBadNodeExpiration();\n    \n    log.info(\n        \"Starting \"\n            + this.getClass().getSimpleName()\n            + \" autoReplicaFailoverWorkLoopDelay={} autoReplicaFailoverWaitAfterExpiration={} autoReplicaFailoverBadNodeExpiration={}\",\n        workLoopDelay, waitAfterExpiration, badNodeExpiration);\n\n    baseUrlForBadNodes = CacheBuilder.newBuilder()\n        .concurrencyLevel(1).expireAfterWrite(badNodeExpiration, TimeUnit.MILLISECONDS).build();\n    \n    // TODO: Speed up our work loop when live_nodes changes??\n\n    updateExecutor = updateShardHandler.getUpdateExecutor();\n\n    \n    // TODO: perhaps do a health ping periodically to each node (scaryish)\n    // And/OR work on JIRA issue around self health checks (SOLR-5805)\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":0,"author":"Ryan Ernst","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerAutoReplicaFailoverThread#OverseerAutoReplicaFailoverThread(CloudConfig,ZkStateReader,UpdateShardHandler).mjava","pathOld":"/dev/null","sourceNew":"  public OverseerAutoReplicaFailoverThread(CloudConfig config, ZkStateReader zkStateReader,\n      UpdateShardHandler updateShardHandler) {\n    this.zkStateReader = zkStateReader;\n    \n    this.workLoopDelay = config.getAutoReplicaFailoverWorkLoopDelay();\n    this.waitAfterExpiration = config.getAutoReplicaFailoverWaitAfterExpiration();\n    int badNodeExpiration = config.getAutoReplicaFailoverBadNodeExpiration();\n    \n    log.info(\n        \"Starting \"\n            + this.getClass().getSimpleName()\n            + \" autoReplicaFailoverWorkLoopDelay={} autoReplicaFailoverWaitAfterExpiration={} autoReplicaFailoverBadNodeExpiration={}\",\n        workLoopDelay, waitAfterExpiration, badNodeExpiration);\n\n    baseUrlForBadNodes = CacheBuilder.newBuilder()\n        .concurrencyLevel(1).expireAfterWrite(badNodeExpiration, TimeUnit.MILLISECONDS).build();\n    \n    // TODO: Speed up our work loop when live_nodes changes??\n\n    updateExecutor = updateShardHandler.getUpdateExecutor();\n\n    \n    // TODO: perhaps do a health ping periodically to each node (scaryish)\n    // And/OR work on JIRA issue around self health checks (SOLR-5805)\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"962cd4f5e313777f35da8f521265323e84184929","date":1474533758,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerAutoReplicaFailoverThread#OverseerAutoReplicaFailoverThread(CloudConfig,ZkStateReader,UpdateShardHandler).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerAutoReplicaFailoverThread#OverseerAutoReplicaFailoverThread(CloudConfig,ZkStateReader,UpdateShardHandler).mjava","sourceNew":"  public OverseerAutoReplicaFailoverThread(CloudConfig config, ZkStateReader zkStateReader,\n      UpdateShardHandler updateShardHandler) {\n    this.zkStateReader = zkStateReader;\n    \n    this.workLoopDelay = config.getAutoReplicaFailoverWorkLoopDelay();\n    this.waitAfterExpiration = config.getAutoReplicaFailoverWaitAfterExpiration();\n    int badNodeExpiration = config.getAutoReplicaFailoverBadNodeExpiration();\n    \n    log.debug(\n        \"Starting \"\n            + this.getClass().getSimpleName()\n            + \" autoReplicaFailoverWorkLoopDelay={} autoReplicaFailoverWaitAfterExpiration={} autoReplicaFailoverBadNodeExpiration={}\",\n        workLoopDelay, waitAfterExpiration, badNodeExpiration);\n\n    baseUrlForBadNodes = CacheBuilder.newBuilder()\n        .concurrencyLevel(1).expireAfterWrite(badNodeExpiration, TimeUnit.MILLISECONDS).build();\n    \n    // TODO: Speed up our work loop when live_nodes changes??\n\n    updateExecutor = updateShardHandler.getUpdateExecutor();\n\n    \n    // TODO: perhaps do a health ping periodically to each node (scaryish)\n    // And/OR work on JIRA issue around self health checks (SOLR-5805)\n  }\n\n","sourceOld":"  public OverseerAutoReplicaFailoverThread(CloudConfig config, ZkStateReader zkStateReader,\n      UpdateShardHandler updateShardHandler) {\n    this.zkStateReader = zkStateReader;\n    \n    this.workLoopDelay = config.getAutoReplicaFailoverWorkLoopDelay();\n    this.waitAfterExpiration = config.getAutoReplicaFailoverWaitAfterExpiration();\n    int badNodeExpiration = config.getAutoReplicaFailoverBadNodeExpiration();\n    \n    log.info(\n        \"Starting \"\n            + this.getClass().getSimpleName()\n            + \" autoReplicaFailoverWorkLoopDelay={} autoReplicaFailoverWaitAfterExpiration={} autoReplicaFailoverBadNodeExpiration={}\",\n        workLoopDelay, waitAfterExpiration, badNodeExpiration);\n\n    baseUrlForBadNodes = CacheBuilder.newBuilder()\n        .concurrencyLevel(1).expireAfterWrite(badNodeExpiration, TimeUnit.MILLISECONDS).build();\n    \n    // TODO: Speed up our work loop when live_nodes changes??\n\n    updateExecutor = updateShardHandler.getUpdateExecutor();\n\n    \n    // TODO: perhaps do a health ping periodically to each node (scaryish)\n    // And/OR work on JIRA issue around self health checks (SOLR-5805)\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerAutoReplicaFailoverThread#OverseerAutoReplicaFailoverThread(CloudConfig,ZkStateReader,UpdateShardHandler).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerAutoReplicaFailoverThread#OverseerAutoReplicaFailoverThread(CloudConfig,ZkStateReader,UpdateShardHandler).mjava","sourceNew":"  public OverseerAutoReplicaFailoverThread(CloudConfig config, ZkStateReader zkStateReader,\n      UpdateShardHandler updateShardHandler) {\n    this.zkStateReader = zkStateReader;\n    \n    this.workLoopDelay = config.getAutoReplicaFailoverWorkLoopDelay();\n    this.waitAfterExpiration = config.getAutoReplicaFailoverWaitAfterExpiration();\n    int badNodeExpiration = config.getAutoReplicaFailoverBadNodeExpiration();\n    \n    log.debug(\n        \"Starting \"\n            + this.getClass().getSimpleName()\n            + \" autoReplicaFailoverWorkLoopDelay={} autoReplicaFailoverWaitAfterExpiration={} autoReplicaFailoverBadNodeExpiration={}\",\n        workLoopDelay, waitAfterExpiration, badNodeExpiration);\n\n    baseUrlForBadNodes = CacheBuilder.newBuilder()\n        .concurrencyLevel(1).expireAfterWrite(badNodeExpiration, TimeUnit.MILLISECONDS).build();\n    \n    // TODO: Speed up our work loop when live_nodes changes??\n\n    updateExecutor = updateShardHandler.getUpdateExecutor();\n\n    \n    // TODO: perhaps do a health ping periodically to each node (scaryish)\n    // And/OR work on JIRA issue around self health checks (SOLR-5805)\n  }\n\n","sourceOld":"  public OverseerAutoReplicaFailoverThread(CloudConfig config, ZkStateReader zkStateReader,\n      UpdateShardHandler updateShardHandler) {\n    this.zkStateReader = zkStateReader;\n    \n    this.workLoopDelay = config.getAutoReplicaFailoverWorkLoopDelay();\n    this.waitAfterExpiration = config.getAutoReplicaFailoverWaitAfterExpiration();\n    int badNodeExpiration = config.getAutoReplicaFailoverBadNodeExpiration();\n    \n    log.info(\n        \"Starting \"\n            + this.getClass().getSimpleName()\n            + \" autoReplicaFailoverWorkLoopDelay={} autoReplicaFailoverWaitAfterExpiration={} autoReplicaFailoverBadNodeExpiration={}\",\n        workLoopDelay, waitAfterExpiration, badNodeExpiration);\n\n    baseUrlForBadNodes = CacheBuilder.newBuilder()\n        .concurrencyLevel(1).expireAfterWrite(badNodeExpiration, TimeUnit.MILLISECONDS).build();\n    \n    // TODO: Speed up our work loop when live_nodes changes??\n\n    updateExecutor = updateShardHandler.getUpdateExecutor();\n\n    \n    // TODO: perhaps do a health ping periodically to each node (scaryish)\n    // And/OR work on JIRA issue around self health checks (SOLR-5805)\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/OverseerAutoReplicaFailoverThread#OverseerAutoReplicaFailoverThread(CloudConfig,ZkStateReader,UpdateShardHandler).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerAutoReplicaFailoverThread#OverseerAutoReplicaFailoverThread(CloudConfig,ZkStateReader,UpdateShardHandler).mjava","sourceNew":"  public OverseerAutoReplicaFailoverThread(CloudConfig config, ZkStateReader zkStateReader,\n      UpdateShardHandler updateShardHandler) {\n    this.zkStateReader = zkStateReader;\n    \n    this.workLoopDelay = config.getAutoReplicaFailoverWorkLoopDelay();\n    this.waitAfterExpiration = config.getAutoReplicaFailoverWaitAfterExpiration();\n    int badNodeExpiration = config.getAutoReplicaFailoverBadNodeExpiration();\n    \n    log.debug(\n        \"Starting \"\n            + this.getClass().getSimpleName()\n            + \" autoReplicaFailoverWorkLoopDelay={} autoReplicaFailoverWaitAfterExpiration={} autoReplicaFailoverBadNodeExpiration={}\",\n        workLoopDelay, waitAfterExpiration, badNodeExpiration);\n\n    baseUrlForBadNodes = CacheBuilder.newBuilder()\n        .concurrencyLevel(1).expireAfterWrite(badNodeExpiration, TimeUnit.MILLISECONDS).build();\n    \n    // TODO: Speed up our work loop when live_nodes changes??\n\n    updateExecutor = updateShardHandler.getUpdateExecutor();\n\n    \n    // TODO: perhaps do a health ping periodically to each node (scaryish)\n    // And/OR work on JIRA issue around self health checks (SOLR-5805)\n  }\n\n","sourceOld":"  public OverseerAutoReplicaFailoverThread(CloudConfig config, ZkStateReader zkStateReader,\n      UpdateShardHandler updateShardHandler) {\n    this.zkStateReader = zkStateReader;\n    \n    this.workLoopDelay = config.getAutoReplicaFailoverWorkLoopDelay();\n    this.waitAfterExpiration = config.getAutoReplicaFailoverWaitAfterExpiration();\n    int badNodeExpiration = config.getAutoReplicaFailoverBadNodeExpiration();\n    \n    log.info(\n        \"Starting \"\n            + this.getClass().getSimpleName()\n            + \" autoReplicaFailoverWorkLoopDelay={} autoReplicaFailoverWaitAfterExpiration={} autoReplicaFailoverBadNodeExpiration={}\",\n        workLoopDelay, waitAfterExpiration, badNodeExpiration);\n\n    baseUrlForBadNodes = CacheBuilder.newBuilder()\n        .concurrencyLevel(1).expireAfterWrite(badNodeExpiration, TimeUnit.MILLISECONDS).build();\n    \n    // TODO: Speed up our work loop when live_nodes changes??\n\n    updateExecutor = updateShardHandler.getUpdateExecutor();\n\n    \n    // TODO: perhaps do a health ping periodically to each node (scaryish)\n    // And/OR work on JIRA issue around self health checks (SOLR-5805)\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"717e5ceb2acae36d422ec75e5a4ce9fac40506e1","date":1501239603,"type":4,"author":"Cao Manh Dat","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerAutoReplicaFailoverThread#OverseerAutoReplicaFailoverThread(CloudConfig,ZkStateReader,UpdateShardHandler).mjava","sourceNew":null,"sourceOld":"  public OverseerAutoReplicaFailoverThread(CloudConfig config, ZkStateReader zkStateReader,\n      UpdateShardHandler updateShardHandler) {\n    this.zkStateReader = zkStateReader;\n    \n    this.workLoopDelay = config.getAutoReplicaFailoverWorkLoopDelay();\n    this.waitAfterExpiration = config.getAutoReplicaFailoverWaitAfterExpiration();\n    int badNodeExpiration = config.getAutoReplicaFailoverBadNodeExpiration();\n    \n    log.debug(\n        \"Starting \"\n            + this.getClass().getSimpleName()\n            + \" autoReplicaFailoverWorkLoopDelay={} autoReplicaFailoverWaitAfterExpiration={} autoReplicaFailoverBadNodeExpiration={}\",\n        workLoopDelay, waitAfterExpiration, badNodeExpiration);\n\n    baseUrlForBadNodes = CacheBuilder.newBuilder()\n        .concurrencyLevel(1).expireAfterWrite(badNodeExpiration, TimeUnit.MILLISECONDS).build();\n    \n    // TODO: Speed up our work loop when live_nodes changes??\n\n    updateExecutor = updateShardHandler.getUpdateExecutor();\n\n    \n    // TODO: perhaps do a health ping periodically to each node (scaryish)\n    // And/OR work on JIRA issue around self health checks (SOLR-5805)\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c304e97e7c1d472bc70e801b35ee78583916c6cd","date":1507105431,"type":4,"author":"Noble Paul","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerAutoReplicaFailoverThread#OverseerAutoReplicaFailoverThread(CloudConfig,ZkStateReader,UpdateShardHandler).mjava","sourceNew":null,"sourceOld":"  public OverseerAutoReplicaFailoverThread(CloudConfig config, ZkStateReader zkStateReader,\n      UpdateShardHandler updateShardHandler) {\n    this.zkStateReader = zkStateReader;\n    \n    this.workLoopDelay = config.getAutoReplicaFailoverWorkLoopDelay();\n    this.waitAfterExpiration = config.getAutoReplicaFailoverWaitAfterExpiration();\n    int badNodeExpiration = config.getAutoReplicaFailoverBadNodeExpiration();\n    \n    log.debug(\n        \"Starting \"\n            + this.getClass().getSimpleName()\n            + \" autoReplicaFailoverWorkLoopDelay={} autoReplicaFailoverWaitAfterExpiration={} autoReplicaFailoverBadNodeExpiration={}\",\n        workLoopDelay, waitAfterExpiration, badNodeExpiration);\n\n    baseUrlForBadNodes = CacheBuilder.newBuilder()\n        .concurrencyLevel(1).expireAfterWrite(badNodeExpiration, TimeUnit.MILLISECONDS).build();\n    \n    // TODO: Speed up our work loop when live_nodes changes??\n\n    updateExecutor = updateShardHandler.getUpdateExecutor();\n\n    \n    // TODO: perhaps do a health ping periodically to each node (scaryish)\n    // And/OR work on JIRA issue around self health checks (SOLR-5805)\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"560c18d71dad43d675158783c3840f8c80d6d39c","date":1507105532,"type":4,"author":"Cao Manh Dat","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/cloud/OverseerAutoReplicaFailoverThread#OverseerAutoReplicaFailoverThread(CloudConfig,ZkStateReader,UpdateShardHandler).mjava","sourceNew":null,"sourceOld":"  public OverseerAutoReplicaFailoverThread(CloudConfig config, ZkStateReader zkStateReader,\n      UpdateShardHandler updateShardHandler) {\n    this.zkStateReader = zkStateReader;\n    \n    this.workLoopDelay = config.getAutoReplicaFailoverWorkLoopDelay();\n    this.waitAfterExpiration = config.getAutoReplicaFailoverWaitAfterExpiration();\n    int badNodeExpiration = config.getAutoReplicaFailoverBadNodeExpiration();\n    \n    log.debug(\n        \"Starting \"\n            + this.getClass().getSimpleName()\n            + \" autoReplicaFailoverWorkLoopDelay={} autoReplicaFailoverWaitAfterExpiration={} autoReplicaFailoverBadNodeExpiration={}\",\n        workLoopDelay, waitAfterExpiration, badNodeExpiration);\n\n    baseUrlForBadNodes = CacheBuilder.newBuilder()\n        .concurrencyLevel(1).expireAfterWrite(badNodeExpiration, TimeUnit.MILLISECONDS).build();\n    \n    // TODO: Speed up our work loop when live_nodes changes??\n\n    updateExecutor = updateShardHandler.getUpdateExecutor();\n\n    \n    // TODO: perhaps do a health ping periodically to each node (scaryish)\n    // And/OR work on JIRA issue around self health checks (SOLR-5805)\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"962cd4f5e313777f35da8f521265323e84184929":["e15199583d3635cb940942caed05132dd6c4c7c6"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","e15199583d3635cb940942caed05132dd6c4c7c6"],"c304e97e7c1d472bc70e801b35ee78583916c6cd":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","717e5ceb2acae36d422ec75e5a4ce9fac40506e1"],"717e5ceb2acae36d422ec75e5a4ce9fac40506e1":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["e15199583d3635cb940942caed05132dd6c4c7c6","962cd4f5e313777f35da8f521265323e84184929"],"560c18d71dad43d675158783c3840f8c80d6d39c":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","c304e97e7c1d472bc70e801b35ee78583916c6cd"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["e15199583d3635cb940942caed05132dd6c4c7c6","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["560c18d71dad43d675158783c3840f8c80d6d39c"],"e15199583d3635cb940942caed05132dd6c4c7c6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"962cd4f5e313777f35da8f521265323e84184929":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"c304e97e7c1d472bc70e801b35ee78583916c6cd":["560c18d71dad43d675158783c3840f8c80d6d39c"],"717e5ceb2acae36d422ec75e5a4ce9fac40506e1":["c304e97e7c1d472bc70e801b35ee78583916c6cd"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","e15199583d3635cb940942caed05132dd6c4c7c6"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["c304e97e7c1d472bc70e801b35ee78583916c6cd","717e5ceb2acae36d422ec75e5a4ce9fac40506e1","560c18d71dad43d675158783c3840f8c80d6d39c","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"560c18d71dad43d675158783c3840f8c80d6d39c":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"e15199583d3635cb940942caed05132dd6c4c7c6":["962cd4f5e313777f35da8f521265323e84184929","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","17e5da53e4e5bd659e22add9bba1cfa222e7e30d","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}