{"path":"lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","commits":[{"id":"7d1467e0527cb2aeb9d7a05c26948ac9d82d81fa","date":1349450075,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (compressionMode == matchingFieldsReader.getCompressionMode() // same compression mode\n                && (numBufferedDocs == 0 || bufferedDocs.length >= chunkSize) // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to uncompress, just copy data\n              endWithPreviousDocument();\n              if (bufferedDocs.length >= chunkSize) {\n                flush();\n              }\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // uncompress\n              it.uncompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                addRawDocument(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["c33c9c4ca6dc47739595c708779c537e8fb8813d","c33c9c4ca6dc47739595c708779c537e8fb8813d","b547be573e7e33d9a40568f89ceb5642382c96e2","b547be573e7e33d9a40568f89ceb5642382c96e2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5af6a67fb827380f7fe2fdf3baa34b10b783f2f1","date":1351696336,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (compressionMode == matchingFieldsReader.getCompressionMode() // same compression mode\n                && (numBufferedDocs == 0 || bufferedDocs.length >= chunkSize) // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              endWithPreviousDocument();\n              if (bufferedDocs.length >= chunkSize) {\n                flush();\n              }\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                addRawDocument(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (compressionMode == matchingFieldsReader.getCompressionMode() // same compression mode\n                && (numBufferedDocs == 0 || bufferedDocs.length >= chunkSize) // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to uncompress, just copy data\n              endWithPreviousDocument();\n              if (bufferedDocs.length >= chunkSize) {\n                flush();\n              }\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // uncompress\n              it.uncompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                addRawDocument(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":["c33c9c4ca6dc47739595c708779c537e8fb8813d","c33c9c4ca6dc47739595c708779c537e8fb8813d"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"eba3cb2a268b9fb6f5be011fbaaf698699dcf24c","date":1352305464,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (compressionMode == matchingFieldsReader.getCompressionMode() // same compression mode\n                && (numBufferedDocs == 0 || triggerFlush()) // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              endWithPreviousDocument();\n              if (triggerFlush()) {\n                flush();\n              }\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument(it.numStoredFields[diff]);\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (compressionMode == matchingFieldsReader.getCompressionMode() // same compression mode\n                && (numBufferedDocs == 0 || bufferedDocs.length >= chunkSize) // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              endWithPreviousDocument();\n              if (bufferedDocs.length >= chunkSize) {\n                flush();\n              }\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                addRawDocument(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":["c33c9c4ca6dc47739595c708779c537e8fb8813d","c33c9c4ca6dc47739595c708779c537e8fb8813d"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5e04b732c631a77cbbd25b6ce43c2a8abb1e9e69","date":1352818449,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (compressionMode == matchingFieldsReader.getCompressionMode() // same compression mode\n                && (numBufferedDocs == 0 || triggerFlush()) // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              endWithPreviousDocument();\n              if (triggerFlush()) {\n                flush();\n              }\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument(it.numStoredFields[diff]);\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (compressionMode == matchingFieldsReader.getCompressionMode() // same compression mode\n                && (numBufferedDocs == 0 || triggerFlush()) // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              endWithPreviousDocument();\n              if (triggerFlush()) {\n                flush();\n              }\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument(it.numStoredFields[diff]);\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"407687e67faf6e1f02a211ca078d8e3eed631027","date":1355157407,"type":5,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (compressionMode == matchingFieldsReader.getCompressionMode() // same compression mode\n                && (numBufferedDocs == 0 || triggerFlush()) // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              endWithPreviousDocument();\n              if (triggerFlush()) {\n                flush();\n              }\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument(it.numStoredFields[diff]);\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (compressionMode == matchingFieldsReader.getCompressionMode() // same compression mode\n                && (numBufferedDocs == 0 || triggerFlush()) // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              endWithPreviousDocument();\n              if (triggerFlush()) {\n                flush();\n              }\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument(it.numStoredFields[diff]);\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"5af6a67fb827380f7fe2fdf3baa34b10b783f2f1":["7d1467e0527cb2aeb9d7a05c26948ac9d82d81fa"],"5e04b732c631a77cbbd25b6ce43c2a8abb1e9e69":["eba3cb2a268b9fb6f5be011fbaaf698699dcf24c"],"7d1467e0527cb2aeb9d7a05c26948ac9d82d81fa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"407687e67faf6e1f02a211ca078d8e3eed631027":["eba3cb2a268b9fb6f5be011fbaaf698699dcf24c","5e04b732c631a77cbbd25b6ce43c2a8abb1e9e69"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["5e04b732c631a77cbbd25b6ce43c2a8abb1e9e69"],"eba3cb2a268b9fb6f5be011fbaaf698699dcf24c":["5af6a67fb827380f7fe2fdf3baa34b10b783f2f1"]},"commit2Childs":{"5af6a67fb827380f7fe2fdf3baa34b10b783f2f1":["eba3cb2a268b9fb6f5be011fbaaf698699dcf24c"],"5e04b732c631a77cbbd25b6ce43c2a8abb1e9e69":["407687e67faf6e1f02a211ca078d8e3eed631027","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"7d1467e0527cb2aeb9d7a05c26948ac9d82d81fa":["5af6a67fb827380f7fe2fdf3baa34b10b783f2f1"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["7d1467e0527cb2aeb9d7a05c26948ac9d82d81fa"],"407687e67faf6e1f02a211ca078d8e3eed631027":[],"eba3cb2a268b9fb6f5be011fbaaf698699dcf24c":["5e04b732c631a77cbbd25b6ce43c2a8abb1e9e69","407687e67faf6e1f02a211ca078d8e3eed631027"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["407687e67faf6e1f02a211ca078d8e3eed631027","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}