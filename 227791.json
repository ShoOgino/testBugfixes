{"path":"src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","commits":[{"id":"4ceaa9738cad9616d1831286111af106e13e0e4b","date":1114543820,"type":1,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","pathOld":"src/java/org/apache/lucene/search/MultiSearcher#prepareWeight(Query).mjava","sourceNew":"  /**\n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    Set terms = new HashSet();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    HashMap dfMap = new HashMap();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], new Integer(aggregatedDfs[i]));\n    }\n\n    // step4\n    int numDocs = maxDoc();\n    CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs);\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","sourceOld":"  /**\n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  private Weight prepareWeight(Query original) throws IOException {\n    // step 1\n    Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    Set terms = new HashSet();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    HashMap dfMap = new HashMap();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], new Integer(aggregatedDfs[i]));\n    }\n\n    // step4\n    int numDocs = maxDoc();\n    CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs);\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3530bdb35df564c8916e1fa475608edb79a1191b","date":1114565957,"type":3,"author":"Erik Hatcher","isMerge":false,"pathNew":"src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","pathOld":"src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","sourceNew":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    Set terms = new HashSet();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    HashMap dfMap = new HashMap();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], new Integer(aggregatedDfs[i]));\n    }\n\n    // step4\n    int numDocs = maxDoc();\n    CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs);\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","sourceOld":"  /**\n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    Set terms = new HashSet();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    HashMap dfMap = new HashMap();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], new Integer(aggregatedDfs[i]));\n    }\n\n    // step4\n    int numDocs = maxDoc();\n    CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs);\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"94b168f99ebebb8d23639b67c3f4f8939cd4f5be","date":1177535443,"type":3,"author":"Doron Cohen","isMerge":false,"pathNew":"src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","pathOld":"src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","sourceNew":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    Set terms = new HashSet();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    HashMap dfMap = new HashMap();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], new Integer(aggregatedDfs[i]));\n    }\n\n    // step4\n    int numDocs = maxDoc();\n    CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","sourceOld":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    Set terms = new HashSet();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    HashMap dfMap = new HashMap();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], new Integer(aggregatedDfs[i]));\n    }\n\n    // step4\n    int numDocs = maxDoc();\n    CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs);\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","bugFix":["402061809f3a4629ea0c449e33e9f94a9772f3c3"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"052fac7830290bd38a04cddee1a121ee07656b56","date":1245780702,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/search/MultiSearcher#createQueryWeight(Query).mjava","pathOld":"src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","sourceNew":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  protected QueryWeight createQueryWeight(Query original) throws IOException {\n    // step 1\n    Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    Set terms = new HashSet();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    HashMap dfMap = new HashMap();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], new Integer(aggregatedDfs[i]));\n    }\n\n    // step4\n    int numDocs = maxDoc();\n    CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.queryWeight(cacheSim);\n  }\n\n","sourceOld":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    Set terms = new HashSet();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    HashMap dfMap = new HashMap();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], new Integer(aggregatedDfs[i]));\n    }\n\n    // step4\n    int numDocs = maxDoc();\n    CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe941135bdfc28c81e20b4d21422f8726af34925","date":1250040150,"type":1,"author":"Mark Robert Miller","isMerge":false,"pathNew":"src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","pathOld":"src/java/org/apache/lucene/search/MultiSearcher#createQueryWeight(Query).mjava","sourceNew":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    Set terms = new HashSet();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    HashMap dfMap = new HashMap();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], new Integer(aggregatedDfs[i]));\n    }\n\n    // step4\n    int numDocs = maxDoc();\n    CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","sourceOld":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  protected QueryWeight createQueryWeight(Query original) throws IOException {\n    // step 1\n    Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    Set terms = new HashSet();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    HashMap dfMap = new HashMap();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], new Integer(aggregatedDfs[i]));\n    }\n\n    // step4\n    int numDocs = maxDoc();\n    CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.queryWeight(cacheSim);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6bcde5e3f23911110baa101ed062b544162825b5","date":1254521804,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","pathOld":"src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","sourceNew":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    Set terms = new HashSet();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    HashMap dfMap = new HashMap();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], Integer.valueOf(aggregatedDfs[i]));\n    }\n\n    // step4\n    int numDocs = maxDoc();\n    CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","sourceOld":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    Set terms = new HashSet();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    HashMap dfMap = new HashMap();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], new Integer(aggregatedDfs[i]));\n    }\n\n    // step4\n    int numDocs = maxDoc();\n    CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ef82ff03e4016c705811b2658e81471a645c0e49","date":1255900293,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","pathOld":"src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","sourceNew":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    Set<Term> terms = new HashSet<Term>();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    HashMap<Term,Integer> dfMap = new HashMap<Term,Integer>();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], Integer.valueOf(aggregatedDfs[i]));\n    }\n\n    // step4\n    int numDocs = maxDoc();\n    CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","sourceOld":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    Set terms = new HashSet();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    HashMap dfMap = new HashMap();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], Integer.valueOf(aggregatedDfs[i]));\n    }\n\n    // step4\n    int numDocs = maxDoc();\n    CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8a9e385641d717e641408d8fbbc62be8fc766357","date":1256746606,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","pathOld":"src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","sourceNew":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  @Override\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    Set<Term> terms = new HashSet<Term>();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    HashMap<Term,Integer> dfMap = new HashMap<Term,Integer>();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], Integer.valueOf(aggregatedDfs[i]));\n    }\n\n    // step4\n    int numDocs = maxDoc();\n    CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","sourceOld":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    Set<Term> terms = new HashSet<Term>();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    HashMap<Term,Integer> dfMap = new HashMap<Term,Integer>();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], Integer.valueOf(aggregatedDfs[i]));\n    }\n\n    // step4\n    int numDocs = maxDoc();\n    CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7325af1b9f607a9e8e30785e8de8ffd1d4c08ddf","date":1257873376,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","pathOld":"src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","sourceNew":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  @Override\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    final Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    final Set<Term> terms = new HashSet<Term>();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    final Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    final HashMap<Term,Integer> dfMap = new HashMap<Term,Integer>();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], Integer.valueOf(aggregatedDfs[i]));\n    }\n\n    // step4\n    final int numDocs = maxDoc();\n    final CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","sourceOld":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  @Override\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    Set<Term> terms = new HashSet<Term>();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    HashMap<Term,Integer> dfMap = new HashMap<Term,Integer>();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], Integer.valueOf(aggregatedDfs[i]));\n    }\n\n    // step4\n    int numDocs = maxDoc();\n    CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"15663e8ac5d62b8a3eccef533d0384f3b4c139ea","date":1260310472,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","pathOld":"src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","sourceNew":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  @Override\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    final Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    final Set<Term> terms = new HashSet<Term>();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    final Map<Term,Integer> dfMap = createDocFrequencyMap(terms);\n\n    // step4\n    final int numDocs = maxDoc();\n    final CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","sourceOld":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  @Override\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    final Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    final Set<Term> terms = new HashSet<Term>();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    final Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    final HashMap<Term,Integer> dfMap = new HashMap<Term,Integer>();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], Integer.valueOf(aggregatedDfs[i]));\n    }\n\n    // step4\n    final int numDocs = maxDoc();\n    final CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","pathOld":"src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","sourceNew":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  @Override\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    final Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    final Set<Term> terms = new HashSet<Term>();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    final Map<Term,Integer> dfMap = createDocFrequencyMap(terms);\n\n    // step4\n    final int numDocs = maxDoc();\n    final CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","sourceOld":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  @Override\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    final Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    final Set<Term> terms = new HashSet<Term>();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    final Map<Term,Integer> dfMap = createDocFrequencyMap(terms);\n\n    // step4\n    final int numDocs = maxDoc();\n    final CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"15663e8ac5d62b8a3eccef533d0384f3b4c139ea":["7325af1b9f607a9e8e30785e8de8ffd1d4c08ddf"],"6bcde5e3f23911110baa101ed062b544162825b5":["fe941135bdfc28c81e20b4d21422f8726af34925"],"4ceaa9738cad9616d1831286111af106e13e0e4b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"94b168f99ebebb8d23639b67c3f4f8939cd4f5be":["3530bdb35df564c8916e1fa475608edb79a1191b"],"8a9e385641d717e641408d8fbbc62be8fc766357":["ef82ff03e4016c705811b2658e81471a645c0e49"],"fe941135bdfc28c81e20b4d21422f8726af34925":["052fac7830290bd38a04cddee1a121ee07656b56"],"7325af1b9f607a9e8e30785e8de8ffd1d4c08ddf":["8a9e385641d717e641408d8fbbc62be8fc766357"],"ef82ff03e4016c705811b2658e81471a645c0e49":["6bcde5e3f23911110baa101ed062b544162825b5"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3530bdb35df564c8916e1fa475608edb79a1191b":["4ceaa9738cad9616d1831286111af106e13e0e4b"],"052fac7830290bd38a04cddee1a121ee07656b56":["94b168f99ebebb8d23639b67c3f4f8939cd4f5be"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["15663e8ac5d62b8a3eccef533d0384f3b4c139ea"]},"commit2Childs":{"15663e8ac5d62b8a3eccef533d0384f3b4c139ea":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"6bcde5e3f23911110baa101ed062b544162825b5":["ef82ff03e4016c705811b2658e81471a645c0e49"],"4ceaa9738cad9616d1831286111af106e13e0e4b":["3530bdb35df564c8916e1fa475608edb79a1191b"],"94b168f99ebebb8d23639b67c3f4f8939cd4f5be":["052fac7830290bd38a04cddee1a121ee07656b56"],"8a9e385641d717e641408d8fbbc62be8fc766357":["7325af1b9f607a9e8e30785e8de8ffd1d4c08ddf"],"fe941135bdfc28c81e20b4d21422f8726af34925":["6bcde5e3f23911110baa101ed062b544162825b5"],"7325af1b9f607a9e8e30785e8de8ffd1d4c08ddf":["15663e8ac5d62b8a3eccef533d0384f3b4c139ea"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4ceaa9738cad9616d1831286111af106e13e0e4b"],"ef82ff03e4016c705811b2658e81471a645c0e49":["8a9e385641d717e641408d8fbbc62be8fc766357"],"3530bdb35df564c8916e1fa475608edb79a1191b":["94b168f99ebebb8d23639b67c3f4f8939cd4f5be"],"052fac7830290bd38a04cddee1a121ee07656b56":["fe941135bdfc28c81e20b4d21422f8726af34925"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}