{"path":"lucene/backwards/src/demo/org/apache/lucene/demo/HTMLDocument#Document(File).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/backwards/src/demo/org/apache/lucene/demo/HTMLDocument#Document(File).mjava","pathOld":"backwards/src/demo/org/apache/lucene/demo/HTMLDocument#Document(File).mjava","sourceNew":"  public static Document Document(File f)\n       throws IOException, InterruptedException  {\n    // make a new, empty document\n    Document doc = new Document();\n\n    // Add the url as a field named \"path\".  Use a field that is \n    // indexed (i.e. searchable), but don't tokenize the field into words.\n    doc.add(new Field(\"path\", f.getPath().replace(dirSep, '/'), Field.Store.YES,\n        Field.Index.NOT_ANALYZED));\n\n    // Add the last modified date of the file a field named \"modified\".  \n    // Use a field that is indexed (i.e. searchable), but don't tokenize\n    // the field into words.\n    doc.add(new Field(\"modified\",\n        DateTools.timeToString(f.lastModified(), DateTools.Resolution.MINUTE),\n        Field.Store.YES, Field.Index.NOT_ANALYZED));\n\n    // Add the uid as a field, so that index can be incrementally maintained.\n    // This field is not stored with document, it is indexed, but it is not\n    // tokenized prior to indexing.\n    doc.add(new Field(\"uid\", uid(f), Field.Store.NO, Field.Index.NOT_ANALYZED));\n\n    FileInputStream fis = new FileInputStream(f);\n    HTMLParser parser = new HTMLParser(fis);\n      \n    // Add the tag-stripped contents as a Reader-valued Text field so it will\n    // get tokenized and indexed.\n    doc.add(new Field(\"contents\", parser.getReader()));\n\n    // Add the summary as a field that is stored and returned with\n    // hit documents for display.\n    doc.add(new Field(\"summary\", parser.getSummary(), Field.Store.YES, Field.Index.NO));\n\n    // Add the title as a field that it can be searched and that is stored.\n    doc.add(new Field(\"title\", parser.getTitle(), Field.Store.YES, Field.Index.ANALYZED));\n\n    // return the document\n    return doc;\n  }\n\n","sourceOld":"  public static Document Document(File f)\n       throws IOException, InterruptedException  {\n    // make a new, empty document\n    Document doc = new Document();\n\n    // Add the url as a field named \"path\".  Use a field that is \n    // indexed (i.e. searchable), but don't tokenize the field into words.\n    doc.add(new Field(\"path\", f.getPath().replace(dirSep, '/'), Field.Store.YES,\n        Field.Index.NOT_ANALYZED));\n\n    // Add the last modified date of the file a field named \"modified\".  \n    // Use a field that is indexed (i.e. searchable), but don't tokenize\n    // the field into words.\n    doc.add(new Field(\"modified\",\n        DateTools.timeToString(f.lastModified(), DateTools.Resolution.MINUTE),\n        Field.Store.YES, Field.Index.NOT_ANALYZED));\n\n    // Add the uid as a field, so that index can be incrementally maintained.\n    // This field is not stored with document, it is indexed, but it is not\n    // tokenized prior to indexing.\n    doc.add(new Field(\"uid\", uid(f), Field.Store.NO, Field.Index.NOT_ANALYZED));\n\n    FileInputStream fis = new FileInputStream(f);\n    HTMLParser parser = new HTMLParser(fis);\n      \n    // Add the tag-stripped contents as a Reader-valued Text field so it will\n    // get tokenized and indexed.\n    doc.add(new Field(\"contents\", parser.getReader()));\n\n    // Add the summary as a field that is stored and returned with\n    // hit documents for display.\n    doc.add(new Field(\"summary\", parser.getSummary(), Field.Store.YES, Field.Index.NO));\n\n    // Add the title as a field that it can be searched and that is stored.\n    doc.add(new Field(\"title\", parser.getTitle(), Field.Store.YES, Field.Index.ANALYZED));\n\n    // return the document\n    return doc;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6","date":1272983566,"type":4,"author":"Uwe Schindler","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/backwards/src/demo/org/apache/lucene/demo/HTMLDocument#Document(File).mjava","sourceNew":null,"sourceOld":"  public static Document Document(File f)\n       throws IOException, InterruptedException  {\n    // make a new, empty document\n    Document doc = new Document();\n\n    // Add the url as a field named \"path\".  Use a field that is \n    // indexed (i.e. searchable), but don't tokenize the field into words.\n    doc.add(new Field(\"path\", f.getPath().replace(dirSep, '/'), Field.Store.YES,\n        Field.Index.NOT_ANALYZED));\n\n    // Add the last modified date of the file a field named \"modified\".  \n    // Use a field that is indexed (i.e. searchable), but don't tokenize\n    // the field into words.\n    doc.add(new Field(\"modified\",\n        DateTools.timeToString(f.lastModified(), DateTools.Resolution.MINUTE),\n        Field.Store.YES, Field.Index.NOT_ANALYZED));\n\n    // Add the uid as a field, so that index can be incrementally maintained.\n    // This field is not stored with document, it is indexed, but it is not\n    // tokenized prior to indexing.\n    doc.add(new Field(\"uid\", uid(f), Field.Store.NO, Field.Index.NOT_ANALYZED));\n\n    FileInputStream fis = new FileInputStream(f);\n    HTMLParser parser = new HTMLParser(fis);\n      \n    // Add the tag-stripped contents as a Reader-valued Text field so it will\n    // get tokenized and indexed.\n    doc.add(new Field(\"contents\", parser.getReader()));\n\n    // Add the summary as a field that is stored and returned with\n    // hit documents for display.\n    doc.add(new Field(\"summary\", parser.getSummary(), Field.Store.YES, Field.Index.NO));\n\n    // Add the title as a field that it can be searched and that is stored.\n    doc.add(new Field(\"title\", parser.getTitle(), Field.Store.YES, Field.Index.ANALYZED));\n\n    // return the document\n    return doc;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}