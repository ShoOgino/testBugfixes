{"path":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","commits":[{"id":"4831dd345148fcd7c33877b449ade21fc45459d8","date":1363963811,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,Term[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i], true)) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, Term terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i].bytes(), true)) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eee5f2a24465d2c9a5f86ab84b7c35041a30fda8","date":1373996650,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i], true)) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"71393760b816981b3e59704441c8bd6f32276046","date":1376329629,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","date":1376366778,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i], true)) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4dcface3bab82444afcba8add9fb24a2d7276330","date":1376585535,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset || end > contentLength) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset || end > contentLength) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dd81b1d062b9688a18721a1adfc489577479856a","date":1390711758,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = terms[off.id];\n        if (term == null) {\n          // multitermquery match, pull from payload\n          term = off.dp.getPayload();\n          assert term != null;\n        }\n        current.addMatch(start, end, term);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset || end > contentLength) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset || end > contentLength) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = terms[off.id];\n        if (term == null) {\n          // multitermquery match, pull from payload\n          term = off.dp.getPayload();\n          assert term != null;\n        }\n        current.addMatch(start, end, term);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset || end > contentLength) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = terms[off.id];\n        if (term == null) {\n          // multitermquery match, pull from payload\n          term = off.dp.getPayload();\n          assert term != null;\n        }\n        current.addMatch(start, end, term);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset || end > contentLength) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":5,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,PostingsEnum[],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, PostingsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      PostingsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.postings(null, null, PostingsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final PostingsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = terms[off.id];\n        if (term == null) {\n          // multitermquery match, pull from payload\n          term = off.dp.getPayload();\n          assert term != null;\n        }\n        current.addMatch(start, end, term);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset || end > contentLength) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = terms[off.id];\n        if (term == null) {\n          // multitermquery match, pull from payload\n          term = off.dp.getPayload();\n          assert term != null;\n        }\n        current.addMatch(start, end, term);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset || end > contentLength) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","bugFix":null,"bugIntro":["ad0d09e969f4763b0df4230f8e3f74357872a4e4"],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["dd81b1d062b9688a18721a1adfc489577479856a"],"4dcface3bab82444afcba8add9fb24a2d7276330":["71393760b816981b3e59704441c8bd6f32276046"],"4831dd345148fcd7c33877b449ade21fc45459d8":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","4dcface3bab82444afcba8add9fb24a2d7276330"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["4831dd345148fcd7c33877b449ade21fc45459d8"],"51f5280f31484820499077f41fcdfe92d527d9dc":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"dd81b1d062b9688a18721a1adfc489577479856a":["4dcface3bab82444afcba8add9fb24a2d7276330"],"71393760b816981b3e59704441c8bd6f32276046":["eee5f2a24465d2c9a5f86ab84b7c35041a30fda8"],"eee5f2a24465d2c9a5f86ab84b7c35041a30fda8":["4831dd345148fcd7c33877b449ade21fc45459d8"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["51f5280f31484820499077f41fcdfe92d527d9dc"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["51f5280f31484820499077f41fcdfe92d527d9dc"],"4dcface3bab82444afcba8add9fb24a2d7276330":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","dd81b1d062b9688a18721a1adfc489577479856a"],"4831dd345148fcd7c33877b449ade21fc45459d8":["8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","eee5f2a24465d2c9a5f86ab84b7c35041a30fda8"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4831dd345148fcd7c33877b449ade21fc45459d8"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576"],"dd81b1d062b9688a18721a1adfc489577479856a":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"71393760b816981b3e59704441c8bd6f32276046":["4dcface3bab82444afcba8add9fb24a2d7276330"],"51f5280f31484820499077f41fcdfe92d527d9dc":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"eee5f2a24465d2c9a5f86ab84b7c35041a30fda8":["71393760b816981b3e59704441c8bd6f32276046"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}