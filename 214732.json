{"path":"solr/core/src/java/org/apache/solr/search/GraphTermsQParserPlugin.GraphTermsQuery#createWeight(IndexSearcher,boolean,float).mjava","commits":[{"id":"02e175abd2c4c1611c5a9647486ae8ba249a94c1","date":1468327116,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/GraphTermsQParserPlugin.GraphTermsQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/GraphTermsQParserPlugin.GraphTermsQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"    @Override\n    public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n      List<TermContext> finalContexts = new ArrayList();\n      List<Term> finalTerms = new ArrayList();\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      TermContext[] termContexts = new TermContext[this.queryTerms.length];\n      collectTermContext(searcher.getIndexReader(), contexts, termContexts, this.queryTerms);\n      for(int i=0; i<termContexts.length; i++) {\n        TermContext termContext = termContexts[i];\n        if(termContext != null && termContext.docFreq() <= this.maxDocFreq) {\n          finalContexts.add(termContext);\n          finalTerms.add(queryTerms[i]);\n        }\n      }\n\n      return new ConstantScoreWeight(this, boost) {\n\n        @Override\n        public void extractTerms(Set<Term> terms) {\n          // no-op\n          // This query is for abuse cases when the number of terms is too high to\n          // run efficiently as a BooleanQuery. So likewise we hide its terms in\n          // order to protect highlighters\n        }\n\n        private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n          final LeafReader reader = context.reader();\n          final Fields fields = reader.fields();\n          Terms terms = fields.terms(field);\n          if(terms == null) {\n            return new WeightOrDocIdSet(new BitDocIdSet(new FixedBitSet(reader.maxDoc()), 0));\n          }\n          TermsEnum  termsEnum = terms.iterator();\n          PostingsEnum docs = null;\n          DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n          for (int i=0; i<finalContexts.size(); i++) {\n            TermContext termContext = finalContexts.get(i);\n            TermState termState = termContext.get(context.ord);\n            if(termState != null) {\n              Term term = finalTerms.get(i);\n              termsEnum.seekExact(term.bytes(), termContext.get(context.ord));\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            }\n          }\n          return new WeightOrDocIdSet(builder.build());\n        }\n\n        private Scorer scorer(DocIdSet set) throws IOException {\n          if (set == null) {\n            return null;\n          }\n          final DocIdSetIterator disi = set.iterator();\n          if (disi == null) {\n            return null;\n          }\n          return new ConstantScoreScorer(this, score(), disi);\n        }\n\n        @Override\n        public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.bulkScorer(context);\n          } else {\n            final Scorer scorer = scorer(weightOrBitSet.set);\n            if (scorer == null) {\n              return null;\n            }\n            return new DefaultBulkScorer(scorer);\n          }\n        }\n\n        @Override\n        public Scorer scorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.scorer(context);\n          } else {\n            return scorer(weightOrBitSet.set);\n          }\n        }\n      };\n    }\n\n","sourceOld":"    @Override\n    public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n\n      List<TermContext> finalContexts = new ArrayList();\n      List<Term> finalTerms = new ArrayList();\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      TermContext[] termContexts = new TermContext[this.queryTerms.length];\n      collectTermContext(searcher.getIndexReader(), contexts, termContexts, this.queryTerms);\n      for(int i=0; i<termContexts.length; i++) {\n        TermContext termContext = termContexts[i];\n        if(termContext != null && termContext.docFreq() <= this.maxDocFreq) {\n          finalContexts.add(termContext);\n          finalTerms.add(queryTerms[i]);\n        }\n      }\n\n      return new ConstantScoreWeight(this) {\n\n        @Override\n        public void extractTerms(Set<Term> terms) {\n          // no-op\n          // This query is for abuse cases when the number of terms is too high to\n          // run efficiently as a BooleanQuery. So likewise we hide its terms in\n          // order to protect highlighters\n        }\n\n        private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n          final LeafReader reader = context.reader();\n          final Fields fields = reader.fields();\n          Terms terms = fields.terms(field);\n          if(terms == null) {\n            return new WeightOrDocIdSet(new BitDocIdSet(new FixedBitSet(reader.maxDoc()), 0));\n          }\n          TermsEnum  termsEnum = terms.iterator();\n          PostingsEnum docs = null;\n          DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n          for (int i=0; i<finalContexts.size(); i++) {\n            TermContext termContext = finalContexts.get(i);\n            TermState termState = termContext.get(context.ord);\n            if(termState != null) {\n              Term term = finalTerms.get(i);\n              termsEnum.seekExact(term.bytes(), termContext.get(context.ord));\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            }\n          }\n          return new WeightOrDocIdSet(builder.build());\n        }\n\n        private Scorer scorer(DocIdSet set) throws IOException {\n          if (set == null) {\n            return null;\n          }\n          final DocIdSetIterator disi = set.iterator();\n          if (disi == null) {\n            return null;\n          }\n          return new ConstantScoreScorer(this, score(), disi);\n        }\n\n        @Override\n        public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.bulkScorer(context);\n          } else {\n            final Scorer scorer = scorer(weightOrBitSet.set);\n            if (scorer == null) {\n              return null;\n            }\n            return new DefaultBulkScorer(scorer);\n          }\n        }\n\n        @Override\n        public Scorer scorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.scorer(context);\n          } else {\n            return scorer(weightOrBitSet.set);\n          }\n        }\n      };\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/GraphTermsQParserPlugin.GraphTermsQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"/dev/null","sourceNew":"    @Override\n    public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n      List<TermContext> finalContexts = new ArrayList();\n      List<Term> finalTerms = new ArrayList();\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      TermContext[] termContexts = new TermContext[this.queryTerms.length];\n      collectTermContext(searcher.getIndexReader(), contexts, termContexts, this.queryTerms);\n      for(int i=0; i<termContexts.length; i++) {\n        TermContext termContext = termContexts[i];\n        if(termContext != null && termContext.docFreq() <= this.maxDocFreq) {\n          finalContexts.add(termContext);\n          finalTerms.add(queryTerms[i]);\n        }\n      }\n\n      return new ConstantScoreWeight(this, boost) {\n\n        @Override\n        public void extractTerms(Set<Term> terms) {\n          // no-op\n          // This query is for abuse cases when the number of terms is too high to\n          // run efficiently as a BooleanQuery. So likewise we hide its terms in\n          // order to protect highlighters\n        }\n\n        private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n          final LeafReader reader = context.reader();\n          final Fields fields = reader.fields();\n          Terms terms = fields.terms(field);\n          if(terms == null) {\n            return new WeightOrDocIdSet(new BitDocIdSet(new FixedBitSet(reader.maxDoc()), 0));\n          }\n          TermsEnum  termsEnum = terms.iterator();\n          PostingsEnum docs = null;\n          DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n          for (int i=0; i<finalContexts.size(); i++) {\n            TermContext termContext = finalContexts.get(i);\n            TermState termState = termContext.get(context.ord);\n            if(termState != null) {\n              Term term = finalTerms.get(i);\n              termsEnum.seekExact(term.bytes(), termContext.get(context.ord));\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            }\n          }\n          return new WeightOrDocIdSet(builder.build());\n        }\n\n        private Scorer scorer(DocIdSet set) throws IOException {\n          if (set == null) {\n            return null;\n          }\n          final DocIdSetIterator disi = set.iterator();\n          if (disi == null) {\n            return null;\n          }\n          return new ConstantScoreScorer(this, score(), disi);\n        }\n\n        @Override\n        public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.bulkScorer(context);\n          } else {\n            final Scorer scorer = scorer(weightOrBitSet.set);\n            if (scorer == null) {\n              return null;\n            }\n            return new DefaultBulkScorer(scorer);\n          }\n        }\n\n        @Override\n        public Scorer scorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.scorer(context);\n          } else {\n            return scorer(weightOrBitSet.set);\n          }\n        }\n      };\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b","date":1497408244,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/GraphTermsQParserPlugin.GraphTermsQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/GraphTermsQParserPlugin.GraphTermsQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"    @Override\n    public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n      List<TermContext> finalContexts = new ArrayList();\n      List<Term> finalTerms = new ArrayList();\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      TermContext[] termContexts = new TermContext[this.queryTerms.length];\n      collectTermContext(searcher.getIndexReader(), contexts, termContexts, this.queryTerms);\n      for(int i=0; i<termContexts.length; i++) {\n        TermContext termContext = termContexts[i];\n        if(termContext != null && termContext.docFreq() <= this.maxDocFreq) {\n          finalContexts.add(termContext);\n          finalTerms.add(queryTerms[i]);\n        }\n      }\n\n      return new ConstantScoreWeight(this, boost) {\n\n        @Override\n        public void extractTerms(Set<Term> terms) {\n          // no-op\n          // This query is for abuse cases when the number of terms is too high to\n          // run efficiently as a BooleanQuery. So likewise we hide its terms in\n          // order to protect highlighters\n        }\n\n        private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n          final LeafReader reader = context.reader();\n          Terms terms = reader.terms(field);\n          if(terms == null) {\n            return new WeightOrDocIdSet(new BitDocIdSet(new FixedBitSet(reader.maxDoc()), 0));\n          }\n          TermsEnum  termsEnum = terms.iterator();\n          PostingsEnum docs = null;\n          DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n          for (int i=0; i<finalContexts.size(); i++) {\n            TermContext termContext = finalContexts.get(i);\n            TermState termState = termContext.get(context.ord);\n            if(termState != null) {\n              Term term = finalTerms.get(i);\n              termsEnum.seekExact(term.bytes(), termContext.get(context.ord));\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            }\n          }\n          return new WeightOrDocIdSet(builder.build());\n        }\n\n        private Scorer scorer(DocIdSet set) throws IOException {\n          if (set == null) {\n            return null;\n          }\n          final DocIdSetIterator disi = set.iterator();\n          if (disi == null) {\n            return null;\n          }\n          return new ConstantScoreScorer(this, score(), disi);\n        }\n\n        @Override\n        public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.bulkScorer(context);\n          } else {\n            final Scorer scorer = scorer(weightOrBitSet.set);\n            if (scorer == null) {\n              return null;\n            }\n            return new DefaultBulkScorer(scorer);\n          }\n        }\n\n        @Override\n        public Scorer scorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.scorer(context);\n          } else {\n            return scorer(weightOrBitSet.set);\n          }\n        }\n      };\n    }\n\n","sourceOld":"    @Override\n    public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n      List<TermContext> finalContexts = new ArrayList();\n      List<Term> finalTerms = new ArrayList();\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      TermContext[] termContexts = new TermContext[this.queryTerms.length];\n      collectTermContext(searcher.getIndexReader(), contexts, termContexts, this.queryTerms);\n      for(int i=0; i<termContexts.length; i++) {\n        TermContext termContext = termContexts[i];\n        if(termContext != null && termContext.docFreq() <= this.maxDocFreq) {\n          finalContexts.add(termContext);\n          finalTerms.add(queryTerms[i]);\n        }\n      }\n\n      return new ConstantScoreWeight(this, boost) {\n\n        @Override\n        public void extractTerms(Set<Term> terms) {\n          // no-op\n          // This query is for abuse cases when the number of terms is too high to\n          // run efficiently as a BooleanQuery. So likewise we hide its terms in\n          // order to protect highlighters\n        }\n\n        private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n          final LeafReader reader = context.reader();\n          final Fields fields = reader.fields();\n          Terms terms = fields.terms(field);\n          if(terms == null) {\n            return new WeightOrDocIdSet(new BitDocIdSet(new FixedBitSet(reader.maxDoc()), 0));\n          }\n          TermsEnum  termsEnum = terms.iterator();\n          PostingsEnum docs = null;\n          DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n          for (int i=0; i<finalContexts.size(); i++) {\n            TermContext termContext = finalContexts.get(i);\n            TermState termState = termContext.get(context.ord);\n            if(termState != null) {\n              Term term = finalTerms.get(i);\n              termsEnum.seekExact(term.bytes(), termContext.get(context.ord));\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            }\n          }\n          return new WeightOrDocIdSet(builder.build());\n        }\n\n        private Scorer scorer(DocIdSet set) throws IOException {\n          if (set == null) {\n            return null;\n          }\n          final DocIdSetIterator disi = set.iterator();\n          if (disi == null) {\n            return null;\n          }\n          return new ConstantScoreScorer(this, score(), disi);\n        }\n\n        @Override\n        public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.bulkScorer(context);\n          } else {\n            final Scorer scorer = scorer(weightOrBitSet.set);\n            if (scorer == null) {\n              return null;\n            }\n            return new DefaultBulkScorer(scorer);\n          }\n        }\n\n        @Override\n        public Scorer scorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.scorer(context);\n          } else {\n            return scorer(weightOrBitSet.set);\n          }\n        }\n      };\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","date":1498028748,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/GraphTermsQParserPlugin.GraphTermsQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/GraphTermsQParserPlugin.GraphTermsQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"    @Override\n    public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n      List<TermContext> finalContexts = new ArrayList();\n      List<Term> finalTerms = new ArrayList();\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      TermContext[] termContexts = new TermContext[this.queryTerms.length];\n      collectTermContext(searcher.getIndexReader(), contexts, termContexts, this.queryTerms);\n      for(int i=0; i<termContexts.length; i++) {\n        TermContext termContext = termContexts[i];\n        if(termContext != null && termContext.docFreq() <= this.maxDocFreq) {\n          finalContexts.add(termContext);\n          finalTerms.add(queryTerms[i]);\n        }\n      }\n\n      return new ConstantScoreWeight(this, boost) {\n\n        @Override\n        public void extractTerms(Set<Term> terms) {\n          // no-op\n          // This query is for abuse cases when the number of terms is too high to\n          // run efficiently as a BooleanQuery. So likewise we hide its terms in\n          // order to protect highlighters\n        }\n\n        private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n          final LeafReader reader = context.reader();\n          Terms terms = reader.terms(field);\n          if(terms == null) {\n            return new WeightOrDocIdSet(new BitDocIdSet(new FixedBitSet(reader.maxDoc()), 0));\n          }\n          TermsEnum  termsEnum = terms.iterator();\n          PostingsEnum docs = null;\n          DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n          for (int i=0; i<finalContexts.size(); i++) {\n            TermContext termContext = finalContexts.get(i);\n            TermState termState = termContext.get(context.ord);\n            if(termState != null) {\n              Term term = finalTerms.get(i);\n              termsEnum.seekExact(term.bytes(), termContext.get(context.ord));\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            }\n          }\n          return new WeightOrDocIdSet(builder.build());\n        }\n\n        private Scorer scorer(DocIdSet set) throws IOException {\n          if (set == null) {\n            return null;\n          }\n          final DocIdSetIterator disi = set.iterator();\n          if (disi == null) {\n            return null;\n          }\n          return new ConstantScoreScorer(this, score(), disi);\n        }\n\n        @Override\n        public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.bulkScorer(context);\n          } else {\n            final Scorer scorer = scorer(weightOrBitSet.set);\n            if (scorer == null) {\n              return null;\n            }\n            return new DefaultBulkScorer(scorer);\n          }\n        }\n\n        @Override\n        public Scorer scorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.scorer(context);\n          } else {\n            return scorer(weightOrBitSet.set);\n          }\n        }\n      };\n    }\n\n","sourceOld":"    @Override\n    public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n      List<TermContext> finalContexts = new ArrayList();\n      List<Term> finalTerms = new ArrayList();\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      TermContext[] termContexts = new TermContext[this.queryTerms.length];\n      collectTermContext(searcher.getIndexReader(), contexts, termContexts, this.queryTerms);\n      for(int i=0; i<termContexts.length; i++) {\n        TermContext termContext = termContexts[i];\n        if(termContext != null && termContext.docFreq() <= this.maxDocFreq) {\n          finalContexts.add(termContext);\n          finalTerms.add(queryTerms[i]);\n        }\n      }\n\n      return new ConstantScoreWeight(this, boost) {\n\n        @Override\n        public void extractTerms(Set<Term> terms) {\n          // no-op\n          // This query is for abuse cases when the number of terms is too high to\n          // run efficiently as a BooleanQuery. So likewise we hide its terms in\n          // order to protect highlighters\n        }\n\n        private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n          final LeafReader reader = context.reader();\n          final Fields fields = reader.fields();\n          Terms terms = fields.terms(field);\n          if(terms == null) {\n            return new WeightOrDocIdSet(new BitDocIdSet(new FixedBitSet(reader.maxDoc()), 0));\n          }\n          TermsEnum  termsEnum = terms.iterator();\n          PostingsEnum docs = null;\n          DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n          for (int i=0; i<finalContexts.size(); i++) {\n            TermContext termContext = finalContexts.get(i);\n            TermState termState = termContext.get(context.ord);\n            if(termState != null) {\n              Term term = finalTerms.get(i);\n              termsEnum.seekExact(term.bytes(), termContext.get(context.ord));\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            }\n          }\n          return new WeightOrDocIdSet(builder.build());\n        }\n\n        private Scorer scorer(DocIdSet set) throws IOException {\n          if (set == null) {\n            return null;\n          }\n          final DocIdSetIterator disi = set.iterator();\n          if (disi == null) {\n            return null;\n          }\n          return new ConstantScoreScorer(this, score(), disi);\n        }\n\n        @Override\n        public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.bulkScorer(context);\n          } else {\n            final Scorer scorer = scorer(weightOrBitSet.set);\n            if (scorer == null) {\n              return null;\n            }\n            return new DefaultBulkScorer(scorer);\n          }\n        }\n\n        @Override\n        public Scorer scorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.scorer(context);\n          } else {\n            return scorer(weightOrBitSet.set);\n          }\n        }\n      };\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/GraphTermsQParserPlugin.GraphTermsQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/GraphTermsQParserPlugin.GraphTermsQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"    @Override\n    public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n      List<TermContext> finalContexts = new ArrayList();\n      List<Term> finalTerms = new ArrayList();\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      TermContext[] termContexts = new TermContext[this.queryTerms.length];\n      collectTermContext(searcher.getIndexReader(), contexts, termContexts, this.queryTerms);\n      for(int i=0; i<termContexts.length; i++) {\n        TermContext termContext = termContexts[i];\n        if(termContext != null && termContext.docFreq() <= this.maxDocFreq) {\n          finalContexts.add(termContext);\n          finalTerms.add(queryTerms[i]);\n        }\n      }\n\n      return new ConstantScoreWeight(this, boost) {\n\n        @Override\n        public void extractTerms(Set<Term> terms) {\n          // no-op\n          // This query is for abuse cases when the number of terms is too high to\n          // run efficiently as a BooleanQuery. So likewise we hide its terms in\n          // order to protect highlighters\n        }\n\n        private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n          final LeafReader reader = context.reader();\n          Terms terms = reader.terms(field);\n          if(terms == null) {\n            return new WeightOrDocIdSet(new BitDocIdSet(new FixedBitSet(reader.maxDoc()), 0));\n          }\n          TermsEnum  termsEnum = terms.iterator();\n          PostingsEnum docs = null;\n          DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n          for (int i=0; i<finalContexts.size(); i++) {\n            TermContext termContext = finalContexts.get(i);\n            TermState termState = termContext.get(context.ord);\n            if(termState != null) {\n              Term term = finalTerms.get(i);\n              termsEnum.seekExact(term.bytes(), termContext.get(context.ord));\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            }\n          }\n          return new WeightOrDocIdSet(builder.build());\n        }\n\n        private Scorer scorer(DocIdSet set) throws IOException {\n          if (set == null) {\n            return null;\n          }\n          final DocIdSetIterator disi = set.iterator();\n          if (disi == null) {\n            return null;\n          }\n          return new ConstantScoreScorer(this, score(), disi);\n        }\n\n        @Override\n        public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.bulkScorer(context);\n          } else {\n            final Scorer scorer = scorer(weightOrBitSet.set);\n            if (scorer == null) {\n              return null;\n            }\n            return new DefaultBulkScorer(scorer);\n          }\n        }\n\n        @Override\n        public Scorer scorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.scorer(context);\n          } else {\n            return scorer(weightOrBitSet.set);\n          }\n        }\n      };\n    }\n\n","sourceOld":"    @Override\n    public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n      List<TermContext> finalContexts = new ArrayList();\n      List<Term> finalTerms = new ArrayList();\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      TermContext[] termContexts = new TermContext[this.queryTerms.length];\n      collectTermContext(searcher.getIndexReader(), contexts, termContexts, this.queryTerms);\n      for(int i=0; i<termContexts.length; i++) {\n        TermContext termContext = termContexts[i];\n        if(termContext != null && termContext.docFreq() <= this.maxDocFreq) {\n          finalContexts.add(termContext);\n          finalTerms.add(queryTerms[i]);\n        }\n      }\n\n      return new ConstantScoreWeight(this, boost) {\n\n        @Override\n        public void extractTerms(Set<Term> terms) {\n          // no-op\n          // This query is for abuse cases when the number of terms is too high to\n          // run efficiently as a BooleanQuery. So likewise we hide its terms in\n          // order to protect highlighters\n        }\n\n        private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n          final LeafReader reader = context.reader();\n          final Fields fields = reader.fields();\n          Terms terms = fields.terms(field);\n          if(terms == null) {\n            return new WeightOrDocIdSet(new BitDocIdSet(new FixedBitSet(reader.maxDoc()), 0));\n          }\n          TermsEnum  termsEnum = terms.iterator();\n          PostingsEnum docs = null;\n          DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n          for (int i=0; i<finalContexts.size(); i++) {\n            TermContext termContext = finalContexts.get(i);\n            TermState termState = termContext.get(context.ord);\n            if(termState != null) {\n              Term term = finalTerms.get(i);\n              termsEnum.seekExact(term.bytes(), termContext.get(context.ord));\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            }\n          }\n          return new WeightOrDocIdSet(builder.build());\n        }\n\n        private Scorer scorer(DocIdSet set) throws IOException {\n          if (set == null) {\n            return null;\n          }\n          final DocIdSetIterator disi = set.iterator();\n          if (disi == null) {\n            return null;\n          }\n          return new ConstantScoreScorer(this, score(), disi);\n        }\n\n        @Override\n        public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.bulkScorer(context);\n          } else {\n            final Scorer scorer = scorer(weightOrBitSet.set);\n            if (scorer == null) {\n              return null;\n            }\n            return new DefaultBulkScorer(scorer);\n          }\n        }\n\n        @Override\n        public Scorer scorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.scorer(context);\n          } else {\n            return scorer(weightOrBitSet.set);\n          }\n        }\n      };\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"706a7a3396c030cc66dda92a0492eb492131c4c0","date":1509705614,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/GraphTermsQParserPlugin.GraphTermsQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/GraphTermsQParserPlugin.GraphTermsQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"    @Override\n    public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n      List<TermContext> finalContexts = new ArrayList();\n      List<Term> finalTerms = new ArrayList();\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      TermContext[] termContexts = new TermContext[this.queryTerms.length];\n      collectTermContext(searcher.getIndexReader(), contexts, termContexts, this.queryTerms);\n      for(int i=0; i<termContexts.length; i++) {\n        TermContext termContext = termContexts[i];\n        if(termContext != null && termContext.docFreq() <= this.maxDocFreq) {\n          finalContexts.add(termContext);\n          finalTerms.add(queryTerms[i]);\n        }\n      }\n\n      return new ConstantScoreWeight(this, boost) {\n\n        @Override\n        public void extractTerms(Set<Term> terms) {\n          // no-op\n          // This query is for abuse cases when the number of terms is too high to\n          // run efficiently as a BooleanQuery. So likewise we hide its terms in\n          // order to protect highlighters\n        }\n\n        private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n          final LeafReader reader = context.reader();\n          Terms terms = reader.terms(field);\n          if(terms == null) {\n            return new WeightOrDocIdSet(new BitDocIdSet(new FixedBitSet(reader.maxDoc()), 0));\n          }\n          TermsEnum  termsEnum = terms.iterator();\n          PostingsEnum docs = null;\n          DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n          for (int i=0; i<finalContexts.size(); i++) {\n            TermContext termContext = finalContexts.get(i);\n            TermState termState = termContext.get(context.ord);\n            if(termState != null) {\n              Term term = finalTerms.get(i);\n              termsEnum.seekExact(term.bytes(), termContext.get(context.ord));\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            }\n          }\n          return new WeightOrDocIdSet(builder.build());\n        }\n\n        private Scorer scorer(DocIdSet set) throws IOException {\n          if (set == null) {\n            return null;\n          }\n          final DocIdSetIterator disi = set.iterator();\n          if (disi == null) {\n            return null;\n          }\n          return new ConstantScoreScorer(this, score(), disi);\n        }\n\n        @Override\n        public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.bulkScorer(context);\n          } else {\n            final Scorer scorer = scorer(weightOrBitSet.set);\n            if (scorer == null) {\n              return null;\n            }\n            return new DefaultBulkScorer(scorer);\n          }\n        }\n\n        @Override\n        public Scorer scorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.scorer(context);\n          } else {\n            return scorer(weightOrBitSet.set);\n          }\n        }\n\n        @Override\n        public IndexReader.CacheHelper getCacheHelper(LeafReaderContext context) {\n          return context.reader().getCoreCacheHelper();\n        }\n      };\n    }\n\n","sourceOld":"    @Override\n    public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n      List<TermContext> finalContexts = new ArrayList();\n      List<Term> finalTerms = new ArrayList();\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      TermContext[] termContexts = new TermContext[this.queryTerms.length];\n      collectTermContext(searcher.getIndexReader(), contexts, termContexts, this.queryTerms);\n      for(int i=0; i<termContexts.length; i++) {\n        TermContext termContext = termContexts[i];\n        if(termContext != null && termContext.docFreq() <= this.maxDocFreq) {\n          finalContexts.add(termContext);\n          finalTerms.add(queryTerms[i]);\n        }\n      }\n\n      return new ConstantScoreWeight(this, boost) {\n\n        @Override\n        public void extractTerms(Set<Term> terms) {\n          // no-op\n          // This query is for abuse cases when the number of terms is too high to\n          // run efficiently as a BooleanQuery. So likewise we hide its terms in\n          // order to protect highlighters\n        }\n\n        private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n          final LeafReader reader = context.reader();\n          Terms terms = reader.terms(field);\n          if(terms == null) {\n            return new WeightOrDocIdSet(new BitDocIdSet(new FixedBitSet(reader.maxDoc()), 0));\n          }\n          TermsEnum  termsEnum = terms.iterator();\n          PostingsEnum docs = null;\n          DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n          for (int i=0; i<finalContexts.size(); i++) {\n            TermContext termContext = finalContexts.get(i);\n            TermState termState = termContext.get(context.ord);\n            if(termState != null) {\n              Term term = finalTerms.get(i);\n              termsEnum.seekExact(term.bytes(), termContext.get(context.ord));\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            }\n          }\n          return new WeightOrDocIdSet(builder.build());\n        }\n\n        private Scorer scorer(DocIdSet set) throws IOException {\n          if (set == null) {\n            return null;\n          }\n          final DocIdSetIterator disi = set.iterator();\n          if (disi == null) {\n            return null;\n          }\n          return new ConstantScoreScorer(this, score(), disi);\n        }\n\n        @Override\n        public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.bulkScorer(context);\n          } else {\n            final Scorer scorer = scorer(weightOrBitSet.set);\n            if (scorer == null) {\n              return null;\n            }\n            return new DefaultBulkScorer(scorer);\n          }\n        }\n\n        @Override\n        public Scorer scorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.scorer(context);\n          } else {\n            return scorer(weightOrBitSet.set);\n          }\n        }\n      };\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d523b8189b211dd1630166aa77b8c88bb48b3fcc","date":1510144168,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/GraphTermsQParserPlugin.GraphTermsQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/GraphTermsQParserPlugin.GraphTermsQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"    @Override\n    public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n      List<TermContext> finalContexts = new ArrayList();\n      List<Term> finalTerms = new ArrayList();\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      TermContext[] termContexts = new TermContext[this.queryTerms.length];\n      collectTermContext(searcher.getIndexReader(), contexts, termContexts, this.queryTerms);\n      for(int i=0; i<termContexts.length; i++) {\n        TermContext termContext = termContexts[i];\n        if(termContext != null && termContext.docFreq() <= this.maxDocFreq) {\n          finalContexts.add(termContext);\n          finalTerms.add(queryTerms[i]);\n        }\n      }\n\n      return new ConstantScoreWeight(this, boost) {\n\n        @Override\n        public void extractTerms(Set<Term> terms) {\n          // no-op\n          // This query is for abuse cases when the number of terms is too high to\n          // run efficiently as a BooleanQuery. So likewise we hide its terms in\n          // order to protect highlighters\n        }\n\n        private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n          final LeafReader reader = context.reader();\n          Terms terms = reader.terms(field);\n          if(terms == null) {\n            return new WeightOrDocIdSet(new BitDocIdSet(new FixedBitSet(reader.maxDoc()), 0));\n          }\n          TermsEnum  termsEnum = terms.iterator();\n          PostingsEnum docs = null;\n          DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n          for (int i=0; i<finalContexts.size(); i++) {\n            TermContext termContext = finalContexts.get(i);\n            TermState termState = termContext.get(context.ord);\n            if(termState != null) {\n              Term term = finalTerms.get(i);\n              termsEnum.seekExact(term.bytes(), termContext.get(context.ord));\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            }\n          }\n          return new WeightOrDocIdSet(builder.build());\n        }\n\n        private Scorer scorer(DocIdSet set) throws IOException {\n          if (set == null) {\n            return null;\n          }\n          final DocIdSetIterator disi = set.iterator();\n          if (disi == null) {\n            return null;\n          }\n          return new ConstantScoreScorer(this, score(), disi);\n        }\n\n        @Override\n        public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.bulkScorer(context);\n          } else {\n            final Scorer scorer = scorer(weightOrBitSet.set);\n            if (scorer == null) {\n              return null;\n            }\n            return new DefaultBulkScorer(scorer);\n          }\n        }\n\n        @Override\n        public Scorer scorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.scorer(context);\n          } else {\n            return scorer(weightOrBitSet.set);\n          }\n        }\n\n        @Override\n        public IndexReader.CacheHelper getCacheHelper(LeafReaderContext context) {\n          return context.reader().getCoreCacheHelper();\n        }\n      };\n    }\n\n","sourceOld":"    @Override\n    public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n      List<TermContext> finalContexts = new ArrayList();\n      List<Term> finalTerms = new ArrayList();\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      TermContext[] termContexts = new TermContext[this.queryTerms.length];\n      collectTermContext(searcher.getIndexReader(), contexts, termContexts, this.queryTerms);\n      for(int i=0; i<termContexts.length; i++) {\n        TermContext termContext = termContexts[i];\n        if(termContext != null && termContext.docFreq() <= this.maxDocFreq) {\n          finalContexts.add(termContext);\n          finalTerms.add(queryTerms[i]);\n        }\n      }\n\n      return new ConstantScoreWeight(this, boost) {\n\n        @Override\n        public void extractTerms(Set<Term> terms) {\n          // no-op\n          // This query is for abuse cases when the number of terms is too high to\n          // run efficiently as a BooleanQuery. So likewise we hide its terms in\n          // order to protect highlighters\n        }\n\n        private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n          final LeafReader reader = context.reader();\n          Terms terms = reader.terms(field);\n          if(terms == null) {\n            return new WeightOrDocIdSet(new BitDocIdSet(new FixedBitSet(reader.maxDoc()), 0));\n          }\n          TermsEnum  termsEnum = terms.iterator();\n          PostingsEnum docs = null;\n          DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n          for (int i=0; i<finalContexts.size(); i++) {\n            TermContext termContext = finalContexts.get(i);\n            TermState termState = termContext.get(context.ord);\n            if(termState != null) {\n              Term term = finalTerms.get(i);\n              termsEnum.seekExact(term.bytes(), termContext.get(context.ord));\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            }\n          }\n          return new WeightOrDocIdSet(builder.build());\n        }\n\n        private Scorer scorer(DocIdSet set) throws IOException {\n          if (set == null) {\n            return null;\n          }\n          final DocIdSetIterator disi = set.iterator();\n          if (disi == null) {\n            return null;\n          }\n          return new ConstantScoreScorer(this, score(), disi);\n        }\n\n        @Override\n        public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.bulkScorer(context);\n          } else {\n            final Scorer scorer = scorer(weightOrBitSet.set);\n            if (scorer == null) {\n              return null;\n            }\n            return new DefaultBulkScorer(scorer);\n          }\n        }\n\n        @Override\n        public Scorer scorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.scorer(context);\n          } else {\n            return scorer(weightOrBitSet.set);\n          }\n        }\n      };\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"da1460d7a5dea2658e7b8e4f6e632e53ade440ac","date":1510316270,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/GraphTermsQParserPlugin.GraphTermsQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/GraphTermsQParserPlugin.GraphTermsQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"    @Override\n    public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n      List<TermContext> finalContexts = new ArrayList();\n      List<Term> finalTerms = new ArrayList();\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      TermContext[] termContexts = new TermContext[this.queryTerms.length];\n      collectTermContext(searcher.getIndexReader(), contexts, termContexts, this.queryTerms);\n      for(int i=0; i<termContexts.length; i++) {\n        TermContext termContext = termContexts[i];\n        if(termContext != null && termContext.docFreq() <= this.maxDocFreq) {\n          finalContexts.add(termContext);\n          finalTerms.add(queryTerms[i]);\n        }\n      }\n\n      return new ConstantScoreWeight(this, boost) {\n\n        @Override\n        public void extractTerms(Set<Term> terms) {\n          // no-op\n          // This query is for abuse cases when the number of terms is too high to\n          // run efficiently as a BooleanQuery. So likewise we hide its terms in\n          // order to protect highlighters\n        }\n\n        private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n          final LeafReader reader = context.reader();\n          Terms terms = reader.terms(field);\n          if(terms == null) {\n            return new WeightOrDocIdSet(new BitDocIdSet(new FixedBitSet(reader.maxDoc()), 0));\n          }\n          TermsEnum  termsEnum = terms.iterator();\n          PostingsEnum docs = null;\n          DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n          for (int i=0; i<finalContexts.size(); i++) {\n            TermContext termContext = finalContexts.get(i);\n            TermState termState = termContext.get(context.ord);\n            if(termState != null) {\n              Term term = finalTerms.get(i);\n              termsEnum.seekExact(term.bytes(), termContext.get(context.ord));\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            }\n          }\n          return new WeightOrDocIdSet(builder.build());\n        }\n\n        private Scorer scorer(DocIdSet set) throws IOException {\n          if (set == null) {\n            return null;\n          }\n          final DocIdSetIterator disi = set.iterator();\n          if (disi == null) {\n            return null;\n          }\n          return new ConstantScoreScorer(this, score(), disi);\n        }\n\n        @Override\n        public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.bulkScorer(context);\n          } else {\n            final Scorer scorer = scorer(weightOrBitSet.set);\n            if (scorer == null) {\n              return null;\n            }\n            return new DefaultBulkScorer(scorer);\n          }\n        }\n\n        @Override\n        public Scorer scorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.scorer(context);\n          } else {\n            return scorer(weightOrBitSet.set);\n          }\n        }\n\n        @Override\n        public boolean isCacheable(LeafReaderContext ctx) {\n          return true;\n        }\n\n      };\n    }\n\n","sourceOld":"    @Override\n    public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n      List<TermContext> finalContexts = new ArrayList();\n      List<Term> finalTerms = new ArrayList();\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      TermContext[] termContexts = new TermContext[this.queryTerms.length];\n      collectTermContext(searcher.getIndexReader(), contexts, termContexts, this.queryTerms);\n      for(int i=0; i<termContexts.length; i++) {\n        TermContext termContext = termContexts[i];\n        if(termContext != null && termContext.docFreq() <= this.maxDocFreq) {\n          finalContexts.add(termContext);\n          finalTerms.add(queryTerms[i]);\n        }\n      }\n\n      return new ConstantScoreWeight(this, boost) {\n\n        @Override\n        public void extractTerms(Set<Term> terms) {\n          // no-op\n          // This query is for abuse cases when the number of terms is too high to\n          // run efficiently as a BooleanQuery. So likewise we hide its terms in\n          // order to protect highlighters\n        }\n\n        private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n          final LeafReader reader = context.reader();\n          Terms terms = reader.terms(field);\n          if(terms == null) {\n            return new WeightOrDocIdSet(new BitDocIdSet(new FixedBitSet(reader.maxDoc()), 0));\n          }\n          TermsEnum  termsEnum = terms.iterator();\n          PostingsEnum docs = null;\n          DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n          for (int i=0; i<finalContexts.size(); i++) {\n            TermContext termContext = finalContexts.get(i);\n            TermState termState = termContext.get(context.ord);\n            if(termState != null) {\n              Term term = finalTerms.get(i);\n              termsEnum.seekExact(term.bytes(), termContext.get(context.ord));\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            }\n          }\n          return new WeightOrDocIdSet(builder.build());\n        }\n\n        private Scorer scorer(DocIdSet set) throws IOException {\n          if (set == null) {\n            return null;\n          }\n          final DocIdSetIterator disi = set.iterator();\n          if (disi == null) {\n            return null;\n          }\n          return new ConstantScoreScorer(this, score(), disi);\n        }\n\n        @Override\n        public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.bulkScorer(context);\n          } else {\n            final Scorer scorer = scorer(weightOrBitSet.set);\n            if (scorer == null) {\n              return null;\n            }\n            return new DefaultBulkScorer(scorer);\n          }\n        }\n\n        @Override\n        public Scorer scorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.scorer(context);\n          } else {\n            return scorer(weightOrBitSet.set);\n          }\n        }\n\n        @Override\n        public IndexReader.CacheHelper getCacheHelper(LeafReaderContext context) {\n          return context.reader().getCoreCacheHelper();\n        }\n      };\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9fc47cb7b4346802411bb432f501ed0673d7119e","date":1512640179,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/GraphTermsQParserPlugin.GraphTermsQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/GraphTermsQParserPlugin.GraphTermsQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"    @Override\n    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n\n      List<TermContext> finalContexts = new ArrayList();\n      List<Term> finalTerms = new ArrayList();\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      TermContext[] termContexts = new TermContext[this.queryTerms.length];\n      collectTermContext(searcher.getIndexReader(), contexts, termContexts, this.queryTerms);\n      for(int i=0; i<termContexts.length; i++) {\n        TermContext termContext = termContexts[i];\n        if(termContext != null && termContext.docFreq() <= this.maxDocFreq) {\n          finalContexts.add(termContext);\n          finalTerms.add(queryTerms[i]);\n        }\n      }\n\n      return new ConstantScoreWeight(this, boost) {\n\n        @Override\n        public void extractTerms(Set<Term> terms) {\n          // no-op\n          // This query is for abuse cases when the number of terms is too high to\n          // run efficiently as a BooleanQuery. So likewise we hide its terms in\n          // order to protect highlighters\n        }\n\n        private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n          final LeafReader reader = context.reader();\n          Terms terms = reader.terms(field);\n          if(terms == null) {\n            return new WeightOrDocIdSet(new BitDocIdSet(new FixedBitSet(reader.maxDoc()), 0));\n          }\n          TermsEnum  termsEnum = terms.iterator();\n          PostingsEnum docs = null;\n          DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n          for (int i=0; i<finalContexts.size(); i++) {\n            TermContext termContext = finalContexts.get(i);\n            TermState termState = termContext.get(context.ord);\n            if(termState != null) {\n              Term term = finalTerms.get(i);\n              termsEnum.seekExact(term.bytes(), termContext.get(context.ord));\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            }\n          }\n          return new WeightOrDocIdSet(builder.build());\n        }\n\n        private Scorer scorer(DocIdSet set) throws IOException {\n          if (set == null) {\n            return null;\n          }\n          final DocIdSetIterator disi = set.iterator();\n          if (disi == null) {\n            return null;\n          }\n          return new ConstantScoreScorer(this, score(), disi);\n        }\n\n        @Override\n        public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.bulkScorer(context);\n          } else {\n            final Scorer scorer = scorer(weightOrBitSet.set);\n            if (scorer == null) {\n              return null;\n            }\n            return new DefaultBulkScorer(scorer);\n          }\n        }\n\n        @Override\n        public Scorer scorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.scorer(context);\n          } else {\n            return scorer(weightOrBitSet.set);\n          }\n        }\n\n        @Override\n        public boolean isCacheable(LeafReaderContext ctx) {\n          return true;\n        }\n\n      };\n    }\n\n","sourceOld":"    @Override\n    public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n      List<TermContext> finalContexts = new ArrayList();\n      List<Term> finalTerms = new ArrayList();\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      TermContext[] termContexts = new TermContext[this.queryTerms.length];\n      collectTermContext(searcher.getIndexReader(), contexts, termContexts, this.queryTerms);\n      for(int i=0; i<termContexts.length; i++) {\n        TermContext termContext = termContexts[i];\n        if(termContext != null && termContext.docFreq() <= this.maxDocFreq) {\n          finalContexts.add(termContext);\n          finalTerms.add(queryTerms[i]);\n        }\n      }\n\n      return new ConstantScoreWeight(this, boost) {\n\n        @Override\n        public void extractTerms(Set<Term> terms) {\n          // no-op\n          // This query is for abuse cases when the number of terms is too high to\n          // run efficiently as a BooleanQuery. So likewise we hide its terms in\n          // order to protect highlighters\n        }\n\n        private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n          final LeafReader reader = context.reader();\n          Terms terms = reader.terms(field);\n          if(terms == null) {\n            return new WeightOrDocIdSet(new BitDocIdSet(new FixedBitSet(reader.maxDoc()), 0));\n          }\n          TermsEnum  termsEnum = terms.iterator();\n          PostingsEnum docs = null;\n          DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n          for (int i=0; i<finalContexts.size(); i++) {\n            TermContext termContext = finalContexts.get(i);\n            TermState termState = termContext.get(context.ord);\n            if(termState != null) {\n              Term term = finalTerms.get(i);\n              termsEnum.seekExact(term.bytes(), termContext.get(context.ord));\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            }\n          }\n          return new WeightOrDocIdSet(builder.build());\n        }\n\n        private Scorer scorer(DocIdSet set) throws IOException {\n          if (set == null) {\n            return null;\n          }\n          final DocIdSetIterator disi = set.iterator();\n          if (disi == null) {\n            return null;\n          }\n          return new ConstantScoreScorer(this, score(), disi);\n        }\n\n        @Override\n        public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.bulkScorer(context);\n          } else {\n            final Scorer scorer = scorer(weightOrBitSet.set);\n            if (scorer == null) {\n              return null;\n            }\n            return new DefaultBulkScorer(scorer);\n          }\n        }\n\n        @Override\n        public Scorer scorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.scorer(context);\n          } else {\n            return scorer(weightOrBitSet.set);\n          }\n        }\n\n        @Override\n        public boolean isCacheable(LeafReaderContext ctx) {\n          return true;\n        }\n\n      };\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"417142ff08fda9cf0b72d5133e63097a166c6458","date":1512729693,"type":5,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/GraphTermsQParserPlugin.GraphTermsQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/GraphTermsQParserPlugin.GraphTermsQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"    @Override\n    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n\n      List<TermContext> finalContexts = new ArrayList();\n      List<Term> finalTerms = new ArrayList();\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      TermContext[] termContexts = new TermContext[this.queryTerms.length];\n      collectTermContext(searcher.getIndexReader(), contexts, termContexts, this.queryTerms);\n      for(int i=0; i<termContexts.length; i++) {\n        TermContext termContext = termContexts[i];\n        if(termContext != null && termContext.docFreq() <= this.maxDocFreq) {\n          finalContexts.add(termContext);\n          finalTerms.add(queryTerms[i]);\n        }\n      }\n\n      return new ConstantScoreWeight(this, boost) {\n\n        @Override\n        public void extractTerms(Set<Term> terms) {\n          // no-op\n          // This query is for abuse cases when the number of terms is too high to\n          // run efficiently as a BooleanQuery. So likewise we hide its terms in\n          // order to protect highlighters\n        }\n\n        private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n          final LeafReader reader = context.reader();\n          Terms terms = reader.terms(field);\n          if(terms == null) {\n            return new WeightOrDocIdSet(new BitDocIdSet(new FixedBitSet(reader.maxDoc()), 0));\n          }\n          TermsEnum  termsEnum = terms.iterator();\n          PostingsEnum docs = null;\n          DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n          for (int i=0; i<finalContexts.size(); i++) {\n            TermContext termContext = finalContexts.get(i);\n            TermState termState = termContext.get(context.ord);\n            if(termState != null) {\n              Term term = finalTerms.get(i);\n              termsEnum.seekExact(term.bytes(), termContext.get(context.ord));\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            }\n          }\n          return new WeightOrDocIdSet(builder.build());\n        }\n\n        private Scorer scorer(DocIdSet set) throws IOException {\n          if (set == null) {\n            return null;\n          }\n          final DocIdSetIterator disi = set.iterator();\n          if (disi == null) {\n            return null;\n          }\n          return new ConstantScoreScorer(this, score(), disi);\n        }\n\n        @Override\n        public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.bulkScorer(context);\n          } else {\n            final Scorer scorer = scorer(weightOrBitSet.set);\n            if (scorer == null) {\n              return null;\n            }\n            return new DefaultBulkScorer(scorer);\n          }\n        }\n\n        @Override\n        public Scorer scorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.scorer(context);\n          } else {\n            return scorer(weightOrBitSet.set);\n          }\n        }\n\n        @Override\n        public boolean isCacheable(LeafReaderContext ctx) {\n          return true;\n        }\n\n      };\n    }\n\n","sourceOld":"    @Override\n    public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n      List<TermContext> finalContexts = new ArrayList();\n      List<Term> finalTerms = new ArrayList();\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      TermContext[] termContexts = new TermContext[this.queryTerms.length];\n      collectTermContext(searcher.getIndexReader(), contexts, termContexts, this.queryTerms);\n      for(int i=0; i<termContexts.length; i++) {\n        TermContext termContext = termContexts[i];\n        if(termContext != null && termContext.docFreq() <= this.maxDocFreq) {\n          finalContexts.add(termContext);\n          finalTerms.add(queryTerms[i]);\n        }\n      }\n\n      return new ConstantScoreWeight(this, boost) {\n\n        @Override\n        public void extractTerms(Set<Term> terms) {\n          // no-op\n          // This query is for abuse cases when the number of terms is too high to\n          // run efficiently as a BooleanQuery. So likewise we hide its terms in\n          // order to protect highlighters\n        }\n\n        private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n          final LeafReader reader = context.reader();\n          Terms terms = reader.terms(field);\n          if(terms == null) {\n            return new WeightOrDocIdSet(new BitDocIdSet(new FixedBitSet(reader.maxDoc()), 0));\n          }\n          TermsEnum  termsEnum = terms.iterator();\n          PostingsEnum docs = null;\n          DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n          for (int i=0; i<finalContexts.size(); i++) {\n            TermContext termContext = finalContexts.get(i);\n            TermState termState = termContext.get(context.ord);\n            if(termState != null) {\n              Term term = finalTerms.get(i);\n              termsEnum.seekExact(term.bytes(), termContext.get(context.ord));\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            }\n          }\n          return new WeightOrDocIdSet(builder.build());\n        }\n\n        private Scorer scorer(DocIdSet set) throws IOException {\n          if (set == null) {\n            return null;\n          }\n          final DocIdSetIterator disi = set.iterator();\n          if (disi == null) {\n            return null;\n          }\n          return new ConstantScoreScorer(this, score(), disi);\n        }\n\n        @Override\n        public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.bulkScorer(context);\n          } else {\n            final Scorer scorer = scorer(weightOrBitSet.set);\n            if (scorer == null) {\n              return null;\n            }\n            return new DefaultBulkScorer(scorer);\n          }\n        }\n\n        @Override\n        public Scorer scorer(LeafReaderContext context) throws IOException {\n          final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n          if (weightOrBitSet.weight != null) {\n            return weightOrBitSet.weight.scorer(context);\n          } else {\n            return scorer(weightOrBitSet.set);\n          }\n        }\n\n        @Override\n        public boolean isCacheable(LeafReaderContext ctx) {\n          return true;\n        }\n\n      };\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"706a7a3396c030cc66dda92a0492eb492131c4c0":["28288370235ed02234a64753cdbf0c6ec096304a"],"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b":["02e175abd2c4c1611c5a9647486ae8ba249a94c1"],"da1460d7a5dea2658e7b8e4f6e632e53ade440ac":["d523b8189b211dd1630166aa77b8c88bb48b3fcc"],"d523b8189b211dd1630166aa77b8c88bb48b3fcc":["28288370235ed02234a64753cdbf0c6ec096304a","706a7a3396c030cc66dda92a0492eb492131c4c0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"02e175abd2c4c1611c5a9647486ae8ba249a94c1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"9fc47cb7b4346802411bb432f501ed0673d7119e":["da1460d7a5dea2658e7b8e4f6e632e53ade440ac"],"28288370235ed02234a64753cdbf0c6ec096304a":["02e175abd2c4c1611c5a9647486ae8ba249a94c1","e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":["02e175abd2c4c1611c5a9647486ae8ba249a94c1","e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","02e175abd2c4c1611c5a9647486ae8ba249a94c1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["417142ff08fda9cf0b72d5133e63097a166c6458"],"417142ff08fda9cf0b72d5133e63097a166c6458":["da1460d7a5dea2658e7b8e4f6e632e53ade440ac","9fc47cb7b4346802411bb432f501ed0673d7119e"]},"commit2Childs":{"706a7a3396c030cc66dda92a0492eb492131c4c0":["d523b8189b211dd1630166aa77b8c88bb48b3fcc"],"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b":["28288370235ed02234a64753cdbf0c6ec096304a","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"da1460d7a5dea2658e7b8e4f6e632e53ade440ac":["9fc47cb7b4346802411bb432f501ed0673d7119e","417142ff08fda9cf0b72d5133e63097a166c6458"],"d523b8189b211dd1630166aa77b8c88bb48b3fcc":["da1460d7a5dea2658e7b8e4f6e632e53ade440ac"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["02e175abd2c4c1611c5a9647486ae8ba249a94c1","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"02e175abd2c4c1611c5a9647486ae8ba249a94c1":["e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b","28288370235ed02234a64753cdbf0c6ec096304a","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"9fc47cb7b4346802411bb432f501ed0673d7119e":["417142ff08fda9cf0b72d5133e63097a166c6458"],"28288370235ed02234a64753cdbf0c6ec096304a":["706a7a3396c030cc66dda92a0492eb492131c4c0","d523b8189b211dd1630166aa77b8c88bb48b3fcc"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":[],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"417142ff08fda9cf0b72d5133e63097a166c6458":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}