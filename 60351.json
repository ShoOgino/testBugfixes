{"path":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldOffsetStrategy#createAutomataOffsetsFromTerms(Terms,int).mjava","commits":[{"id":"f2e9861e4a2b724d9fc51b618714c579491b78d7","date":1479244606,"type":0,"author":"David Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldOffsetStrategy#createAutomataOffsetsFromTerms(Terms,int).mjava","pathOld":"/dev/null","sourceNew":"  protected List<OffsetsEnum> createAutomataOffsetsFromTerms(Terms termsIndex, int doc) throws IOException {\n    List<List<PostingsEnum>> automataPostings = new ArrayList<>(automata.length);\n    for (int i = 0; i < automata.length; i++) {\n      automataPostings.add(new ArrayList<>());\n    }\n\n    TermsEnum termsEnum = termsIndex.iterator();\n    BytesRef term;\n    CharsRefBuilder refBuilder = new CharsRefBuilder();\n    while ((term = termsEnum.next()) != null) {\n      for (int i = 0; i < automata.length; i++) {\n        CharacterRunAutomaton automaton = automata[i];\n        refBuilder.copyUTF8Bytes(term);\n        if (automaton.run(refBuilder.chars(), 0, refBuilder.length())) {\n          PostingsEnum postings = termsEnum.postings(null, PostingsEnum.OFFSETS);\n          if (doc == postings.advance(doc)) {\n            automataPostings.get(i).add(postings);\n          }\n        }\n      }\n    }\n\n    List<OffsetsEnum> offsetsEnums = new ArrayList<>(automata.length); //will be at most this long\n    for (int i = 0; i < automata.length; i++) {\n      CharacterRunAutomaton automaton = automata[i];\n      List<PostingsEnum> postingsEnums = automataPostings.get(i);\n      int size = postingsEnums.size();\n      if (size > 0) { //only add if we have offsets\n        BytesRef wildcardTerm = new BytesRef(automaton.toString());\n        if (size == 1) { //don't wrap in a composite if there's only one OffsetsEnum\n          offsetsEnums.add(new OffsetsEnum(wildcardTerm, postingsEnums.get(0)));\n        } else {\n          offsetsEnums.add(new OffsetsEnum(wildcardTerm, new CompositeOffsetsPostingsEnum(postingsEnums)));\n        }\n      }\n    }\n\n    return offsetsEnums;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a1ef55e1fff7ff44354432770ad8bc19be1fcc75","date":1479266056,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldOffsetStrategy#createAutomataOffsetsFromTerms(Terms,int).mjava","pathOld":"/dev/null","sourceNew":"  protected List<OffsetsEnum> createAutomataOffsetsFromTerms(Terms termsIndex, int doc) throws IOException {\n    List<List<PostingsEnum>> automataPostings = new ArrayList<>(automata.length);\n    for (int i = 0; i < automata.length; i++) {\n      automataPostings.add(new ArrayList<>());\n    }\n\n    TermsEnum termsEnum = termsIndex.iterator();\n    BytesRef term;\n    CharsRefBuilder refBuilder = new CharsRefBuilder();\n    while ((term = termsEnum.next()) != null) {\n      for (int i = 0; i < automata.length; i++) {\n        CharacterRunAutomaton automaton = automata[i];\n        refBuilder.copyUTF8Bytes(term);\n        if (automaton.run(refBuilder.chars(), 0, refBuilder.length())) {\n          PostingsEnum postings = termsEnum.postings(null, PostingsEnum.OFFSETS);\n          if (doc == postings.advance(doc)) {\n            automataPostings.get(i).add(postings);\n          }\n        }\n      }\n    }\n\n    List<OffsetsEnum> offsetsEnums = new ArrayList<>(automata.length); //will be at most this long\n    for (int i = 0; i < automata.length; i++) {\n      CharacterRunAutomaton automaton = automata[i];\n      List<PostingsEnum> postingsEnums = automataPostings.get(i);\n      int size = postingsEnums.size();\n      if (size > 0) { //only add if we have offsets\n        BytesRef wildcardTerm = new BytesRef(automaton.toString());\n        if (size == 1) { //don't wrap in a composite if there's only one OffsetsEnum\n          offsetsEnums.add(new OffsetsEnum(wildcardTerm, postingsEnums.get(0)));\n        } else {\n          offsetsEnums.add(new OffsetsEnum(wildcardTerm, new CompositeOffsetsPostingsEnum(postingsEnums)));\n        }\n      }\n    }\n\n    return offsetsEnums;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eef54e3d232eae0e9fc18d75e9b0c3d9ce04b571","date":1515642580,"type":5,"author":"David Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldOffsetStrategy#createOffsetsEnumsForAutomata(Terms,int,List[OffsetsEnum]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldOffsetStrategy#createAutomataOffsetsFromTerms(Terms,int).mjava","sourceNew":"  protected void createOffsetsEnumsForAutomata(Terms termsIndex, int doc, List<OffsetsEnum> results) throws IOException {\n    List<List<PostingsEnum>> automataPostings = new ArrayList<>(automata.length);\n    for (int i = 0; i < automata.length; i++) {\n      automataPostings.add(new ArrayList<>());\n    }\n\n    TermsEnum termsEnum = termsIndex.iterator();\n    BytesRef term;\n\n    CharsRefBuilder refBuilder = new CharsRefBuilder();\n    while ((term = termsEnum.next()) != null) {\n      for (int i = 0; i < automata.length; i++) {\n        CharacterRunAutomaton automaton = automata[i];\n        refBuilder.copyUTF8Bytes(term);\n        if (automaton.run(refBuilder.chars(), 0, refBuilder.length())) {\n          PostingsEnum postings = termsEnum.postings(null, PostingsEnum.OFFSETS);\n          if (doc == postings.advance(doc)) {\n            automataPostings.get(i).add(postings);\n          }\n        }\n      }\n    }\n\n    for (int i = 0; i < automata.length; i++) {\n      CharacterRunAutomaton automaton = automata[i];\n      List<PostingsEnum> postingsEnums = automataPostings.get(i);\n      int size = postingsEnums.size();\n      if (size > 0) { //only add if we have offsets\n        BytesRef wildcardTerm = new BytesRef(automaton.toString());\n        if (size == 1) { //don't wrap in a composite if there's only one OffsetsEnum\n          results.add(new OffsetsEnum.OfPostings(wildcardTerm, postingsEnums.get(0)));\n        } else {\n          results.add(new OffsetsEnum.OfPostings(wildcardTerm, new CompositeOffsetsPostingsEnum(postingsEnums)));\n        }\n      }\n    }\n\n  }\n\n","sourceOld":"  protected List<OffsetsEnum> createAutomataOffsetsFromTerms(Terms termsIndex, int doc) throws IOException {\n    List<List<PostingsEnum>> automataPostings = new ArrayList<>(automata.length);\n    for (int i = 0; i < automata.length; i++) {\n      automataPostings.add(new ArrayList<>());\n    }\n\n    TermsEnum termsEnum = termsIndex.iterator();\n    BytesRef term;\n    CharsRefBuilder refBuilder = new CharsRefBuilder();\n    while ((term = termsEnum.next()) != null) {\n      for (int i = 0; i < automata.length; i++) {\n        CharacterRunAutomaton automaton = automata[i];\n        refBuilder.copyUTF8Bytes(term);\n        if (automaton.run(refBuilder.chars(), 0, refBuilder.length())) {\n          PostingsEnum postings = termsEnum.postings(null, PostingsEnum.OFFSETS);\n          if (doc == postings.advance(doc)) {\n            automataPostings.get(i).add(postings);\n          }\n        }\n      }\n    }\n\n    List<OffsetsEnum> offsetsEnums = new ArrayList<>(automata.length); //will be at most this long\n    for (int i = 0; i < automata.length; i++) {\n      CharacterRunAutomaton automaton = automata[i];\n      List<PostingsEnum> postingsEnums = automataPostings.get(i);\n      int size = postingsEnums.size();\n      if (size > 0) { //only add if we have offsets\n        BytesRef wildcardTerm = new BytesRef(automaton.toString());\n        if (size == 1) { //don't wrap in a composite if there's only one OffsetsEnum\n          offsetsEnums.add(new OffsetsEnum(wildcardTerm, postingsEnums.get(0)));\n        } else {\n          offsetsEnums.add(new OffsetsEnum(wildcardTerm, new CompositeOffsetsPostingsEnum(postingsEnums)));\n        }\n      }\n    }\n\n    return offsetsEnums;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b94236357aaa22b76c10629851fe4e376e0cea82","date":1516710914,"type":4,"author":"Karl Wright","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldOffsetStrategy#createAutomataOffsetsFromTerms(Terms,int).mjava","sourceNew":null,"sourceOld":"  protected List<OffsetsEnum> createAutomataOffsetsFromTerms(Terms termsIndex, int doc) throws IOException {\n    List<List<PostingsEnum>> automataPostings = new ArrayList<>(automata.length);\n    for (int i = 0; i < automata.length; i++) {\n      automataPostings.add(new ArrayList<>());\n    }\n\n    TermsEnum termsEnum = termsIndex.iterator();\n    BytesRef term;\n    CharsRefBuilder refBuilder = new CharsRefBuilder();\n    while ((term = termsEnum.next()) != null) {\n      for (int i = 0; i < automata.length; i++) {\n        CharacterRunAutomaton automaton = automata[i];\n        refBuilder.copyUTF8Bytes(term);\n        if (automaton.run(refBuilder.chars(), 0, refBuilder.length())) {\n          PostingsEnum postings = termsEnum.postings(null, PostingsEnum.OFFSETS);\n          if (doc == postings.advance(doc)) {\n            automataPostings.get(i).add(postings);\n          }\n        }\n      }\n    }\n\n    List<OffsetsEnum> offsetsEnums = new ArrayList<>(automata.length); //will be at most this long\n    for (int i = 0; i < automata.length; i++) {\n      CharacterRunAutomaton automaton = automata[i];\n      List<PostingsEnum> postingsEnums = automataPostings.get(i);\n      int size = postingsEnums.size();\n      if (size > 0) { //only add if we have offsets\n        BytesRef wildcardTerm = new BytesRef(automaton.toString());\n        if (size == 1) { //don't wrap in a composite if there's only one OffsetsEnum\n          offsetsEnums.add(new OffsetsEnum(wildcardTerm, postingsEnums.get(0)));\n        } else {\n          offsetsEnums.add(new OffsetsEnum(wildcardTerm, new CompositeOffsetsPostingsEnum(postingsEnums)));\n        }\n      }\n    }\n\n    return offsetsEnums;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b94236357aaa22b76c10629851fe4e376e0cea82":["f2e9861e4a2b724d9fc51b618714c579491b78d7","eef54e3d232eae0e9fc18d75e9b0c3d9ce04b571"],"f2e9861e4a2b724d9fc51b618714c579491b78d7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a1ef55e1fff7ff44354432770ad8bc19be1fcc75":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f2e9861e4a2b724d9fc51b618714c579491b78d7"],"eef54e3d232eae0e9fc18d75e9b0c3d9ce04b571":["f2e9861e4a2b724d9fc51b618714c579491b78d7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b94236357aaa22b76c10629851fe4e376e0cea82"]},"commit2Childs":{"b94236357aaa22b76c10629851fe4e376e0cea82":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"f2e9861e4a2b724d9fc51b618714c579491b78d7":["b94236357aaa22b76c10629851fe4e376e0cea82","a1ef55e1fff7ff44354432770ad8bc19be1fcc75","eef54e3d232eae0e9fc18d75e9b0c3d9ce04b571"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["f2e9861e4a2b724d9fc51b618714c579491b78d7","a1ef55e1fff7ff44354432770ad8bc19be1fcc75"],"a1ef55e1fff7ff44354432770ad8bc19be1fcc75":[],"eef54e3d232eae0e9fc18d75e9b0c3d9ce04b571":["b94236357aaa22b76c10629851fe4e376e0cea82"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a1ef55e1fff7ff44354432770ad8bc19be1fcc75","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}