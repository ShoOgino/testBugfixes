{"path":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","commits":[{"id":"0beaed456aa3358e5e4a99ea2aea994ef6c81de3","date":1365434191,"type":0,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(15);\n    printLayout();\n\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    DocRouter router = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION).getRouter();\n    Slice shard1 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n    DocRouter.Range shard1Range = shard1.getRange() != null ? shard1.getRange() : router.fullRange();\n    final List<DocRouter.Range> ranges = router.partitionRange(2, shard1Range);\n    final int[] docCounts = new int[ranges.size()];\n    int numReplicas = shard1.getReplicas().size();\n\n    final SolrServer solrServer = clients.get(0);\n    solrServer.deleteByQuery(\"*:*\");\n    for (int i = 0; i < 100; i++) {\n      indexr(\"id\", i);\n\n      // todo - hook in custom hashing\n      byte[] bytes = String.valueOf(i).getBytes(\"UTF-8\");\n      int hash = Hash.murmurhash3_x86_32(bytes, 0, bytes.length, 0);\n      for (int i2 = 0; i2 < ranges.size(); i2++) {\n        DocRouter.Range range = ranges.get(i2);\n        if (range.includes(hash))\n          docCounts[i2]++;\n      }\n    }\n    solrServer.commit();\n\n    waitForRecoveriesToFinish(false);\n\n    Thread indexThread = new Thread() {\n      @Override\n      public void run() {\n        for (int i = 101; i < 201; i++) {\n          try {\n            indexr(\"id\", i);\n\n            // todo - hook in custom hashing\n            byte[] bytes = String.valueOf(i).getBytes(\"UTF-8\");\n            int hash = Hash.murmurhash3_x86_32(bytes, 0, bytes.length, 0);\n            for (int i2 = 0; i2 < ranges.size(); i2++) {\n              DocRouter.Range range = ranges.get(i2);\n              if (range.includes(hash))\n                docCounts[i2]++;\n            }\n            Thread.sleep(100);\n          } catch (Exception e) {\n            log.error(\"Exception while adding doc\", e);\n          }\n        }\n      }\n    };\n    indexThread.start();\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionParams.CollectionAction.SPLITSHARD.toString());\n    params.set(\"collection\", \"collection1\");\n    params.set(\"shard\", SHARD1);\n    SolrRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n\n    String baseUrl = ((HttpSolrServer) shardToJetty.get(SHARD1).get(0).client.solrClient)\n        .getBaseURL();\n    baseUrl = baseUrl.substring(0, baseUrl.length() - \"collection1\".length());\n\n    HttpSolrServer baseServer = new HttpSolrServer(baseUrl);\n    baseServer.setConnectionTimeout(15000);\n    baseServer.setSoTimeout((int) (CollectionsHandler.DEFAULT_ZK_TIMEOUT * 5));\n    baseServer.request(request);\n\n    log.info(\"Layout after split: \\n\");\n    printLayout();\n\n    indexThread.join();\n\n    solrServer.commit(); // distributed commit on all shards\n\n    SolrQuery query = new SolrQuery(\"*:*\").setRows(1000).setFields(\"id\", \"_version_\");\n    query.set(\"distrib\", false);\n\n    ZkCoreNodeProps shard1_0 = getLeaderUrlFromZk(AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1_0);\n    HttpSolrServer shard1_0Server = new HttpSolrServer(shard1_0.getCoreUrl());\n    QueryResponse response = shard1_0Server.query(query);\n    long shard10Count = response.getResults().getNumFound();\n//    log.info(\"Resp: shard: shard1_0 url: \" + shard1_0.getCoreUrl() + \"\\n\" + response.getResponse());\n\n    ZkCoreNodeProps shard1_1 = getLeaderUrlFromZk(AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1_1);\n    HttpSolrServer shard1_1Server = new HttpSolrServer(shard1_1.getCoreUrl());\n    QueryResponse response2 = shard1_1Server.query(query);\n    long shard11Count = response2.getResults().getNumFound();\n    //log.info(\"Resp: shard: shard1_1 url: \" + shard1_1.getCoreUrl() + \"\\n\" + response.getResponse());\n\n    for (int i = 0; i < docCounts.length; i++) {\n      int docCount = docCounts[i];\n      log.info(\"Expected docCount for shard1_{} = {}\", i, docCount);\n    }\n\n    // DEBUGGING CODE\n    log.info(\"Actual docCount for shard1_0 = {}\", shard10Count);\n    log.info(\"Actual docCount for shard1_1 = {}\", shard11Count);\n    Map<String, String> idVsVersion = new HashMap<String, String>();\n    Map<String, SolrDocument> shard10Docs = new HashMap<String, SolrDocument>();\n    Map<String, SolrDocument> shard11Docs = new HashMap<String, SolrDocument>();\n    for (int i = 0; i < response.getResults().size(); i++) {\n      SolrDocument document = response.getResults().get(i);\n      idVsVersion.put(document.getFieldValue(\"id\").toString(), document.getFieldValue(\"_version_\").toString());\n      SolrDocument old = shard10Docs.put(document.getFieldValue(\"id\").toString(), document);\n      if (old != null) {\n        log.error(\"EXTRA: ID: \" + document.getFieldValue(\"id\") + \" on shard1_0. Old version: \" + old.getFieldValue(\"_version_\") + \" new version: \" + document.getFieldValue(\"_version_\"));\n      }\n    }\n    for (int i = 0; i < response2.getResults().size(); i++) {\n      SolrDocument document = response2.getResults().get(i);\n      String value = document.getFieldValue(\"id\").toString();\n      String version = idVsVersion.get(value);\n      if (version != null) {\n        log.error(\"DUPLICATE: ID: \" + value + \" , shard1_0Version: \" + version + \" shard1_1Version:\" + document.getFieldValue(\"_version_\"));\n      }\n      SolrDocument old = shard11Docs.put(document.getFieldValue(\"id\").toString(), document);\n      if (old != null) {\n        log.error(\"EXTRA: ID: \" + document.getFieldValue(\"id\") + \" on shard1_1. Old version: \" + old.getFieldValue(\"_version_\") + \" new version: \" + document.getFieldValue(\"_version_\"));\n      }\n    }\n    // END DEBUGGING CODE\n\n    assertEquals(\"Wrong doc count on shard1_0\", docCounts[0], shard10Count);\n    assertEquals(\"Wrong doc count on shard1_1\", docCounts[1], shard11Count);\n\n    Slice slice1_0 = null, slice1_1 = null;\n    int i = 0;\n    for (i = 0; i < 10; i++) {\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      zkStateReader.updateClusterState(true);\n      clusterState = zkStateReader.getClusterState();\n      slice1_0 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, \"shard1_0\");\n      slice1_1 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, \"shard1_1\");\n      if (Slice.ACTIVE.equals(slice1_0.getState()) && Slice.ACTIVE.equals(slice1_1.getState()))\n        break;\n      Thread.sleep(500);\n    }\n\n    log.info(\"ShardSplitTest waited for {} ms for shard state to be set to active\", i * 500);\n\n    assertNotNull(\"Cluster state does not contain shard1_0\", slice1_0);\n    assertNotNull(\"Cluster state does not contain shard1_0\", slice1_1);\n    assertEquals(\"shard1_0 is not active\", Slice.ACTIVE, slice1_0.getState());\n    assertEquals(\"shard1_1 is not active\", Slice.ACTIVE, slice1_1.getState());\n    assertEquals(\"Wrong number of replicas created for shard1_0\", numReplicas, slice1_0.getReplicas().size());\n    assertEquals(\"Wrong number of replicas created for shard1_1\", numReplicas, slice1_1.getReplicas().size());\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n\n    // todo - more and better tests\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["2dbed1dd58810b079506c1e4cd13ce80e646faed"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8735477f53284dd67c6335828378cadf20cddabc","date":1365956061,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    DocRouter router = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION).getRouter();\n    Slice shard1 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n    DocRouter.Range shard1Range = shard1.getRange() != null ? shard1.getRange() : router.fullRange();\n    final List<DocRouter.Range> ranges = router.partitionRange(2, shard1Range);\n    final int[] docCounts = new int[ranges.size()];\n    int numReplicas = shard1.getReplicas().size();\n\n    del(\"*:*\");\n    for (int id = 0; id < 100; id++) {\n      indexAndUpdateCount(ranges, docCounts, id);\n    }\n    commit();\n\n    Thread indexThread = new Thread() {\n      @Override\n      public void run() {\n        for (int id = 101; id < atLeast(401); id++) {\n          try {\n            indexAndUpdateCount(ranges, docCounts, id);\n            Thread.sleep(atLeast(25));\n          } catch (Exception e) {\n            log.error(\"Exception while adding doc\", e);\n          }\n        }\n      }\n    };\n    indexThread.start();\n\n    splitShard(SHARD1);\n\n    log.info(\"Layout after split: \\n\");\n    printLayout();\n\n    indexThread.join();\n\n    commit();\n\n    checkDocCountsAndShardStates(docCounts, numReplicas);\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(15);\n    printLayout();\n\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    DocRouter router = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION).getRouter();\n    Slice shard1 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n    DocRouter.Range shard1Range = shard1.getRange() != null ? shard1.getRange() : router.fullRange();\n    final List<DocRouter.Range> ranges = router.partitionRange(2, shard1Range);\n    final int[] docCounts = new int[ranges.size()];\n    int numReplicas = shard1.getReplicas().size();\n\n    final SolrServer solrServer = clients.get(0);\n    solrServer.deleteByQuery(\"*:*\");\n    for (int i = 0; i < 100; i++) {\n      indexr(\"id\", i);\n\n      // todo - hook in custom hashing\n      byte[] bytes = String.valueOf(i).getBytes(\"UTF-8\");\n      int hash = Hash.murmurhash3_x86_32(bytes, 0, bytes.length, 0);\n      for (int i2 = 0; i2 < ranges.size(); i2++) {\n        DocRouter.Range range = ranges.get(i2);\n        if (range.includes(hash))\n          docCounts[i2]++;\n      }\n    }\n    solrServer.commit();\n\n    waitForRecoveriesToFinish(false);\n\n    Thread indexThread = new Thread() {\n      @Override\n      public void run() {\n        for (int i = 101; i < 201; i++) {\n          try {\n            indexr(\"id\", i);\n\n            // todo - hook in custom hashing\n            byte[] bytes = String.valueOf(i).getBytes(\"UTF-8\");\n            int hash = Hash.murmurhash3_x86_32(bytes, 0, bytes.length, 0);\n            for (int i2 = 0; i2 < ranges.size(); i2++) {\n              DocRouter.Range range = ranges.get(i2);\n              if (range.includes(hash))\n                docCounts[i2]++;\n            }\n            Thread.sleep(100);\n          } catch (Exception e) {\n            log.error(\"Exception while adding doc\", e);\n          }\n        }\n      }\n    };\n    indexThread.start();\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionParams.CollectionAction.SPLITSHARD.toString());\n    params.set(\"collection\", \"collection1\");\n    params.set(\"shard\", SHARD1);\n    SolrRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n\n    String baseUrl = ((HttpSolrServer) shardToJetty.get(SHARD1).get(0).client.solrClient)\n        .getBaseURL();\n    baseUrl = baseUrl.substring(0, baseUrl.length() - \"collection1\".length());\n\n    HttpSolrServer baseServer = new HttpSolrServer(baseUrl);\n    baseServer.setConnectionTimeout(15000);\n    baseServer.setSoTimeout((int) (CollectionsHandler.DEFAULT_ZK_TIMEOUT * 5));\n    baseServer.request(request);\n\n    log.info(\"Layout after split: \\n\");\n    printLayout();\n\n    indexThread.join();\n\n    solrServer.commit(); // distributed commit on all shards\n\n    SolrQuery query = new SolrQuery(\"*:*\").setRows(1000).setFields(\"id\", \"_version_\");\n    query.set(\"distrib\", false);\n\n    ZkCoreNodeProps shard1_0 = getLeaderUrlFromZk(AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1_0);\n    HttpSolrServer shard1_0Server = new HttpSolrServer(shard1_0.getCoreUrl());\n    QueryResponse response = shard1_0Server.query(query);\n    long shard10Count = response.getResults().getNumFound();\n//    log.info(\"Resp: shard: shard1_0 url: \" + shard1_0.getCoreUrl() + \"\\n\" + response.getResponse());\n\n    ZkCoreNodeProps shard1_1 = getLeaderUrlFromZk(AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1_1);\n    HttpSolrServer shard1_1Server = new HttpSolrServer(shard1_1.getCoreUrl());\n    QueryResponse response2 = shard1_1Server.query(query);\n    long shard11Count = response2.getResults().getNumFound();\n    //log.info(\"Resp: shard: shard1_1 url: \" + shard1_1.getCoreUrl() + \"\\n\" + response.getResponse());\n\n    for (int i = 0; i < docCounts.length; i++) {\n      int docCount = docCounts[i];\n      log.info(\"Expected docCount for shard1_{} = {}\", i, docCount);\n    }\n\n    // DEBUGGING CODE\n    log.info(\"Actual docCount for shard1_0 = {}\", shard10Count);\n    log.info(\"Actual docCount for shard1_1 = {}\", shard11Count);\n    Map<String, String> idVsVersion = new HashMap<String, String>();\n    Map<String, SolrDocument> shard10Docs = new HashMap<String, SolrDocument>();\n    Map<String, SolrDocument> shard11Docs = new HashMap<String, SolrDocument>();\n    for (int i = 0; i < response.getResults().size(); i++) {\n      SolrDocument document = response.getResults().get(i);\n      idVsVersion.put(document.getFieldValue(\"id\").toString(), document.getFieldValue(\"_version_\").toString());\n      SolrDocument old = shard10Docs.put(document.getFieldValue(\"id\").toString(), document);\n      if (old != null) {\n        log.error(\"EXTRA: ID: \" + document.getFieldValue(\"id\") + \" on shard1_0. Old version: \" + old.getFieldValue(\"_version_\") + \" new version: \" + document.getFieldValue(\"_version_\"));\n      }\n    }\n    for (int i = 0; i < response2.getResults().size(); i++) {\n      SolrDocument document = response2.getResults().get(i);\n      String value = document.getFieldValue(\"id\").toString();\n      String version = idVsVersion.get(value);\n      if (version != null) {\n        log.error(\"DUPLICATE: ID: \" + value + \" , shard1_0Version: \" + version + \" shard1_1Version:\" + document.getFieldValue(\"_version_\"));\n      }\n      SolrDocument old = shard11Docs.put(document.getFieldValue(\"id\").toString(), document);\n      if (old != null) {\n        log.error(\"EXTRA: ID: \" + document.getFieldValue(\"id\") + \" on shard1_1. Old version: \" + old.getFieldValue(\"_version_\") + \" new version: \" + document.getFieldValue(\"_version_\"));\n      }\n    }\n    // END DEBUGGING CODE\n\n    assertEquals(\"Wrong doc count on shard1_0\", docCounts[0], shard10Count);\n    assertEquals(\"Wrong doc count on shard1_1\", docCounts[1], shard11Count);\n\n    Slice slice1_0 = null, slice1_1 = null;\n    int i = 0;\n    for (i = 0; i < 10; i++) {\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      zkStateReader.updateClusterState(true);\n      clusterState = zkStateReader.getClusterState();\n      slice1_0 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, \"shard1_0\");\n      slice1_1 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, \"shard1_1\");\n      if (Slice.ACTIVE.equals(slice1_0.getState()) && Slice.ACTIVE.equals(slice1_1.getState()))\n        break;\n      Thread.sleep(500);\n    }\n\n    log.info(\"ShardSplitTest waited for {} ms for shard state to be set to active\", i * 500);\n\n    assertNotNull(\"Cluster state does not contain shard1_0\", slice1_0);\n    assertNotNull(\"Cluster state does not contain shard1_0\", slice1_1);\n    assertEquals(\"shard1_0 is not active\", Slice.ACTIVE, slice1_0.getState());\n    assertEquals(\"shard1_1 is not active\", Slice.ACTIVE, slice1_1.getState());\n    assertEquals(\"Wrong number of replicas created for shard1_0\", numReplicas, slice1_0.getReplicas().size());\n    assertEquals(\"Wrong number of replicas created for shard1_1\", numReplicas, slice1_1.getReplicas().size());\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n\n    // todo - more and better tests\n  }\n\n","bugFix":null,"bugIntro":["2dbed1dd58810b079506c1e4cd13ce80e646faed","6776b3c3ed554ace17893a807da5b7a0a6d364c8"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6776b3c3ed554ace17893a807da5b7a0a6d364c8","date":1367964133,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    DocRouter router = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION).getRouter();\n    Slice shard1 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n    DocRouter.Range shard1Range = shard1.getRange() != null ? shard1.getRange() : router.fullRange();\n    final List<DocRouter.Range> ranges = router.partitionRange(2, shard1Range);\n    final int[] docCounts = new int[ranges.size()];\n    int numReplicas = shard1.getReplicas().size();\n\n    del(\"*:*\");\n    for (int id = 0; id < 100; id++) {\n      indexAndUpdateCount(ranges, docCounts, id);\n    }\n    commit();\n\n    Thread indexThread = new Thread() {\n      @Override\n      public void run() {\n        int max = atLeast(401);\n        for (int id = 101; id < max; id++) {\n          try {\n            indexAndUpdateCount(ranges, docCounts, id);\n            Thread.sleep(atLeast(25));\n          } catch (Exception e) {\n            log.error(\"Exception while adding doc\", e);\n          }\n        }\n      }\n    };\n    indexThread.start();\n\n    splitShard(SHARD1);\n\n    log.info(\"Layout after split: \\n\");\n    printLayout();\n\n    indexThread.join();\n\n    commit();\n\n    checkDocCountsAndShardStates(docCounts, numReplicas);\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    DocRouter router = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION).getRouter();\n    Slice shard1 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n    DocRouter.Range shard1Range = shard1.getRange() != null ? shard1.getRange() : router.fullRange();\n    final List<DocRouter.Range> ranges = router.partitionRange(2, shard1Range);\n    final int[] docCounts = new int[ranges.size()];\n    int numReplicas = shard1.getReplicas().size();\n\n    del(\"*:*\");\n    for (int id = 0; id < 100; id++) {\n      indexAndUpdateCount(ranges, docCounts, id);\n    }\n    commit();\n\n    Thread indexThread = new Thread() {\n      @Override\n      public void run() {\n        for (int id = 101; id < atLeast(401); id++) {\n          try {\n            indexAndUpdateCount(ranges, docCounts, id);\n            Thread.sleep(atLeast(25));\n          } catch (Exception e) {\n            log.error(\"Exception while adding doc\", e);\n          }\n        }\n      }\n    };\n    indexThread.start();\n\n    splitShard(SHARD1);\n\n    log.info(\"Layout after split: \\n\");\n    printLayout();\n\n    indexThread.join();\n\n    commit();\n\n    checkDocCountsAndShardStates(docCounts, numReplicas);\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n  }\n\n","bugFix":["8735477f53284dd67c6335828378cadf20cddabc"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2dbed1dd58810b079506c1e4cd13ce80e646faed","date":1368050251,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    final DocRouter router = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION).getRouter();\n    Slice shard1 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n    DocRouter.Range shard1Range = shard1.getRange() != null ? shard1.getRange() : router.fullRange();\n    final List<DocRouter.Range> ranges = router.partitionRange(2, shard1Range);\n    final int[] docCounts = new int[ranges.size()];\n    int numReplicas = shard1.getReplicas().size();\n\n    del(\"*:*\");\n    for (int id = 0; id <= 100; id++) {\n      String shardKey = \"\" + (char)('a' + (id % 26)); // See comment in ShardRoutingTest for hash distribution\n      indexAndUpdateCount(router, ranges, docCounts, shardKey + \"!\" + String.valueOf(id));\n    }\n    commit();\n\n    Thread indexThread = new Thread() {\n      @Override\n      public void run() {\n        int max = atLeast(401);\n        for (int id = 101; id < max; id++) {\n          try {\n            indexAndUpdateCount(router, ranges, docCounts, String.valueOf(id));\n            Thread.sleep(atLeast(25));\n          } catch (Exception e) {\n            log.error(\"Exception while adding doc\", e);\n          }\n        }\n      }\n    };\n    indexThread.start();\n\n    splitShard(SHARD1);\n\n    log.info(\"Layout after split: \\n\");\n    printLayout();\n\n    indexThread.join();\n\n    commit();\n\n    checkDocCountsAndShardStates(docCounts, numReplicas);\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    DocRouter router = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION).getRouter();\n    Slice shard1 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n    DocRouter.Range shard1Range = shard1.getRange() != null ? shard1.getRange() : router.fullRange();\n    final List<DocRouter.Range> ranges = router.partitionRange(2, shard1Range);\n    final int[] docCounts = new int[ranges.size()];\n    int numReplicas = shard1.getReplicas().size();\n\n    del(\"*:*\");\n    for (int id = 0; id < 100; id++) {\n      indexAndUpdateCount(ranges, docCounts, id);\n    }\n    commit();\n\n    Thread indexThread = new Thread() {\n      @Override\n      public void run() {\n        int max = atLeast(401);\n        for (int id = 101; id < max; id++) {\n          try {\n            indexAndUpdateCount(ranges, docCounts, id);\n            Thread.sleep(atLeast(25));\n          } catch (Exception e) {\n            log.error(\"Exception while adding doc\", e);\n          }\n        }\n      }\n    };\n    indexThread.start();\n\n    splitShard(SHARD1);\n\n    log.info(\"Layout after split: \\n\");\n    printLayout();\n\n    indexThread.join();\n\n    commit();\n\n    checkDocCountsAndShardStates(docCounts, numReplicas);\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n  }\n\n","bugFix":["0beaed456aa3358e5e4a99ea2aea994ef6c81de3","8735477f53284dd67c6335828378cadf20cddabc"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8f1ea787bab5bdb5e72685e55424898da05509b6","date":1370289750,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    final DocRouter router = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION).getRouter();\n    Slice shard1 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n    DocRouter.Range shard1Range = shard1.getRange() != null ? shard1.getRange() : router.fullRange();\n    final List<DocRouter.Range> ranges = router.partitionRange(2, shard1Range);\n    final int[] docCounts = new int[ranges.size()];\n    int numReplicas = shard1.getReplicas().size();\n\n    del(\"*:*\");\n    for (int id = 0; id <= 100; id++) {\n      String shardKey = \"\" + (char)('a' + (id % 26)); // See comment in ShardRoutingTest for hash distribution\n      indexAndUpdateCount(router, ranges, docCounts, shardKey + \"!\" + String.valueOf(id), id);\n    }\n    commit();\n\n    Thread indexThread = new Thread() {\n      @Override\n      public void run() {\n        Random random = random();\n        int max = atLeast(random, 401);\n        int sleep = atLeast(random, 25);\n        log.info(\"SHARDSPLITTEST: Going to add \" + max + \" number of docs at 1 doc per \" + sleep + \"ms\");\n        Set<String> deleted = new HashSet<String>();\n        for (int id = 101; id < max; id++) {\n          try {\n            indexAndUpdateCount(router, ranges, docCounts, String.valueOf(id), id);\n            Thread.sleep(sleep);\n            if (usually(random))  {\n              String delId = String.valueOf(random.nextInt(id - 101 + 1) + 101);\n              if (deleted.contains(delId))  continue;\n              try {\n                deleteAndUpdateCount(router, ranges, docCounts, delId);\n                deleted.add(delId);\n              } catch (Exception e) {\n                log.error(\"Exception while deleting docs\", e);\n              }\n            }\n          } catch (Exception e) {\n            log.error(\"Exception while adding docs\", e);\n          }\n        }\n      }\n    };\n    indexThread.start();\n\n    try {\n      splitShard(SHARD1);\n      log.info(\"Layout after split: \\n\");\n      printLayout();\n    } finally {\n      try {\n        indexThread.join();\n      } catch (InterruptedException e) {\n        log.error(\"Indexing thread interrupted\", e);\n      }\n    }\n\n    commit();\n    checkDocCountsAndShardStates(docCounts, numReplicas);\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    final DocRouter router = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION).getRouter();\n    Slice shard1 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n    DocRouter.Range shard1Range = shard1.getRange() != null ? shard1.getRange() : router.fullRange();\n    final List<DocRouter.Range> ranges = router.partitionRange(2, shard1Range);\n    final int[] docCounts = new int[ranges.size()];\n    int numReplicas = shard1.getReplicas().size();\n\n    del(\"*:*\");\n    for (int id = 0; id <= 100; id++) {\n      String shardKey = \"\" + (char)('a' + (id % 26)); // See comment in ShardRoutingTest for hash distribution\n      indexAndUpdateCount(router, ranges, docCounts, shardKey + \"!\" + String.valueOf(id));\n    }\n    commit();\n\n    Thread indexThread = new Thread() {\n      @Override\n      public void run() {\n        int max = atLeast(401);\n        for (int id = 101; id < max; id++) {\n          try {\n            indexAndUpdateCount(router, ranges, docCounts, String.valueOf(id));\n            Thread.sleep(atLeast(25));\n          } catch (Exception e) {\n            log.error(\"Exception while adding doc\", e);\n          }\n        }\n      }\n    };\n    indexThread.start();\n\n    splitShard(SHARD1);\n\n    log.info(\"Layout after split: \\n\");\n    printLayout();\n\n    indexThread.join();\n\n    commit();\n\n    checkDocCountsAndShardStates(docCounts, numReplicas);\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n  }\n\n","bugFix":null,"bugIntro":["0220ee39df0e359431efa5115aeb0729982e3c96"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"15c66d71c28f85069ad65f352d70d281b944f197","date":1372734643,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    final DocRouter router = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION).getRouter();\n    Slice shard1 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n    DocRouter.Range shard1Range = shard1.getRange() != null ? shard1.getRange() : router.fullRange();\n    final List<DocRouter.Range> ranges = router.partitionRange(2, shard1Range);\n    final int[] docCounts = new int[ranges.size()];\n    int numReplicas = shard1.getReplicas().size();\n\n    del(\"*:*\");\n    for (int id = 0; id <= 100; id++) {\n      String shardKey = \"\" + (char)('a' + (id % 26)); // See comment in ShardRoutingTest for hash distribution\n      indexAndUpdateCount(router, ranges, docCounts, shardKey + \"!\" + String.valueOf(id), id);\n    }\n    commit();\n\n    Thread indexThread = new Thread() {\n      @Override\n      public void run() {\n        Random random = random();\n        int max = atLeast(random, 401);\n        int sleep = atLeast(random, 25);\n        log.info(\"SHARDSPLITTEST: Going to add \" + max + \" number of docs at 1 doc per \" + sleep + \"ms\");\n        Set<String> deleted = new HashSet<String>();\n        for (int id = 101; id < max; id++) {\n          try {\n            indexAndUpdateCount(router, ranges, docCounts, String.valueOf(id), id);\n            Thread.sleep(sleep);\n            if (usually(random))  {\n              String delId = String.valueOf(random.nextInt(id - 101 + 1) + 101);\n              if (deleted.contains(delId))  continue;\n              try {\n                deleteAndUpdateCount(router, ranges, docCounts, delId);\n                deleted.add(delId);\n              } catch (Exception e) {\n                log.error(\"Exception while deleting docs\", e);\n              }\n            }\n          } catch (Exception e) {\n            log.error(\"Exception while adding docs\", e);\n          }\n        }\n      }\n    };\n    indexThread.start();\n\n    try {\n      splitShard(SHARD1);\n      log.info(\"Layout after split: \\n\");\n      printLayout();\n    } finally {\n      try {\n        indexThread.join();\n      } catch (InterruptedException e) {\n        log.error(\"Indexing thread interrupted\", e);\n      }\n    }\n\n    commit();\n    \n    try {\n      checkDocCountsAndShardStates(docCounts, numReplicas);\n    } catch (HttpSolrServer.RemoteSolrException e) {\n      if (e.code() != 500) {\n        throw e;\n      }\n      \n      // if we get a 500 error, the split should be retried ... let's wait and see if it works...\n      Slice slice1_0 = null, slice1_1 = null;\n      int i = 0;\n      for (i = 0; i < 60; i++) {\n        ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n        zkStateReader.updateClusterState(true);\n        clusterState = zkStateReader.getClusterState();\n        slice1_0 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, \"shard1_0\");\n        slice1_1 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, \"shard1_1\");\n        if (slice1_0 != null  && slice1_1 != null) {\n          break;\n        }\n        Thread.sleep(500);\n      }\n\n      if (slice1_0 == null  || slice1_1 == null) {\n        throw e;\n      }\n    }\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    final DocRouter router = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION).getRouter();\n    Slice shard1 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n    DocRouter.Range shard1Range = shard1.getRange() != null ? shard1.getRange() : router.fullRange();\n    final List<DocRouter.Range> ranges = router.partitionRange(2, shard1Range);\n    final int[] docCounts = new int[ranges.size()];\n    int numReplicas = shard1.getReplicas().size();\n\n    del(\"*:*\");\n    for (int id = 0; id <= 100; id++) {\n      String shardKey = \"\" + (char)('a' + (id % 26)); // See comment in ShardRoutingTest for hash distribution\n      indexAndUpdateCount(router, ranges, docCounts, shardKey + \"!\" + String.valueOf(id), id);\n    }\n    commit();\n\n    Thread indexThread = new Thread() {\n      @Override\n      public void run() {\n        Random random = random();\n        int max = atLeast(random, 401);\n        int sleep = atLeast(random, 25);\n        log.info(\"SHARDSPLITTEST: Going to add \" + max + \" number of docs at 1 doc per \" + sleep + \"ms\");\n        Set<String> deleted = new HashSet<String>();\n        for (int id = 101; id < max; id++) {\n          try {\n            indexAndUpdateCount(router, ranges, docCounts, String.valueOf(id), id);\n            Thread.sleep(sleep);\n            if (usually(random))  {\n              String delId = String.valueOf(random.nextInt(id - 101 + 1) + 101);\n              if (deleted.contains(delId))  continue;\n              try {\n                deleteAndUpdateCount(router, ranges, docCounts, delId);\n                deleted.add(delId);\n              } catch (Exception e) {\n                log.error(\"Exception while deleting docs\", e);\n              }\n            }\n          } catch (Exception e) {\n            log.error(\"Exception while adding docs\", e);\n          }\n        }\n      }\n    };\n    indexThread.start();\n\n    try {\n      splitShard(SHARD1);\n      log.info(\"Layout after split: \\n\");\n      printLayout();\n    } finally {\n      try {\n        indexThread.join();\n      } catch (InterruptedException e) {\n        log.error(\"Indexing thread interrupted\", e);\n      }\n    }\n\n    commit();\n    checkDocCountsAndShardStates(docCounts, numReplicas);\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n  }\n\n","bugFix":null,"bugIntro":["0220ee39df0e359431efa5115aeb0729982e3c96"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0220ee39df0e359431efa5115aeb0729982e3c96","date":1372771370,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    final DocRouter router = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION).getRouter();\n    Slice shard1 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n    DocRouter.Range shard1Range = shard1.getRange() != null ? shard1.getRange() : router.fullRange();\n    final List<DocRouter.Range> ranges = router.partitionRange(2, shard1Range);\n    final int[] docCounts = new int[ranges.size()];\n    int numReplicas = shard1.getReplicas().size();\n\n    del(\"*:*\");\n    for (int id = 0; id <= 100; id++) {\n      String shardKey = \"\" + (char)('a' + (id % 26)); // See comment in ShardRoutingTest for hash distribution\n      indexAndUpdateCount(router, ranges, docCounts, shardKey + \"!\" + String.valueOf(id), id);\n    }\n    commit();\n\n    Thread indexThread = new Thread() {\n      @Override\n      public void run() {\n        Random random = random();\n        int max = atLeast(random, 401);\n        int sleep = atLeast(random, 25);\n        log.info(\"SHARDSPLITTEST: Going to add \" + max + \" number of docs at 1 doc per \" + sleep + \"ms\");\n        Set<String> deleted = new HashSet<String>();\n        for (int id = 101; id < max; id++) {\n          try {\n            indexAndUpdateCount(router, ranges, docCounts, String.valueOf(id), id);\n            Thread.sleep(sleep);\n            if (usually(random))  {\n              String delId = String.valueOf(random.nextInt(id - 101 + 1) + 101);\n              if (deleted.contains(delId))  continue;\n              try {\n                deleteAndUpdateCount(router, ranges, docCounts, delId);\n                deleted.add(delId);\n              } catch (Exception e) {\n                log.error(\"Exception while deleting docs\", e);\n              }\n            }\n          } catch (Exception e) {\n            log.error(\"Exception while adding docs\", e);\n          }\n        }\n      }\n    };\n    indexThread.start();\n\n    try {\n      for (int i = 0; i < 3; i++) {\n        try {\n          splitShard(SHARD1);\n          log.info(\"Layout after split: \\n\");\n          printLayout();\n          break;\n        } catch (HttpSolrServer.RemoteSolrException e) {\n          if (e.code() != 500)  {\n            throw e;\n          }\n          log.error(\"SPLITSHARD failed. \" + (i < 2 ? \" Retring split\" : \"\"), e);\n          if (i == 2) {\n            fail(\"SPLITSHARD was not successful even after three tries\");\n          }\n        }\n      }\n    } finally {\n      try {\n        indexThread.join();\n      } catch (InterruptedException e) {\n        log.error(\"Indexing thread interrupted\", e);\n      }\n    }\n\n    commit();\n    checkDocCountsAndShardStates(docCounts, numReplicas);\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    final DocRouter router = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION).getRouter();\n    Slice shard1 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n    DocRouter.Range shard1Range = shard1.getRange() != null ? shard1.getRange() : router.fullRange();\n    final List<DocRouter.Range> ranges = router.partitionRange(2, shard1Range);\n    final int[] docCounts = new int[ranges.size()];\n    int numReplicas = shard1.getReplicas().size();\n\n    del(\"*:*\");\n    for (int id = 0; id <= 100; id++) {\n      String shardKey = \"\" + (char)('a' + (id % 26)); // See comment in ShardRoutingTest for hash distribution\n      indexAndUpdateCount(router, ranges, docCounts, shardKey + \"!\" + String.valueOf(id), id);\n    }\n    commit();\n\n    Thread indexThread = new Thread() {\n      @Override\n      public void run() {\n        Random random = random();\n        int max = atLeast(random, 401);\n        int sleep = atLeast(random, 25);\n        log.info(\"SHARDSPLITTEST: Going to add \" + max + \" number of docs at 1 doc per \" + sleep + \"ms\");\n        Set<String> deleted = new HashSet<String>();\n        for (int id = 101; id < max; id++) {\n          try {\n            indexAndUpdateCount(router, ranges, docCounts, String.valueOf(id), id);\n            Thread.sleep(sleep);\n            if (usually(random))  {\n              String delId = String.valueOf(random.nextInt(id - 101 + 1) + 101);\n              if (deleted.contains(delId))  continue;\n              try {\n                deleteAndUpdateCount(router, ranges, docCounts, delId);\n                deleted.add(delId);\n              } catch (Exception e) {\n                log.error(\"Exception while deleting docs\", e);\n              }\n            }\n          } catch (Exception e) {\n            log.error(\"Exception while adding docs\", e);\n          }\n        }\n      }\n    };\n    indexThread.start();\n\n    try {\n      splitShard(SHARD1);\n      log.info(\"Layout after split: \\n\");\n      printLayout();\n    } finally {\n      try {\n        indexThread.join();\n      } catch (InterruptedException e) {\n        log.error(\"Indexing thread interrupted\", e);\n      }\n    }\n\n    commit();\n    \n    try {\n      checkDocCountsAndShardStates(docCounts, numReplicas);\n    } catch (HttpSolrServer.RemoteSolrException e) {\n      if (e.code() != 500) {\n        throw e;\n      }\n      \n      // if we get a 500 error, the split should be retried ... let's wait and see if it works...\n      Slice slice1_0 = null, slice1_1 = null;\n      int i = 0;\n      for (i = 0; i < 60; i++) {\n        ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n        zkStateReader.updateClusterState(true);\n        clusterState = zkStateReader.getClusterState();\n        slice1_0 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, \"shard1_0\");\n        slice1_1 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, \"shard1_1\");\n        if (slice1_0 != null  && slice1_1 != null) {\n          break;\n        }\n        Thread.sleep(500);\n      }\n\n      if (slice1_0 == null  || slice1_1 == null) {\n        throw e;\n      }\n    }\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n  }\n\n","bugFix":["8f1ea787bab5bdb5e72685e55424898da05509b6","15c66d71c28f85069ad65f352d70d281b944f197"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"96adbab674ae121f8b6b3e10474070b4bd97a219","date":1373614333,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    final DocRouter router = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION).getRouter();\n    Slice shard1 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n    DocRouter.Range shard1Range = shard1.getRange() != null ? shard1.getRange() : router.fullRange();\n    final List<DocRouter.Range> ranges = router.partitionRange(2, shard1Range);\n    final int[] docCounts = new int[ranges.size()];\n    int numReplicas = shard1.getReplicas().size();\n\n    del(\"*:*\");\n    for (int id = 0; id <= 100; id++) {\n      String shardKey = \"\" + (char)('a' + (id % 26)); // See comment in ShardRoutingTest for hash distribution\n      indexAndUpdateCount(router, ranges, docCounts, shardKey + \"!\" + String.valueOf(id), id);\n    }\n    commit();\n\n    Thread indexThread = new Thread() {\n      @Override\n      public void run() {\n        Random random = random();\n        int max = atLeast(random, 401);\n        int sleep = atLeast(random, 25);\n        log.info(\"SHARDSPLITTEST: Going to add \" + max + \" number of docs at 1 doc per \" + sleep + \"ms\");\n        Set<String> deleted = new HashSet<String>();\n        for (int id = 101; id < max; id++) {\n          try {\n            indexAndUpdateCount(router, ranges, docCounts, String.valueOf(id), id);\n            Thread.sleep(sleep);\n            if (usually(random))  {\n              String delId = String.valueOf(random.nextInt(id - 101 + 1) + 101);\n              if (deleted.contains(delId))  continue;\n              try {\n                deleteAndUpdateCount(router, ranges, docCounts, delId);\n                deleted.add(delId);\n              } catch (Exception e) {\n                log.error(\"Exception while deleting docs\", e);\n              }\n            }\n          } catch (Exception e) {\n            log.error(\"Exception while adding docs\", e);\n          }\n        }\n      }\n    };\n    indexThread.start();\n\n    try {\n      for (int i = 0; i < 3; i++) {\n        try {\n          splitShard(SHARD1);\n          log.info(\"Layout after split: \\n\");\n          printLayout();\n          break;\n        } catch (HttpSolrServer.RemoteSolrException e) {\n          if (e.code() != 500)  {\n            throw e;\n          }\n          log.error(\"SPLITSHARD failed. \" + (i < 2 ? \" Retring split\" : \"\"), e);\n          if (i == 2) {\n            fail(\"SPLITSHARD was not successful even after three tries\");\n          }\n        }\n      }\n    } finally {\n      try {\n        indexThread.join();\n      } catch (InterruptedException e) {\n        log.error(\"Indexing thread interrupted\", e);\n      }\n    }\n\n    checkDocCountsAndShardStates(docCounts, numReplicas);\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    final DocRouter router = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION).getRouter();\n    Slice shard1 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n    DocRouter.Range shard1Range = shard1.getRange() != null ? shard1.getRange() : router.fullRange();\n    final List<DocRouter.Range> ranges = router.partitionRange(2, shard1Range);\n    final int[] docCounts = new int[ranges.size()];\n    int numReplicas = shard1.getReplicas().size();\n\n    del(\"*:*\");\n    for (int id = 0; id <= 100; id++) {\n      String shardKey = \"\" + (char)('a' + (id % 26)); // See comment in ShardRoutingTest for hash distribution\n      indexAndUpdateCount(router, ranges, docCounts, shardKey + \"!\" + String.valueOf(id), id);\n    }\n    commit();\n\n    Thread indexThread = new Thread() {\n      @Override\n      public void run() {\n        Random random = random();\n        int max = atLeast(random, 401);\n        int sleep = atLeast(random, 25);\n        log.info(\"SHARDSPLITTEST: Going to add \" + max + \" number of docs at 1 doc per \" + sleep + \"ms\");\n        Set<String> deleted = new HashSet<String>();\n        for (int id = 101; id < max; id++) {\n          try {\n            indexAndUpdateCount(router, ranges, docCounts, String.valueOf(id), id);\n            Thread.sleep(sleep);\n            if (usually(random))  {\n              String delId = String.valueOf(random.nextInt(id - 101 + 1) + 101);\n              if (deleted.contains(delId))  continue;\n              try {\n                deleteAndUpdateCount(router, ranges, docCounts, delId);\n                deleted.add(delId);\n              } catch (Exception e) {\n                log.error(\"Exception while deleting docs\", e);\n              }\n            }\n          } catch (Exception e) {\n            log.error(\"Exception while adding docs\", e);\n          }\n        }\n      }\n    };\n    indexThread.start();\n\n    try {\n      for (int i = 0; i < 3; i++) {\n        try {\n          splitShard(SHARD1);\n          log.info(\"Layout after split: \\n\");\n          printLayout();\n          break;\n        } catch (HttpSolrServer.RemoteSolrException e) {\n          if (e.code() != 500)  {\n            throw e;\n          }\n          log.error(\"SPLITSHARD failed. \" + (i < 2 ? \" Retring split\" : \"\"), e);\n          if (i == 2) {\n            fail(\"SPLITSHARD was not successful even after three tries\");\n          }\n        }\n      }\n    } finally {\n      try {\n        indexThread.join();\n      } catch (InterruptedException e) {\n        log.error(\"Indexing thread interrupted\", e);\n      }\n    }\n\n    commit();\n    checkDocCountsAndShardStates(docCounts, numReplicas);\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    final DocRouter router = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION).getRouter();\n    Slice shard1 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n    DocRouter.Range shard1Range = shard1.getRange() != null ? shard1.getRange() : router.fullRange();\n    final List<DocRouter.Range> ranges = router.partitionRange(2, shard1Range);\n    final int[] docCounts = new int[ranges.size()];\n    int numReplicas = shard1.getReplicas().size();\n\n    del(\"*:*\");\n    for (int id = 0; id <= 100; id++) {\n      String shardKey = \"\" + (char)('a' + (id % 26)); // See comment in ShardRoutingTest for hash distribution\n      indexAndUpdateCount(router, ranges, docCounts, shardKey + \"!\" + String.valueOf(id), id);\n    }\n    commit();\n\n    Thread indexThread = new Thread() {\n      @Override\n      public void run() {\n        Random random = random();\n        int max = atLeast(random, 401);\n        int sleep = atLeast(random, 25);\n        log.info(\"SHARDSPLITTEST: Going to add \" + max + \" number of docs at 1 doc per \" + sleep + \"ms\");\n        Set<String> deleted = new HashSet<String>();\n        for (int id = 101; id < max; id++) {\n          try {\n            indexAndUpdateCount(router, ranges, docCounts, String.valueOf(id), id);\n            Thread.sleep(sleep);\n            if (usually(random))  {\n              String delId = String.valueOf(random.nextInt(id - 101 + 1) + 101);\n              if (deleted.contains(delId))  continue;\n              try {\n                deleteAndUpdateCount(router, ranges, docCounts, delId);\n                deleted.add(delId);\n              } catch (Exception e) {\n                log.error(\"Exception while deleting docs\", e);\n              }\n            }\n          } catch (Exception e) {\n            log.error(\"Exception while adding docs\", e);\n          }\n        }\n      }\n    };\n    indexThread.start();\n\n    try {\n      for (int i = 0; i < 3; i++) {\n        try {\n          splitShard(SHARD1);\n          log.info(\"Layout after split: \\n\");\n          printLayout();\n          break;\n        } catch (HttpSolrServer.RemoteSolrException e) {\n          if (e.code() != 500)  {\n            throw e;\n          }\n          log.error(\"SPLITSHARD failed. \" + (i < 2 ? \" Retring split\" : \"\"), e);\n          if (i == 2) {\n            fail(\"SPLITSHARD was not successful even after three tries\");\n          }\n        }\n      }\n    } finally {\n      try {\n        indexThread.join();\n      } catch (InterruptedException e) {\n        log.error(\"Indexing thread interrupted\", e);\n      }\n    }\n\n    checkDocCountsAndShardStates(docCounts, numReplicas);\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    final DocRouter router = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION).getRouter();\n    Slice shard1 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n    DocRouter.Range shard1Range = shard1.getRange() != null ? shard1.getRange() : router.fullRange();\n    final List<DocRouter.Range> ranges = router.partitionRange(2, shard1Range);\n    final int[] docCounts = new int[ranges.size()];\n    int numReplicas = shard1.getReplicas().size();\n\n    del(\"*:*\");\n    for (int id = 0; id <= 100; id++) {\n      String shardKey = \"\" + (char)('a' + (id % 26)); // See comment in ShardRoutingTest for hash distribution\n      indexAndUpdateCount(router, ranges, docCounts, shardKey + \"!\" + String.valueOf(id), id);\n    }\n    commit();\n\n    Thread indexThread = new Thread() {\n      @Override\n      public void run() {\n        Random random = random();\n        int max = atLeast(random, 401);\n        int sleep = atLeast(random, 25);\n        log.info(\"SHARDSPLITTEST: Going to add \" + max + \" number of docs at 1 doc per \" + sleep + \"ms\");\n        Set<String> deleted = new HashSet<String>();\n        for (int id = 101; id < max; id++) {\n          try {\n            indexAndUpdateCount(router, ranges, docCounts, String.valueOf(id), id);\n            Thread.sleep(sleep);\n            if (usually(random))  {\n              String delId = String.valueOf(random.nextInt(id - 101 + 1) + 101);\n              if (deleted.contains(delId))  continue;\n              try {\n                deleteAndUpdateCount(router, ranges, docCounts, delId);\n                deleted.add(delId);\n              } catch (Exception e) {\n                log.error(\"Exception while deleting docs\", e);\n              }\n            }\n          } catch (Exception e) {\n            log.error(\"Exception while adding docs\", e);\n          }\n        }\n      }\n    };\n    indexThread.start();\n\n    try {\n      splitShard(SHARD1);\n      log.info(\"Layout after split: \\n\");\n      printLayout();\n    } finally {\n      try {\n        indexThread.join();\n      } catch (InterruptedException e) {\n        log.error(\"Indexing thread interrupted\", e);\n      }\n    }\n\n    commit();\n    checkDocCountsAndShardStates(docCounts, numReplicas);\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1816753738ff1f27f11b38030e83c0ded050b7a4","date":1380106089,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    splitByUniqueKeyTest();\n    splitByRouteFieldTest();\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    final DocRouter router = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION).getRouter();\n    Slice shard1 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);\n    DocRouter.Range shard1Range = shard1.getRange() != null ? shard1.getRange() : router.fullRange();\n    final List<DocRouter.Range> ranges = router.partitionRange(2, shard1Range);\n    final int[] docCounts = new int[ranges.size()];\n    int numReplicas = shard1.getReplicas().size();\n\n    del(\"*:*\");\n    for (int id = 0; id <= 100; id++) {\n      String shardKey = \"\" + (char)('a' + (id % 26)); // See comment in ShardRoutingTest for hash distribution\n      indexAndUpdateCount(router, ranges, docCounts, shardKey + \"!\" + String.valueOf(id), id);\n    }\n    commit();\n\n    Thread indexThread = new Thread() {\n      @Override\n      public void run() {\n        Random random = random();\n        int max = atLeast(random, 401);\n        int sleep = atLeast(random, 25);\n        log.info(\"SHARDSPLITTEST: Going to add \" + max + \" number of docs at 1 doc per \" + sleep + \"ms\");\n        Set<String> deleted = new HashSet<String>();\n        for (int id = 101; id < max; id++) {\n          try {\n            indexAndUpdateCount(router, ranges, docCounts, String.valueOf(id), id);\n            Thread.sleep(sleep);\n            if (usually(random))  {\n              String delId = String.valueOf(random.nextInt(id - 101 + 1) + 101);\n              if (deleted.contains(delId))  continue;\n              try {\n                deleteAndUpdateCount(router, ranges, docCounts, delId);\n                deleted.add(delId);\n              } catch (Exception e) {\n                log.error(\"Exception while deleting docs\", e);\n              }\n            }\n          } catch (Exception e) {\n            log.error(\"Exception while adding docs\", e);\n          }\n        }\n      }\n    };\n    indexThread.start();\n\n    try {\n      for (int i = 0; i < 3; i++) {\n        try {\n          splitShard(SHARD1);\n          log.info(\"Layout after split: \\n\");\n          printLayout();\n          break;\n        } catch (HttpSolrServer.RemoteSolrException e) {\n          if (e.code() != 500)  {\n            throw e;\n          }\n          log.error(\"SPLITSHARD failed. \" + (i < 2 ? \" Retring split\" : \"\"), e);\n          if (i == 2) {\n            fail(\"SPLITSHARD was not successful even after three tries\");\n          }\n        }\n      }\n    } finally {\n      try {\n        indexThread.join();\n      } catch (InterruptedException e) {\n        log.error(\"Indexing thread interrupted\", e);\n      }\n    }\n\n    checkDocCountsAndShardStates(docCounts, numReplicas);\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n  }\n\n","bugFix":null,"bugIntro":["5aa6dcd736e5c400d1c763ae8fa2fe5aedb75132"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d9d5abf772262a05c74afddcadc95c4bdab07f1f","date":1381747682,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    splitByUniqueKeyTest();\n    splitByRouteFieldTest();\n    splitByRouteKeyTest();\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    splitByUniqueKeyTest();\n    splitByRouteFieldTest();\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cee263b0163fa111cfda384934079baae77fccac","date":1382523980,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    incompleteOrOverlappingCustomRangeTest();\n    splitByUniqueKeyTest();\n    splitByRouteFieldTest();\n    splitByRouteKeyTest();\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    splitByUniqueKeyTest();\n    splitByRouteFieldTest();\n    splitByRouteKeyTest();\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4ddb837927b3de29503b68a05fec256665edab50","date":1400760638,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    if (usually()) {\n      log.info(\"Using legacyCloud=false for cluster\");\n      CollectionsAPIDistributedZkTest.setClusterProp(cloudClient, \"legacyCloud\", \"false\");\n    }\n    incompleteOrOverlappingCustomRangeTest();\n    splitByUniqueKeyTest();\n    splitByRouteFieldTest();\n    splitByRouteKeyTest();\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    incompleteOrOverlappingCustomRangeTest();\n    splitByUniqueKeyTest();\n    splitByRouteFieldTest();\n    splitByRouteKeyTest();\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n\n  }\n\n","bugFix":null,"bugIntro":["5ad9c35f926b4bf8da0336d1300efc709c8d5a56","5ad9c35f926b4bf8da0336d1300efc709c8d5a56"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"abb23fcc2461782ab204e61213240feb77d355aa","date":1422029612,"type":5,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#doTest().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    if (usually()) {\n      log.info(\"Using legacyCloud=false for cluster\");\n      CollectionsAPIDistributedZkTest.setClusterProp(cloudClient, \"legacyCloud\", \"false\");\n    }\n    incompleteOrOverlappingCustomRangeTest();\n    splitByUniqueKeyTest();\n    splitByRouteFieldTest();\n    splitByRouteKeyTest();\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    if (usually()) {\n      log.info(\"Using legacyCloud=false for cluster\");\n      CollectionsAPIDistributedZkTest.setClusterProp(cloudClient, \"legacyCloud\", \"false\");\n    }\n    incompleteOrOverlappingCustomRangeTest();\n    splitByUniqueKeyTest();\n    splitByRouteFieldTest();\n    splitByRouteKeyTest();\n\n    // todo can't call waitForThingsToLevelOut because it looks for jettys of all shards\n    // and the new sub-shards don't have any.\n    waitForRecoveriesToFinish(true);\n    //waitForThingsToLevelOut(15);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"1816753738ff1f27f11b38030e83c0ded050b7a4":["96adbab674ae121f8b6b3e10474070b4bd97a219"],"2dbed1dd58810b079506c1e4cd13ce80e646faed":["6776b3c3ed554ace17893a807da5b7a0a6d364c8"],"4ddb837927b3de29503b68a05fec256665edab50":["cee263b0163fa111cfda384934079baae77fccac"],"0220ee39df0e359431efa5115aeb0729982e3c96":["15c66d71c28f85069ad65f352d70d281b944f197"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["8f1ea787bab5bdb5e72685e55424898da05509b6","96adbab674ae121f8b6b3e10474070b4bd97a219"],"abb23fcc2461782ab204e61213240feb77d355aa":["4ddb837927b3de29503b68a05fec256665edab50"],"6776b3c3ed554ace17893a807da5b7a0a6d364c8":["8735477f53284dd67c6335828378cadf20cddabc"],"15c66d71c28f85069ad65f352d70d281b944f197":["8f1ea787bab5bdb5e72685e55424898da05509b6"],"8f1ea787bab5bdb5e72685e55424898da05509b6":["2dbed1dd58810b079506c1e4cd13ce80e646faed"],"cee263b0163fa111cfda384934079baae77fccac":["d9d5abf772262a05c74afddcadc95c4bdab07f1f"],"8735477f53284dd67c6335828378cadf20cddabc":["0beaed456aa3358e5e4a99ea2aea994ef6c81de3"],"0beaed456aa3358e5e4a99ea2aea994ef6c81de3":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"96adbab674ae121f8b6b3e10474070b4bd97a219":["0220ee39df0e359431efa5115aeb0729982e3c96"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d9d5abf772262a05c74afddcadc95c4bdab07f1f":["1816753738ff1f27f11b38030e83c0ded050b7a4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["abb23fcc2461782ab204e61213240feb77d355aa"]},"commit2Childs":{"1816753738ff1f27f11b38030e83c0ded050b7a4":["d9d5abf772262a05c74afddcadc95c4bdab07f1f"],"2dbed1dd58810b079506c1e4cd13ce80e646faed":["8f1ea787bab5bdb5e72685e55424898da05509b6"],"4ddb837927b3de29503b68a05fec256665edab50":["abb23fcc2461782ab204e61213240feb77d355aa"],"0220ee39df0e359431efa5115aeb0729982e3c96":["96adbab674ae121f8b6b3e10474070b4bd97a219"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"6776b3c3ed554ace17893a807da5b7a0a6d364c8":["2dbed1dd58810b079506c1e4cd13ce80e646faed"],"abb23fcc2461782ab204e61213240feb77d355aa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"15c66d71c28f85069ad65f352d70d281b944f197":["0220ee39df0e359431efa5115aeb0729982e3c96"],"8f1ea787bab5bdb5e72685e55424898da05509b6":["37a0f60745e53927c4c876cfe5b5a58170f0646c","15c66d71c28f85069ad65f352d70d281b944f197"],"cee263b0163fa111cfda384934079baae77fccac":["4ddb837927b3de29503b68a05fec256665edab50"],"8735477f53284dd67c6335828378cadf20cddabc":["6776b3c3ed554ace17893a807da5b7a0a6d364c8"],"0beaed456aa3358e5e4a99ea2aea994ef6c81de3":["8735477f53284dd67c6335828378cadf20cddabc"],"96adbab674ae121f8b6b3e10474070b4bd97a219":["1816753738ff1f27f11b38030e83c0ded050b7a4","37a0f60745e53927c4c876cfe5b5a58170f0646c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0beaed456aa3358e5e4a99ea2aea994ef6c81de3"],"d9d5abf772262a05c74afddcadc95c4bdab07f1f":["cee263b0163fa111cfda384934079baae77fccac"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["37a0f60745e53927c4c876cfe5b5a58170f0646c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}