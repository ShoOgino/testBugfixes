{"path":"sandbox/contributions/lucli/src/lucli/LuceneMethods#invertDocument(Document).mjava","commits":[{"id":"66a601acf439105252becef7e0f1cb6b2e296bce","date":1071108465,"type":0,"author":"Erik Hatcher","isMerge":false,"pathNew":"sandbox/contributions/lucli/src/lucli/LuceneMethods#invertDocument(Document).mjava","pathOld":"/dev/null","sourceNew":"\t// Copied from DocumentWriter\n\t// Tokenizes the fields of a document into Postings.\n\tprivate void invertDocument(Document doc)\n\t\tthrows IOException {\n\n\t\tHashtable tokenHash = new Hashtable();\n\t\tfinal int maxFieldLength = 10000;\n\n\t\tAnalyzer analyzer = new StandardAnalyzer();\n\t\tEnumeration fields = doc.fields();\n\t\twhile (fields.hasMoreElements()) {\n\t\t\tField field = (Field) fields.nextElement();\n\t\t\tString fieldName = field.name();\n\n\n\t\t\tif (field.isIndexed()) {\n\t\t\t\tif (field.isTokenized()) {     // un-tokenized field\n\t\t\t\t\tReader reader;        // find or make Reader\n\t\t\t\t\tif (field.readerValue() != null)\n\t\t\t\t\t\treader = field.readerValue();\n\t\t\t\t\telse if (field.stringValue() != null)\n\t\t\t\t\t\treader = new StringReader(field.stringValue());\n\t\t\t\t\telse\n\t\t\t\t\t\tthrow new IllegalArgumentException\n\t\t\t\t\t\t\t(\"field must have either String or Reader value\");\n\n\t\t\t\t\tint position = 0;\n\t\t\t\t\t// Tokenize field and add to postingTable\n\t\t\t\t\tTokenStream stream = analyzer.tokenStream(fieldName, reader);\n\t\t\t\t\ttry {\n\t\t\t\t\t\tfor (Token t = stream.next(); t != null; t = stream.next()) {\n\t\t\t\t\t\t\tposition += (t.getPositionIncrement() - 1);\n\t\t\t\t\t\t\tposition++;\n\t\t\t\t\t\t\tString name = t.termText();\n\t\t\t\t\t\t\tInteger Count = (Integer)tokenHash.get(name);\n\t\t\t\t\t\t\tif (Count == null) { // not in there yet\n\t\t\t\t\t\t\t\ttokenHash.put(name, new Integer(1)); //first one\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tint count = Count.intValue();\n\t\t\t\t\t\t\t\ttokenHash.put(name, new Integer (count+1));\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif (position > maxFieldLength) break;\n\t\t\t\t\t\t}\n\t\t\t\t\t} finally {\n\t\t\t\t\t\tstream.close();\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t}\n\t\t}\n\t\tEntry[] sortedHash = getSortedHashtableEntries(tokenHash);\n\t\tfor (int ii = 0; ii < sortedHash.length && ii < 10; ii ++) {\n\t\t\tEntry currentEntry = sortedHash[ii];\n\t\t\tmessage((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n\t\t}\n\t}\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e98e42b41456ce3f4f653a8e6c3abc34a23f41a8","date":1075080975,"type":3,"author":"Erik Hatcher","isMerge":false,"pathNew":"sandbox/contributions/lucli/src/lucli/LuceneMethods#invertDocument(Document).mjava","pathOld":"sandbox/contributions/lucli/src/lucli/LuceneMethods#invertDocument(Document).mjava","sourceNew":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Hashtable tokenHash = new Hashtable();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = new StandardAnalyzer();\n    Enumeration fields = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field) fields.nextElement();\n      String fieldName = field.name();\n\n\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              position++;\n              String name = t.termText();\n              Integer Count = (Integer) tokenHash.get(name);\n              if (Count == null) { // not in there yet\n                tokenHash.put(name, new Integer(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenHash.put(name, new Integer(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Entry[] sortedHash = getSortedHashtableEntries(tokenHash);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Entry currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","sourceOld":"\t// Copied from DocumentWriter\n\t// Tokenizes the fields of a document into Postings.\n\tprivate void invertDocument(Document doc)\n\t\tthrows IOException {\n\n\t\tHashtable tokenHash = new Hashtable();\n\t\tfinal int maxFieldLength = 10000;\n\n\t\tAnalyzer analyzer = new StandardAnalyzer();\n\t\tEnumeration fields = doc.fields();\n\t\twhile (fields.hasMoreElements()) {\n\t\t\tField field = (Field) fields.nextElement();\n\t\t\tString fieldName = field.name();\n\n\n\t\t\tif (field.isIndexed()) {\n\t\t\t\tif (field.isTokenized()) {     // un-tokenized field\n\t\t\t\t\tReader reader;        // find or make Reader\n\t\t\t\t\tif (field.readerValue() != null)\n\t\t\t\t\t\treader = field.readerValue();\n\t\t\t\t\telse if (field.stringValue() != null)\n\t\t\t\t\t\treader = new StringReader(field.stringValue());\n\t\t\t\t\telse\n\t\t\t\t\t\tthrow new IllegalArgumentException\n\t\t\t\t\t\t\t(\"field must have either String or Reader value\");\n\n\t\t\t\t\tint position = 0;\n\t\t\t\t\t// Tokenize field and add to postingTable\n\t\t\t\t\tTokenStream stream = analyzer.tokenStream(fieldName, reader);\n\t\t\t\t\ttry {\n\t\t\t\t\t\tfor (Token t = stream.next(); t != null; t = stream.next()) {\n\t\t\t\t\t\t\tposition += (t.getPositionIncrement() - 1);\n\t\t\t\t\t\t\tposition++;\n\t\t\t\t\t\t\tString name = t.termText();\n\t\t\t\t\t\t\tInteger Count = (Integer)tokenHash.get(name);\n\t\t\t\t\t\t\tif (Count == null) { // not in there yet\n\t\t\t\t\t\t\t\ttokenHash.put(name, new Integer(1)); //first one\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tint count = Count.intValue();\n\t\t\t\t\t\t\t\ttokenHash.put(name, new Integer (count+1));\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif (position > maxFieldLength) break;\n\t\t\t\t\t\t}\n\t\t\t\t\t} finally {\n\t\t\t\t\t\tstream.close();\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t}\n\t\t}\n\t\tEntry[] sortedHash = getSortedHashtableEntries(tokenHash);\n\t\tfor (int ii = 0; ii < sortedHash.length && ii < 10; ii ++) {\n\t\t\tEntry currentEntry = sortedHash[ii];\n\t\t\tmessage((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a458de6913aa2012d9bbfd9b571ad1f8ab3b1b9a","date":1107704112,"type":5,"author":"Erik Hatcher","isMerge":false,"pathNew":"contrib/lucli/src/lucli/LuceneMethods#invertDocument(Document).mjava","pathOld":"sandbox/contributions/lucli/src/lucli/LuceneMethods#invertDocument(Document).mjava","sourceNew":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Hashtable tokenHash = new Hashtable();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = new StandardAnalyzer();\n    Enumeration fields = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field) fields.nextElement();\n      String fieldName = field.name();\n\n\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              position++;\n              String name = t.termText();\n              Integer Count = (Integer) tokenHash.get(name);\n              if (Count == null) { // not in there yet\n                tokenHash.put(name, new Integer(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenHash.put(name, new Integer(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Entry[] sortedHash = getSortedHashtableEntries(tokenHash);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Entry currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","sourceOld":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Hashtable tokenHash = new Hashtable();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = new StandardAnalyzer();\n    Enumeration fields = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field) fields.nextElement();\n      String fieldName = field.name();\n\n\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              position++;\n              String name = t.termText();\n              Integer Count = (Integer) tokenHash.get(name);\n              if (Count == null) { // not in there yet\n                tokenHash.put(name, new Integer(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenHash.put(name, new Integer(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Entry[] sortedHash = getSortedHashtableEntries(tokenHash);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Entry currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a458de6913aa2012d9bbfd9b571ad1f8ab3b1b9a":["e98e42b41456ce3f4f653a8e6c3abc34a23f41a8"],"66a601acf439105252becef7e0f1cb6b2e296bce":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"e98e42b41456ce3f4f653a8e6c3abc34a23f41a8":["66a601acf439105252becef7e0f1cb6b2e296bce"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a458de6913aa2012d9bbfd9b571ad1f8ab3b1b9a"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["66a601acf439105252becef7e0f1cb6b2e296bce"],"a458de6913aa2012d9bbfd9b571ad1f8ab3b1b9a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"66a601acf439105252becef7e0f1cb6b2e296bce":["e98e42b41456ce3f4f653a8e6c3abc34a23f41a8"],"e98e42b41456ce3f4f653a8e6c3abc34a23f41a8":["a458de6913aa2012d9bbfd9b571ad1f8ab3b1b9a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}