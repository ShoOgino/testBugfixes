{"path":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","commits":[{"id":"6146c07c0dee1ae1e42926167acd127fed5ef59d","date":1516129420,"type":1,"author":"Varun Thacker","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","sourceNew":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n\n      //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas .\n      //Value is still the same as the original. maybe test with different values that the original for better test coverage\n      if (random().nextBoolean())  {\n        restore.setReplicationFactor(replFactor);\n      }\n      if (backupCollection.getReplicas().size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode((int)Math.ceil(backupCollection.getReplicas().size()/cluster.getJettySolrRunners().size()));\n      }\n      \n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        if (restore.getMaxShardsPerNode() != null) {\n          restore.setMaxShardsPerNode(restore.getMaxShardsPerNode() * 2);\n        } else {\n          restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n        }\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    assertEquals(\"Different count of nrtReplicas. Backup collection state=\" + backupCollection + \"\\nRestore \" +\n        \"collection state=\" + restoreCollection, replFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(\"Different count of pullReplicas. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numPullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(\"Different count of TlogReplica. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n\n      //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas .\n      //Value is still the same as the original. maybe test with different values that the original for better test coverage\n      if (random().nextBoolean())  {\n        restore.setReplicationFactor(replFactor);\n      }\n      if (backupCollection.getReplicas().size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode((int)Math.ceil(backupCollection.getReplicas().size()/cluster.getJettySolrRunners().size()));\n      }\n      \n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        if (restore.getMaxShardsPerNode() != null) {\n          restore.setMaxShardsPerNode(restore.getMaxShardsPerNode() * 2);\n        } else {\n          restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n        }\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    assertEquals(\"Different count of nrtReplicas. Backup collection state=\" + backupCollection + \"\\nRestore \" +\n        \"collection state=\" + restoreCollection, replFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(\"Different count of pullReplicas. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numPullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(\"Different count of TlogReplica. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b94236357aaa22b76c10629851fe4e376e0cea82","date":1516710914,"type":1,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","sourceNew":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n\n      //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas .\n      //Value is still the same as the original. maybe test with different values that the original for better test coverage\n      if (random().nextBoolean())  {\n        restore.setReplicationFactor(replFactor);\n      }\n      if (backupCollection.getReplicas().size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode((int)Math.ceil(backupCollection.getReplicas().size()/cluster.getJettySolrRunners().size()));\n      }\n      \n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        if (restore.getMaxShardsPerNode() != null) {\n          restore.setMaxShardsPerNode(restore.getMaxShardsPerNode() * 2);\n        } else {\n          restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n        }\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    assertEquals(\"Different count of nrtReplicas. Backup collection state=\" + backupCollection + \"\\nRestore \" +\n        \"collection state=\" + restoreCollection, replFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(\"Different count of pullReplicas. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numPullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(\"Different count of TlogReplica. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n\n      //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas .\n      //Value is still the same as the original. maybe test with different values that the original for better test coverage\n      if (random().nextBoolean())  {\n        restore.setReplicationFactor(replFactor);\n      }\n      if (backupCollection.getReplicas().size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode((int)Math.ceil(backupCollection.getReplicas().size()/cluster.getJettySolrRunners().size()));\n      }\n      \n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        if (restore.getMaxShardsPerNode() != null) {\n          restore.setMaxShardsPerNode(restore.getMaxShardsPerNode() * 2);\n        } else {\n          restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n        }\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    assertEquals(\"Different count of nrtReplicas. Backup collection state=\" + backupCollection + \"\\nRestore \" +\n        \"collection state=\" + restoreCollection, replFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(\"Different count of pullReplicas. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numPullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(\"Different count of TlogReplica. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"076d58da25128e8a4c511abf07c5d86c4ebddcbf","date":1523546428,"type":3,"author":"Varun Thacker","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","sourceNew":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n\n      //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas .\n      //Value is still the same as the original. maybe test with different values that the original for better test coverage\n      if (random().nextBoolean())  {\n        restore.setReplicationFactor(replFactor);\n      }\n      if (backupCollection.getReplicas().size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode((int)Math.ceil(backupCollection.getReplicas().size()/cluster.getJettySolrRunners().size()));\n      }\n\n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        if (restore.getMaxShardsPerNode() != null) {\n          restore.setMaxShardsPerNode(restore.getMaxShardsPerNode() * 2);\n        } else {\n          restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n        }\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    assertEquals(\"Different count of nrtReplicas. Backup collection state=\" + backupCollection + \"\\nRestore \" +\n        \"collection state=\" + restoreCollection, replFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(\"Different count of pullReplicas. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numPullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(\"Different count of TlogReplica. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:\" + restoredCollectionDocCount + \", number of newly added documents to the restored collection: \" + numberNewDocsIndexed + \", after indexing: \" + restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n\n      //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas .\n      //Value is still the same as the original. maybe test with different values that the original for better test coverage\n      if (random().nextBoolean())  {\n        restore.setReplicationFactor(replFactor);\n      }\n      if (backupCollection.getReplicas().size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode((int)Math.ceil(backupCollection.getReplicas().size()/cluster.getJettySolrRunners().size()));\n      }\n      \n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        if (restore.getMaxShardsPerNode() != null) {\n          restore.setMaxShardsPerNode(restore.getMaxShardsPerNode() * 2);\n        } else {\n          restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n        }\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    assertEquals(\"Different count of nrtReplicas. Backup collection state=\" + backupCollection + \"\\nRestore \" +\n        \"collection state=\" + restoreCollection, replFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(\"Different count of pullReplicas. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numPullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(\"Different count of TlogReplica. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":["c5c99ad021f3da085fcb66220598a8f91dc5e453","c84db5cfadecb30316e2cc080dd63920b5e158bc"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"742168028ecb4838c124d27f836df9637be2f769","date":1529417708,"type":5,"author":"Varun Thacker","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String,int).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","sourceNew":"  private void testBackupAndRestore(String collectionName, int backupReplFactor) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    int restoreReplcationFactor = replFactor;\n    int restoreTlogReplicas = numTlogReplicas;\n    int restorePullReplicas = numPullReplicas;\n    boolean setExternalReplicationFactor = false;\n    if (random().nextBoolean()) { //Override replicationFactor / tLogReplicas / pullReplicas\n      setExternalReplicationFactor = true;\n      restoreTlogReplicas = TestUtil.nextInt(random(), 0, 1);\n      restoreReplcationFactor = TestUtil.nextInt(random(), 1, 2);\n      restorePullReplicas = TestUtil.nextInt(random(), 0, 1);\n    }\n    int numShards = backupCollection.getActiveSlices().size();\n\n    int restoreReplFactor = restoreReplcationFactor + restoreTlogReplicas + restorePullReplicas;\n\n    boolean isMaxShardsPerNodeExternal = false;\n    int restoreMaxShardsPerNode = -1;\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n      //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas.\n      if (setExternalReplicationFactor)  {\n        restore.setReplicationFactor(restoreReplcationFactor);\n        restore.setTlogReplicas(restoreTlogReplicas);\n        restore.setPullReplicas(restorePullReplicas);\n      }\n\n\n      if (restoreReplFactor > backupReplFactor) { //else the backup maxShardsPerNode should be enough\n        restoreMaxShardsPerNode = (int)Math.ceil((restoreReplFactor * numShards/(double) cluster.getJettySolrRunners().size()));\n        log.info(\"numShards={} restoreReplFactor={} maxShardsPerNode={} totalNodes={}\",\n            numShards, restoreReplFactor, restoreMaxShardsPerNode, cluster.getJettySolrRunners().size());\n        isMaxShardsPerNodeExternal = true;\n\n        restore.setMaxShardsPerNode(restoreMaxShardsPerNode);\n      }\n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        if (restore.getMaxShardsPerNode() != null) {\n          restore.setMaxShardsPerNode(restore.getMaxShardsPerNode() * 2);\n        } else {\n          restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n        }\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getReplicationFactor().intValue());\n    assertEquals(restoreCollection.toString(), restoreReplcationFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restorePullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(restoreCollection.toString(), restoreTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n    if (isMaxShardsPerNodeExternal) {\n      assertEquals(restoreCollectionName, restoreMaxShardsPerNode, restoreCollection.getMaxShardsPerNode());\n    } else {\n      assertEquals(restoreCollectionName, backupCollection.getMaxShardsPerNode(), restoreCollection.getMaxShardsPerNode());\n    }\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:\" + restoredCollectionDocCount + \", number of newly added documents to the restored collection: \" + numberNewDocsIndexed + \", after indexing: \" + restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","sourceOld":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n\n      //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas .\n      //Value is still the same as the original. maybe test with different values that the original for better test coverage\n      if (random().nextBoolean())  {\n        restore.setReplicationFactor(replFactor);\n      }\n      if (backupCollection.getReplicas().size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode((int)Math.ceil(backupCollection.getReplicas().size()/cluster.getJettySolrRunners().size()));\n      }\n\n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        if (restore.getMaxShardsPerNode() != null) {\n          restore.setMaxShardsPerNode(restore.getMaxShardsPerNode() * 2);\n        } else {\n          restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n        }\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    assertEquals(\"Different count of nrtReplicas. Backup collection state=\" + backupCollection + \"\\nRestore \" +\n        \"collection state=\" + restoreCollection, replFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(\"Different count of pullReplicas. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numPullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(\"Different count of TlogReplica. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:\" + restoredCollectionDocCount + \", number of newly added documents to the restored collection: \" + numberNewDocsIndexed + \", after indexing: \" + restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26","date":1531589977,"type":4,"author":"Michael Braun","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","sourceNew":null,"sourceOld":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n\n      //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas .\n      //Value is still the same as the original. maybe test with different values that the original for better test coverage\n      if (random().nextBoolean())  {\n        restore.setReplicationFactor(replFactor);\n      }\n      if (backupCollection.getReplicas().size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode((int)Math.ceil(backupCollection.getReplicas().size()/cluster.getJettySolrRunners().size()));\n      }\n\n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        if (restore.getMaxShardsPerNode() != null) {\n          restore.setMaxShardsPerNode(restore.getMaxShardsPerNode() * 2);\n        } else {\n          restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n        }\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    assertEquals(\"Different count of nrtReplicas. Backup collection state=\" + backupCollection + \"\\nRestore \" +\n        \"collection state=\" + restoreCollection, replFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(\"Different count of pullReplicas. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numPullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(\"Different count of TlogReplica. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:\" + restoredCollectionDocCount + \", number of newly added documents to the restored collection: \" + numberNewDocsIndexed + \", after indexing: \" + restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","date":1531905561,"type":4,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/AbstractCloudBackupRestoreTestCase#testBackupAndRestore(String).mjava","sourceNew":null,"sourceOld":"  private void testBackupAndRestore(String collectionName) throws Exception {\n    String backupLocation = getBackupLocation();\n    String backupName = \"mytestbackup\";\n\n    CloudSolrClient client = cluster.getSolrClient();\n    DocCollection backupCollection = client.getZkStateReader().getClusterState().getCollection(collectionName);\n\n    Map<String, Integer> origShardToDocCount = getShardToDocCountMap(client, backupCollection);\n    assert origShardToDocCount.isEmpty() == false;\n\n    log.info(\"Triggering Backup command\");\n\n    {\n      CollectionAdminRequest.Backup backup = CollectionAdminRequest.backupCollection(collectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n      if (random().nextBoolean()) {\n        assertEquals(0, backup.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, backup.processAndWait(client, 30));//async\n      }\n    }\n\n    log.info(\"Triggering Restore command\");\n\n    String restoreCollectionName = collectionName + \"_restored\";\n    boolean sameConfig = random().nextBoolean();\n\n    {\n      CollectionAdminRequest.Restore restore = CollectionAdminRequest.restoreCollection(restoreCollectionName, backupName)\n          .setLocation(backupLocation).setRepositoryName(getBackupRepoName());\n\n\n      //explicitly specify the replicationFactor/pullReplicas/nrtReplicas/tlogReplicas .\n      //Value is still the same as the original. maybe test with different values that the original for better test coverage\n      if (random().nextBoolean())  {\n        restore.setReplicationFactor(replFactor);\n      }\n      if (backupCollection.getReplicas().size() > cluster.getJettySolrRunners().size()) {\n        // may need to increase maxShardsPerNode (e.g. if it was shard split, then now we need more)\n        restore.setMaxShardsPerNode((int)Math.ceil(backupCollection.getReplicas().size()/cluster.getJettySolrRunners().size()));\n      }\n\n\n      if (rarely()) { // Try with createNodeSet configuration\n        int nodeSetSize = cluster.getJettySolrRunners().size() / 2;\n        List<String> nodeStrs = new ArrayList<>(nodeSetSize);\n        Iterator<JettySolrRunner> iter = cluster.getJettySolrRunners().iterator();\n        for (int i = 0; i < nodeSetSize ; i++) {\n          nodeStrs.add(iter.next().getNodeName());\n        }\n        restore.setCreateNodeSet(String.join(\",\", nodeStrs));\n        restore.setCreateNodeSetShuffle(usually());\n        // we need to double maxShardsPerNode value since we reduced number of available nodes by half.\n        if (restore.getMaxShardsPerNode() != null) {\n          restore.setMaxShardsPerNode(restore.getMaxShardsPerNode() * 2);\n        } else {\n          restore.setMaxShardsPerNode(origShardToDocCount.size() * 2);\n        }\n      }\n\n      Properties props = new Properties();\n      props.setProperty(\"customKey\", \"customVal\");\n      restore.setProperties(props);\n\n      if (sameConfig==false) {\n        restore.setConfigName(\"customConfigName\");\n      }\n      if (random().nextBoolean()) {\n        assertEquals(0, restore.process(client).getStatus());\n      } else {\n        assertEquals(RequestStatusState.COMPLETED, restore.processAndWait(client, 30));//async\n      }\n      AbstractDistribZkTestBase.waitForRecoveriesToFinish(\n          restoreCollectionName, cluster.getSolrClient().getZkStateReader(), log.isDebugEnabled(), true, 30);\n    }\n\n    //Check the number of results are the same\n    DocCollection restoreCollection = client.getZkStateReader().getClusterState().getCollection(restoreCollectionName);\n    assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    //Re-index same docs (should be identical docs given same random seed) and test we have the same result.  Helps\n    //  test we reconstituted the hash ranges / doc router.\n    if (!(restoreCollection.getRouter() instanceof ImplicitDocRouter) && random().nextBoolean()) {\n      indexDocs(restoreCollectionName, false);\n      assertEquals(origShardToDocCount, getShardToDocCountMap(client, restoreCollection));\n    }\n\n    assertEquals(backupCollection.getReplicationFactor(), restoreCollection.getReplicationFactor());\n    assertEquals(backupCollection.getAutoAddReplicas(), restoreCollection.getAutoAddReplicas());\n    assertEquals(backupCollection.getActiveSlices().iterator().next().getReplicas().size(),\n        restoreCollection.getActiveSlices().iterator().next().getReplicas().size());\n    assertEquals(sameConfig ? \"conf1\" : \"customConfigName\",\n        cluster.getSolrClient().getZkStateReader().readConfigName(restoreCollectionName));\n\n    Map<String, Integer> numReplicasByNodeName = new HashMap<>();\n    restoreCollection.getReplicas().forEach(x -> {\n      numReplicasByNodeName.put(x.getNodeName(), numReplicasByNodeName.getOrDefault(x.getNodeName(), 0) + 1);\n    });\n    numReplicasByNodeName.forEach((k, v) -> {\n      assertTrue(\"Node \" + k + \" has \" + v + \" replicas. Expected num replicas : \" + restoreCollection.getMaxShardsPerNode() ,\n          v <= restoreCollection.getMaxShardsPerNode());\n    });\n\n    assertEquals(\"Different count of nrtReplicas. Backup collection state=\" + backupCollection + \"\\nRestore \" +\n        \"collection state=\" + restoreCollection, replFactor, restoreCollection.getNumNrtReplicas().intValue());\n    assertEquals(\"Different count of pullReplicas. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numPullReplicas, restoreCollection.getNumPullReplicas().intValue());\n    assertEquals(\"Different count of TlogReplica. Backup collection state=\" + backupCollection + \"\\nRestore\" +\n        \" collection state=\" + restoreCollection, numTlogReplicas, restoreCollection.getNumTlogReplicas().intValue());\n\n    assertEquals(\"Restore collection should use stateFormat=2\", 2, restoreCollection.getStateFormat());\n\n    //SOLR-12605: Add more docs after restore is complete to see if they are getting added fine\n    //explicitly querying the leaders. If we use CloudSolrClient there is no guarantee that we'll hit a nrtReplica\n    {\n      Map<String, Integer> restoredCollectionPerShardCount =  getShardToDocCountMap(client, restoreCollection);\n      long restoredCollectionDocCount = restoredCollectionPerShardCount.values().stream().mapToInt(Number::intValue).sum();\n      int numberNewDocsIndexed = indexDocs(restoreCollectionName, true);\n      Map<String, Integer> restoredCollectionPerShardCountAfterIndexing = getShardToDocCountMap(client, restoreCollection);\n      int restoredCollectionFinalDocCount = restoredCollectionPerShardCountAfterIndexing.values().stream().mapToInt(Number::intValue).sum();\n\n      log.info(\"Original doc count in restored collection:\" + restoredCollectionDocCount + \", number of newly added documents to the restored collection: \" + numberNewDocsIndexed + \", after indexing: \" + restoredCollectionFinalDocCount);\n      assertEquals((restoredCollectionDocCount + numberNewDocsIndexed), restoredCollectionFinalDocCount);\n    }\n\n    // assert added core properties:\n    // DWS: did via manual inspection.\n    // TODO Find the applicable core.properties on the file system but how?\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"076d58da25128e8a4c511abf07c5d86c4ebddcbf":["b94236357aaa22b76c10629851fe4e376e0cea82"],"b94236357aaa22b76c10629851fe4e376e0cea82":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","6146c07c0dee1ae1e42926167acd127fed5ef59d"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":["076d58da25128e8a4c511abf07c5d86c4ebddcbf","742168028ecb4838c124d27f836df9637be2f769"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"6146c07c0dee1ae1e42926167acd127fed5ef59d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"742168028ecb4838c124d27f836df9637be2f769":["076d58da25128e8a4c511abf07c5d86c4ebddcbf"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["742168028ecb4838c124d27f836df9637be2f769"],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":["076d58da25128e8a4c511abf07c5d86c4ebddcbf","742168028ecb4838c124d27f836df9637be2f769"]},"commit2Childs":{"076d58da25128e8a4c511abf07c5d86c4ebddcbf":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","742168028ecb4838c124d27f836df9637be2f769","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"b94236357aaa22b76c10629851fe4e376e0cea82":["076d58da25128e8a4c511abf07c5d86c4ebddcbf"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b94236357aaa22b76c10629851fe4e376e0cea82","6146c07c0dee1ae1e42926167acd127fed5ef59d"],"6146c07c0dee1ae1e42926167acd127fed5ef59d":["b94236357aaa22b76c10629851fe4e376e0cea82"],"742168028ecb4838c124d27f836df9637be2f769":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","cd5edd1f2b162a5cfa08efd17851a07373a96817","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":[]},"heads":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","cd5edd1f2b162a5cfa08efd17851a07373a96817","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}