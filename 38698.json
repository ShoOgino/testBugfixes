{"path":"lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter#split(Version,IndexReader,Directory[],boolean).mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter#split(Version,IndexReader,Directory[],boolean).mjava","pathOld":"lucene/contrib/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter#split(Version,IndexReader,Directory[],boolean).mjava","sourceNew":"  /**\n   * Split source index into multiple parts.\n   * @param in source index, can have deletions, can have\n   * multiple segments (or multiple readers).\n   * @param outputs list of directories where the output parts will be stored.\n   * @param seq if true, then the source index will be split into equal\n   * increasing ranges of document id-s. If false, source document id-s will be\n   * assigned in a deterministic round-robin fashion to one of the output splits.\n   * @throws IOException\n   */\n  public void split(Version version, IndexReader in, Directory[] outputs, boolean seq) throws IOException {\n    if (outputs == null || outputs.length < 2) {\n      throw new IOException(\"Invalid number of outputs.\");\n    }\n    if (in == null || in.numDocs() < 2) {\n      throw new IOException(\"Not enough documents for splitting\");\n    }\n    int numParts = outputs.length;\n    // wrap a potentially read-only input\n    // this way we don't have to preserve original deletions because neither\n    // deleteDocument(int) or undeleteAll() is applied to the wrapped input index.\n    FakeDeleteIndexReader input = new FakeDeleteIndexReader(in);\n    int maxDoc = input.maxDoc();\n    int partLen = maxDoc / numParts;\n    for (int i = 0; i < numParts; i++) {\n      input.undeleteAll();\n      if (seq) { // sequential range\n        int lo = partLen * i;\n        int hi = lo + partLen;\n        // below range\n        for (int j = 0; j < lo; j++) {\n          input.deleteDocument(j);\n        }\n        // above range - last part collects all id-s that remained due to\n        // integer rounding errors\n        if (i < numParts - 1) {\n          for (int j = hi; j < maxDoc; j++) {\n            input.deleteDocument(j);\n          }\n        }\n      } else {\n        // round-robin\n        for (int j = 0; j < maxDoc; j++) {\n          if ((j + numParts - i) % numParts != 0) {\n            input.deleteDocument(j);\n          }\n        }\n      }\n      IndexWriter w = new IndexWriter(outputs[i], new IndexWriterConfig(\n          version,\n          null)\n          .setOpenMode(OpenMode.CREATE));\n      System.err.println(\"Writing part \" + (i + 1) + \" ...\");\n      // pass the subreaders directly, as our wrapper's numDocs/hasDeletetions are not up-to-date\n      w.addIndexes(input.getSequentialSubReaders());\n      w.close();\n    }\n    System.err.println(\"Done.\");\n  }\n\n","sourceOld":"  /**\n   * Split source index into multiple parts.\n   * @param in source index, can have deletions, can have\n   * multiple segments (or multiple readers).\n   * @param outputs list of directories where the output parts will be stored.\n   * @param seq if true, then the source index will be split into equal\n   * increasing ranges of document id-s. If false, source document id-s will be\n   * assigned in a deterministic round-robin fashion to one of the output splits.\n   * @throws IOException\n   */\n  public void split(Version version, IndexReader in, Directory[] outputs, boolean seq) throws IOException {\n    if (outputs == null || outputs.length < 2) {\n      throw new IOException(\"Invalid number of outputs.\");\n    }\n    if (in == null || in.numDocs() < 2) {\n      throw new IOException(\"Not enough documents for splitting\");\n    }\n    int numParts = outputs.length;\n    // wrap a potentially read-only input\n    // this way we don't have to preserve original deletions because neither\n    // deleteDocument(int) or undeleteAll() is applied to the wrapped input index.\n    FakeDeleteIndexReader input = new FakeDeleteIndexReader(in);\n    int maxDoc = input.maxDoc();\n    int partLen = maxDoc / numParts;\n    for (int i = 0; i < numParts; i++) {\n      input.undeleteAll();\n      if (seq) { // sequential range\n        int lo = partLen * i;\n        int hi = lo + partLen;\n        // below range\n        for (int j = 0; j < lo; j++) {\n          input.deleteDocument(j);\n        }\n        // above range - last part collects all id-s that remained due to\n        // integer rounding errors\n        if (i < numParts - 1) {\n          for (int j = hi; j < maxDoc; j++) {\n            input.deleteDocument(j);\n          }\n        }\n      } else {\n        // round-robin\n        for (int j = 0; j < maxDoc; j++) {\n          if ((j + numParts - i) % numParts != 0) {\n            input.deleteDocument(j);\n          }\n        }\n      }\n      IndexWriter w = new IndexWriter(outputs[i], new IndexWriterConfig(\n          version,\n          null)\n          .setOpenMode(OpenMode.CREATE));\n      System.err.println(\"Writing part \" + (i + 1) + \" ...\");\n      // pass the subreaders directly, as our wrapper's numDocs/hasDeletetions are not up-to-date\n      w.addIndexes(input.getSequentialSubReaders());\n      w.close();\n    }\n    System.err.println(\"Done.\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4c5ae929ce8aa0c4856f0d6bfd4c196bc2d3eb9c","date":1340090669,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter#split(Version,IndexReader,Directory[],boolean).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter#split(Version,IndexReader,Directory[],boolean).mjava","sourceNew":"  /**\n   * Split source index into multiple parts.\n   * @param in source index, can have deletions, can have\n   * multiple segments (or multiple readers).\n   * @param outputs list of directories where the output parts will be stored.\n   * @param seq if true, then the source index will be split into equal\n   * increasing ranges of document id-s. If false, source document id-s will be\n   * assigned in a deterministic round-robin fashion to one of the output splits.\n   * @throws IOException\n   */\n  public void split(Version version, IndexReader in, Directory[] outputs, boolean seq) throws IOException {\n    if (outputs == null || outputs.length < 2) {\n      throw new IOException(\"Invalid number of outputs.\");\n    }\n    if (in == null || in.numDocs() < 2) {\n      throw new IOException(\"Not enough documents for splitting\");\n    }\n    int numParts = outputs.length;\n    // wrap a potentially read-only input\n    // this way we don't have to preserve original deletions because neither\n    // deleteDocument(int) or undeleteAll() is applied to the wrapped input index.\n    FakeDeleteIndexReader input = new FakeDeleteIndexReader(in);\n    int maxDoc = input.maxDoc();\n    int partLen = maxDoc / numParts;\n    for (int i = 0; i < numParts; i++) {\n      input.undeleteAll();\n      if (seq) { // sequential range\n        int lo = partLen * i;\n        int hi = lo + partLen;\n        // below range\n        for (int j = 0; j < lo; j++) {\n          input.deleteDocument(j);\n        }\n        // above range - last part collects all id-s that remained due to\n        // integer rounding errors\n        if (i < numParts - 1) {\n          for (int j = hi; j < maxDoc; j++) {\n            input.deleteDocument(j);\n          }\n        }\n      } else {\n        // round-robin\n        for (int j = 0; j < maxDoc; j++) {\n          if ((j + numParts - i) % numParts != 0) {\n            input.deleteDocument(j);\n          }\n        }\n      }\n      IndexWriter w = new IndexWriter(outputs[i], new IndexWriterConfig(\n          version,\n          null)\n          .setOpenMode(OpenMode.CREATE));\n      System.err.println(\"Writing part \" + (i + 1) + \" ...\");\n      // pass the subreaders directly, as our wrapper's numDocs/hasDeletetions are not up-to-date\n      final List<? extends FakeDeleteAtomicIndexReader> sr = input.getSequentialSubReaders();\n      w.addIndexes(sr.toArray(new IndexReader[sr.size()])); // TODO: maybe take List<IR> here?\n      w.close();\n    }\n    System.err.println(\"Done.\");\n  }\n\n","sourceOld":"  /**\n   * Split source index into multiple parts.\n   * @param in source index, can have deletions, can have\n   * multiple segments (or multiple readers).\n   * @param outputs list of directories where the output parts will be stored.\n   * @param seq if true, then the source index will be split into equal\n   * increasing ranges of document id-s. If false, source document id-s will be\n   * assigned in a deterministic round-robin fashion to one of the output splits.\n   * @throws IOException\n   */\n  public void split(Version version, IndexReader in, Directory[] outputs, boolean seq) throws IOException {\n    if (outputs == null || outputs.length < 2) {\n      throw new IOException(\"Invalid number of outputs.\");\n    }\n    if (in == null || in.numDocs() < 2) {\n      throw new IOException(\"Not enough documents for splitting\");\n    }\n    int numParts = outputs.length;\n    // wrap a potentially read-only input\n    // this way we don't have to preserve original deletions because neither\n    // deleteDocument(int) or undeleteAll() is applied to the wrapped input index.\n    FakeDeleteIndexReader input = new FakeDeleteIndexReader(in);\n    int maxDoc = input.maxDoc();\n    int partLen = maxDoc / numParts;\n    for (int i = 0; i < numParts; i++) {\n      input.undeleteAll();\n      if (seq) { // sequential range\n        int lo = partLen * i;\n        int hi = lo + partLen;\n        // below range\n        for (int j = 0; j < lo; j++) {\n          input.deleteDocument(j);\n        }\n        // above range - last part collects all id-s that remained due to\n        // integer rounding errors\n        if (i < numParts - 1) {\n          for (int j = hi; j < maxDoc; j++) {\n            input.deleteDocument(j);\n          }\n        }\n      } else {\n        // round-robin\n        for (int j = 0; j < maxDoc; j++) {\n          if ((j + numParts - i) % numParts != 0) {\n            input.deleteDocument(j);\n          }\n        }\n      }\n      IndexWriter w = new IndexWriter(outputs[i], new IndexWriterConfig(\n          version,\n          null)\n          .setOpenMode(OpenMode.CREATE));\n      System.err.println(\"Writing part \" + (i + 1) + \" ...\");\n      // pass the subreaders directly, as our wrapper's numDocs/hasDeletetions are not up-to-date\n      w.addIndexes(input.getSequentialSubReaders());\n      w.close();\n    }\n    System.err.println(\"Done.\");\n  }\n\n","bugFix":null,"bugIntro":["0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c7bd1fdddb8e84c1857d1a55c32ced51f0ed2069","date":1348430063,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter#split(Version,IndexReader,Directory[],boolean).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter#split(Version,IndexReader,Directory[],boolean).mjava","sourceNew":"  /**\n   * Split source index into multiple parts.\n   * @param in source index, can have deletions, can have\n   * multiple segments (or multiple readers).\n   * @param outputs list of directories where the output parts will be stored.\n   * @param seq if true, then the source index will be split into equal\n   * increasing ranges of document id-s. If false, source document id-s will be\n   * assigned in a deterministic round-robin fashion to one of the output splits.\n   * @throws IOException If there is a low-level I/O error\n   */\n  public void split(Version version, IndexReader in, Directory[] outputs, boolean seq) throws IOException {\n    if (outputs == null || outputs.length < 2) {\n      throw new IOException(\"Invalid number of outputs.\");\n    }\n    if (in == null || in.numDocs() < 2) {\n      throw new IOException(\"Not enough documents for splitting\");\n    }\n    int numParts = outputs.length;\n    // wrap a potentially read-only input\n    // this way we don't have to preserve original deletions because neither\n    // deleteDocument(int) or undeleteAll() is applied to the wrapped input index.\n    FakeDeleteIndexReader input = new FakeDeleteIndexReader(in);\n    int maxDoc = input.maxDoc();\n    int partLen = maxDoc / numParts;\n    for (int i = 0; i < numParts; i++) {\n      input.undeleteAll();\n      if (seq) { // sequential range\n        int lo = partLen * i;\n        int hi = lo + partLen;\n        // below range\n        for (int j = 0; j < lo; j++) {\n          input.deleteDocument(j);\n        }\n        // above range - last part collects all id-s that remained due to\n        // integer rounding errors\n        if (i < numParts - 1) {\n          for (int j = hi; j < maxDoc; j++) {\n            input.deleteDocument(j);\n          }\n        }\n      } else {\n        // round-robin\n        for (int j = 0; j < maxDoc; j++) {\n          if ((j + numParts - i) % numParts != 0) {\n            input.deleteDocument(j);\n          }\n        }\n      }\n      IndexWriter w = new IndexWriter(outputs[i], new IndexWriterConfig(\n          version,\n          null)\n          .setOpenMode(OpenMode.CREATE));\n      System.err.println(\"Writing part \" + (i + 1) + \" ...\");\n      // pass the subreaders directly, as our wrapper's numDocs/hasDeletetions are not up-to-date\n      final List<? extends FakeDeleteAtomicIndexReader> sr = input.getSequentialSubReaders();\n      w.addIndexes(sr.toArray(new IndexReader[sr.size()])); // TODO: maybe take List<IR> here?\n      w.close();\n    }\n    System.err.println(\"Done.\");\n  }\n\n","sourceOld":"  /**\n   * Split source index into multiple parts.\n   * @param in source index, can have deletions, can have\n   * multiple segments (or multiple readers).\n   * @param outputs list of directories where the output parts will be stored.\n   * @param seq if true, then the source index will be split into equal\n   * increasing ranges of document id-s. If false, source document id-s will be\n   * assigned in a deterministic round-robin fashion to one of the output splits.\n   * @throws IOException\n   */\n  public void split(Version version, IndexReader in, Directory[] outputs, boolean seq) throws IOException {\n    if (outputs == null || outputs.length < 2) {\n      throw new IOException(\"Invalid number of outputs.\");\n    }\n    if (in == null || in.numDocs() < 2) {\n      throw new IOException(\"Not enough documents for splitting\");\n    }\n    int numParts = outputs.length;\n    // wrap a potentially read-only input\n    // this way we don't have to preserve original deletions because neither\n    // deleteDocument(int) or undeleteAll() is applied to the wrapped input index.\n    FakeDeleteIndexReader input = new FakeDeleteIndexReader(in);\n    int maxDoc = input.maxDoc();\n    int partLen = maxDoc / numParts;\n    for (int i = 0; i < numParts; i++) {\n      input.undeleteAll();\n      if (seq) { // sequential range\n        int lo = partLen * i;\n        int hi = lo + partLen;\n        // below range\n        for (int j = 0; j < lo; j++) {\n          input.deleteDocument(j);\n        }\n        // above range - last part collects all id-s that remained due to\n        // integer rounding errors\n        if (i < numParts - 1) {\n          for (int j = hi; j < maxDoc; j++) {\n            input.deleteDocument(j);\n          }\n        }\n      } else {\n        // round-robin\n        for (int j = 0; j < maxDoc; j++) {\n          if ((j + numParts - i) % numParts != 0) {\n            input.deleteDocument(j);\n          }\n        }\n      }\n      IndexWriter w = new IndexWriter(outputs[i], new IndexWriterConfig(\n          version,\n          null)\n          .setOpenMode(OpenMode.CREATE));\n      System.err.println(\"Writing part \" + (i + 1) + \" ...\");\n      // pass the subreaders directly, as our wrapper's numDocs/hasDeletetions are not up-to-date\n      final List<? extends FakeDeleteAtomicIndexReader> sr = input.getSequentialSubReaders();\n      w.addIndexes(sr.toArray(new IndexReader[sr.size()])); // TODO: maybe take List<IR> here?\n      w.close();\n    }\n    System.err.println(\"Done.\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter#split(Version,IndexReader,Directory[],boolean).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter#split(Version,IndexReader,Directory[],boolean).mjava","sourceNew":"  /**\n   * Split source index into multiple parts.\n   * @param in source index, can have deletions, can have\n   * multiple segments (or multiple readers).\n   * @param outputs list of directories where the output parts will be stored.\n   * @param seq if true, then the source index will be split into equal\n   * increasing ranges of document id-s. If false, source document id-s will be\n   * assigned in a deterministic round-robin fashion to one of the output splits.\n   * @throws IOException If there is a low-level I/O error\n   */\n  public void split(Version version, IndexReader in, Directory[] outputs, boolean seq) throws IOException {\n    if (outputs == null || outputs.length < 2) {\n      throw new IOException(\"Invalid number of outputs.\");\n    }\n    if (in == null || in.numDocs() < 2) {\n      throw new IOException(\"Not enough documents for splitting\");\n    }\n    int numParts = outputs.length;\n    // wrap a potentially read-only input\n    // this way we don't have to preserve original deletions because neither\n    // deleteDocument(int) or undeleteAll() is applied to the wrapped input index.\n    FakeDeleteIndexReader input = new FakeDeleteIndexReader(in);\n    int maxDoc = input.maxDoc();\n    int partLen = maxDoc / numParts;\n    for (int i = 0; i < numParts; i++) {\n      input.undeleteAll();\n      if (seq) { // sequential range\n        int lo = partLen * i;\n        int hi = lo + partLen;\n        // below range\n        for (int j = 0; j < lo; j++) {\n          input.deleteDocument(j);\n        }\n        // above range - last part collects all id-s that remained due to\n        // integer rounding errors\n        if (i < numParts - 1) {\n          for (int j = hi; j < maxDoc; j++) {\n            input.deleteDocument(j);\n          }\n        }\n      } else {\n        // round-robin\n        for (int j = 0; j < maxDoc; j++) {\n          if ((j + numParts - i) % numParts != 0) {\n            input.deleteDocument(j);\n          }\n        }\n      }\n      IndexWriter w = new IndexWriter(outputs[i], new IndexWriterConfig(\n          version,\n          null)\n          .setOpenMode(OpenMode.CREATE));\n      System.err.println(\"Writing part \" + (i + 1) + \" ...\");\n      // pass the subreaders directly, as our wrapper's numDocs/hasDeletetions are not up-to-date\n      final List<? extends FakeDeleteAtomicIndexReader> sr = input.getSequentialSubReaders();\n      w.addIndexes(sr.toArray(new IndexReader[sr.size()])); // TODO: maybe take List<IR> here?\n      w.shutdown();\n    }\n    System.err.println(\"Done.\");\n  }\n\n","sourceOld":"  /**\n   * Split source index into multiple parts.\n   * @param in source index, can have deletions, can have\n   * multiple segments (or multiple readers).\n   * @param outputs list of directories where the output parts will be stored.\n   * @param seq if true, then the source index will be split into equal\n   * increasing ranges of document id-s. If false, source document id-s will be\n   * assigned in a deterministic round-robin fashion to one of the output splits.\n   * @throws IOException If there is a low-level I/O error\n   */\n  public void split(Version version, IndexReader in, Directory[] outputs, boolean seq) throws IOException {\n    if (outputs == null || outputs.length < 2) {\n      throw new IOException(\"Invalid number of outputs.\");\n    }\n    if (in == null || in.numDocs() < 2) {\n      throw new IOException(\"Not enough documents for splitting\");\n    }\n    int numParts = outputs.length;\n    // wrap a potentially read-only input\n    // this way we don't have to preserve original deletions because neither\n    // deleteDocument(int) or undeleteAll() is applied to the wrapped input index.\n    FakeDeleteIndexReader input = new FakeDeleteIndexReader(in);\n    int maxDoc = input.maxDoc();\n    int partLen = maxDoc / numParts;\n    for (int i = 0; i < numParts; i++) {\n      input.undeleteAll();\n      if (seq) { // sequential range\n        int lo = partLen * i;\n        int hi = lo + partLen;\n        // below range\n        for (int j = 0; j < lo; j++) {\n          input.deleteDocument(j);\n        }\n        // above range - last part collects all id-s that remained due to\n        // integer rounding errors\n        if (i < numParts - 1) {\n          for (int j = hi; j < maxDoc; j++) {\n            input.deleteDocument(j);\n          }\n        }\n      } else {\n        // round-robin\n        for (int j = 0; j < maxDoc; j++) {\n          if ((j + numParts - i) % numParts != 0) {\n            input.deleteDocument(j);\n          }\n        }\n      }\n      IndexWriter w = new IndexWriter(outputs[i], new IndexWriterConfig(\n          version,\n          null)\n          .setOpenMode(OpenMode.CREATE));\n      System.err.println(\"Writing part \" + (i + 1) + \" ...\");\n      // pass the subreaders directly, as our wrapper's numDocs/hasDeletetions are not up-to-date\n      final List<? extends FakeDeleteAtomicIndexReader> sr = input.getSequentialSubReaders();\n      w.addIndexes(sr.toArray(new IndexReader[sr.size()])); // TODO: maybe take List<IR> here?\n      w.close();\n    }\n    System.err.println(\"Done.\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter#split(Version,IndexReader,Directory[],boolean).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter#split(Version,IndexReader,Directory[],boolean).mjava","sourceNew":"  /**\n   * Split source index into multiple parts.\n   * @param in source index, can have deletions, can have\n   * multiple segments (or multiple readers).\n   * @param outputs list of directories where the output parts will be stored.\n   * @param seq if true, then the source index will be split into equal\n   * increasing ranges of document id-s. If false, source document id-s will be\n   * assigned in a deterministic round-robin fashion to one of the output splits.\n   * @throws IOException If there is a low-level I/O error\n   */\n  public void split(Version version, IndexReader in, Directory[] outputs, boolean seq) throws IOException {\n    if (outputs == null || outputs.length < 2) {\n      throw new IOException(\"Invalid number of outputs.\");\n    }\n    if (in == null || in.numDocs() < 2) {\n      throw new IOException(\"Not enough documents for splitting\");\n    }\n    int numParts = outputs.length;\n    // wrap a potentially read-only input\n    // this way we don't have to preserve original deletions because neither\n    // deleteDocument(int) or undeleteAll() is applied to the wrapped input index.\n    FakeDeleteIndexReader input = new FakeDeleteIndexReader(in);\n    int maxDoc = input.maxDoc();\n    int partLen = maxDoc / numParts;\n    for (int i = 0; i < numParts; i++) {\n      input.undeleteAll();\n      if (seq) { // sequential range\n        int lo = partLen * i;\n        int hi = lo + partLen;\n        // below range\n        for (int j = 0; j < lo; j++) {\n          input.deleteDocument(j);\n        }\n        // above range - last part collects all id-s that remained due to\n        // integer rounding errors\n        if (i < numParts - 1) {\n          for (int j = hi; j < maxDoc; j++) {\n            input.deleteDocument(j);\n          }\n        }\n      } else {\n        // round-robin\n        for (int j = 0; j < maxDoc; j++) {\n          if ((j + numParts - i) % numParts != 0) {\n            input.deleteDocument(j);\n          }\n        }\n      }\n      IndexWriter w = new IndexWriter(outputs[i], new IndexWriterConfig(null)\n          .setOpenMode(OpenMode.CREATE));\n      System.err.println(\"Writing part \" + (i + 1) + \" ...\");\n      // pass the subreaders directly, as our wrapper's numDocs/hasDeletetions are not up-to-date\n      final List<? extends FakeDeleteAtomicIndexReader> sr = input.getSequentialSubReaders();\n      w.addIndexes(sr.toArray(new IndexReader[sr.size()])); // TODO: maybe take List<IR> here?\n      w.close();\n    }\n    System.err.println(\"Done.\");\n  }\n\n","sourceOld":"  /**\n   * Split source index into multiple parts.\n   * @param in source index, can have deletions, can have\n   * multiple segments (or multiple readers).\n   * @param outputs list of directories where the output parts will be stored.\n   * @param seq if true, then the source index will be split into equal\n   * increasing ranges of document id-s. If false, source document id-s will be\n   * assigned in a deterministic round-robin fashion to one of the output splits.\n   * @throws IOException If there is a low-level I/O error\n   */\n  public void split(Version version, IndexReader in, Directory[] outputs, boolean seq) throws IOException {\n    if (outputs == null || outputs.length < 2) {\n      throw new IOException(\"Invalid number of outputs.\");\n    }\n    if (in == null || in.numDocs() < 2) {\n      throw new IOException(\"Not enough documents for splitting\");\n    }\n    int numParts = outputs.length;\n    // wrap a potentially read-only input\n    // this way we don't have to preserve original deletions because neither\n    // deleteDocument(int) or undeleteAll() is applied to the wrapped input index.\n    FakeDeleteIndexReader input = new FakeDeleteIndexReader(in);\n    int maxDoc = input.maxDoc();\n    int partLen = maxDoc / numParts;\n    for (int i = 0; i < numParts; i++) {\n      input.undeleteAll();\n      if (seq) { // sequential range\n        int lo = partLen * i;\n        int hi = lo + partLen;\n        // below range\n        for (int j = 0; j < lo; j++) {\n          input.deleteDocument(j);\n        }\n        // above range - last part collects all id-s that remained due to\n        // integer rounding errors\n        if (i < numParts - 1) {\n          for (int j = hi; j < maxDoc; j++) {\n            input.deleteDocument(j);\n          }\n        }\n      } else {\n        // round-robin\n        for (int j = 0; j < maxDoc; j++) {\n          if ((j + numParts - i) % numParts != 0) {\n            input.deleteDocument(j);\n          }\n        }\n      }\n      IndexWriter w = new IndexWriter(outputs[i], new IndexWriterConfig(\n          version,\n          null)\n          .setOpenMode(OpenMode.CREATE));\n      System.err.println(\"Writing part \" + (i + 1) + \" ...\");\n      // pass the subreaders directly, as our wrapper's numDocs/hasDeletetions are not up-to-date\n      final List<? extends FakeDeleteAtomicIndexReader> sr = input.getSequentialSubReaders();\n      w.addIndexes(sr.toArray(new IndexReader[sr.size()])); // TODO: maybe take List<IR> here?\n      w.shutdown();\n    }\n    System.err.println(\"Done.\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter#split(Version,IndexReader,Directory[],boolean).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter#split(Version,IndexReader,Directory[],boolean).mjava","sourceNew":"  /**\n   * Split source index into multiple parts.\n   * @param in source index, can have deletions, can have\n   * multiple segments (or multiple readers).\n   * @param outputs list of directories where the output parts will be stored.\n   * @param seq if true, then the source index will be split into equal\n   * increasing ranges of document id-s. If false, source document id-s will be\n   * assigned in a deterministic round-robin fashion to one of the output splits.\n   * @throws IOException If there is a low-level I/O error\n   */\n  public void split(Version version, IndexReader in, Directory[] outputs, boolean seq) throws IOException {\n    if (outputs == null || outputs.length < 2) {\n      throw new IOException(\"Invalid number of outputs.\");\n    }\n    if (in == null || in.numDocs() < 2) {\n      throw new IOException(\"Not enough documents for splitting\");\n    }\n    int numParts = outputs.length;\n    // wrap a potentially read-only input\n    // this way we don't have to preserve original deletions because neither\n    // deleteDocument(int) or undeleteAll() is applied to the wrapped input index.\n    FakeDeleteIndexReader input = new FakeDeleteIndexReader(in);\n    int maxDoc = input.maxDoc();\n    int partLen = maxDoc / numParts;\n    for (int i = 0; i < numParts; i++) {\n      input.undeleteAll();\n      if (seq) { // sequential range\n        int lo = partLen * i;\n        int hi = lo + partLen;\n        // below range\n        for (int j = 0; j < lo; j++) {\n          input.deleteDocument(j);\n        }\n        // above range - last part collects all id-s that remained due to\n        // integer rounding errors\n        if (i < numParts - 1) {\n          for (int j = hi; j < maxDoc; j++) {\n            input.deleteDocument(j);\n          }\n        }\n      } else {\n        // round-robin\n        for (int j = 0; j < maxDoc; j++) {\n          if ((j + numParts - i) % numParts != 0) {\n            input.deleteDocument(j);\n          }\n        }\n      }\n      IndexWriter w = new IndexWriter(outputs[i], new IndexWriterConfig(null)\n          .setOpenMode(OpenMode.CREATE));\n      System.err.println(\"Writing part \" + (i + 1) + \" ...\");\n      // pass the subreaders directly, as our wrapper's numDocs/hasDeletetions are not up-to-date\n      final List<? extends FakeDeleteLeafIndexReader> sr = input.getSequentialSubReaders();\n      w.addIndexes(sr.toArray(new IndexReader[sr.size()])); // TODO: maybe take List<IR> here?\n      w.close();\n    }\n    System.err.println(\"Done.\");\n  }\n\n","sourceOld":"  /**\n   * Split source index into multiple parts.\n   * @param in source index, can have deletions, can have\n   * multiple segments (or multiple readers).\n   * @param outputs list of directories where the output parts will be stored.\n   * @param seq if true, then the source index will be split into equal\n   * increasing ranges of document id-s. If false, source document id-s will be\n   * assigned in a deterministic round-robin fashion to one of the output splits.\n   * @throws IOException If there is a low-level I/O error\n   */\n  public void split(Version version, IndexReader in, Directory[] outputs, boolean seq) throws IOException {\n    if (outputs == null || outputs.length < 2) {\n      throw new IOException(\"Invalid number of outputs.\");\n    }\n    if (in == null || in.numDocs() < 2) {\n      throw new IOException(\"Not enough documents for splitting\");\n    }\n    int numParts = outputs.length;\n    // wrap a potentially read-only input\n    // this way we don't have to preserve original deletions because neither\n    // deleteDocument(int) or undeleteAll() is applied to the wrapped input index.\n    FakeDeleteIndexReader input = new FakeDeleteIndexReader(in);\n    int maxDoc = input.maxDoc();\n    int partLen = maxDoc / numParts;\n    for (int i = 0; i < numParts; i++) {\n      input.undeleteAll();\n      if (seq) { // sequential range\n        int lo = partLen * i;\n        int hi = lo + partLen;\n        // below range\n        for (int j = 0; j < lo; j++) {\n          input.deleteDocument(j);\n        }\n        // above range - last part collects all id-s that remained due to\n        // integer rounding errors\n        if (i < numParts - 1) {\n          for (int j = hi; j < maxDoc; j++) {\n            input.deleteDocument(j);\n          }\n        }\n      } else {\n        // round-robin\n        for (int j = 0; j < maxDoc; j++) {\n          if ((j + numParts - i) % numParts != 0) {\n            input.deleteDocument(j);\n          }\n        }\n      }\n      IndexWriter w = new IndexWriter(outputs[i], new IndexWriterConfig(null)\n          .setOpenMode(OpenMode.CREATE));\n      System.err.println(\"Writing part \" + (i + 1) + \" ...\");\n      // pass the subreaders directly, as our wrapper's numDocs/hasDeletetions are not up-to-date\n      final List<? extends FakeDeleteAtomicIndexReader> sr = input.getSequentialSubReaders();\n      w.addIndexes(sr.toArray(new IndexReader[sr.size()])); // TODO: maybe take List<IR> here?\n      w.close();\n    }\n    System.err.println(\"Done.\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa","date":1420599177,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter#split(Version,IndexReader,Directory[],boolean).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter#split(Version,IndexReader,Directory[],boolean).mjava","sourceNew":"  /**\n   * Split source index into multiple parts.\n   * @param in source index, can have deletions, can have\n   * multiple segments (or multiple readers).\n   * @param outputs list of directories where the output parts will be stored.\n   * @param seq if true, then the source index will be split into equal\n   * increasing ranges of document id-s. If false, source document id-s will be\n   * assigned in a deterministic round-robin fashion to one of the output splits.\n   * @throws IOException If there is a low-level I/O error\n   */\n  public void split(Version version, IndexReader in, Directory[] outputs, boolean seq) throws IOException {\n    if (outputs == null || outputs.length < 2) {\n      throw new IOException(\"Invalid number of outputs.\");\n    }\n    if (in == null || in.numDocs() < 2) {\n      throw new IOException(\"Not enough documents for splitting\");\n    }\n    int numParts = outputs.length;\n    // wrap a potentially read-only input\n    // this way we don't have to preserve original deletions because neither\n    // deleteDocument(int) or undeleteAll() is applied to the wrapped input index.\n    FakeDeleteIndexReader input = new FakeDeleteIndexReader(in);\n    int maxDoc = input.maxDoc();\n    int partLen = maxDoc / numParts;\n    for (int i = 0; i < numParts; i++) {\n      input.undeleteAll();\n      if (seq) { // sequential range\n        int lo = partLen * i;\n        int hi = lo + partLen;\n        // below range\n        for (int j = 0; j < lo; j++) {\n          input.deleteDocument(j);\n        }\n        // above range - last part collects all id-s that remained due to\n        // integer rounding errors\n        if (i < numParts - 1) {\n          for (int j = hi; j < maxDoc; j++) {\n            input.deleteDocument(j);\n          }\n        }\n      } else {\n        // round-robin\n        for (int j = 0; j < maxDoc; j++) {\n          if ((j + numParts - i) % numParts != 0) {\n            input.deleteDocument(j);\n          }\n        }\n      }\n      IndexWriter w = new IndexWriter(outputs[i], new IndexWriterConfig(null)\n          .setOpenMode(OpenMode.CREATE));\n      System.err.println(\"Writing part \" + (i + 1) + \" ...\");\n      // pass the subreaders directly, as our wrapper's numDocs/hasDeletetions are not up-to-date\n      final List<? extends FakeDeleteLeafIndexReader> sr = input.getSequentialSubReaders();\n      w.addIndexes(sr.toArray(new LeafReader[sr.size()])); // TODO: maybe take List<IR> here?\n      w.close();\n    }\n    System.err.println(\"Done.\");\n  }\n\n","sourceOld":"  /**\n   * Split source index into multiple parts.\n   * @param in source index, can have deletions, can have\n   * multiple segments (or multiple readers).\n   * @param outputs list of directories where the output parts will be stored.\n   * @param seq if true, then the source index will be split into equal\n   * increasing ranges of document id-s. If false, source document id-s will be\n   * assigned in a deterministic round-robin fashion to one of the output splits.\n   * @throws IOException If there is a low-level I/O error\n   */\n  public void split(Version version, IndexReader in, Directory[] outputs, boolean seq) throws IOException {\n    if (outputs == null || outputs.length < 2) {\n      throw new IOException(\"Invalid number of outputs.\");\n    }\n    if (in == null || in.numDocs() < 2) {\n      throw new IOException(\"Not enough documents for splitting\");\n    }\n    int numParts = outputs.length;\n    // wrap a potentially read-only input\n    // this way we don't have to preserve original deletions because neither\n    // deleteDocument(int) or undeleteAll() is applied to the wrapped input index.\n    FakeDeleteIndexReader input = new FakeDeleteIndexReader(in);\n    int maxDoc = input.maxDoc();\n    int partLen = maxDoc / numParts;\n    for (int i = 0; i < numParts; i++) {\n      input.undeleteAll();\n      if (seq) { // sequential range\n        int lo = partLen * i;\n        int hi = lo + partLen;\n        // below range\n        for (int j = 0; j < lo; j++) {\n          input.deleteDocument(j);\n        }\n        // above range - last part collects all id-s that remained due to\n        // integer rounding errors\n        if (i < numParts - 1) {\n          for (int j = hi; j < maxDoc; j++) {\n            input.deleteDocument(j);\n          }\n        }\n      } else {\n        // round-robin\n        for (int j = 0; j < maxDoc; j++) {\n          if ((j + numParts - i) % numParts != 0) {\n            input.deleteDocument(j);\n          }\n        }\n      }\n      IndexWriter w = new IndexWriter(outputs[i], new IndexWriterConfig(null)\n          .setOpenMode(OpenMode.CREATE));\n      System.err.println(\"Writing part \" + (i + 1) + \" ...\");\n      // pass the subreaders directly, as our wrapper's numDocs/hasDeletetions are not up-to-date\n      final List<? extends FakeDeleteLeafIndexReader> sr = input.getSequentialSubReaders();\n      w.addIndexes(sr.toArray(new IndexReader[sr.size()])); // TODO: maybe take List<IR> here?\n      w.close();\n    }\n    System.err.println(\"Done.\");\n  }\n\n","bugFix":["4c5ae929ce8aa0c4856f0d6bfd4c196bc2d3eb9c"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"505bff044e47a553f461b6f4484d1d08faf4ac85","date":1420728783,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter#split(IndexReader,Directory[],boolean).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter#split(Version,IndexReader,Directory[],boolean).mjava","sourceNew":"  /**\n   * Split source index into multiple parts.\n   * @param in source index, can have deletions, can have\n   * multiple segments (or multiple readers).\n   * @param outputs list of directories where the output parts will be stored.\n   * @param seq if true, then the source index will be split into equal\n   * increasing ranges of document id-s. If false, source document id-s will be\n   * assigned in a deterministic round-robin fashion to one of the output splits.\n   * @throws IOException If there is a low-level I/O error\n   */\n  public void split(IndexReader in, Directory[] outputs, boolean seq) throws IOException {\n    if (outputs == null || outputs.length < 2) {\n      throw new IOException(\"Invalid number of outputs.\");\n    }\n    if (in == null || in.numDocs() < 2) {\n      throw new IOException(\"Not enough documents for splitting\");\n    }\n    int numParts = outputs.length;\n    // wrap a potentially read-only input\n    // this way we don't have to preserve original deletions because neither\n    // deleteDocument(int) or undeleteAll() is applied to the wrapped input index.\n    FakeDeleteIndexReader input = new FakeDeleteIndexReader(in);\n    int maxDoc = input.maxDoc();\n    int partLen = maxDoc / numParts;\n    for (int i = 0; i < numParts; i++) {\n      input.undeleteAll();\n      if (seq) { // sequential range\n        int lo = partLen * i;\n        int hi = lo + partLen;\n        // below range\n        for (int j = 0; j < lo; j++) {\n          input.deleteDocument(j);\n        }\n        // above range - last part collects all id-s that remained due to\n        // integer rounding errors\n        if (i < numParts - 1) {\n          for (int j = hi; j < maxDoc; j++) {\n            input.deleteDocument(j);\n          }\n        }\n      } else {\n        // round-robin\n        for (int j = 0; j < maxDoc; j++) {\n          if ((j + numParts - i) % numParts != 0) {\n            input.deleteDocument(j);\n          }\n        }\n      }\n      IndexWriter w = new IndexWriter(outputs[i], new IndexWriterConfig(null)\n          .setOpenMode(OpenMode.CREATE));\n      System.err.println(\"Writing part \" + (i + 1) + \" ...\");\n      // pass the subreaders directly, as our wrapper's numDocs/hasDeletetions are not up-to-date\n      final List<? extends FakeDeleteLeafIndexReader> sr = input.getSequentialSubReaders();\n      w.addIndexes(sr.toArray(new CodecReader[sr.size()])); // TODO: maybe take List<IR> here?\n      w.close();\n    }\n    System.err.println(\"Done.\");\n  }\n\n","sourceOld":"  /**\n   * Split source index into multiple parts.\n   * @param in source index, can have deletions, can have\n   * multiple segments (or multiple readers).\n   * @param outputs list of directories where the output parts will be stored.\n   * @param seq if true, then the source index will be split into equal\n   * increasing ranges of document id-s. If false, source document id-s will be\n   * assigned in a deterministic round-robin fashion to one of the output splits.\n   * @throws IOException If there is a low-level I/O error\n   */\n  public void split(Version version, IndexReader in, Directory[] outputs, boolean seq) throws IOException {\n    if (outputs == null || outputs.length < 2) {\n      throw new IOException(\"Invalid number of outputs.\");\n    }\n    if (in == null || in.numDocs() < 2) {\n      throw new IOException(\"Not enough documents for splitting\");\n    }\n    int numParts = outputs.length;\n    // wrap a potentially read-only input\n    // this way we don't have to preserve original deletions because neither\n    // deleteDocument(int) or undeleteAll() is applied to the wrapped input index.\n    FakeDeleteIndexReader input = new FakeDeleteIndexReader(in);\n    int maxDoc = input.maxDoc();\n    int partLen = maxDoc / numParts;\n    for (int i = 0; i < numParts; i++) {\n      input.undeleteAll();\n      if (seq) { // sequential range\n        int lo = partLen * i;\n        int hi = lo + partLen;\n        // below range\n        for (int j = 0; j < lo; j++) {\n          input.deleteDocument(j);\n        }\n        // above range - last part collects all id-s that remained due to\n        // integer rounding errors\n        if (i < numParts - 1) {\n          for (int j = hi; j < maxDoc; j++) {\n            input.deleteDocument(j);\n          }\n        }\n      } else {\n        // round-robin\n        for (int j = 0; j < maxDoc; j++) {\n          if ((j + numParts - i) % numParts != 0) {\n            input.deleteDocument(j);\n          }\n        }\n      }\n      IndexWriter w = new IndexWriter(outputs[i], new IndexWriterConfig(null)\n          .setOpenMode(OpenMode.CREATE));\n      System.err.println(\"Writing part \" + (i + 1) + \" ...\");\n      // pass the subreaders directly, as our wrapper's numDocs/hasDeletetions are not up-to-date\n      final List<? extends FakeDeleteLeafIndexReader> sr = input.getSequentialSubReaders();\n      w.addIndexes(sr.toArray(new LeafReader[sr.size()])); // TODO: maybe take List<IR> here?\n      w.close();\n    }\n    System.err.println(\"Done.\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"c7bd1fdddb8e84c1857d1a55c32ced51f0ed2069":["4c5ae929ce8aa0c4856f0d6bfd4c196bc2d3eb9c"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4c5ae929ce8aa0c4856f0d6bfd4c196bc2d3eb9c":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["c7bd1fdddb8e84c1857d1a55c32ced51f0ed2069"],"505bff044e47a553f461b6f4484d1d08faf4ac85":["0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["505bff044e47a553f461b6f4484d1d08faf4ac85"]},"commit2Childs":{"c7bd1fdddb8e84c1857d1a55c32ced51f0ed2069":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["4c5ae929ce8aa0c4856f0d6bfd4c196bc2d3eb9c"],"4c5ae929ce8aa0c4856f0d6bfd4c196bc2d3eb9c":["c7bd1fdddb8e84c1857d1a55c32ced51f0ed2069"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa":["505bff044e47a553f461b6f4484d1d08faf4ac85"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa"],"505bff044e47a553f461b6f4484d1d08faf4ac85":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}