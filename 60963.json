{"path":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,PostingsEnum[],int).mjava","commits":[{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":1,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,PostingsEnum[],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, PostingsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      PostingsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.postings(null, null, PostingsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final PostingsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = terms[off.id];\n        if (term == null) {\n          // multitermquery match, pull from payload\n          term = off.dp.getPayload();\n          assert term != null;\n        }\n        current.addMatch(start, end, term);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset || end > contentLength) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = terms[off.id];\n        if (term == null) {\n          // multitermquery match, pull from payload\n          term = off.dp.getPayload();\n          assert term != null;\n        }\n        current.addMatch(start, end, term);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset || end > contentLength) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e73063b92d958076ef4ae8beb5f493e8ccdcecb4","date":1424177215,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,PostingsEnum[],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,PostingsEnum[],int).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, PostingsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      PostingsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.postings(null, null, PostingsEnum.OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final PostingsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = terms[off.id];\n        if (term == null) {\n          // multitermquery match, pull from payload\n          term = off.dp.getPayload();\n          assert term != null;\n        }\n        current.addMatch(start, end, term);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset || end > contentLength) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, PostingsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      PostingsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.postings(null, null, PostingsEnum.FLAG_OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final PostingsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = terms[off.id];\n        if (term == null) {\n          // multitermquery match, pull from payload\n          term = off.dp.getPayload();\n          assert term != null;\n        }\n        current.addMatch(start, end, term);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset || end > contentLength) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cb5311f0bff57ce15a23909f4cfb953773630534","date":1424827033,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,PostingsEnum[],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,PostingsEnum[],int).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, PostingsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      PostingsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        // nocommit: check\n        de = postings[i] = termsEnum.postings(null, null, PostingsEnum.OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final PostingsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = terms[off.id];\n        if (term == null) {\n          // multitermquery match, pull from payload\n          term = off.dp.getPayload();\n          assert term != null;\n        }\n        current.addMatch(start, end, term);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset || end > contentLength) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, PostingsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      PostingsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.postings(null, null, PostingsEnum.OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final PostingsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = terms[off.id];\n        if (term == null) {\n          // multitermquery match, pull from payload\n          term = off.dp.getPayload();\n          assert term != null;\n        }\n        current.addMatch(start, end, term);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset || end > contentLength) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1530e3091abcc262e32b7d5b7ba8675aa3fff9cc","date":1427826153,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,PostingsEnum[],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,PostingsEnum[],int).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, PostingsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      PostingsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.postings(null, null, PostingsEnum.OFFSETS);\n        assert de != null;\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final PostingsEnum dp = off.dp;\n      int start = dp.startOffset();\n      assert start >= 0;\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = terms[off.id];\n        if (term == null) {\n          // multitermquery match, pull from payload\n          term = off.dp.getPayload();\n          assert term != null;\n        }\n        current.addMatch(start, end, term);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset || end > contentLength) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, PostingsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      PostingsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        // nocommit: check\n        de = postings[i] = termsEnum.postings(null, null, PostingsEnum.OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final PostingsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = terms[off.id];\n        if (term == null) {\n          // multitermquery match, pull from payload\n          term = off.dp.getPayload();\n          assert term != null;\n        }\n        current.addMatch(start, end, term);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset || end > contentLength) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6b4e3cd382d0d075a0f1725649c084bb6510c483","date":1428096423,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,PostingsEnum[],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,PostingsEnum[],int).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, PostingsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      PostingsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.postings(null, null, PostingsEnum.OFFSETS);\n        assert de != null;\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final PostingsEnum dp = off.dp;\n      int start = dp.startOffset();\n      assert start >= 0;\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = terms[off.id];\n        if (term == null) {\n          // multitermquery match, pull from payload\n          term = off.dp.getPayload();\n          assert term != null;\n        }\n        current.addMatch(start, end, term);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset || end > contentLength) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, PostingsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      PostingsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.postings(null, null, PostingsEnum.OFFSETS);\n        if (de == null) {\n          // no positions available\n          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final PostingsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = terms[off.id];\n        if (term == null) {\n          // multitermquery match, pull from payload\n          term = off.dp.getPayload();\n          assert term != null;\n        }\n        current.addMatch(start, end, term);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset || end > contentLength) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,PostingsEnum[],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,PostingsEnum[],int).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, PostingsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      PostingsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.postings(null, PostingsEnum.OFFSETS);\n        assert de != null;\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final PostingsEnum dp = off.dp;\n      int start = dp.startOffset();\n      assert start >= 0;\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = terms[off.id];\n        if (term == null) {\n          // multitermquery match, pull from payload\n          term = off.dp.getPayload();\n          assert term != null;\n        }\n        current.addMatch(start, end, term);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset || end > contentLength) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, PostingsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      PostingsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.postings(null, null, PostingsEnum.OFFSETS);\n        assert de != null;\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final PostingsEnum dp = off.dp;\n      int start = dp.startOffset();\n      assert start >= 0;\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = terms[off.id];\n        if (term == null) {\n          // multitermquery match, pull from payload\n          term = off.dp.getPayload();\n          assert term != null;\n        }\n        current.addMatch(start, end, term);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset || end > contentLength) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ad0d09e969f4763b0df4230f8e3f74357872a4e4","date":1459188769,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,PostingsEnum[],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,PostingsEnum[],int).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, PostingsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer must not be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      PostingsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.postings(null, PostingsEnum.OFFSETS);\n        assert de != null;\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final PostingsEnum dp = off.dp;\n      int start = dp.startOffset();\n      assert start >= 0;\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = terms[off.id];\n        if (term == null) {\n          // multitermquery match, pull from payload\n          term = off.dp.getPayload();\n          assert term != null;\n        }\n        current.addMatch(start, end, term);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset || end > contentLength) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, PostingsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer cannot be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      PostingsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.postings(null, PostingsEnum.OFFSETS);\n        assert de != null;\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final PostingsEnum dp = off.dp;\n      int start = dp.startOffset();\n      assert start >= 0;\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = terms[off.id];\n        if (term == null) {\n          // multitermquery match, pull from payload\n          term = off.dp.getPayload();\n          assert term != null;\n        }\n        current.addMatch(start, end, term);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset || end > contentLength) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","bugFix":["51f5280f31484820499077f41fcdfe92d527d9dc"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"381618eac2691bb34ab9a3fca76ad55c6274517e","date":1495564791,"type":4,"author":"David Smiley","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,PostingsEnum[],int).mjava","sourceNew":null,"sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, PostingsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer must not be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      PostingsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.postings(null, PostingsEnum.OFFSETS);\n        assert de != null;\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final PostingsEnum dp = off.dp;\n      int start = dp.startOffset();\n      assert start >= 0;\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = terms[off.id];\n        if (term == null) {\n          // multitermquery match, pull from payload\n          term = off.dp.getPayload();\n          assert term != null;\n        }\n        current.addMatch(start, end, term);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset || end > contentLength) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e9017cf144952056066919f1ebc7897ff9bd71b1","date":1496757600,"type":4,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightDoc(String,BytesRef[],int,BreakIterator,int,TermsEnum,PostingsEnum[],int).mjava","sourceNew":null,"sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, PostingsEnum[] postings, int n) throws IOException {\n    PassageScorer scorer = getScorer(field);\n    if (scorer == null) {\n      throw new NullPointerException(\"PassageScorer must not be null\");\n    }\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      PostingsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i])) {\n          continue; // term not found\n        }\n        de = postings[i] = termsEnum.postings(null, PostingsEnum.OFFSETS);\n        assert de != null;\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (left.score < right.score) {\n          return -1;\n        } else if (left.score > right.score) {\n          return 1;\n        } else {\n          return left.startOffset - right.startOffset;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final PostingsEnum dp = off.dp;\n      int start = dp.startOffset();\n      assert start >= 0;\n      int end = dp.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid \n      // hits may exist (they are sorted by start). so we pretend like we never \n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      if (start >= current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          for (Passage p : passages) {\n            p.sort();\n          }\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = terms[off.id];\n        if (term == null) {\n          // multitermquery match, pull from payload\n          term = off.dp.getPayload();\n          assert term != null;\n        }\n        current.addMatch(start, end, term);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset || end > contentLength) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n\n    // Dead code but compiler disagrees:\n    assert false;\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"381618eac2691bb34ab9a3fca76ad55c6274517e":["ad0d09e969f4763b0df4230f8e3f74357872a4e4"],"cb5311f0bff57ce15a23909f4cfb953773630534":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"e9017cf144952056066919f1ebc7897ff9bd71b1":["ad0d09e969f4763b0df4230f8e3f74357872a4e4","381618eac2691bb34ab9a3fca76ad55c6274517e"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["6b4e3cd382d0d075a0f1725649c084bb6510c483"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["51f5280f31484820499077f41fcdfe92d527d9dc"],"1530e3091abcc262e32b7d5b7ba8675aa3fff9cc":["cb5311f0bff57ce15a23909f4cfb953773630534"],"6b4e3cd382d0d075a0f1725649c084bb6510c483":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4","1530e3091abcc262e32b7d5b7ba8675aa3fff9cc"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"ad0d09e969f4763b0df4230f8e3f74357872a4e4":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"51f5280f31484820499077f41fcdfe92d527d9dc":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["381618eac2691bb34ab9a3fca76ad55c6274517e"]},"commit2Childs":{"381618eac2691bb34ab9a3fca76ad55c6274517e":["e9017cf144952056066919f1ebc7897ff9bd71b1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cb5311f0bff57ce15a23909f4cfb953773630534":["1530e3091abcc262e32b7d5b7ba8675aa3fff9cc"],"e9017cf144952056066919f1ebc7897ff9bd71b1":[],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["cb5311f0bff57ce15a23909f4cfb953773630534","6b4e3cd382d0d075a0f1725649c084bb6510c483"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["ad0d09e969f4763b0df4230f8e3f74357872a4e4"],"1530e3091abcc262e32b7d5b7ba8675aa3fff9cc":["6b4e3cd382d0d075a0f1725649c084bb6510c483"],"6b4e3cd382d0d075a0f1725649c084bb6510c483":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["51f5280f31484820499077f41fcdfe92d527d9dc"],"ad0d09e969f4763b0df4230f8e3f74357872a4e4":["381618eac2691bb34ab9a3fca76ad55c6274517e","e9017cf144952056066919f1ebc7897ff9bd71b1"],"51f5280f31484820499077f41fcdfe92d527d9dc":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["e9017cf144952056066919f1ebc7897ff9bd71b1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}