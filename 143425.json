{"path":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","commits":[{"id":"2eceb3be6e29e3ad7ee08d5025a431c3812aa0d9","date":1392385887,"type":0,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap, int offsetGap) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, null, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["ca02c6cbb1ba28fd0bd36bd0f8a2ac84be5f0e71"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ca02c6cbb1ba28fd0bd36bd0f8a2ac84be5f0e71","date":1400675008,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap, int offsetGap) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, null, -1, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap, int offsetGap) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, null, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":["2eceb3be6e29e3ad7ee08d5025a431c3812aa0d9"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a656b32c3aa151037a8c52e9b134acc3cbf482bc","date":1400688195,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap, int offsetGap) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, null, -1, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap, int offsetGap) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, null, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7605579001505896d48b07160075a5c8b8e128e","date":1400758727,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap, int offsetGap) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, null, -1, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap, int offsetGap) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, null, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"761333d77c7f29123c00c93b107b743f32f012e6","date":1411986072,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap, int offsetGap) {\n    try {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, null, -1, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap, int offsetGap) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, null, -1, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d9a47902d6207303f5ed3e7aaca62ca33433af66","date":1412435312,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap, int offsetGap) {\n    try {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, null, -1, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap, int offsetGap) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, null, -1, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"05da2d758a6089e737cdfc230e57a51b472b94b6","date":1413392310,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap, int offsetGap) {\n    try {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, -1, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap, int offsetGap) {\n    try {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, null, -1, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c93b0dbaa6abe99bc8d1b476bcacc27b324b2b84","date":1413458798,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap, int offsetGap) {\n    try {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, -1, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap, int offsetGap) {\n    try {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, null, -1, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3184874f7f3aca850248483485b4995343066875","date":1413876758,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap, int offsetGap) {\n    try {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS, null, -1, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap, int offsetGap) {\n    try {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, -1, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a22eafe3f72a4c2945eaad9547e6c78816978f4","date":1413956657,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap, int offsetGap) {\n    try {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS, null, -1, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap, int offsetGap) {\n    try {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS , null, -1, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2bb2842e561df4e8e9ad89010605fc86ac265465","date":1414768208,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap, int offsetGap) {\n    try {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, fieldInfos.size(), false, false, false,\n                          this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS,\n                          DocValuesType.NO, -1, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap, int offsetGap) {\n    try {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, fieldInfos.size(), false, false, false, this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS, null, -1, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f95ce1375367b92d411a06175eab3915fe93c6bc","date":1414788502,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap, int offsetGap) {\n    try {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, fieldInfos.size(), false, false, false,\n                          this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS,\n                          DocValuesType.NONE, -1, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap, int offsetGap) {\n    try {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, fieldInfos.size(), false, false, false,\n                          this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS,\n                          DocValuesType.NO, -1, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f657d9837900f4519ca1cbd5e98d86d4bba4dab","date":1417790596,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap, int offsetGap) {\n    try {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      FieldInfo fieldInfo;\n      if ((info = fields.get(fieldName)) != null) {\n        fieldInfo = info.fieldInfo;\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        fieldInfo = new FieldInfo(fieldName, fields.size(), false, false, false,\n            this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS,\n            DocValuesType.NONE, -1, null);\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(fieldInfo, terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap, int offsetGap) {\n    try {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info = null;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      if ((info = fields.get(fieldName)) != null) {\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      if (!fieldInfos.containsKey(fieldName)) {\n        fieldInfos.put(fieldName, \n            new FieldInfo(fieldName, fieldInfos.size(), false, false, false,\n                          this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS,\n                          DocValuesType.NONE, -1, null));\n      }\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e88ae259732b6a9caf4c8f3a2e5a19c7b54ddbcb","date":1420550360,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param tokenStream\n   *            the token stream to retrieve tokens from. It's guaranteed to be closed no matter what.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream tokenStream, float boost, int positionIncrementGap,\n                       int offsetGap) {\n    try (TokenStream stream = tokenStream) {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n        throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n        throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      FieldInfo fieldInfo;\n      if ((info = fields.get(fieldName)) != null) {\n        fieldInfo = info.fieldInfo;\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        fieldInfo = new FieldInfo(fieldName, fields.size(), false, false, this.storePayloads,\n            this.storeOffsets\n                ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS,\n            DocValuesType.NONE, -1, null);\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      PayloadAttribute payloadAtt = storePayloads ? stream.addAttribute(PayloadAttribute.class) : null;\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        postingsWriter.writeInt(pos);\n        if (storeOffsets) {\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        if (storePayloads) {\n          final BytesRef payload = payloadAtt.getPayload();\n          int pIndex = payload == null ? -1 : payloadsBytesRefs.append(payload);\n          postingsWriter.writeInt(pIndex);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(fieldInfo, terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost, int positionIncrementGap, int offsetGap) {\n    try {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      FieldInfo fieldInfo;\n      if ((info = fields.get(fieldName)) != null) {\n        fieldInfo = info.fieldInfo;\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        fieldInfo = new FieldInfo(fieldName, fields.size(), false, false, false,\n            this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS,\n            DocValuesType.NONE, -1, null);\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        if (!storeOffsets) {\n          postingsWriter.writeInt(pos);\n        } else {\n          postingsWriter.writeInt(pos);\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(fieldInfo, terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n      }\n    } catch (Exception e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) {\n          stream.close();\n        }\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["03f665d0b9240c1ee79baeac03a20def3275d816","bb04834a792874aacf8d8b111a39603c23fbd777"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"79700663e164dece87bed4adfd3e28bab6cb1385","date":1425241849,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param tokenStream\n   *            the token stream to retrieve tokens from. It's guaranteed to be closed no matter what.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream tokenStream, float boost, int positionIncrementGap,\n                       int offsetGap) {\n    try (TokenStream stream = tokenStream) {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n        throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n        throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      FieldInfo fieldInfo;\n      if ((info = fields.get(fieldName)) != null) {\n        fieldInfo = info.fieldInfo;\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        fieldInfo = new FieldInfo(fieldName, fields.size(), false, false, this.storePayloads,\n            this.storeOffsets\n                ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS,\n            DocValuesType.NONE, -1, Collections.emptyMap());\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      PayloadAttribute payloadAtt = storePayloads ? stream.addAttribute(PayloadAttribute.class) : null;\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        postingsWriter.writeInt(pos);\n        if (storeOffsets) {\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        if (storePayloads) {\n          final BytesRef payload = payloadAtt.getPayload();\n          int pIndex = payload == null ? -1 : payloadsBytesRefs.append(payload);\n          postingsWriter.writeInt(pIndex);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(fieldInfo, terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param tokenStream\n   *            the token stream to retrieve tokens from. It's guaranteed to be closed no matter what.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream tokenStream, float boost, int positionIncrementGap,\n                       int offsetGap) {\n    try (TokenStream stream = tokenStream) {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n        throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n        throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      FieldInfo fieldInfo;\n      if ((info = fields.get(fieldName)) != null) {\n        fieldInfo = info.fieldInfo;\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        fieldInfo = new FieldInfo(fieldName, fields.size(), false, false, this.storePayloads,\n            this.storeOffsets\n                ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS,\n            DocValuesType.NONE, -1, null);\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      PayloadAttribute payloadAtt = storePayloads ? stream.addAttribute(PayloadAttribute.class) : null;\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        postingsWriter.writeInt(pos);\n        if (storeOffsets) {\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        if (storePayloads) {\n          final BytesRef payload = payloadAtt.getPayload();\n          int pIndex = payload == null ? -1 : payloadsBytesRefs.append(payload);\n          postingsWriter.writeInt(pIndex);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(fieldInfo, terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"299a2348fa24151d150182211b6208a38e5e3450","date":1425304608,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param tokenStream\n   *            the token stream to retrieve tokens from. It's guaranteed to be closed no matter what.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream tokenStream, float boost, int positionIncrementGap,\n                       int offsetGap) {\n    try (TokenStream stream = tokenStream) {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n        throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n        throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      FieldInfo fieldInfo;\n      if ((info = fields.get(fieldName)) != null) {\n        fieldInfo = info.fieldInfo;\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        fieldInfo = new FieldInfo(fieldName, fields.size(), false, false, this.storePayloads,\n            this.storeOffsets\n                ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS,\n            DocValuesType.NONE, -1, Collections.emptyMap());\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      PayloadAttribute payloadAtt = storePayloads ? stream.addAttribute(PayloadAttribute.class) : null;\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        postingsWriter.writeInt(pos);\n        if (storeOffsets) {\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        if (storePayloads) {\n          final BytesRef payload = payloadAtt.getPayload();\n          int pIndex = payload == null ? -1 : payloadsBytesRefs.append(payload);\n          postingsWriter.writeInt(pIndex);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(fieldInfo, terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param tokenStream\n   *            the token stream to retrieve tokens from. It's guaranteed to be closed no matter what.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream tokenStream, float boost, int positionIncrementGap,\n                       int offsetGap) {\n    try (TokenStream stream = tokenStream) {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n        throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n        throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      FieldInfo fieldInfo;\n      if ((info = fields.get(fieldName)) != null) {\n        fieldInfo = info.fieldInfo;\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        fieldInfo = new FieldInfo(fieldName, fields.size(), false, false, this.storePayloads,\n            this.storeOffsets\n                ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS,\n            DocValuesType.NONE, -1, null);\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      PayloadAttribute payloadAtt = storePayloads ? stream.addAttribute(PayloadAttribute.class) : null;\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        postingsWriter.writeInt(pos);\n        if (storeOffsets) {\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        if (storePayloads) {\n          final BytesRef payload = payloadAtt.getPayload();\n          int pIndex = payload == null ? -1 : payloadsBytesRefs.append(payload);\n          postingsWriter.writeInt(pIndex);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(fieldInfo, terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param tokenStream\n   *            the token stream to retrieve tokens from. It's guaranteed to be closed no matter what.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream tokenStream, float boost, int positionIncrementGap,\n                       int offsetGap) {\n    try (TokenStream stream = tokenStream) {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n        throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n        throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      FieldInfo fieldInfo;\n      if ((info = fields.get(fieldName)) != null) {\n        fieldInfo = info.fieldInfo;\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        fieldInfo = new FieldInfo(fieldName, fields.size(), false, false, this.storePayloads,\n            this.storeOffsets\n                ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS,\n            DocValuesType.NONE, -1, Collections.emptyMap());\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      PayloadAttribute payloadAtt = storePayloads ? stream.addAttribute(PayloadAttribute.class) : null;\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        postingsWriter.writeInt(pos);\n        if (storeOffsets) {\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        if (storePayloads) {\n          final BytesRef payload = payloadAtt.getPayload();\n          int pIndex = payload == null ? -1 : payloadsBytesRefs.append(payload);\n          postingsWriter.writeInt(pIndex);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(fieldInfo, terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param tokenStream\n   *            the token stream to retrieve tokens from. It's guaranteed to be closed no matter what.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream tokenStream, float boost, int positionIncrementGap,\n                       int offsetGap) {\n    try (TokenStream stream = tokenStream) {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n        throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n        throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      FieldInfo fieldInfo;\n      if ((info = fields.get(fieldName)) != null) {\n        fieldInfo = info.fieldInfo;\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        fieldInfo = new FieldInfo(fieldName, fields.size(), false, false, this.storePayloads,\n            this.storeOffsets\n                ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS,\n            DocValuesType.NONE, -1, null);\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      PayloadAttribute payloadAtt = storePayloads ? stream.addAttribute(PayloadAttribute.class) : null;\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        postingsWriter.writeInt(pos);\n        if (storeOffsets) {\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        if (storePayloads) {\n          final BytesRef payload = payloadAtt.getPayload();\n          int pIndex = payload == null ? -1 : payloadsBytesRefs.append(payload);\n          postingsWriter.writeInt(pIndex);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(fieldInfo, terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"03f665d0b9240c1ee79baeac03a20def3275d816","date":1428405689,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param tokenStream\n   *            the token stream to retrieve tokens from. It's guaranteed to be closed no matter what.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream tokenStream, float boost, int positionIncrementGap,\n                       int offsetGap) {\n    try (TokenStream stream = tokenStream) {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n        throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n        throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      FieldInfo fieldInfo;\n      if ((info = fields.get(fieldName)) != null) {\n        fieldInfo = info.fieldInfo;\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        fieldInfo = new FieldInfo(fieldName, fields.size(), true, false, this.storePayloads,\n            this.storeOffsets\n                ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS,\n            DocValuesType.NONE, -1, Collections.emptyMap());\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      PayloadAttribute payloadAtt = storePayloads ? stream.addAttribute(PayloadAttribute.class) : null;\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        postingsWriter.writeInt(pos);\n        if (storeOffsets) {\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        if (storePayloads) {\n          final BytesRef payload = payloadAtt.getPayload();\n          int pIndex = payload == null ? -1 : payloadsBytesRefs.append(payload);\n          postingsWriter.writeInt(pIndex);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(fieldInfo, terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param tokenStream\n   *            the token stream to retrieve tokens from. It's guaranteed to be closed no matter what.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream tokenStream, float boost, int positionIncrementGap,\n                       int offsetGap) {\n    try (TokenStream stream = tokenStream) {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n        throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n        throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      FieldInfo fieldInfo;\n      if ((info = fields.get(fieldName)) != null) {\n        fieldInfo = info.fieldInfo;\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        fieldInfo = new FieldInfo(fieldName, fields.size(), false, false, this.storePayloads,\n            this.storeOffsets\n                ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS,\n            DocValuesType.NONE, -1, Collections.emptyMap());\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      PayloadAttribute payloadAtt = storePayloads ? stream.addAttribute(PayloadAttribute.class) : null;\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        postingsWriter.writeInt(pos);\n        if (storeOffsets) {\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        if (storePayloads) {\n          final BytesRef payload = payloadAtt.getPayload();\n          int pIndex = payload == null ? -1 : payloadsBytesRefs.append(payload);\n          postingsWriter.writeInt(pIndex);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(fieldInfo, terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n","bugFix":["e88ae259732b6a9caf4c8f3a2e5a19c7b54ddbcb"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bb04834a792874aacf8d8b111a39603c23fbd777","date":1428406678,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param tokenStream\n   *            the token stream to retrieve tokens from. It's guaranteed to be closed no matter what.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream tokenStream, float boost, int positionIncrementGap,\n                       int offsetGap) {\n    try (TokenStream stream = tokenStream) {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n        throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n        throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      FieldInfo fieldInfo;\n      if ((info = fields.get(fieldName)) != null) {\n        fieldInfo = info.fieldInfo;\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        fieldInfo = new FieldInfo(fieldName, fields.size(), true, false, this.storePayloads,\n            this.storeOffsets\n                ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS,\n            DocValuesType.NONE, -1, Collections.emptyMap());\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      PayloadAttribute payloadAtt = storePayloads ? stream.addAttribute(PayloadAttribute.class) : null;\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        postingsWriter.writeInt(pos);\n        if (storeOffsets) {\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        if (storePayloads) {\n          final BytesRef payload = payloadAtt.getPayload();\n          final int pIndex;\n          if (payload == null || payload.length == 0) {\n            pIndex = -1;\n          } else {\n            pIndex = payloadsBytesRefs.append(payload);\n          }\n          postingsWriter.writeInt(pIndex);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(fieldInfo, terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param tokenStream\n   *            the token stream to retrieve tokens from. It's guaranteed to be closed no matter what.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream tokenStream, float boost, int positionIncrementGap,\n                       int offsetGap) {\n    try (TokenStream stream = tokenStream) {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n        throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n        throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      FieldInfo fieldInfo;\n      if ((info = fields.get(fieldName)) != null) {\n        fieldInfo = info.fieldInfo;\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        fieldInfo = new FieldInfo(fieldName, fields.size(), true, false, this.storePayloads,\n            this.storeOffsets\n                ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS,\n            DocValuesType.NONE, -1, Collections.emptyMap());\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      PayloadAttribute payloadAtt = storePayloads ? stream.addAttribute(PayloadAttribute.class) : null;\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        postingsWriter.writeInt(pos);\n        if (storeOffsets) {\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        if (storePayloads) {\n          final BytesRef payload = payloadAtt.getPayload();\n          int pIndex = payload == null ? -1 : payloadsBytesRefs.append(payload);\n          postingsWriter.writeInt(pIndex);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(fieldInfo, terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n","bugFix":["e88ae259732b6a9caf4c8f3a2e5a19c7b54ddbcb"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"804b857d1066ab5185b3b9101bde41b0b71426ec","date":1435846169,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param tokenStream\n   *            the token stream to retrieve tokens from. It's guaranteed to be closed no matter what.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream tokenStream, float boost, int positionIncrementGap,\n                       int offsetGap) {\n    try (TokenStream stream = tokenStream) {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n        throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n        throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      FieldInfo fieldInfo;\n      if ((info = fields.get(fieldName)) != null) {\n        fieldInfo = info.fieldInfo;\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        fieldInfo = new FieldInfo(fieldName, fields.size(), true, false, this.storePayloads,\n            this.storeOffsets\n                ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS,\n            DocValuesType.NONE, -1, Collections.emptyMap());\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      PayloadAttribute payloadAtt = storePayloads ? stream.addAttribute(PayloadAttribute.class) : null;\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(termAtt.getBytesRef());\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        postingsWriter.writeInt(pos);\n        if (storeOffsets) {\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        if (storePayloads) {\n          final BytesRef payload = payloadAtt.getPayload();\n          final int pIndex;\n          if (payload == null || payload.length == 0) {\n            pIndex = -1;\n          } else {\n            pIndex = payloadsBytesRefs.append(payload);\n          }\n          postingsWriter.writeInt(pIndex);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(fieldInfo, terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param tokenStream\n   *            the token stream to retrieve tokens from. It's guaranteed to be closed no matter what.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream tokenStream, float boost, int positionIncrementGap,\n                       int offsetGap) {\n    try (TokenStream stream = tokenStream) {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n        throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n        throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      FieldInfo fieldInfo;\n      if ((info = fields.get(fieldName)) != null) {\n        fieldInfo = info.fieldInfo;\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        fieldInfo = new FieldInfo(fieldName, fields.size(), true, false, this.storePayloads,\n            this.storeOffsets\n                ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS,\n            DocValuesType.NONE, -1, Collections.emptyMap());\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      PayloadAttribute payloadAtt = storePayloads ? stream.addAttribute(PayloadAttribute.class) : null;\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(ref);\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        postingsWriter.writeInt(pos);\n        if (storeOffsets) {\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        if (storePayloads) {\n          final BytesRef payload = payloadAtt.getPayload();\n          final int pIndex;\n          if (payload == null || payload.length == 0) {\n            pIndex = -1;\n          } else {\n            pIndex = payloadsBytesRefs.append(payload);\n          }\n          postingsWriter.writeInt(pIndex);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(fieldInfo, terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ca792c26af46bd6c4a08d81117c60440cf6a7e3d","date":1445938295,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param tokenStream\n   *            the token stream to retrieve tokens from. It's guaranteed to be closed no matter what.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream tokenStream, float boost, int positionIncrementGap,\n                       int offsetGap) {\n    try (TokenStream stream = tokenStream) {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n        throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n        throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      FieldInfo fieldInfo;\n      if ((info = fields.get(fieldName)) != null) {\n        fieldInfo = info.fieldInfo;\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        fieldInfo = new FieldInfo(fieldName, fields.size(), true, false, this.storePayloads,\n                                  this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS,\n                                  DocValuesType.NONE, -1, Collections.emptyMap(), 0, 0);\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      PayloadAttribute payloadAtt = storePayloads ? stream.addAttribute(PayloadAttribute.class) : null;\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(termAtt.getBytesRef());\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        postingsWriter.writeInt(pos);\n        if (storeOffsets) {\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        if (storePayloads) {\n          final BytesRef payload = payloadAtt.getPayload();\n          final int pIndex;\n          if (payload == null || payload.length == 0) {\n            pIndex = -1;\n          } else {\n            pIndex = payloadsBytesRefs.append(payload);\n          }\n          postingsWriter.writeInt(pIndex);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(fieldInfo, terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param tokenStream\n   *            the token stream to retrieve tokens from. It's guaranteed to be closed no matter what.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream tokenStream, float boost, int positionIncrementGap,\n                       int offsetGap) {\n    try (TokenStream stream = tokenStream) {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n        throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n        throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      FieldInfo fieldInfo;\n      if ((info = fields.get(fieldName)) != null) {\n        fieldInfo = info.fieldInfo;\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        fieldInfo = new FieldInfo(fieldName, fields.size(), true, false, this.storePayloads,\n            this.storeOffsets\n                ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS,\n            DocValuesType.NONE, -1, Collections.emptyMap());\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      PayloadAttribute payloadAtt = storePayloads ? stream.addAttribute(PayloadAttribute.class) : null;\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(termAtt.getBytesRef());\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        postingsWriter.writeInt(pos);\n        if (storeOffsets) {\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        if (storePayloads) {\n          final BytesRef payload = payloadAtt.getPayload();\n          final int pIndex;\n          if (payload == null || payload.length == 0) {\n            pIndex = -1;\n          } else {\n            pIndex = payloadsBytesRefs.append(payload);\n          }\n          postingsWriter.writeInt(pIndex);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(fieldInfo, terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"253a79e1af11467dd01315b1919025d288aa0ccb","date":1458032260,"type":3,"author":"Martijn van Groningen","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param tokenStream\n   *            the token stream to retrieve tokens from. It's guaranteed to be closed no matter what.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream tokenStream, float boost, int positionIncrementGap, int offsetGap) {\n    addField(fieldName, tokenStream, boost, positionIncrementGap, offsetGap, DocValuesType.NONE, null);\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param tokenStream\n   *            the token stream to retrieve tokens from. It's guaranteed to be closed no matter what.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream tokenStream, float boost, int positionIncrementGap,\n                       int offsetGap) {\n    try (TokenStream stream = tokenStream) {\n      if (frozen)\n        throw new IllegalArgumentException(\"Cannot call addField() when MemoryIndex is frozen\");\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n        throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n        throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      final BytesRefHash terms;\n      final SliceByteStartArray sliceArray;\n      Info info;\n      long sumTotalTermFreq = 0;\n      int offset = 0;\n      FieldInfo fieldInfo;\n      if ((info = fields.get(fieldName)) != null) {\n        fieldInfo = info.fieldInfo;\n        numTokens = info.numTokens;\n        numOverlapTokens = info.numOverlapTokens;\n        pos = info.lastPosition + positionIncrementGap;\n        offset = info.lastOffset + offsetGap;\n        terms = info.terms;\n        boost *= info.boost;\n        sliceArray = info.sliceArray;\n        sumTotalTermFreq = info.sumTotalTermFreq;\n      } else {\n        fieldInfo = new FieldInfo(fieldName, fields.size(), true, false, this.storePayloads,\n                                  this.storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS,\n                                  DocValuesType.NONE, -1, Collections.emptyMap(), 0, 0);\n        sliceArray = new SliceByteStartArray(BytesRefHash.DEFAULT_CAPACITY);\n        terms = new BytesRefHash(byteBlockPool, BytesRefHash.DEFAULT_CAPACITY, sliceArray);\n      }\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      PayloadAttribute payloadAtt = storePayloads ? stream.addAttribute(PayloadAttribute.class) : null;\n      stream.reset();\n      \n      while (stream.incrementToken()) {\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        int ord = terms.add(termAtt.getBytesRef());\n        if (ord < 0) {\n          ord = (-ord) - 1;\n          postingsWriter.reset(sliceArray.end[ord]);\n        } else {\n          sliceArray.start[ord] = postingsWriter.startNewSlice();\n        }\n        sliceArray.freq[ord]++;\n        sumTotalTermFreq++;\n        postingsWriter.writeInt(pos);\n        if (storeOffsets) {\n          postingsWriter.writeInt(offsetAtt.startOffset() + offset);\n          postingsWriter.writeInt(offsetAtt.endOffset() + offset);\n        }\n        if (storePayloads) {\n          final BytesRef payload = payloadAtt.getPayload();\n          final int pIndex;\n          if (payload == null || payload.length == 0) {\n            pIndex = -1;\n          } else {\n            pIndex = payloadsBytesRefs.append(payload);\n          }\n          postingsWriter.writeInt(pIndex);\n        }\n        sliceArray.end[ord] = postingsWriter.getCurrentOffset();\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        fields.put(fieldName, new Info(fieldInfo, terms, sliceArray, numTokens, numOverlapTokens, boost, pos, offsetAtt.endOffset() + offset, sumTotalTermFreq));\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n","bugFix":null,"bugIntro":["74d5d70ec9df9b59ea6d0dbdb5f7af1991ba7275"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"74d5d70ec9df9b59ea6d0dbdb5f7af1991ba7275","date":1458043999,"type":3,"author":"Martijn van Groningen","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param tokenStream\n   *            the token stream to retrieve tokens from. It's guaranteed to be closed no matter what.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream tokenStream, float boost, int positionIncrementGap, int offsetGap) {\n    addField(fieldName, tokenStream, boost, positionIncrementGap, offsetGap, DocValuesType.NONE, null, 0, 0, null);\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param tokenStream\n   *            the token stream to retrieve tokens from. It's guaranteed to be closed no matter what.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream tokenStream, float boost, int positionIncrementGap, int offsetGap) {\n    addField(fieldName, tokenStream, boost, positionIncrementGap, offsetGap, DocValuesType.NONE, null);\n  }\n\n","bugFix":["253a79e1af11467dd01315b1919025d288aa0ccb"],"bugIntro":["89b68d01c34172936f1aa2a8b9abf0e1bc68415f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"89b68d01c34172936f1aa2a8b9abf0e1bc68415f","date":1486637198,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param tokenStream\n   *            the token stream to retrieve tokens from. It's guaranteed to be closed no matter what.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream tokenStream, float boost, int positionIncrementGap, int offsetGap) {\n    Info info = getInfo(fieldName, defaultFieldType);\n    storeTerms(info, tokenStream, boost, positionIncrementGap, offsetGap);\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param tokenStream\n   *            the token stream to retrieve tokens from. It's guaranteed to be closed no matter what.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream tokenStream, float boost, int positionIncrementGap, int offsetGap) {\n    addField(fieldName, tokenStream, boost, positionIncrementGap, offsetGap, DocValuesType.NONE, null, 0, 0, null);\n  }\n\n","bugFix":["74d5d70ec9df9b59ea6d0dbdb5f7af1991ba7275"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"acd9883560fd89e6448b2b447302fe543040cd4f","date":1488478696,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,int,int).mjava","pathOld":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float,int,int).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param tokenStream\n   *            the token stream to retrieve tokens from. It's guaranteed to be closed no matter what.\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   */\n  public void addField(String fieldName, TokenStream tokenStream, int positionIncrementGap, int offsetGap) {\n    Info info = getInfo(fieldName, defaultFieldType);\n    storeTerms(info, tokenStream, positionIncrementGap, offsetGap);\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene <code>KeywordTokenizer</code> or similar utilities.\n   * \n   *\n   * @param fieldName\n   *            a name to be associated with the text\n   * @param tokenStream\n   *            the token stream to retrieve tokens from. It's guaranteed to be closed no matter what.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @param positionIncrementGap\n   *            the position increment gap if fields with the same name are added more than once\n   * @param offsetGap\n   *            the offset gap if fields with the same name are added more than once\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream tokenStream, float boost, int positionIncrementGap, int offsetGap) {\n    Info info = getInfo(fieldName, defaultFieldType);\n    storeTerms(info, tokenStream, boost, positionIncrementGap, offsetGap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"89b68d01c34172936f1aa2a8b9abf0e1bc68415f":["74d5d70ec9df9b59ea6d0dbdb5f7af1991ba7275"],"2bb2842e561df4e8e9ad89010605fc86ac265465":["3184874f7f3aca850248483485b4995343066875"],"b7605579001505896d48b07160075a5c8b8e128e":["2eceb3be6e29e3ad7ee08d5025a431c3812aa0d9","ca02c6cbb1ba28fd0bd36bd0f8a2ac84be5f0e71"],"2eceb3be6e29e3ad7ee08d5025a431c3812aa0d9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["e88ae259732b6a9caf4c8f3a2e5a19c7b54ddbcb","79700663e164dece87bed4adfd3e28bab6cb1385"],"bb04834a792874aacf8d8b111a39603c23fbd777":["03f665d0b9240c1ee79baeac03a20def3275d816"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"79700663e164dece87bed4adfd3e28bab6cb1385":["e88ae259732b6a9caf4c8f3a2e5a19c7b54ddbcb"],"299a2348fa24151d150182211b6208a38e5e3450":["e88ae259732b6a9caf4c8f3a2e5a19c7b54ddbcb","79700663e164dece87bed4adfd3e28bab6cb1385"],"761333d77c7f29123c00c93b107b743f32f012e6":["ca02c6cbb1ba28fd0bd36bd0f8a2ac84be5f0e71"],"804b857d1066ab5185b3b9101bde41b0b71426ec":["bb04834a792874aacf8d8b111a39603c23fbd777"],"05da2d758a6089e737cdfc230e57a51b472b94b6":["761333d77c7f29123c00c93b107b743f32f012e6"],"c93b0dbaa6abe99bc8d1b476bcacc27b324b2b84":["d9a47902d6207303f5ed3e7aaca62ca33433af66","05da2d758a6089e737cdfc230e57a51b472b94b6"],"03f665d0b9240c1ee79baeac03a20def3275d816":["79700663e164dece87bed4adfd3e28bab6cb1385"],"e88ae259732b6a9caf4c8f3a2e5a19c7b54ddbcb":["0f657d9837900f4519ca1cbd5e98d86d4bba4dab"],"d9a47902d6207303f5ed3e7aaca62ca33433af66":["ca02c6cbb1ba28fd0bd36bd0f8a2ac84be5f0e71","761333d77c7f29123c00c93b107b743f32f012e6"],"0f657d9837900f4519ca1cbd5e98d86d4bba4dab":["f95ce1375367b92d411a06175eab3915fe93c6bc"],"f95ce1375367b92d411a06175eab3915fe93c6bc":["2bb2842e561df4e8e9ad89010605fc86ac265465"],"3184874f7f3aca850248483485b4995343066875":["05da2d758a6089e737cdfc230e57a51b472b94b6"],"74d5d70ec9df9b59ea6d0dbdb5f7af1991ba7275":["253a79e1af11467dd01315b1919025d288aa0ccb"],"0a22eafe3f72a4c2945eaad9547e6c78816978f4":["c93b0dbaa6abe99bc8d1b476bcacc27b324b2b84","3184874f7f3aca850248483485b4995343066875"],"253a79e1af11467dd01315b1919025d288aa0ccb":["ca792c26af46bd6c4a08d81117c60440cf6a7e3d"],"ca792c26af46bd6c4a08d81117c60440cf6a7e3d":["804b857d1066ab5185b3b9101bde41b0b71426ec"],"a656b32c3aa151037a8c52e9b134acc3cbf482bc":["2eceb3be6e29e3ad7ee08d5025a431c3812aa0d9","ca02c6cbb1ba28fd0bd36bd0f8a2ac84be5f0e71"],"acd9883560fd89e6448b2b447302fe543040cd4f":["89b68d01c34172936f1aa2a8b9abf0e1bc68415f"],"ca02c6cbb1ba28fd0bd36bd0f8a2ac84be5f0e71":["2eceb3be6e29e3ad7ee08d5025a431c3812aa0d9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["acd9883560fd89e6448b2b447302fe543040cd4f"]},"commit2Childs":{"89b68d01c34172936f1aa2a8b9abf0e1bc68415f":["acd9883560fd89e6448b2b447302fe543040cd4f"],"2bb2842e561df4e8e9ad89010605fc86ac265465":["f95ce1375367b92d411a06175eab3915fe93c6bc"],"b7605579001505896d48b07160075a5c8b8e128e":[],"2eceb3be6e29e3ad7ee08d5025a431c3812aa0d9":["b7605579001505896d48b07160075a5c8b8e128e","a656b32c3aa151037a8c52e9b134acc3cbf482bc","ca02c6cbb1ba28fd0bd36bd0f8a2ac84be5f0e71"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"bb04834a792874aacf8d8b111a39603c23fbd777":["804b857d1066ab5185b3b9101bde41b0b71426ec"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["2eceb3be6e29e3ad7ee08d5025a431c3812aa0d9"],"79700663e164dece87bed4adfd3e28bab6cb1385":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","299a2348fa24151d150182211b6208a38e5e3450","03f665d0b9240c1ee79baeac03a20def3275d816"],"299a2348fa24151d150182211b6208a38e5e3450":[],"761333d77c7f29123c00c93b107b743f32f012e6":["05da2d758a6089e737cdfc230e57a51b472b94b6","d9a47902d6207303f5ed3e7aaca62ca33433af66"],"804b857d1066ab5185b3b9101bde41b0b71426ec":["ca792c26af46bd6c4a08d81117c60440cf6a7e3d"],"05da2d758a6089e737cdfc230e57a51b472b94b6":["c93b0dbaa6abe99bc8d1b476bcacc27b324b2b84","3184874f7f3aca850248483485b4995343066875"],"c93b0dbaa6abe99bc8d1b476bcacc27b324b2b84":["0a22eafe3f72a4c2945eaad9547e6c78816978f4"],"03f665d0b9240c1ee79baeac03a20def3275d816":["bb04834a792874aacf8d8b111a39603c23fbd777"],"e88ae259732b6a9caf4c8f3a2e5a19c7b54ddbcb":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","79700663e164dece87bed4adfd3e28bab6cb1385","299a2348fa24151d150182211b6208a38e5e3450"],"d9a47902d6207303f5ed3e7aaca62ca33433af66":["c93b0dbaa6abe99bc8d1b476bcacc27b324b2b84"],"3184874f7f3aca850248483485b4995343066875":["2bb2842e561df4e8e9ad89010605fc86ac265465","0a22eafe3f72a4c2945eaad9547e6c78816978f4"],"0f657d9837900f4519ca1cbd5e98d86d4bba4dab":["e88ae259732b6a9caf4c8f3a2e5a19c7b54ddbcb"],"f95ce1375367b92d411a06175eab3915fe93c6bc":["0f657d9837900f4519ca1cbd5e98d86d4bba4dab"],"74d5d70ec9df9b59ea6d0dbdb5f7af1991ba7275":["89b68d01c34172936f1aa2a8b9abf0e1bc68415f"],"0a22eafe3f72a4c2945eaad9547e6c78816978f4":[],"253a79e1af11467dd01315b1919025d288aa0ccb":["74d5d70ec9df9b59ea6d0dbdb5f7af1991ba7275"],"ca792c26af46bd6c4a08d81117c60440cf6a7e3d":["253a79e1af11467dd01315b1919025d288aa0ccb"],"a656b32c3aa151037a8c52e9b134acc3cbf482bc":[],"acd9883560fd89e6448b2b447302fe543040cd4f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ca02c6cbb1ba28fd0bd36bd0f8a2ac84be5f0e71":["b7605579001505896d48b07160075a5c8b8e128e","761333d77c7f29123c00c93b107b743f32f012e6","d9a47902d6207303f5ed3e7aaca62ca33433af66","a656b32c3aa151037a8c52e9b134acc3cbf482bc"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b7605579001505896d48b07160075a5c8b8e128e","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","299a2348fa24151d150182211b6208a38e5e3450","0a22eafe3f72a4c2945eaad9547e6c78816978f4","a656b32c3aa151037a8c52e9b134acc3cbf482bc","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}