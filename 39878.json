{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter#testTokenStream().mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter#testTokenStream().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter#testTokenStream().mjava","sourceNew":"  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(in)  h i j k ll cccc bbb aa\n  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(out) i i jj kkk llll cc b a\n  //\n  //    h, 0, 1 =>    i, 0, 1\n  //    i, 2, 3 =>    i, 2, 3\n  //    j, 4, 5 =>   jj, 4, 5\n  //    k, 6, 7 =>  kkk, 6, 7\n  //   ll, 8,10 => llll, 8,10\n  // cccc,11,15 =>   cc,11,15\n  //  bbb,16,19 =>    b,16,19\n  //   aa,20,22 =>    a,20,22\n  //\n  public void testTokenStream() throws Exception {\n    String testString = \"h i j k ll cccc bbb aa\";\n    CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( testString ) ) );\n    TokenStream ts = new MockTokenizer(cs, MockTokenizer.WHITESPACE, false);\n    assertTokenStreamContents(ts,\n      new String[]{\"i\",\"i\",\"jj\",\"kkk\",\"llll\",\"cc\",\"b\",\"a\"},\n      new int[]{0,2,4,6,8,11,16,20},\n      new int[]{1,3,5,7,10,15,19,22},\n      testString.length()\n    );\n  }\n\n","sourceOld":"  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(in)  h i j k ll cccc bbb aa\n  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(out) i i jj kkk llll cc b a\n  //\n  //    h, 0, 1 =>    i, 0, 1\n  //    i, 2, 3 =>    i, 2, 3\n  //    j, 4, 5 =>   jj, 4, 5\n  //    k, 6, 7 =>  kkk, 6, 7\n  //   ll, 8,10 => llll, 8,10\n  // cccc,11,15 =>   cc,11,15\n  //  bbb,16,19 =>    b,16,19\n  //   aa,20,22 =>    a,20,22\n  //\n  public void testTokenStream() throws Exception {\n    String testString = \"h i j k ll cccc bbb aa\";\n    CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( testString ) ) );\n    TokenStream ts = new MockTokenizer(cs, MockTokenizer.WHITESPACE, false);\n    assertTokenStreamContents(ts,\n      new String[]{\"i\",\"i\",\"jj\",\"kkk\",\"llll\",\"cc\",\"b\",\"a\"},\n      new int[]{0,2,4,6,8,11,16,20},\n      new int[]{1,3,5,7,10,15,19,22},\n      testString.length()\n    );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fc706b1e03a539d44d99998108feb684bb44cbb2","date":1342522408,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter#testTokenStream().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter#testTokenStream().mjava","sourceNew":"  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(in)  h i j k ll cccc bbb aa\n  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(out) i i jj kkk llll cc b a\n  //\n  //    h, 0, 1 =>    i, 0, 1\n  //    i, 2, 3 =>    i, 2, 3\n  //    j, 4, 5 =>   jj, 4, 5\n  //    k, 6, 7 =>  kkk, 6, 7\n  //   ll, 8,10 => llll, 8,10\n  // cccc,11,15 =>   cc,11,15\n  //  bbb,16,19 =>    b,16,19\n  //   aa,20,22 =>    a,20,22\n  //\n  public void testTokenStream() throws Exception {\n    String testString = \"h i j k ll cccc bbb aa\";\n    CharFilter cs = new MappingCharFilter( normMap, new StringReader( testString ) );\n    TokenStream ts = new MockTokenizer(cs, MockTokenizer.WHITESPACE, false);\n    assertTokenStreamContents(ts,\n      new String[]{\"i\",\"i\",\"jj\",\"kkk\",\"llll\",\"cc\",\"b\",\"a\"},\n      new int[]{0,2,4,6,8,11,16,20},\n      new int[]{1,3,5,7,10,15,19,22},\n      testString.length()\n    );\n  }\n\n","sourceOld":"  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(in)  h i j k ll cccc bbb aa\n  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(out) i i jj kkk llll cc b a\n  //\n  //    h, 0, 1 =>    i, 0, 1\n  //    i, 2, 3 =>    i, 2, 3\n  //    j, 4, 5 =>   jj, 4, 5\n  //    k, 6, 7 =>  kkk, 6, 7\n  //   ll, 8,10 => llll, 8,10\n  // cccc,11,15 =>   cc,11,15\n  //  bbb,16,19 =>    b,16,19\n  //   aa,20,22 =>    a,20,22\n  //\n  public void testTokenStream() throws Exception {\n    String testString = \"h i j k ll cccc bbb aa\";\n    CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( testString ) ) );\n    TokenStream ts = new MockTokenizer(cs, MockTokenizer.WHITESPACE, false);\n    assertTokenStreamContents(ts,\n      new String[]{\"i\",\"i\",\"jj\",\"kkk\",\"llll\",\"cc\",\"b\",\"a\"},\n      new int[]{0,2,4,6,8,11,16,20},\n      new int[]{1,3,5,7,10,15,19,22},\n      testString.length()\n    );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe33227f6805edab2036cbb80645cc4e2d1fa424","date":1342713534,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter#testTokenStream().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter#testTokenStream().mjava","sourceNew":"  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(in)  h i j k ll cccc bbb aa\n  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(out) i i jj kkk llll cc b a\n  //\n  //    h, 0, 1 =>    i, 0, 1\n  //    i, 2, 3 =>    i, 2, 3\n  //    j, 4, 5 =>   jj, 4, 5\n  //    k, 6, 7 =>  kkk, 6, 7\n  //   ll, 8,10 => llll, 8,10\n  // cccc,11,15 =>   cc,11,15\n  //  bbb,16,19 =>    b,16,19\n  //   aa,20,22 =>    a,20,22\n  //\n  public void testTokenStream() throws Exception {\n    String testString = \"h i j k ll cccc bbb aa\";\n    CharFilter cs = new MappingCharFilter( normMap, new StringReader( testString ) );\n    TokenStream ts = new MockTokenizer(cs, MockTokenizer.WHITESPACE, false);\n    assertTokenStreamContents(ts,\n      new String[]{\"i\",\"i\",\"jj\",\"kkk\",\"llll\",\"cc\",\"b\",\"a\"},\n      new int[]{0,2,4,6,8,11,16,20},\n      new int[]{1,3,5,7,10,15,19,22},\n      testString.length()\n    );\n  }\n\n","sourceOld":"  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(in)  h i j k ll cccc bbb aa\n  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(out) i i jj kkk llll cc b a\n  //\n  //    h, 0, 1 =>    i, 0, 1\n  //    i, 2, 3 =>    i, 2, 3\n  //    j, 4, 5 =>   jj, 4, 5\n  //    k, 6, 7 =>  kkk, 6, 7\n  //   ll, 8,10 => llll, 8,10\n  // cccc,11,15 =>   cc,11,15\n  //  bbb,16,19 =>    b,16,19\n  //   aa,20,22 =>    a,20,22\n  //\n  public void testTokenStream() throws Exception {\n    String testString = \"h i j k ll cccc bbb aa\";\n    CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( testString ) ) );\n    TokenStream ts = new MockTokenizer(cs, MockTokenizer.WHITESPACE, false);\n    assertTokenStreamContents(ts,\n      new String[]{\"i\",\"i\",\"jj\",\"kkk\",\"llll\",\"cc\",\"b\",\"a\"},\n      new int[]{0,2,4,6,8,11,16,20},\n      new int[]{1,3,5,7,10,15,19,22},\n      testString.length()\n    );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aba371508186796cc6151d8223a5b4e16d02e26e","date":1343474871,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter#testTokenStream().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter#testTokenStream().mjava","sourceNew":"  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(in)  h i j k ll cccc bbb aa\n  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(out) i i jj kkk llll cc b a\n  //\n  //    h, 0, 1 =>    i, 0, 1\n  //    i, 2, 3 =>    i, 2, 3\n  //    j, 4, 5 =>   jj, 4, 5\n  //    k, 6, 7 =>  kkk, 6, 7\n  //   ll, 8,10 => llll, 8,10\n  // cccc,11,15 =>   cc,11,15\n  //  bbb,16,19 =>    b,16,19\n  //   aa,20,22 =>    a,20,22\n  //\n  public void testTokenStream() throws Exception {\n    String testString = \"h i j k ll cccc bbb aa\";\n    CharFilter cs = new MappingCharFilter( normMap, new StringReader( testString ) );\n    TokenStream ts = new MockTokenizer(cs, MockTokenizer.WHITESPACE, false);\n    assertTokenStreamContents(ts,\n      new String[]{\"i\",\"i\",\"jj\",\"kkk\",\"llll\",\"cc\",\"b\",\"a\"},\n      new int[]{0,2,4,6,8,11,16,20},\n      new int[]{1,3,5,7,10,15,19,22},\n      testString.length()\n    );\n  }\n\n","sourceOld":"  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(in)  h i j k ll cccc bbb aa\n  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(out) i i jj kkk llll cc b a\n  //\n  //    h, 0, 1 =>    i, 0, 1\n  //    i, 2, 3 =>    i, 2, 3\n  //    j, 4, 5 =>   jj, 4, 5\n  //    k, 6, 7 =>  kkk, 6, 7\n  //   ll, 8,10 => llll, 8,10\n  // cccc,11,15 =>   cc,11,15\n  //  bbb,16,19 =>    b,16,19\n  //   aa,20,22 =>    a,20,22\n  //\n  public void testTokenStream() throws Exception {\n    String testString = \"h i j k ll cccc bbb aa\";\n    CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( testString ) ) );\n    TokenStream ts = new MockTokenizer(cs, MockTokenizer.WHITESPACE, false);\n    assertTokenStreamContents(ts,\n      new String[]{\"i\",\"i\",\"jj\",\"kkk\",\"llll\",\"cc\",\"b\",\"a\"},\n      new int[]{0,2,4,6,8,11,16,20},\n      new int[]{1,3,5,7,10,15,19,22},\n      testString.length()\n    );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter#testTokenStream().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter#testTokenStream().mjava","sourceNew":"  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(in)  h i j k ll cccc bbb aa\n  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(out) i i jj kkk llll cc b a\n  //\n  //    h, 0, 1 =>    i, 0, 1\n  //    i, 2, 3 =>    i, 2, 3\n  //    j, 4, 5 =>   jj, 4, 5\n  //    k, 6, 7 =>  kkk, 6, 7\n  //   ll, 8,10 => llll, 8,10\n  // cccc,11,15 =>   cc,11,15\n  //  bbb,16,19 =>    b,16,19\n  //   aa,20,22 =>    a,20,22\n  //\n  public void testTokenStream() throws Exception {\n    String testString = \"h i j k ll cccc bbb aa\";\n    CharFilter cs = new MappingCharFilter( normMap, new StringReader( testString ) );\n    TokenStream ts =whitespaceMockTokenizer(cs);\n    assertTokenStreamContents(ts,\n      new String[]{\"i\",\"i\",\"jj\",\"kkk\",\"llll\",\"cc\",\"b\",\"a\"},\n      new int[]{0,2,4,6,8,11,16,20},\n      new int[]{1,3,5,7,10,15,19,22},\n      testString.length()\n    );\n  }\n\n","sourceOld":"  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(in)  h i j k ll cccc bbb aa\n  //\n  //                1111111111222\n  //      01234567890123456789012\n  //(out) i i jj kkk llll cc b a\n  //\n  //    h, 0, 1 =>    i, 0, 1\n  //    i, 2, 3 =>    i, 2, 3\n  //    j, 4, 5 =>   jj, 4, 5\n  //    k, 6, 7 =>  kkk, 6, 7\n  //   ll, 8,10 => llll, 8,10\n  // cccc,11,15 =>   cc,11,15\n  //  bbb,16,19 =>    b,16,19\n  //   aa,20,22 =>    a,20,22\n  //\n  public void testTokenStream() throws Exception {\n    String testString = \"h i j k ll cccc bbb aa\";\n    CharFilter cs = new MappingCharFilter( normMap, new StringReader( testString ) );\n    TokenStream ts = new MockTokenizer(cs, MockTokenizer.WHITESPACE, false);\n    assertTokenStreamContents(ts,\n      new String[]{\"i\",\"i\",\"jj\",\"kkk\",\"llll\",\"cc\",\"b\",\"a\"},\n      new int[]{0,2,4,6,8,11,16,20},\n      new int[]{1,3,5,7,10,15,19,22},\n      testString.length()\n    );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"aba371508186796cc6151d8223a5b4e16d02e26e":["b89678825b68eccaf09e6ab71675fc0b0af1e099","fc706b1e03a539d44d99998108feb684bb44cbb2"],"fc706b1e03a539d44d99998108feb684bb44cbb2":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["fc706b1e03a539d44d99998108feb684bb44cbb2"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["b89678825b68eccaf09e6ab71675fc0b0af1e099","fc706b1e03a539d44d99998108feb684bb44cbb2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"]},"commit2Childs":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["aba371508186796cc6151d8223a5b4e16d02e26e","fc706b1e03a539d44d99998108feb684bb44cbb2","fe33227f6805edab2036cbb80645cc4e2d1fa424"],"aba371508186796cc6151d8223a5b4e16d02e26e":[],"fc706b1e03a539d44d99998108feb684bb44cbb2":["aba371508186796cc6151d8223a5b4e16d02e26e","ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","fe33227f6805edab2036cbb80645cc4e2d1fa424"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["aba371508186796cc6151d8223a5b4e16d02e26e","fe33227f6805edab2036cbb80645cc4e2d1fa424","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}