{"path":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","commits":[{"id":"cbf497fc92342be81ff184a144dfa7c96264116b","date":1275079529,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","pathOld":"/dev/null","sourceNew":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    IndexReader reader = writer.getReader();\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(new Field(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    IndexReader newReader = refreshReader(reader);\n    assertTrue(reader != newReader);\n    reader = newReader;\n    searcher = new IndexSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c","date":1281646583,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","sourceNew":"  public void testEnforceDeletions() throws Exception {\n    Random random = newRandom();\n    Directory dir = newDirectory(random);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = writer.w.getReader();\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(new Field(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    IndexReader newReader = refreshReader(reader);\n    assertTrue(reader != newReader);\n    reader = newReader;\n    searcher = new IndexSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    IndexReader reader = writer.getReader();\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(new Field(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    IndexReader newReader = refreshReader(reader);\n    assertTrue(reader != newReader);\n    reader = newReader;\n    searcher = new IndexSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1f653cfcf159baeaafe5d01682a911e95bba4012","date":1284122058,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","sourceNew":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = writer.w.getReader();\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(new Field(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    IndexReader newReader = refreshReader(reader);\n    assertTrue(reader != newReader);\n    reader = newReader;\n    searcher = new IndexSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testEnforceDeletions() throws Exception {\n    Random random = newRandom();\n    Directory dir = newDirectory(random);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = writer.w.getReader();\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(new Field(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    IndexReader newReader = refreshReader(reader);\n    assertTrue(reader != newReader);\n    reader = newReader;\n    searcher = new IndexSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"132903c28af3aa6f67284b78de91c0f0a99488c2","date":1284282129,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","sourceNew":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = writer.w.getReader();\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    IndexReader newReader = refreshReader(reader);\n    assertTrue(reader != newReader);\n    reader = newReader;\n    searcher = new IndexSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = writer.w.getReader();\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(new Field(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    IndexReader newReader = refreshReader(reader);\n    assertTrue(reader != newReader);\n    reader = newReader;\n    searcher = new IndexSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3f873fa376ffa7e7a90771b8afbc3f80667c7b11","date":1285964346,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","sourceNew":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n                                                     newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setMergeScheduler(new SerialMergeScheduler()));\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = writer.w.getReader();\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher = new IndexSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = writer.w.getReader();\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    IndexReader newReader = refreshReader(reader);\n    assertTrue(reader != newReader);\n    reader = newReader;\n    searcher = new IndexSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"296e4ed69ccbda3c7b5fdb86c7acaa43c9074e01","date":1286712181,"type":3,"author":"Grant Ingersoll","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","sourceNew":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n                                                     newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setMergeScheduler(new SerialMergeScheduler()));\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher = new IndexSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n                                                     newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setMergeScheduler(new SerialMergeScheduler()));\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = writer.w.getReader();\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher = new IndexSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2a186ae8733084223c22044e935e4ef848a143d1","date":1289694819,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","sourceNew":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n                                                     newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setMergeScheduler(new SerialMergeScheduler()));\n    // asserts below requires no unexpected merges:\n    ((LogMergePolicy) writer.w.getMergePolicy()).setMergeFactor(10);\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher = new IndexSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n                                                     newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setMergeScheduler(new SerialMergeScheduler()));\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher = new IndexSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c498d3f8d75170b121f5eda2c6210ac5beb5d411","date":1289726298,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","sourceNew":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n                                                     newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setMergeScheduler(new SerialMergeScheduler()));\n    // asserts below requires no unexpected merges:\n    ((LogMergePolicy) writer.w.getMergePolicy()).setMergeFactor(10);\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher = new IndexSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n                                                     newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setMergeScheduler(new SerialMergeScheduler()));\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher = new IndexSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4e8cc373c801e54cec75daf9f52792cb4b17f536","date":1291116159,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","sourceNew":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher = new IndexSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n                                                     newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setMergeScheduler(new SerialMergeScheduler()));\n    // asserts below requires no unexpected merges:\n    ((LogMergePolicy) writer.w.getMergePolicy()).setMergeFactor(10);\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher = new IndexSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3bb13258feba31ab676502787ab2e1779f129b7a","date":1291596436,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","sourceNew":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher = new IndexSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n                                                     newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setMergeScheduler(new SerialMergeScheduler()));\n    // asserts below requires no unexpected merges:\n    ((LogMergePolicy) writer.w.getMergePolicy()).setMergeFactor(10);\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher = new IndexSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","sourceNew":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher = new IndexSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    IndexReader reader = writer.getReader();\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(new Field(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    IndexReader newReader = refreshReader(reader);\n    assertTrue(reader != newReader);\n    reader = newReader;\n    searcher = new IndexSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eb378f8bdee16a26810e086303a4a86b4930ea12","date":1296410797,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","sourceNew":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w, true);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher = new IndexSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher = new IndexSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"790e1fde4caa765b3faaad3fbcd25c6973450336","date":1296689245,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","sourceNew":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w, true);\n    IndexSearcher searcher = newSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher.close();\n    searcher = newSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    searcher.close();\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w, true);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher = new IndexSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","sourceNew":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w, true);\n    IndexSearcher searcher = newSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher.close();\n    searcher = newSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    searcher.close();\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher = new IndexSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3","date":1297940445,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","sourceNew":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w, true);\n    IndexSearcher searcher = newSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that\n    // it's not there\n    _TestUtil.keepFullyDeletedSegments(writer.w);\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher.close();\n    searcher = newSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    searcher.close();\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w, true);\n    IndexSearcher searcher = newSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher.close();\n    searcher = newSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    searcher.close();\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f1bdbf92da222965b46c0a942c3857ba56e5c638","date":1298297608,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","sourceNew":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w, true);\n    IndexSearcher searcher = newSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that\n    // it's not there\n    _TestUtil.keepFullyDeletedSegments(writer.w);\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher.close();\n    searcher = newSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    searcher.close();\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w, true);\n    IndexSearcher searcher = newSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher.close();\n    searcher = newSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    searcher.close();\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","sourceNew":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w, true);\n    IndexSearcher searcher = newSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that\n    // it's not there\n    _TestUtil.keepFullyDeletedSegments(writer.w);\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher.close();\n    searcher = newSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    searcher.close();\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher = new IndexSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher = new IndexSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","sourceNew":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w, true);\n    IndexSearcher searcher = newSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that\n    // it's not there\n    _TestUtil.keepFullyDeletedSegments(writer.w);\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher.close();\n    searcher = newSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    searcher.close();\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w, true);\n    IndexSearcher searcher = newSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that\n    // it's not there\n    _TestUtil.keepFullyDeletedSegments(writer.w);\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher.close();\n    searcher = newSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    searcher.close();\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","sourceNew":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w, true);\n    IndexSearcher searcher = newSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that\n    // it's not there\n    _TestUtil.keepFullyDeletedSegments(writer.w);\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher.close();\n    searcher = newSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    searcher.close();\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w, true);\n    IndexSearcher searcher = newSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that\n    // it's not there\n    _TestUtil.keepFullyDeletedSegments(writer.w);\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher.close();\n    searcher = newSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    searcher.close();\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c1bb50752d43a65ef1b623eabdb8e865983d3cd6","date":1304257984,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","sourceNew":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w, true);\n    // same reason we don't wrap?\n    IndexSearcher searcher = newSearcher(reader, false);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that\n    // it's not there\n    _TestUtil.keepFullyDeletedSegments(writer.w);\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    searcher.close();\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w, true);\n    IndexSearcher searcher = newSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that\n    // it's not there\n    _TestUtil.keepFullyDeletedSegments(writer.w);\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher.close();\n    searcher = newSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    searcher.close();\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","sourceNew":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w, true);\n    // same reason we don't wrap?\n    IndexSearcher searcher = newSearcher(reader, false);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that\n    // it's not there\n    _TestUtil.keepFullyDeletedSegments(writer.w);\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    searcher.close();\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w, true);\n    IndexSearcher searcher = newSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that\n    // it's not there\n    _TestUtil.keepFullyDeletedSegments(writer.w);\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher.close();\n    searcher = newSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    searcher.close();\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","sourceNew":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w, true);\n    // same reason we don't wrap?\n    IndexSearcher searcher = newSearcher(reader, false);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that\n    // it's not there\n    _TestUtil.keepFullyDeletedSegments(writer.w);\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    searcher.close();\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w, true);\n    IndexSearcher searcher = newSearcher(reader);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that\n    // it's not there\n    _TestUtil.keepFullyDeletedSegments(writer.w);\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher.close();\n    searcher = newSearcher(reader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    searcher.close();\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","sourceNew":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w, true);\n    // same reason we don't wrap?\n    IndexSearcher searcher = newSearcher(reader, false);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setTokenized(false);\n    doc.add(newField(\"id\", \"1\", customType));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that\n    // it's not there\n    _TestUtil.keepFullyDeletedSegments(writer.w);\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    searcher.close();\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w, true);\n    // same reason we don't wrap?\n    IndexSearcher searcher = newSearcher(reader, false);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    doc.add(newField(\"id\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that\n    // it's not there\n    _TestUtil.keepFullyDeletedSegments(writer.w);\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    searcher.close();\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"43369d257d14f61a881aa609962ef95e8a334d3a","date":1318786064,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","sourceNew":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w, true);\n    // same reason we don't wrap?\n    IndexSearcher searcher = newSearcher(reader, false);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setTokenized(false);\n    doc.add(newField(\"id\", \"1\", customType));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that\n    // it's not there\n    _TestUtil.keepFullyDeletedSegments(writer.w);\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    // Deletes nothing:\n    writer.deleteDocuments(new Term(\"foo\", \"bar\"));\n    reader = refreshReader(reader);\n    assertTrue(reader == oldReader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    searcher.close();\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w, true);\n    // same reason we don't wrap?\n    IndexSearcher searcher = newSearcher(reader, false);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setTokenized(false);\n    doc.add(newField(\"id\", \"1\", customType));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that\n    // it's not there\n    _TestUtil.keepFullyDeletedSegments(writer.w);\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    reader = refreshReader(reader);\n    assertTrue(reader != oldReader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    searcher.close();\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6620df8541b174097b1133a4fc370adb2e570524","date":1319544675,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","sourceNew":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w, true);\n    // same reason we don't wrap?\n    IndexSearcher searcher = newSearcher(reader, false);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setTokenized(false);\n    doc.add(newField(\"id\", \"1\", customType));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    int missCount = filter.missCount;\n    assertTrue(missCount > 0);\n    Query constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find 2 hits...\", 2, docs.totalHits);\n    assertTrue(filter.missCount > missCount);\n    missCount = filter.missCount;\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 2, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader2 = reader;\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n    assertTrue(oldReader2 != null);\n\n    searcher.close();\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w, true);\n    // same reason we don't wrap?\n    IndexSearcher searcher = newSearcher(reader, false);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setTokenized(false);\n    doc.add(newField(\"id\", \"1\", customType));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    // ignore deletions\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.IGNORE);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    ConstantScoreQuery constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // now delete the doc, refresh the reader, and see that\n    // it's not there\n    _TestUtil.keepFullyDeletedSegments(writer.w);\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n\n    // force cache to regenerate:\n    filter = new CachingSpanFilter(startFilter, CachingWrapperFilter.DeletesMode.RECACHE);\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    // make sure we get a cache hit when we reopen readers\n    // that had no new deletions\n    // Deletes nothing:\n    writer.deleteDocuments(new Term(\"foo\", \"bar\"));\n    reader = refreshReader(reader);\n    assertTrue(reader == oldReader);\n    int missCount = filter.missCount;\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n\n    searcher.close();\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a015474a0e3185be4c42ed156c0f1e88b90b1ace","date":1321290150,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter#testEnforceDeletions().mjava","sourceNew":null,"sourceOld":"  public void testEnforceDeletions() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(\n        random,\n        dir,\n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergeScheduler(new SerialMergeScheduler()).\n            // asserts below requires no unexpected merges:\n            setMergePolicy(newLogMergePolicy(10))\n    );\n\n    // NOTE: cannot use writer.getReader because RIW (on\n    // flipping a coin) may give us a newly opened reader,\n    // but we use .reopen on this reader below and expect to\n    // (must) get an NRT reader:\n    IndexReader reader = IndexReader.open(writer.w, true);\n    // same reason we don't wrap?\n    IndexSearcher searcher = newSearcher(reader, false);\n\n    // add a doc, refresh the reader, and check that its there\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setTokenized(false);\n    doc.add(newField(\"id\", \"1\", customType));\n    writer.addDocument(doc);\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    TopDocs docs = searcher.search(new MatchAllDocsQuery(), 1);\n    assertEquals(\"Should find a hit...\", 1, docs.totalHits);\n\n    final SpanFilter startFilter = new SpanQueryFilter(new SpanTermQuery(new Term(\"id\", \"1\")));\n\n    CachingSpanFilter filter = new CachingSpanFilter(startFilter);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find a hit...\", 1, docs.totalHits);\n    int missCount = filter.missCount;\n    assertTrue(missCount > 0);\n    Query constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 1, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader = reader;\n\n    writer.addDocument(doc);\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n        \n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should find 2 hits...\", 2, docs.totalHits);\n    assertTrue(filter.missCount > missCount);\n    missCount = filter.missCount;\n\n    constantScore = new ConstantScoreQuery(filter);\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should find a hit...\", 2, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // NOTE: important to hold ref here so GC doesn't clear\n    // the cache entry!  Else the assert below may sometimes\n    // fail:\n    IndexReader oldReader2 = reader;\n\n    // now delete the doc, refresh the reader, and see that it's not there\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    reader = refreshReader(reader);\n    searcher.close();\n    searcher = newSearcher(reader, false);\n\n    docs = searcher.search(new MatchAllDocsQuery(), filter, 1);\n    assertEquals(\"[query + filter] Should *not* find a hit...\", 0, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    docs = searcher.search(constantScore, 1);\n    assertEquals(\"[just filter] Should *not* find a hit...\", 0, docs.totalHits);\n    assertEquals(missCount, filter.missCount);\n\n    // NOTE: silliness to make sure JRE does not optimize\n    // away our holding onto oldReader to prevent\n    // CachingWrapperFilter's WeakHashMap from dropping the\n    // entry:\n    assertTrue(oldReader != null);\n    assertTrue(oldReader2 != null);\n\n    searcher.close();\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"6620df8541b174097b1133a4fc370adb2e570524":["43369d257d14f61a881aa609962ef95e8a334d3a"],"f1bdbf92da222965b46c0a942c3857ba56e5c638":["29ef99d61cda9641b6250bf9567329a6e65f901d","e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["cbf497fc92342be81ff184a144dfa7c96264116b","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3":["790e1fde4caa765b3faaad3fbcd25c6973450336"],"296e4ed69ccbda3c7b5fdb86c7acaa43c9074e01":["3f873fa376ffa7e7a90771b8afbc3f80667c7b11"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["3bb13258feba31ab676502787ab2e1779f129b7a","790e1fde4caa765b3faaad3fbcd25c6973450336"],"3f873fa376ffa7e7a90771b8afbc3f80667c7b11":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["c1bb50752d43a65ef1b623eabdb8e865983d3cd6"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3"],"2a186ae8733084223c22044e935e4ef848a143d1":["296e4ed69ccbda3c7b5fdb86c7acaa43c9074e01"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["cbf497fc92342be81ff184a144dfa7c96264116b"],"c498d3f8d75170b121f5eda2c6210ac5beb5d411":["296e4ed69ccbda3c7b5fdb86c7acaa43c9074e01","2a186ae8733084223c22044e935e4ef848a143d1"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["f1bdbf92da222965b46c0a942c3857ba56e5c638","c1bb50752d43a65ef1b623eabdb8e865983d3cd6"],"962d04139994fce5193143ef35615499a9a96d78":["bde51b089eb7f86171eb3406e38a274743f9b7ac","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"43369d257d14f61a881aa609962ef95e8a334d3a":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"eb378f8bdee16a26810e086303a4a86b4930ea12":["4e8cc373c801e54cec75daf9f52792cb4b17f536"],"c1bb50752d43a65ef1b623eabdb8e865983d3cd6":["f2c5f0cb44df114db4228c8f77861714b5cabaea"],"a015474a0e3185be4c42ed156c0f1e88b90b1ace":["6620df8541b174097b1133a4fc370adb2e570524"],"cbf497fc92342be81ff184a144dfa7c96264116b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"790e1fde4caa765b3faaad3fbcd25c6973450336":["eb378f8bdee16a26810e086303a4a86b4930ea12"],"a3776dccca01c11e7046323cfad46a3b4a471233":["e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3","c1bb50752d43a65ef1b623eabdb8e865983d3cd6"],"3bb13258feba31ab676502787ab2e1779f129b7a":["c498d3f8d75170b121f5eda2c6210ac5beb5d411","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a015474a0e3185be4c42ed156c0f1e88b90b1ace"],"4e8cc373c801e54cec75daf9f52792cb4b17f536":["2a186ae8733084223c22044e935e4ef848a143d1"]},"commit2Childs":{"6620df8541b174097b1133a4fc370adb2e570524":["a015474a0e3185be4c42ed156c0f1e88b90b1ace"],"f1bdbf92da222965b46c0a942c3857ba56e5c638":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["3f873fa376ffa7e7a90771b8afbc3f80667c7b11"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3":["f1bdbf92da222965b46c0a942c3857ba56e5c638","f2c5f0cb44df114db4228c8f77861714b5cabaea","bde51b089eb7f86171eb3406e38a274743f9b7ac","a3776dccca01c11e7046323cfad46a3b4a471233"],"296e4ed69ccbda3c7b5fdb86c7acaa43c9074e01":["2a186ae8733084223c22044e935e4ef848a143d1","c498d3f8d75170b121f5eda2c6210ac5beb5d411"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["962d04139994fce5193143ef35615499a9a96d78","c1bb50752d43a65ef1b623eabdb8e865983d3cd6"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["f1bdbf92da222965b46c0a942c3857ba56e5c638"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"3f873fa376ffa7e7a90771b8afbc3f80667c7b11":["296e4ed69ccbda3c7b5fdb86c7acaa43c9074e01"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cbf497fc92342be81ff184a144dfa7c96264116b"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["43369d257d14f61a881aa609962ef95e8a334d3a"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["962d04139994fce5193143ef35615499a9a96d78"],"2a186ae8733084223c22044e935e4ef848a143d1":["c498d3f8d75170b121f5eda2c6210ac5beb5d411","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"c498d3f8d75170b121f5eda2c6210ac5beb5d411":["3bb13258feba31ab676502787ab2e1779f129b7a"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"962d04139994fce5193143ef35615499a9a96d78":[],"43369d257d14f61a881aa609962ef95e8a334d3a":["6620df8541b174097b1133a4fc370adb2e570524"],"eb378f8bdee16a26810e086303a4a86b4930ea12":["790e1fde4caa765b3faaad3fbcd25c6973450336"],"c1bb50752d43a65ef1b623eabdb8e865983d3cd6":["1509f151d7692d84fae414b2b799ac06ba60fcb4","135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233"],"cbf497fc92342be81ff184a144dfa7c96264116b":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"a015474a0e3185be4c42ed156c0f1e88b90b1ace":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"790e1fde4caa765b3faaad3fbcd25c6973450336":["e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3","29ef99d61cda9641b6250bf9567329a6e65f901d"],"a3776dccca01c11e7046323cfad46a3b4a471233":[],"3bb13258feba31ab676502787ab2e1779f129b7a":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"4e8cc373c801e54cec75daf9f52792cb4b17f536":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","eb378f8bdee16a26810e086303a4a86b4930ea12","3bb13258feba31ab676502787ab2e1779f129b7a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","a3776dccca01c11e7046323cfad46a3b4a471233","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}