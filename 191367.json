{"path":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor).mjava","commits":[{"id":"6013b4c7388f1627659c8f96c44abd10a294d3a6","date":1346343796,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor).mjava","pathOld":"/dev/null","sourceNew":"  private String getLeader(final CloudDescriptor cloudDesc) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId())\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeader = zkStateReader.getLeaderUrl(collection,\n          shardId, 30000);\n      int tries = 0;\n      while (!leaderUrl.equals(clusterStateLeader)) {\n        if (tries == 60) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeader + \" but zookeeper says:\" + leaderUrl);\n        }\n        Thread.sleep(1000);\n        tries++;\n        clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId,\n            30000);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId())\n            .getCoreUrl();\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk\", e);\n    } \n    return leaderUrl;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["7de2ec0ce65962e078e24b2ca174d1cb269b610d"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"05a14b2611ead08655a2b2bdc61632eb31316e57","date":1346366621,"type":0,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor).mjava","pathOld":"/dev/null","sourceNew":"  private String getLeader(final CloudDescriptor cloudDesc) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId())\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeader = zkStateReader.getLeaderUrl(collection,\n          shardId, 30000);\n      int tries = 0;\n      while (!leaderUrl.equals(clusterStateLeader)) {\n        if (tries == 60) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeader + \" but zookeeper says:\" + leaderUrl);\n        }\n        Thread.sleep(1000);\n        tries++;\n        clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId,\n            30000);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId())\n            .getCoreUrl();\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk\", e);\n    } \n    return leaderUrl;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f9a98541130dbb2dd570f39bd89ced65760cad80","date":1355032328,"type":5,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor).mjava","sourceNew":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeader = zkStateReader.getLeaderUrl(collection,\n          shardId, 30000);\n      int tries = 0;\n      while (!leaderUrl.equals(clusterStateLeader)) {\n        if (tries == 60) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeader + \" but zookeeper says:\" + leaderUrl);\n        }\n        Thread.sleep(1000);\n        tries++;\n        clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId,\n            30000);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk\", e);\n    } \n    return leaderUrl;\n  }\n\n","sourceOld":"  private String getLeader(final CloudDescriptor cloudDesc) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId())\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeader = zkStateReader.getLeaderUrl(collection,\n          shardId, 30000);\n      int tries = 0;\n      while (!leaderUrl.equals(clusterStateLeader)) {\n        if (tries == 60) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeader + \" but zookeeper says:\" + leaderUrl);\n        }\n        Thread.sleep(1000);\n        tries++;\n        clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId,\n            30000);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId())\n            .getCoreUrl();\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk\", e);\n    } \n    return leaderUrl;\n  }\n\n","bugFix":null,"bugIntro":["7de2ec0ce65962e078e24b2ca174d1cb269b610d"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"407687e67faf6e1f02a211ca078d8e3eed631027","date":1355157407,"type":4,"author":"Robert Muir","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor).mjava","sourceNew":null,"sourceOld":"  private String getLeader(final CloudDescriptor cloudDesc) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId())\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeader = zkStateReader.getLeaderUrl(collection,\n          shardId, 30000);\n      int tries = 0;\n      while (!leaderUrl.equals(clusterStateLeader)) {\n        if (tries == 60) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeader + \" but zookeeper says:\" + leaderUrl);\n        }\n        Thread.sleep(1000);\n        tries++;\n        clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId,\n            30000);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId())\n            .getCoreUrl();\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk\", e);\n    } \n    return leaderUrl;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"f9a98541130dbb2dd570f39bd89ced65760cad80":["6013b4c7388f1627659c8f96c44abd10a294d3a6"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"407687e67faf6e1f02a211ca078d8e3eed631027":["6013b4c7388f1627659c8f96c44abd10a294d3a6","f9a98541130dbb2dd570f39bd89ced65760cad80"],"05a14b2611ead08655a2b2bdc61632eb31316e57":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","6013b4c7388f1627659c8f96c44abd10a294d3a6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["f9a98541130dbb2dd570f39bd89ced65760cad80"],"6013b4c7388f1627659c8f96c44abd10a294d3a6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"f9a98541130dbb2dd570f39bd89ced65760cad80":["407687e67faf6e1f02a211ca078d8e3eed631027","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["05a14b2611ead08655a2b2bdc61632eb31316e57","6013b4c7388f1627659c8f96c44abd10a294d3a6"],"407687e67faf6e1f02a211ca078d8e3eed631027":[],"05a14b2611ead08655a2b2bdc61632eb31316e57":[],"6013b4c7388f1627659c8f96c44abd10a294d3a6":["f9a98541130dbb2dd570f39bd89ced65760cad80","407687e67faf6e1f02a211ca078d8e3eed631027","05a14b2611ead08655a2b2bdc61632eb31316e57"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["407687e67faf6e1f02a211ca078d8e3eed631027","05a14b2611ead08655a2b2bdc61632eb31316e57","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}