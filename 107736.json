{"path":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","commits":[{"id":"1bae040fd1d5e03e0d8d695a9c25cf4f402e7ffe","date":1381909398,"type":0,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = _TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<String>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(_TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numNumericFields=\" + numNumericFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = _TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateNumericDocValue(updateTerm, \"f\" + field, value);\n      writer.updateNumericDocValue(updateTerm, \"cf\" + field, value * 2);\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        AtomicReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.get(j), f.get(j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","sourceNew":"  @Test\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<String>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numNumericFields=\" + numNumericFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateNumericDocValue(updateTerm, \"f\" + field, value);\n      writer.updateNumericDocValue(updateTerm, \"cf\" + field, value * 2);\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        AtomicReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.get(j), f.get(j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = _TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<String>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(_TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numNumericFields=\" + numNumericFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = _TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateNumericDocValue(updateTerm, \"f\" + field, value);\n      writer.updateNumericDocValue(updateTerm, \"cf\" + field, value * 2);\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        AtomicReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.get(j), f.get(j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","sourceNew":"  @Test\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numNumericFields=\" + numNumericFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateNumericDocValue(updateTerm, \"f\" + field, value);\n      writer.updateNumericDocValue(updateTerm, \"cf\" + field, value * 2);\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        AtomicReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.get(j), f.get(j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<String>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numNumericFields=\" + numNumericFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateNumericDocValue(updateTerm, \"f\" + field, value);\n      writer.updateNumericDocValue(updateTerm, \"cf\" + field, value * 2);\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        AtomicReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.get(j), f.get(j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","sourceNew":"  @Test\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numNumericFields=\" + numNumericFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateNumericDocValue(updateTerm, \"f\" + field, value);\n      writer.updateNumericDocValue(updateTerm, \"cf\" + field, value * 2);\n    }\n\n    writer.shutdown();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        AtomicReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.get(j), f.get(j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numNumericFields=\" + numNumericFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateNumericDocValue(updateTerm, \"f\" + field, value);\n      writer.updateNumericDocValue(updateTerm, \"cf\" + field, value * 2);\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        AtomicReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.get(j), f.get(j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70285ef5917fa2c8feec026d4be4d9c20fa89162","date":1401366288,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","sourceNew":"  @Test\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numNumericFields=\" + numNumericFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new NumericDocValuesField(\"f\"+field, value), new NumericDocValuesField(\"cf\"+field, value*2));\n    }\n\n    writer.shutdown();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        AtomicReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.get(j), f.get(j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numNumericFields=\" + numNumericFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateNumericDocValue(updateTerm, \"f\" + field, value);\n      writer.updateNumericDocValue(updateTerm, \"cf\" + field, value * 2);\n    }\n\n    writer.shutdown();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        AtomicReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.get(j), f.get(j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","sourceNew":"  @Test\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numNumericFields=\" + numNumericFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new NumericDocValuesField(\"f\"+field, value), new NumericDocValuesField(\"cf\"+field, value*2));\n    }\n\n    writer.shutdown();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        AtomicReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.get(j), f.get(j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numNumericFields=\" + numNumericFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new NumericDocValuesField(\"f\"+field, value), new NumericDocValuesField(\"cf\"+field, value*2));\n    }\n\n    writer.shutdown();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        AtomicReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.get(j), f.get(j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","sourceNew":"  @Test\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numNumericFields=\" + numNumericFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new NumericDocValuesField(\"f\"+field, value), new NumericDocValuesField(\"cf\"+field, value*2));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        AtomicReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.get(j), f.get(j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numNumericFields=\" + numNumericFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new NumericDocValuesField(\"f\"+field, value), new NumericDocValuesField(\"cf\"+field, value*2));\n    }\n\n    writer.shutdown();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        AtomicReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.get(j), f.get(j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","sourceNew":"  @Test\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numNumericFields=\" + numNumericFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new NumericDocValuesField(\"f\"+field, value), new NumericDocValuesField(\"cf\"+field, value*2));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (LeafReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        LeafReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.get(j), f.get(j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numNumericFields=\" + numNumericFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new NumericDocValuesField(\"f\"+field, value), new NumericDocValuesField(\"cf\"+field, value*2));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        AtomicReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.get(j), f.get(j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a87ce200bba7d88024e2f1c4012212072ce8a5ae","date":1417031281,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","sourceNew":"  @Test @Nightly\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numNumericFields=\" + numNumericFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new NumericDocValuesField(\"f\"+field, value), new NumericDocValuesField(\"cf\"+field, value*2));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (LeafReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        LeafReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.get(j), f.get(j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  @Test\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numNumericFields=\" + numNumericFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new NumericDocValuesField(\"f\"+field, value), new NumericDocValuesField(\"cf\"+field, value*2));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (LeafReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        LeafReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.get(j), f.get(j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6652c74b2358a0b13223817a6a793bf1c9d0749d","date":1474465301,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","sourceNew":"  @Test @Nightly\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numNumericFields=\" + numNumericFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new NumericDocValuesField(\"f\"+field, value), new NumericDocValuesField(\"cf\"+field, value*2));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (LeafReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        LeafReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(j, f.nextDoc());\n          assertEquals(j, cf.nextDoc());\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.longValue(), f.longValue() * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  @Test @Nightly\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numNumericFields=\" + numNumericFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new NumericDocValuesField(\"f\"+field, value), new NumericDocValuesField(\"cf\"+field, value*2));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (LeafReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        LeafReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.get(j), f.get(j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","sourceNew":"  @Test @Nightly\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numNumericFields=\" + numNumericFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new NumericDocValuesField(\"f\"+field, value), new NumericDocValuesField(\"cf\"+field, value*2));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (LeafReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        LeafReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(j, f.nextDoc());\n          assertEquals(j, cf.nextDoc());\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.longValue(), f.longValue() * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  @Test @Nightly\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numNumericFields=\" + numNumericFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new NumericDocValuesField(\"f\"+field, value), new NumericDocValuesField(\"cf\"+field, value*2));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (LeafReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        LeafReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.get(j), f.get(j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","sourceNew":"  @Test @Nightly\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numNumericFields=\" + numNumericFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new NumericDocValuesField(\"f\"+field, value), new NumericDocValuesField(\"cf\"+field, value*2));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (LeafReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        LeafReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(j, f.nextDoc());\n          assertEquals(j, cf.nextDoc());\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.longValue(), f.longValue() * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  @Test @Nightly\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numNumericFields=\" + numNumericFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new NumericDocValuesField(\"f\"+field, value), new NumericDocValuesField(\"cf\"+field, value*2));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (LeafReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        LeafReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.get(j), f.get(j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f4363cd33f6eff7fb4753574a441e2d18c1022a4","date":1498067235,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","sourceNew":"  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new NumericDocValuesField(\"f\"+field, value), new NumericDocValuesField(\"cf\"+field, value*2));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (LeafReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        LeafReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(j, f.nextDoc());\n          assertEquals(j, cf.nextDoc());\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.longValue(), f.longValue() * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  @Test @Nightly\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numNumericFields=\" + numNumericFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new NumericDocValuesField(\"f\"+field, value), new NumericDocValuesField(\"cf\"+field, value*2));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (LeafReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        LeafReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(j, f.nextDoc());\n          assertEquals(j, cf.nextDoc());\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.longValue(), f.longValue() * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","sourceNew":"  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new NumericDocValuesField(\"f\"+field, value), new NumericDocValuesField(\"cf\"+field, value*2));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (LeafReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        LeafReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(j, f.nextDoc());\n          assertEquals(j, cf.nextDoc());\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.longValue(), f.longValue() * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  @Test @Nightly\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numNumericFields=\" + numNumericFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new NumericDocValuesField(\"f\"+field, value), new NumericDocValuesField(\"cf\"+field, value*2));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (LeafReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        LeafReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(j, f.nextDoc());\n          assertEquals(j, cf.nextDoc());\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.longValue(), f.longValue() * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","sourceNew":"  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new NumericDocValuesField(\"f\"+field, value), new NumericDocValuesField(\"cf\"+field, value*2));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (LeafReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        LeafReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(j, f.nextDoc());\n          assertEquals(j, cf.nextDoc());\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.longValue(), f.longValue() * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  @Test @Nightly\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numNumericFields=\" + numNumericFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new NumericDocValuesField(\"f\"+field, value), new NumericDocValuesField(\"cf\"+field, value*2));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (LeafReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        LeafReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(j, f.nextDoc());\n          assertEquals(j, cf.nextDoc());\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.longValue(), f.longValue() * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"57c6c784f777a2cc8fa014507ea129526822714d","date":1579733373,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates#testTonsOfUpdates().mjava","sourceNew":"  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = TEST_NIGHTLY ? atLeast(20000) : atLeast(200);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new NumericDocValuesField(\"f\"+field, value), new NumericDocValuesField(\"cf\"+field, value*2));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (LeafReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        LeafReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(j, f.nextDoc());\n          assertEquals(j, cf.nextDoc());\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.longValue(), f.longValue() * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numNumericFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n    // build a large index with many NDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numNumericFields; j++) {\n        long val = random.nextInt();\n        doc.add(new NumericDocValuesField(\"f\" + j, val));\n        doc.add(new NumericDocValuesField(\"cf\" + j, val * 2));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during numeric updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numNumericFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new NumericDocValuesField(\"f\"+field, value), new NumericDocValuesField(\"cf\"+field, value*2));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (LeafReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numNumericFields; i++) {\n        LeafReader r = context.reader();\n        NumericDocValues f = r.getNumericDocValues(\"f\" + i);\n        NumericDocValues cf = r.getNumericDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(j, f.nextDoc());\n          assertEquals(j, cf.nextDoc());\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, cf.longValue(), f.longValue() * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["6613659748fe4411a7dcf85266e55db1f95f7315"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["70285ef5917fa2c8feec026d4be4d9c20fa89162"],"6613659748fe4411a7dcf85266e55db1f95f7315":["1bae040fd1d5e03e0d8d695a9c25cf4f402e7ffe"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"70285ef5917fa2c8feec026d4be4d9c20fa89162":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"28288370235ed02234a64753cdbf0c6ec096304a":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a87ce200bba7d88024e2f1c4012212072ce8a5ae","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"57c6c784f777a2cc8fa014507ea129526822714d":["28288370235ed02234a64753cdbf0c6ec096304a"],"a87ce200bba7d88024e2f1c4012212072ce8a5ae":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"1bae040fd1d5e03e0d8d695a9c25cf4f402e7ffe":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["a87ce200bba7d88024e2f1c4012212072ce8a5ae","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["a87ce200bba7d88024e2f1c4012212072ce8a5ae"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["57c6c784f777a2cc8fa014507ea129526822714d"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"6613659748fe4411a7dcf85266e55db1f95f7315":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"70285ef5917fa2c8feec026d4be4d9c20fa89162":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":[],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["a87ce200bba7d88024e2f1c4012212072ce8a5ae"],"28288370235ed02234a64753cdbf0c6ec096304a":["57c6c784f777a2cc8fa014507ea129526822714d"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"57c6c784f777a2cc8fa014507ea129526822714d":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"1bae040fd1d5e03e0d8d695a9c25cf4f402e7ffe":["6613659748fe4411a7dcf85266e55db1f95f7315"],"a87ce200bba7d88024e2f1c4012212072ce8a5ae":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","17e5da53e4e5bd659e22add9bba1cfa222e7e30d","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["f4363cd33f6eff7fb4753574a441e2d18c1022a4","b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1bae040fd1d5e03e0d8d695a9c25cf4f402e7ffe"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["70285ef5917fa2c8feec026d4be4d9c20fa89162"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}