{"path":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#processDocument(Analyzer).mjava","commits":[{"id":"4350b17bd363cd13a95171b8df1ca62ea4c3e71c","date":1183562198,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#processDocument(Analyzer).mjava","pathOld":"/dev/null","sourceNew":"    /** Tokenizes the fields of a document into Postings */\n    void processDocument(Analyzer analyzer)\n      throws IOException {\n\n      final int numFields = numFieldData;\n\n      fdtLocal.writeVInt(numStoredFields);\n\n      if (tvx != null)\n        // If we are writing vectors then we must visit\n        // fields in sorted order so they are written in\n        // sorted order.  TODO: we actually only need to\n        // sort the subset of fields that have vectors\n        // enabled; we could save [small amount of] CPU\n        // here.\n        Arrays.sort(fieldDataArray, 0, numFields);\n\n      // We process the document one field at a time\n      for(int i=0;i<numFields;i++)\n        fieldDataArray[i].processField(analyzer);\n\n      if (numBytesUsed > 0.95 * ramBufferSize)\n        balanceRAM();\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["c9a0deca56efc5191d6b3c41047fd538f3fae1d8","83bbb041887bbef07b8a98d08a0e1713ce137039"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2558ddf9e14a97bc597f5b72bb3ecb5b7f6bba8e","date":1191352543,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#processDocument(Analyzer).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#processDocument(Analyzer).mjava","sourceNew":"    /** Tokenizes the fields of a document into Postings */\n    void processDocument(Analyzer analyzer)\n      throws IOException {\n\n      final int numFields = numFieldData;\n\n      fdtLocal.writeVInt(numStoredFields);\n\n      if (tvx != null)\n        // If we are writing vectors then we must visit\n        // fields in sorted order so they are written in\n        // sorted order.  TODO: we actually only need to\n        // sort the subset of fields that have vectors\n        // enabled; we could save [small amount of] CPU\n        // here.\n        Arrays.sort(fieldDataArray, 0, numFields);\n\n      // We process the document one field at a time\n      for(int i=0;i<numFields;i++)\n        fieldDataArray[i].processField(analyzer);\n\n      if (ramBufferSize != IndexWriter.DISABLE_AUTO_FLUSH\n          && numBytesUsed > 0.95 * ramBufferSize)\n        balanceRAM();\n    }\n\n","sourceOld":"    /** Tokenizes the fields of a document into Postings */\n    void processDocument(Analyzer analyzer)\n      throws IOException {\n\n      final int numFields = numFieldData;\n\n      fdtLocal.writeVInt(numStoredFields);\n\n      if (tvx != null)\n        // If we are writing vectors then we must visit\n        // fields in sorted order so they are written in\n        // sorted order.  TODO: we actually only need to\n        // sort the subset of fields that have vectors\n        // enabled; we could save [small amount of] CPU\n        // here.\n        Arrays.sort(fieldDataArray, 0, numFields);\n\n      // We process the document one field at a time\n      for(int i=0;i<numFields;i++)\n        fieldDataArray[i].processField(analyzer);\n\n      if (numBytesUsed > 0.95 * ramBufferSize)\n        balanceRAM();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c9a0deca56efc5191d6b3c41047fd538f3fae1d8","date":1198156049,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#processDocument(Analyzer).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#processDocument(Analyzer).mjava","sourceNew":"    /** Tokenizes the fields of a document into Postings */\n    void processDocument(Analyzer analyzer)\n      throws IOException {\n\n      final int numFields = numFieldData;\n\n      assert 0 == fdtLocal.length();\n\n      if (tvx != null)\n        // If we are writing vectors then we must visit\n        // fields in sorted order so they are written in\n        // sorted order.  TODO: we actually only need to\n        // sort the subset of fields that have vectors\n        // enabled; we could save [small amount of] CPU\n        // here.\n        Arrays.sort(fieldDataArray, 0, numFields);\n\n      // We process the document one field at a time\n      for(int i=0;i<numFields;i++)\n        fieldDataArray[i].processField(analyzer);\n\n      if (ramBufferSize != IndexWriter.DISABLE_AUTO_FLUSH\n          && numBytesUsed > 0.95 * ramBufferSize)\n        balanceRAM();\n    }\n\n","sourceOld":"    /** Tokenizes the fields of a document into Postings */\n    void processDocument(Analyzer analyzer)\n      throws IOException {\n\n      final int numFields = numFieldData;\n\n      fdtLocal.writeVInt(numStoredFields);\n\n      if (tvx != null)\n        // If we are writing vectors then we must visit\n        // fields in sorted order so they are written in\n        // sorted order.  TODO: we actually only need to\n        // sort the subset of fields that have vectors\n        // enabled; we could save [small amount of] CPU\n        // here.\n        Arrays.sort(fieldDataArray, 0, numFields);\n\n      // We process the document one field at a time\n      for(int i=0;i<numFields;i++)\n        fieldDataArray[i].processField(analyzer);\n\n      if (ramBufferSize != IndexWriter.DISABLE_AUTO_FLUSH\n          && numBytesUsed > 0.95 * ramBufferSize)\n        balanceRAM();\n    }\n\n","bugFix":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5a251aa47d1808cbae42c0e172d698c377430e60","date":1199375390,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#processDocument(Analyzer).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#processDocument(Analyzer).mjava","sourceNew":"    /** Tokenizes the fields of a document into Postings */\n    void processDocument(Analyzer analyzer)\n      throws IOException {\n\n      final int numFields = numFieldData;\n\n      assert 0 == fdtLocal.length();\n\n      if (tvx != null)\n        // If we are writing vectors then we must visit\n        // fields in sorted order so they are written in\n        // sorted order.  TODO: we actually only need to\n        // sort the subset of fields that have vectors\n        // enabled; we could save [small amount of] CPU\n        // here.\n        Arrays.sort(fieldDataArray, 0, numFields);\n\n      // We process the document one field at a time\n      for(int i=0;i<numFields;i++)\n        fieldDataArray[i].processField(analyzer);\n\n      if (maxTermPrefix != null && infoStream != null)\n        infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + maxTermPrefix + \"...'\"); \n\n      if (ramBufferSize != IndexWriter.DISABLE_AUTO_FLUSH\n          && numBytesUsed > 0.95 * ramBufferSize)\n        balanceRAM();\n    }\n\n","sourceOld":"    /** Tokenizes the fields of a document into Postings */\n    void processDocument(Analyzer analyzer)\n      throws IOException {\n\n      final int numFields = numFieldData;\n\n      assert 0 == fdtLocal.length();\n\n      if (tvx != null)\n        // If we are writing vectors then we must visit\n        // fields in sorted order so they are written in\n        // sorted order.  TODO: we actually only need to\n        // sort the subset of fields that have vectors\n        // enabled; we could save [small amount of] CPU\n        // here.\n        Arrays.sort(fieldDataArray, 0, numFields);\n\n      // We process the document one field at a time\n      for(int i=0;i<numFields;i++)\n        fieldDataArray[i].processField(analyzer);\n\n      if (ramBufferSize != IndexWriter.DISABLE_AUTO_FLUSH\n          && numBytesUsed > 0.95 * ramBufferSize)\n        balanceRAM();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"83bbb041887bbef07b8a98d08a0e1713ce137039","date":1200330381,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#processDocument(Analyzer).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#processDocument(Analyzer).mjava","sourceNew":"    /** Tokenizes the fields of a document into Postings */\n    void processDocument(Analyzer analyzer)\n      throws IOException, AbortException {\n\n      final int numFields = numFieldData;\n\n      assert 0 == fdtLocal.length();\n\n      if (tvx != null)\n        // If we are writing vectors then we must visit\n        // fields in sorted order so they are written in\n        // sorted order.  TODO: we actually only need to\n        // sort the subset of fields that have vectors\n        // enabled; we could save [small amount of] CPU\n        // here.\n        Arrays.sort(fieldDataArray, 0, numFields);\n\n      // We process the document one field at a time\n      for(int i=0;i<numFields;i++)\n        fieldDataArray[i].processField(analyzer);\n\n      if (maxTermPrefix != null && infoStream != null)\n        infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + maxTermPrefix + \"...'\"); \n\n      if (ramBufferSize != IndexWriter.DISABLE_AUTO_FLUSH\n          && numBytesUsed > 0.95 * ramBufferSize)\n        balanceRAM();\n    }\n\n","sourceOld":"    /** Tokenizes the fields of a document into Postings */\n    void processDocument(Analyzer analyzer)\n      throws IOException {\n\n      final int numFields = numFieldData;\n\n      assert 0 == fdtLocal.length();\n\n      if (tvx != null)\n        // If we are writing vectors then we must visit\n        // fields in sorted order so they are written in\n        // sorted order.  TODO: we actually only need to\n        // sort the subset of fields that have vectors\n        // enabled; we could save [small amount of] CPU\n        // here.\n        Arrays.sort(fieldDataArray, 0, numFields);\n\n      // We process the document one field at a time\n      for(int i=0;i<numFields;i++)\n        fieldDataArray[i].processField(analyzer);\n\n      if (maxTermPrefix != null && infoStream != null)\n        infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + maxTermPrefix + \"...'\"); \n\n      if (ramBufferSize != IndexWriter.DISABLE_AUTO_FLUSH\n          && numBytesUsed > 0.95 * ramBufferSize)\n        balanceRAM();\n    }\n\n","bugFix":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2684bcb2a921b6b5b76f64ba986564ab1ef0649d","date":1202988124,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#processDocument(Analyzer).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#processDocument(Analyzer).mjava","sourceNew":"    /** Tokenizes the fields of a document into Postings */\n    void processDocument(Analyzer analyzer)\n      throws IOException, AbortException {\n\n      final int numFields = numFieldData;\n      assert clearLastVectorFieldName();\n\n      assert 0 == fdtLocal.length();\n\n      if (tvx != null)\n        // If we are writing vectors then we must visit\n        // fields in sorted order so they are written in\n        // sorted order.  TODO: we actually only need to\n        // sort the subset of fields that have vectors\n        // enabled; we could save [small amount of] CPU\n        // here.\n        quickSort(fieldDataArray, 0, numFields-1);\n\n      // We process the document one field at a time\n      for(int i=0;i<numFields;i++)\n        fieldDataArray[i].processField(analyzer);\n\n      if (maxTermPrefix != null && infoStream != null)\n        infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + maxTermPrefix + \"...'\"); \n    }\n\n","sourceOld":"    /** Tokenizes the fields of a document into Postings */\n    void processDocument(Analyzer analyzer)\n      throws IOException, AbortException {\n\n      final int numFields = numFieldData;\n\n      assert 0 == fdtLocal.length();\n\n      if (tvx != null)\n        // If we are writing vectors then we must visit\n        // fields in sorted order so they are written in\n        // sorted order.  TODO: we actually only need to\n        // sort the subset of fields that have vectors\n        // enabled; we could save [small amount of] CPU\n        // here.\n        Arrays.sort(fieldDataArray, 0, numFields);\n\n      // We process the document one field at a time\n      for(int i=0;i<numFields;i++)\n        fieldDataArray[i].processField(analyzer);\n\n      if (maxTermPrefix != null && infoStream != null)\n        infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + maxTermPrefix + \"...'\"); \n\n      if (ramBufferSize != IndexWriter.DISABLE_AUTO_FLUSH\n          && numBytesUsed > 0.95 * ramBufferSize)\n        balanceRAM();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5a0af3a442be522899177e5e11384a45a6784a3f","date":1205348952,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState#processDocument(Analyzer).mjava","sourceNew":null,"sourceOld":"    /** Tokenizes the fields of a document into Postings */\n    void processDocument(Analyzer analyzer)\n      throws IOException, AbortException {\n\n      final int numFields = numFieldData;\n      assert clearLastVectorFieldName();\n\n      assert 0 == fdtLocal.length();\n\n      if (tvx != null)\n        // If we are writing vectors then we must visit\n        // fields in sorted order so they are written in\n        // sorted order.  TODO: we actually only need to\n        // sort the subset of fields that have vectors\n        // enabled; we could save [small amount of] CPU\n        // here.\n        quickSort(fieldDataArray, 0, numFields-1);\n\n      // We process the document one field at a time\n      for(int i=0;i<numFields;i++)\n        fieldDataArray[i].processField(analyzer);\n\n      if (maxTermPrefix != null && infoStream != null)\n        infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + maxTermPrefix + \"...'\"); \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"83bbb041887bbef07b8a98d08a0e1713ce137039":["5a251aa47d1808cbae42c0e172d698c377430e60"],"c9a0deca56efc5191d6b3c41047fd538f3fae1d8":["2558ddf9e14a97bc597f5b72bb3ecb5b7f6bba8e"],"2684bcb2a921b6b5b76f64ba986564ab1ef0649d":["83bbb041887bbef07b8a98d08a0e1713ce137039"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5a251aa47d1808cbae42c0e172d698c377430e60":["c9a0deca56efc5191d6b3c41047fd538f3fae1d8"],"2558ddf9e14a97bc597f5b72bb3ecb5b7f6bba8e":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"5a0af3a442be522899177e5e11384a45a6784a3f":["2684bcb2a921b6b5b76f64ba986564ab1ef0649d"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["5a0af3a442be522899177e5e11384a45a6784a3f"]},"commit2Childs":{"83bbb041887bbef07b8a98d08a0e1713ce137039":["2684bcb2a921b6b5b76f64ba986564ab1ef0649d"],"c9a0deca56efc5191d6b3c41047fd538f3fae1d8":["5a251aa47d1808cbae42c0e172d698c377430e60"],"2684bcb2a921b6b5b76f64ba986564ab1ef0649d":["5a0af3a442be522899177e5e11384a45a6784a3f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"5a251aa47d1808cbae42c0e172d698c377430e60":["83bbb041887bbef07b8a98d08a0e1713ce137039"],"2558ddf9e14a97bc597f5b72bb3ecb5b7f6bba8e":["c9a0deca56efc5191d6b3c41047fd538f3fae1d8"],"5a0af3a442be522899177e5e11384a45a6784a3f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["2558ddf9e14a97bc597f5b72bb3ecb5b7f6bba8e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}