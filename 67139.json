{"path":"lucene/src/java/org/apache/lucene/index/codecs/standard/StandardTermsDictReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","commits":[{"id":"955c32f886db6f6356c9fcdea6b1f1cb4effda24","date":1270581567,"type":0,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/standard/StandardTermsDictReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","pathOld":"/dev/null","sourceNew":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(BytesRef term, boolean useCache) throws IOException {\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            state.copy(cachedState);\n            seekPending = true;\n            bytesReader.term.copy(term);\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (state.ord != -1) {\n          // we are positioned\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          }\n\n          if (cmp < 0 &&\n              indexReader.nextIndexTerm(state.ord, indexResult) &&\n              termComp.compare(indexResult.term, term) > 0) {\n            // Optimization: requested term is within the\n            // same index block we are now in; skip seeking\n            // (but do scanning):\n            doSeek = false;\n          }\n        }\n\n        // Used only for assert:\n        final long startOrd;\n\n        if (doSeek) {\n\n          // As index to find biggest index term that's <=\n          // our text:\n          indexReader.getIndexOffset(term, indexResult);\n\n          in.seek(indexResult.offset);\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexResult.term);\n          \n          state.ord = indexResult.position-1;\n          assert state.ord >= -1: \"ord=\" + state.ord;\n\n          startOrd = indexResult.position;\n        } else {\n          startOrd = -1;\n        }\n\n        // Now scan:\n        while(next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n\n            if (doSeek && useCache) {\n              // Store in cache\n              FieldAndTerm entryKey = new FieldAndTerm(fieldTerm);\n              cachedState = (TermState) state.clone();\n              // this is fp after current term\n              cachedState.filePointer = in.getFilePointer();\n              termsCache.put(entryKey, cachedState);\n            }\n              \n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert state.ord == startOrd || !indexReader.isIndexTerm(state.ord, state.docFreq, true);\n        }\n\n        return SeekStatus.END;\n      }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"552356653fbcba144cd337e76f4abe8885913817","date":1273339661,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/standard/StandardTermsDictReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/standard/StandardTermsDictReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","sourceNew":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(BytesRef term, boolean useCache) throws IOException {\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            state.copy(cachedState);\n            seekPending = true;\n            bytesReader.term.copy(term);\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (state.ord != -1) {\n          // we are positioned\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          }\n\n          if (cmp < 0 &&\n              fieldIndexReader.nextIndexTerm(state.ord, indexResult) &&\n              termComp.compare(indexResult.term, term) > 0) {\n            // Optimization: requested term is within the\n            // same index block we are now in; skip seeking\n            // (but do scanning):\n            doSeek = false;\n          }\n        }\n\n        // Used only for assert:\n        final long startOrd;\n\n        if (doSeek) {\n\n          // As index to find biggest index term that's <=\n          // our text:\n          fieldIndexReader.getIndexOffset(term, indexResult);\n\n          in.seek(indexResult.offset);\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexResult.term);\n          \n          state.ord = indexResult.position-1;\n          assert state.ord >= -1: \"ord=\" + state.ord;\n\n          startOrd = indexResult.position;\n        } else {\n          startOrd = -1;\n        }\n\n        // Now scan:\n        while(next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n\n            if (doSeek && useCache) {\n              // Store in cache\n              FieldAndTerm entryKey = new FieldAndTerm(fieldTerm);\n              cachedState = (TermState) state.clone();\n              // this is fp after current term\n              cachedState.filePointer = in.getFilePointer();\n              termsCache.put(entryKey, cachedState);\n            }\n              \n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert state.ord == startOrd || !fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true);\n        }\n\n        return SeekStatus.END;\n      }\n\n","sourceOld":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(BytesRef term, boolean useCache) throws IOException {\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            state.copy(cachedState);\n            seekPending = true;\n            bytesReader.term.copy(term);\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (state.ord != -1) {\n          // we are positioned\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          }\n\n          if (cmp < 0 &&\n              indexReader.nextIndexTerm(state.ord, indexResult) &&\n              termComp.compare(indexResult.term, term) > 0) {\n            // Optimization: requested term is within the\n            // same index block we are now in; skip seeking\n            // (but do scanning):\n            doSeek = false;\n          }\n        }\n\n        // Used only for assert:\n        final long startOrd;\n\n        if (doSeek) {\n\n          // As index to find biggest index term that's <=\n          // our text:\n          indexReader.getIndexOffset(term, indexResult);\n\n          in.seek(indexResult.offset);\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexResult.term);\n          \n          state.ord = indexResult.position-1;\n          assert state.ord >= -1: \"ord=\" + state.ord;\n\n          startOrd = indexResult.position;\n        } else {\n          startOrd = -1;\n        }\n\n        // Now scan:\n        while(next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n\n            if (doSeek && useCache) {\n              // Store in cache\n              FieldAndTerm entryKey = new FieldAndTerm(fieldTerm);\n              cachedState = (TermState) state.clone();\n              // this is fp after current term\n              cachedState.filePointer = in.getFilePointer();\n              termsCache.put(entryKey, cachedState);\n            }\n              \n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert state.ord == startOrd || !indexReader.isIndexTerm(state.ord, state.docFreq, true);\n        }\n\n        return SeekStatus.END;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2aafd88401639311b0404e67c94e829e123a0e45","date":1273477632,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/standard/StandardTermsDictReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/standard/StandardTermsDictReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","sourceNew":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(BytesRef term, boolean useCache) throws IOException {\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(new FieldAndTerm(fieldTerm));\n          if (cachedState != null) {\n            state.copy(cachedState);\n            seekPending = true;\n            bytesReader.term.copy(term);\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (state.ord != -1) {\n          // we are positioned\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          }\n\n          if (cmp < 0 &&\n              fieldIndexReader.nextIndexTerm(state.ord, indexResult) &&\n              termComp.compare(indexResult.term, term) > 0) {\n            // Optimization: requested term is within the\n            // same index block we are now in; skip seeking\n            // (but do scanning):\n            doSeek = false;\n          }\n        }\n\n        // Used only for assert:\n        final long startOrd;\n\n        if (doSeek) {\n\n          // As index to find biggest index term that's <=\n          // our text:\n          fieldIndexReader.getIndexOffset(term, indexResult);\n\n          in.seek(indexResult.offset);\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexResult.term);\n          \n          state.ord = indexResult.position-1;\n          assert state.ord >= -1: \"ord=\" + state.ord;\n\n          startOrd = indexResult.position;\n        } else {\n          startOrd = -1;\n        }\n\n        // Now scan:\n        while(next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n\n            if (doSeek && useCache) {\n              // Store in cache\n              FieldAndTerm entryKey = new FieldAndTerm(fieldTerm);\n              cachedState = (TermState) state.clone();\n              // this is fp after current term\n              cachedState.filePointer = in.getFilePointer();\n              termsCache.put(entryKey, cachedState);\n            }\n              \n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert state.ord == startOrd || !fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true): \"state.ord=\" + state.ord + \" startOrd=\" + startOrd + \" ir.isIndexTerm=\" + fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true) + \" state.docFreq=\" + state.docFreq;\n        }\n\n        return SeekStatus.END;\n      }\n\n","sourceOld":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(BytesRef term, boolean useCache) throws IOException {\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            state.copy(cachedState);\n            seekPending = true;\n            bytesReader.term.copy(term);\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (state.ord != -1) {\n          // we are positioned\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          }\n\n          if (cmp < 0 &&\n              fieldIndexReader.nextIndexTerm(state.ord, indexResult) &&\n              termComp.compare(indexResult.term, term) > 0) {\n            // Optimization: requested term is within the\n            // same index block we are now in; skip seeking\n            // (but do scanning):\n            doSeek = false;\n          }\n        }\n\n        // Used only for assert:\n        final long startOrd;\n\n        if (doSeek) {\n\n          // As index to find biggest index term that's <=\n          // our text:\n          fieldIndexReader.getIndexOffset(term, indexResult);\n\n          in.seek(indexResult.offset);\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexResult.term);\n          \n          state.ord = indexResult.position-1;\n          assert state.ord >= -1: \"ord=\" + state.ord;\n\n          startOrd = indexResult.position;\n        } else {\n          startOrd = -1;\n        }\n\n        // Now scan:\n        while(next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n\n            if (doSeek && useCache) {\n              // Store in cache\n              FieldAndTerm entryKey = new FieldAndTerm(fieldTerm);\n              cachedState = (TermState) state.clone();\n              // this is fp after current term\n              cachedState.filePointer = in.getFilePointer();\n              termsCache.put(entryKey, cachedState);\n            }\n              \n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert state.ord == startOrd || !fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true);\n        }\n\n        return SeekStatus.END;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4305c035b0ff6b9c317310bca62a6673049f8d0b","date":1273768894,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/standard/StandardTermsDictReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/standard/StandardTermsDictReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","sourceNew":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(BytesRef term, boolean useCache) throws IOException {\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            state.copy(cachedState);\n            seekPending = true;\n            bytesReader.term.copy(term);\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (state.ord != -1) {\n          // we are positioned\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          }\n\n          if (cmp < 0 &&\n              fieldIndexReader.nextIndexTerm(state.ord, indexResult) &&\n              termComp.compare(indexResult.term, term) > 0) {\n            // Optimization: requested term is within the\n            // same index block we are now in; skip seeking\n            // (but do scanning):\n            doSeek = false;\n          }\n        }\n\n        // Used only for assert:\n        final long startOrd;\n\n        if (doSeek) {\n\n          // As index to find biggest index term that's <=\n          // our text:\n          fieldIndexReader.getIndexOffset(term, indexResult);\n\n          in.seek(indexResult.offset);\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexResult.term);\n          \n          state.ord = indexResult.position-1;\n          assert state.ord >= -1: \"ord=\" + state.ord;\n\n          startOrd = indexResult.position;\n        } else {\n          startOrd = -1;\n        }\n\n        // Now scan:\n        while(next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n\n            if (doSeek && useCache) {\n              // Store in cache\n              FieldAndTerm entryKey = new FieldAndTerm(fieldTerm);\n              cachedState = (TermState) state.clone();\n              // this is fp after current term\n              cachedState.filePointer = in.getFilePointer();\n              termsCache.put(entryKey, cachedState);\n            }\n              \n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert state.ord == startOrd || !fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true): \"state.ord=\" + state.ord + \" startOrd=\" + startOrd + \" ir.isIndexTerm=\" + fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true) + \" state.docFreq=\" + state.docFreq;\n        }\n\n        return SeekStatus.END;\n      }\n\n","sourceOld":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(BytesRef term, boolean useCache) throws IOException {\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(new FieldAndTerm(fieldTerm));\n          if (cachedState != null) {\n            state.copy(cachedState);\n            seekPending = true;\n            bytesReader.term.copy(term);\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (state.ord != -1) {\n          // we are positioned\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          }\n\n          if (cmp < 0 &&\n              fieldIndexReader.nextIndexTerm(state.ord, indexResult) &&\n              termComp.compare(indexResult.term, term) > 0) {\n            // Optimization: requested term is within the\n            // same index block we are now in; skip seeking\n            // (but do scanning):\n            doSeek = false;\n          }\n        }\n\n        // Used only for assert:\n        final long startOrd;\n\n        if (doSeek) {\n\n          // As index to find biggest index term that's <=\n          // our text:\n          fieldIndexReader.getIndexOffset(term, indexResult);\n\n          in.seek(indexResult.offset);\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexResult.term);\n          \n          state.ord = indexResult.position-1;\n          assert state.ord >= -1: \"ord=\" + state.ord;\n\n          startOrd = indexResult.position;\n        } else {\n          startOrd = -1;\n        }\n\n        // Now scan:\n        while(next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n\n            if (doSeek && useCache) {\n              // Store in cache\n              FieldAndTerm entryKey = new FieldAndTerm(fieldTerm);\n              cachedState = (TermState) state.clone();\n              // this is fp after current term\n              cachedState.filePointer = in.getFilePointer();\n              termsCache.put(entryKey, cachedState);\n            }\n              \n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert state.ord == startOrd || !fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true): \"state.ord=\" + state.ord + \" startOrd=\" + startOrd + \" ir.isIndexTerm=\" + fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true) + \" state.docFreq=\" + state.docFreq;\n        }\n\n        return SeekStatus.END;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b21422ff1d1d56499dec481f193b402e5e8def5b","date":1281472367,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/standard/StandardTermsDictReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/standard/StandardTermsDictReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","sourceNew":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(BytesRef term, boolean useCache) throws IOException {\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            state.copy(cachedState);\n            seekPending = true;\n            bytesReader.term.copy(term);\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (state.ord != -1) {\n          // we are positioned\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          }\n\n          if (cmp < 0 &&\n              fieldIndexReader.nextIndexTerm(state.ord, indexResult) &&\n              termComp.compare(indexResult.term, term) > 0) {\n            // Optimization: requested term is within the\n            // same index block we are now in; skip seeking\n            // (but do scanning):\n            doSeek = false;\n          }\n        }\n\n        // Used only for assert:\n        final long startOrd;\n\n        if (doSeek) {\n\n          // As index to find biggest index term that's <=\n          // our text:\n          fieldIndexReader.getIndexOffset(term, indexResult);\n\n          in.seek(indexResult.offset);\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexResult.term);\n          \n          state.ord = indexResult.position-1;\n          assert state.ord >= -1: \"ord=\" + state.ord + \" pos=\" + indexResult.position;\n\n          startOrd = indexResult.position;\n        } else {\n          startOrd = -1;\n        }\n\n        // Now scan:\n        while(next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n\n            if (doSeek && useCache) {\n              // Store in cache\n              FieldAndTerm entryKey = new FieldAndTerm(fieldTerm);\n              cachedState = (TermState) state.clone();\n              // this is fp after current term\n              cachedState.filePointer = in.getFilePointer();\n              termsCache.put(entryKey, cachedState);\n            }\n              \n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert state.ord == startOrd || !fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true): \"state.ord=\" + state.ord + \" startOrd=\" + startOrd + \" ir.isIndexTerm=\" + fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true) + \" state.docFreq=\" + state.docFreq;\n        }\n\n        return SeekStatus.END;\n      }\n\n","sourceOld":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(BytesRef term, boolean useCache) throws IOException {\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            state.copy(cachedState);\n            seekPending = true;\n            bytesReader.term.copy(term);\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (state.ord != -1) {\n          // we are positioned\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          }\n\n          if (cmp < 0 &&\n              fieldIndexReader.nextIndexTerm(state.ord, indexResult) &&\n              termComp.compare(indexResult.term, term) > 0) {\n            // Optimization: requested term is within the\n            // same index block we are now in; skip seeking\n            // (but do scanning):\n            doSeek = false;\n          }\n        }\n\n        // Used only for assert:\n        final long startOrd;\n\n        if (doSeek) {\n\n          // As index to find biggest index term that's <=\n          // our text:\n          fieldIndexReader.getIndexOffset(term, indexResult);\n\n          in.seek(indexResult.offset);\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexResult.term);\n          \n          state.ord = indexResult.position-1;\n          assert state.ord >= -1: \"ord=\" + state.ord;\n\n          startOrd = indexResult.position;\n        } else {\n          startOrd = -1;\n        }\n\n        // Now scan:\n        while(next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n\n            if (doSeek && useCache) {\n              // Store in cache\n              FieldAndTerm entryKey = new FieldAndTerm(fieldTerm);\n              cachedState = (TermState) state.clone();\n              // this is fp after current term\n              cachedState.filePointer = in.getFilePointer();\n              termsCache.put(entryKey, cachedState);\n            }\n              \n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert state.ord == startOrd || !fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true): \"state.ord=\" + state.ord + \" startOrd=\" + startOrd + \" ir.isIndexTerm=\" + fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true) + \" state.docFreq=\" + state.docFreq;\n        }\n\n        return SeekStatus.END;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"99cf56f3a650b908f7017a72f9d23940418f8a52","date":1284891529,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/PrefixCodedTermsReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/standard/StandardTermsDictReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","sourceNew":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(BytesRef term, boolean useCache) throws IOException {\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            state.copy(cachedState);\n            seekPending = true;\n            bytesReader.term.copy(term);\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (state.ord != -1) {\n          // we are positioned\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          }\n\n          if (cmp < 0 &&\n              fieldIndexReader.nextIndexTerm(state.ord, indexResult) &&\n              termComp.compare(indexResult.term, term) > 0) {\n            // Optimization: requested term is within the\n            // same index block we are now in; skip seeking\n            // (but do scanning):\n            doSeek = false;\n          }\n        }\n\n        // Used only for assert:\n        final long startOrd;\n\n        if (doSeek) {\n\n          // As index to find biggest index term that's <=\n          // our text:\n          fieldIndexReader.getIndexOffset(term, indexResult);\n\n          in.seek(indexResult.offset);\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexResult.term);\n          \n          state.ord = indexResult.position-1;\n          assert state.ord >= -1: \"ord=\" + state.ord + \" pos=\" + indexResult.position;\n\n          startOrd = indexResult.position;\n        } else {\n          startOrd = -1;\n        }\n\n        // Now scan:\n        while(next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n\n            if (doSeek && useCache) {\n              // Store in cache\n              FieldAndTerm entryKey = new FieldAndTerm(fieldTerm);\n              cachedState = (TermState) state.clone();\n              // this is fp after current term\n              cachedState.filePointer = in.getFilePointer();\n              termsCache.put(entryKey, cachedState);\n            }\n              \n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert state.ord == startOrd || !fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true): \"state.ord=\" + state.ord + \" startOrd=\" + startOrd + \" ir.isIndexTerm=\" + fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true) + \" state.docFreq=\" + state.docFreq;\n        }\n\n        return SeekStatus.END;\n      }\n\n","sourceOld":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(BytesRef term, boolean useCache) throws IOException {\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            state.copy(cachedState);\n            seekPending = true;\n            bytesReader.term.copy(term);\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (state.ord != -1) {\n          // we are positioned\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          }\n\n          if (cmp < 0 &&\n              fieldIndexReader.nextIndexTerm(state.ord, indexResult) &&\n              termComp.compare(indexResult.term, term) > 0) {\n            // Optimization: requested term is within the\n            // same index block we are now in; skip seeking\n            // (but do scanning):\n            doSeek = false;\n          }\n        }\n\n        // Used only for assert:\n        final long startOrd;\n\n        if (doSeek) {\n\n          // As index to find biggest index term that's <=\n          // our text:\n          fieldIndexReader.getIndexOffset(term, indexResult);\n\n          in.seek(indexResult.offset);\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexResult.term);\n          \n          state.ord = indexResult.position-1;\n          assert state.ord >= -1: \"ord=\" + state.ord + \" pos=\" + indexResult.position;\n\n          startOrd = indexResult.position;\n        } else {\n          startOrd = -1;\n        }\n\n        // Now scan:\n        while(next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n\n            if (doSeek && useCache) {\n              // Store in cache\n              FieldAndTerm entryKey = new FieldAndTerm(fieldTerm);\n              cachedState = (TermState) state.clone();\n              // this is fp after current term\n              cachedState.filePointer = in.getFilePointer();\n              termsCache.put(entryKey, cachedState);\n            }\n              \n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert state.ord == startOrd || !fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true): \"state.ord=\" + state.ord + \" startOrd=\" + startOrd + \" ir.isIndexTerm=\" + fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true) + \" state.docFreq=\" + state.docFreq;\n        }\n\n        return SeekStatus.END;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":4,"author":"Michael Busch","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/standard/StandardTermsDictReader.FieldReader.SegmentTermsEnum#seek(BytesRef,boolean).mjava","sourceNew":null,"sourceOld":"      /** Seeks until the first term that's >= the provided\n       *  text; returns SeekStatus.FOUND if the exact term\n       *  is found, SeekStatus.NOT_FOUND if a different term\n       *  was found, SeekStatus.END if we hit EOF */\n      @Override\n      public SeekStatus seek(BytesRef term, boolean useCache) throws IOException {\n        // Check cache\n        fieldTerm.term = term;\n        TermState cachedState;\n        if (useCache) {\n          cachedState = termsCache.get(fieldTerm);\n          if (cachedState != null) {\n            state.copy(cachedState);\n            seekPending = true;\n            bytesReader.term.copy(term);\n            return SeekStatus.FOUND;\n          }\n        } else {\n          cachedState = null;\n        }\n\n        boolean doSeek = true;\n\n        if (state.ord != -1) {\n          // we are positioned\n\n          final int cmp = termComp.compare(bytesReader.term, term);\n\n          if (cmp == 0) {\n            // already at the requested term\n            return SeekStatus.FOUND;\n          }\n\n          if (cmp < 0 &&\n              fieldIndexReader.nextIndexTerm(state.ord, indexResult) &&\n              termComp.compare(indexResult.term, term) > 0) {\n            // Optimization: requested term is within the\n            // same index block we are now in; skip seeking\n            // (but do scanning):\n            doSeek = false;\n          }\n        }\n\n        // Used only for assert:\n        final long startOrd;\n\n        if (doSeek) {\n\n          // As index to find biggest index term that's <=\n          // our text:\n          fieldIndexReader.getIndexOffset(term, indexResult);\n\n          in.seek(indexResult.offset);\n          seekPending = false;\n\n          // NOTE: the first next() after an index seek is\n          // wasteful, since it redundantly reads the same\n          // bytes into the buffer.  We could avoid storing\n          // those bytes in the primary file, but then when\n          // scanning over an index term we'd have to\n          // special case it:\n          bytesReader.reset(indexResult.term);\n          \n          state.ord = indexResult.position-1;\n          assert state.ord >= -1: \"ord=\" + state.ord;\n\n          startOrd = indexResult.position;\n        } else {\n          startOrd = -1;\n        }\n\n        // Now scan:\n        while(next() != null) {\n          final int cmp = termComp.compare(bytesReader.term, term);\n          if (cmp == 0) {\n\n            if (doSeek && useCache) {\n              // Store in cache\n              FieldAndTerm entryKey = new FieldAndTerm(fieldTerm);\n              cachedState = (TermState) state.clone();\n              // this is fp after current term\n              cachedState.filePointer = in.getFilePointer();\n              termsCache.put(entryKey, cachedState);\n            }\n              \n            return SeekStatus.FOUND;\n          } else if (cmp > 0) {\n            return SeekStatus.NOT_FOUND;\n          }\n          // The purpose of the terms dict index is to seek\n          // the enum to the closest index term before the\n          // term we are looking for.  So, we should never\n          // cross another index term (besides the first\n          // one) while we are scanning:\n          assert state.ord == startOrd || !fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true): \"state.ord=\" + state.ord + \" startOrd=\" + startOrd + \" ir.isIndexTerm=\" + fieldIndexReader.isIndexTerm(state.ord, state.docFreq, true) + \" state.docFreq=\" + state.docFreq;\n        }\n\n        return SeekStatus.END;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"552356653fbcba144cd337e76f4abe8885913817":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"2aafd88401639311b0404e67c94e829e123a0e45":["552356653fbcba144cd337e76f4abe8885913817"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["4305c035b0ff6b9c317310bca62a6673049f8d0b","99cf56f3a650b908f7017a72f9d23940418f8a52"],"b21422ff1d1d56499dec481f193b402e5e8def5b":["4305c035b0ff6b9c317310bca62a6673049f8d0b"],"4305c035b0ff6b9c317310bca62a6673049f8d0b":["2aafd88401639311b0404e67c94e829e123a0e45"],"99cf56f3a650b908f7017a72f9d23940418f8a52":["b21422ff1d1d56499dec481f193b402e5e8def5b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["99cf56f3a650b908f7017a72f9d23940418f8a52"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"552356653fbcba144cd337e76f4abe8885913817":["2aafd88401639311b0404e67c94e829e123a0e45"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["552356653fbcba144cd337e76f4abe8885913817"],"2aafd88401639311b0404e67c94e829e123a0e45":["4305c035b0ff6b9c317310bca62a6673049f8d0b"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":[],"4305c035b0ff6b9c317310bca62a6673049f8d0b":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","b21422ff1d1d56499dec481f193b402e5e8def5b"],"b21422ff1d1d56499dec481f193b402e5e8def5b":["99cf56f3a650b908f7017a72f9d23940418f8a52"],"99cf56f3a650b908f7017a72f9d23940418f8a52":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}