{"path":"lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","commits":[{"id":"bf317f41f8247db62a955791ebb8a5ab3e7c8d47","date":1366724337,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","pathOld":"/dev/null","sourceNew":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    long count = 0;\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long gcd = 0;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      for (Number nv : values) {\n        final long v = nv.longValue();\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n      }\n    } else {\n      for (@SuppressWarnings(\"unused\") Number nv : values) {\n        ++count;\n      }\n    }\n    \n    final long delta = maxValue - minValue;\n\n    final int format;\n    if (uniqueValues != null\n        && (delta < 0L || PackedInts.bitsRequired(uniqueValues.size() - 1) < PackedInts.bitsRequired(delta))\n        && count <= Integer.MAX_VALUE) {\n      format = TABLE_COMPRESSED;\n    } else if (gcd != 0 && gcd != 1) {\n      format = GCD_COMPRESSED;\n    } else {\n      format = DELTA_COMPRESSED;\n    }\n    meta.writeVInt(field.number);\n    meta.writeByte(DiskDocValuesFormat.NUMERIC);\n    meta.writeVInt(format);\n    meta.writeVInt(PackedInts.VERSION_CURRENT);\n    meta.writeLong(data.getFilePointer());\n    meta.writeVLong(count);\n    meta.writeVInt(BLOCK_SIZE);\n\n    switch (format) {\n      case GCD_COMPRESSED:\n        meta.writeLong(minValue);\n        meta.writeLong(gcd);\n        final BlockPackedWriter quotientWriter = new BlockPackedWriter(data, BLOCK_SIZE);\n        for (Number nv : values) {\n          quotientWriter.add((nv.longValue() - minValue) / gcd);\n        }\n        quotientWriter.finish();\n        break;\n      case DELTA_COMPRESSED:\n        final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n        for (Number nv : values) {\n          writer.add(nv.longValue());\n        }\n        writer.finish();\n        break;\n      case TABLE_COMPRESSED:\n        final Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n        final HashMap<Long,Integer> encode = new HashMap<Long,Integer>();\n        meta.writeVInt(decode.length);\n        for (int i = 0; i < decode.length; i++) {\n          meta.writeLong(decode[i]);\n          encode.put(decode[i], i);\n        }\n        final int bitsRequired = PackedInts.bitsRequired(uniqueValues.size() - 1);\n        final PackedInts.Writer ordsWriter = PackedInts.getWriterNoHeader(data, PackedInts.Format.PACKED, (int) count, bitsRequired, PackedInts.DEFAULT_BUFFER_SIZE);\n        for (Number nv : values) {\n          ordsWriter.add(encode.get(nv.longValue()));\n        }\n        ordsWriter.finish();\n        break;\n      default:\n        throw new AssertionError();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9faa42f41b6adb98daf009cf99a4ee239189e469","date":1376648738,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","sourceNew":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    long count = 0;\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long gcd = 0;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      // nocommit: impl null values (ideally smartly)\n      for (Number nv : values) {\n        final long v = nv == null ? 0 : nv.longValue();\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n      }\n    } else {\n      for (@SuppressWarnings(\"unused\") Number nv : values) {\n        ++count;\n      }\n    }\n    \n    final long delta = maxValue - minValue;\n\n    final int format;\n    if (uniqueValues != null\n        && (delta < 0L || PackedInts.bitsRequired(uniqueValues.size() - 1) < PackedInts.bitsRequired(delta))\n        && count <= Integer.MAX_VALUE) {\n      format = TABLE_COMPRESSED;\n    } else if (gcd != 0 && gcd != 1) {\n      format = GCD_COMPRESSED;\n    } else {\n      format = DELTA_COMPRESSED;\n    }\n    meta.writeVInt(field.number);\n    meta.writeByte(DiskDocValuesFormat.NUMERIC);\n    meta.writeVInt(format);\n    meta.writeVInt(PackedInts.VERSION_CURRENT);\n    meta.writeLong(data.getFilePointer());\n    meta.writeVLong(count);\n    meta.writeVInt(BLOCK_SIZE);\n\n    switch (format) {\n      case GCD_COMPRESSED:\n        meta.writeLong(minValue);\n        meta.writeLong(gcd);\n        final BlockPackedWriter quotientWriter = new BlockPackedWriter(data, BLOCK_SIZE);\n        for (Number nv : values) {\n          long value = nv == null ? 0 : nv.longValue();\n          quotientWriter.add((value - minValue) / gcd);\n        }\n        quotientWriter.finish();\n        break;\n      case DELTA_COMPRESSED:\n        final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n        for (Number nv : values) {\n          writer.add(nv == null ? 0 : nv.longValue());\n        }\n        writer.finish();\n        break;\n      case TABLE_COMPRESSED:\n        final Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n        final HashMap<Long,Integer> encode = new HashMap<Long,Integer>();\n        meta.writeVInt(decode.length);\n        for (int i = 0; i < decode.length; i++) {\n          meta.writeLong(decode[i]);\n          encode.put(decode[i], i);\n        }\n        final int bitsRequired = PackedInts.bitsRequired(uniqueValues.size() - 1);\n        final PackedInts.Writer ordsWriter = PackedInts.getWriterNoHeader(data, PackedInts.Format.PACKED, (int) count, bitsRequired, PackedInts.DEFAULT_BUFFER_SIZE);\n        for (Number nv : values) {\n          ordsWriter.add(encode.get(nv == null ? 0 : nv.longValue()));\n        }\n        ordsWriter.finish();\n        break;\n      default:\n        throw new AssertionError();\n    }\n  }\n\n","sourceOld":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    long count = 0;\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long gcd = 0;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      for (Number nv : values) {\n        final long v = nv.longValue();\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n      }\n    } else {\n      for (@SuppressWarnings(\"unused\") Number nv : values) {\n        ++count;\n      }\n    }\n    \n    final long delta = maxValue - minValue;\n\n    final int format;\n    if (uniqueValues != null\n        && (delta < 0L || PackedInts.bitsRequired(uniqueValues.size() - 1) < PackedInts.bitsRequired(delta))\n        && count <= Integer.MAX_VALUE) {\n      format = TABLE_COMPRESSED;\n    } else if (gcd != 0 && gcd != 1) {\n      format = GCD_COMPRESSED;\n    } else {\n      format = DELTA_COMPRESSED;\n    }\n    meta.writeVInt(field.number);\n    meta.writeByte(DiskDocValuesFormat.NUMERIC);\n    meta.writeVInt(format);\n    meta.writeVInt(PackedInts.VERSION_CURRENT);\n    meta.writeLong(data.getFilePointer());\n    meta.writeVLong(count);\n    meta.writeVInt(BLOCK_SIZE);\n\n    switch (format) {\n      case GCD_COMPRESSED:\n        meta.writeLong(minValue);\n        meta.writeLong(gcd);\n        final BlockPackedWriter quotientWriter = new BlockPackedWriter(data, BLOCK_SIZE);\n        for (Number nv : values) {\n          quotientWriter.add((nv.longValue() - minValue) / gcd);\n        }\n        quotientWriter.finish();\n        break;\n      case DELTA_COMPRESSED:\n        final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n        for (Number nv : values) {\n          writer.add(nv.longValue());\n        }\n        writer.finish();\n        break;\n      case TABLE_COMPRESSED:\n        final Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n        final HashMap<Long,Integer> encode = new HashMap<Long,Integer>();\n        meta.writeVInt(decode.length);\n        for (int i = 0; i < decode.length; i++) {\n          meta.writeLong(decode[i]);\n          encode.put(decode[i], i);\n        }\n        final int bitsRequired = PackedInts.bitsRequired(uniqueValues.size() - 1);\n        final PackedInts.Writer ordsWriter = PackedInts.getWriterNoHeader(data, PackedInts.Format.PACKED, (int) count, bitsRequired, PackedInts.DEFAULT_BUFFER_SIZE);\n        for (Number nv : values) {\n          ordsWriter.add(encode.get(nv.longValue()));\n        }\n        ordsWriter.finish();\n        break;\n      default:\n        throw new AssertionError();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"11a746437bc5c0a0b3df0337ed249c387c812871","date":1376687959,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","sourceNew":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    long count = 0;\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long gcd = 0;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      // nocommit: impl null values (ideally smartly)\n      for (Number nv : values) {\n        final long v = nv == null ? 0 : nv.longValue();\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n      }\n    } else {\n      for (@SuppressWarnings(\"unused\") Number nv : values) {\n        ++count;\n      }\n    }\n    \n    final long delta = maxValue - minValue;\n\n    final int format;\n    if (uniqueValues != null\n        && (delta < 0L || PackedInts.bitsRequired(uniqueValues.size() - 1) < PackedInts.bitsRequired(delta))\n        && count <= Integer.MAX_VALUE) {\n      format = TABLE_COMPRESSED;\n    } else if (gcd != 0 && gcd != 1) {\n      format = GCD_COMPRESSED;\n    } else {\n      format = DELTA_COMPRESSED;\n    }\n    meta.writeVInt(field.number);\n    meta.writeByte(Lucene45DocValuesFormat.NUMERIC);\n    meta.writeVInt(format);\n    meta.writeVInt(PackedInts.VERSION_CURRENT);\n    meta.writeLong(data.getFilePointer());\n    meta.writeVLong(count);\n    meta.writeVInt(BLOCK_SIZE);\n\n    switch (format) {\n      case GCD_COMPRESSED:\n        meta.writeLong(minValue);\n        meta.writeLong(gcd);\n        final BlockPackedWriter quotientWriter = new BlockPackedWriter(data, BLOCK_SIZE);\n        for (Number nv : values) {\n          long value = nv == null ? 0 : nv.longValue();\n          quotientWriter.add((value - minValue) / gcd);\n        }\n        quotientWriter.finish();\n        break;\n      case DELTA_COMPRESSED:\n        final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n        for (Number nv : values) {\n          writer.add(nv == null ? 0 : nv.longValue());\n        }\n        writer.finish();\n        break;\n      case TABLE_COMPRESSED:\n        final Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n        final HashMap<Long,Integer> encode = new HashMap<Long,Integer>();\n        meta.writeVInt(decode.length);\n        for (int i = 0; i < decode.length; i++) {\n          meta.writeLong(decode[i]);\n          encode.put(decode[i], i);\n        }\n        final int bitsRequired = PackedInts.bitsRequired(uniqueValues.size() - 1);\n        final PackedInts.Writer ordsWriter = PackedInts.getWriterNoHeader(data, PackedInts.Format.PACKED, (int) count, bitsRequired, PackedInts.DEFAULT_BUFFER_SIZE);\n        for (Number nv : values) {\n          ordsWriter.add(encode.get(nv == null ? 0 : nv.longValue()));\n        }\n        ordsWriter.finish();\n        break;\n      default:\n        throw new AssertionError();\n    }\n  }\n\n","sourceOld":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    long count = 0;\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long gcd = 0;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      // nocommit: impl null values (ideally smartly)\n      for (Number nv : values) {\n        final long v = nv == null ? 0 : nv.longValue();\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n      }\n    } else {\n      for (@SuppressWarnings(\"unused\") Number nv : values) {\n        ++count;\n      }\n    }\n    \n    final long delta = maxValue - minValue;\n\n    final int format;\n    if (uniqueValues != null\n        && (delta < 0L || PackedInts.bitsRequired(uniqueValues.size() - 1) < PackedInts.bitsRequired(delta))\n        && count <= Integer.MAX_VALUE) {\n      format = TABLE_COMPRESSED;\n    } else if (gcd != 0 && gcd != 1) {\n      format = GCD_COMPRESSED;\n    } else {\n      format = DELTA_COMPRESSED;\n    }\n    meta.writeVInt(field.number);\n    meta.writeByte(DiskDocValuesFormat.NUMERIC);\n    meta.writeVInt(format);\n    meta.writeVInt(PackedInts.VERSION_CURRENT);\n    meta.writeLong(data.getFilePointer());\n    meta.writeVLong(count);\n    meta.writeVInt(BLOCK_SIZE);\n\n    switch (format) {\n      case GCD_COMPRESSED:\n        meta.writeLong(minValue);\n        meta.writeLong(gcd);\n        final BlockPackedWriter quotientWriter = new BlockPackedWriter(data, BLOCK_SIZE);\n        for (Number nv : values) {\n          long value = nv == null ? 0 : nv.longValue();\n          quotientWriter.add((value - minValue) / gcd);\n        }\n        quotientWriter.finish();\n        break;\n      case DELTA_COMPRESSED:\n        final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n        for (Number nv : values) {\n          writer.add(nv == null ? 0 : nv.longValue());\n        }\n        writer.finish();\n        break;\n      case TABLE_COMPRESSED:\n        final Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n        final HashMap<Long,Integer> encode = new HashMap<Long,Integer>();\n        meta.writeVInt(decode.length);\n        for (int i = 0; i < decode.length; i++) {\n          meta.writeLong(decode[i]);\n          encode.put(decode[i], i);\n        }\n        final int bitsRequired = PackedInts.bitsRequired(uniqueValues.size() - 1);\n        final PackedInts.Writer ordsWriter = PackedInts.getWriterNoHeader(data, PackedInts.Format.PACKED, (int) count, bitsRequired, PackedInts.DEFAULT_BUFFER_SIZE);\n        for (Number nv : values) {\n          ordsWriter.add(encode.get(nv == null ? 0 : nv.longValue()));\n        }\n        ordsWriter.finish();\n        break;\n      default:\n        throw new AssertionError();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff","date":1377034255,"type":5,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","sourceNew":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    long count = 0;\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long gcd = 0;\n    boolean missing = false;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      for (Number nv : values) {\n        final long v;\n        if (nv == null) {\n          v = 0;\n          missing = true;\n        } else {\n          v = nv.longValue();\n        }\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n      }\n    } else {\n      for (@SuppressWarnings(\"unused\") Number nv : values) {\n        ++count;\n      }\n    }\n    \n    final long delta = maxValue - minValue;\n\n    final int format;\n    if (uniqueValues != null\n        && (delta < 0L || PackedInts.bitsRequired(uniqueValues.size() - 1) < PackedInts.bitsRequired(delta))\n        && count <= Integer.MAX_VALUE) {\n      format = TABLE_COMPRESSED;\n    } else if (gcd != 0 && gcd != 1) {\n      format = GCD_COMPRESSED;\n    } else {\n      format = DELTA_COMPRESSED;\n    }\n    meta.writeVInt(field.number);\n    meta.writeByte(Lucene45DocValuesFormat.NUMERIC);\n    meta.writeVInt(format);\n    if (missing) {\n      meta.writeLong(data.getFilePointer());\n      writeMissingBitset(values);\n    } else {\n      meta.writeLong(-1L);\n    }\n    meta.writeVInt(PackedInts.VERSION_CURRENT);\n    meta.writeLong(data.getFilePointer());\n    meta.writeVLong(count);\n    meta.writeVInt(BLOCK_SIZE);\n\n    switch (format) {\n      case GCD_COMPRESSED:\n        meta.writeLong(minValue);\n        meta.writeLong(gcd);\n        final BlockPackedWriter quotientWriter = new BlockPackedWriter(data, BLOCK_SIZE);\n        for (Number nv : values) {\n          long value = nv == null ? 0 : nv.longValue();\n          quotientWriter.add((value - minValue) / gcd);\n        }\n        quotientWriter.finish();\n        break;\n      case DELTA_COMPRESSED:\n        final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n        for (Number nv : values) {\n          writer.add(nv == null ? 0 : nv.longValue());\n        }\n        writer.finish();\n        break;\n      case TABLE_COMPRESSED:\n        final Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n        final HashMap<Long,Integer> encode = new HashMap<Long,Integer>();\n        meta.writeVInt(decode.length);\n        for (int i = 0; i < decode.length; i++) {\n          meta.writeLong(decode[i]);\n          encode.put(decode[i], i);\n        }\n        final int bitsRequired = PackedInts.bitsRequired(uniqueValues.size() - 1);\n        final PackedInts.Writer ordsWriter = PackedInts.getWriterNoHeader(data, PackedInts.Format.PACKED, (int) count, bitsRequired, PackedInts.DEFAULT_BUFFER_SIZE);\n        for (Number nv : values) {\n          ordsWriter.add(encode.get(nv == null ? 0 : nv.longValue()));\n        }\n        ordsWriter.finish();\n        break;\n      default:\n        throw new AssertionError();\n    }\n  }\n\n","sourceOld":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    long count = 0;\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long gcd = 0;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      for (Number nv : values) {\n        final long v = nv.longValue();\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n      }\n    } else {\n      for (@SuppressWarnings(\"unused\") Number nv : values) {\n        ++count;\n      }\n    }\n    \n    final long delta = maxValue - minValue;\n\n    final int format;\n    if (uniqueValues != null\n        && (delta < 0L || PackedInts.bitsRequired(uniqueValues.size() - 1) < PackedInts.bitsRequired(delta))\n        && count <= Integer.MAX_VALUE) {\n      format = TABLE_COMPRESSED;\n    } else if (gcd != 0 && gcd != 1) {\n      format = GCD_COMPRESSED;\n    } else {\n      format = DELTA_COMPRESSED;\n    }\n    meta.writeVInt(field.number);\n    meta.writeByte(DiskDocValuesFormat.NUMERIC);\n    meta.writeVInt(format);\n    meta.writeVInt(PackedInts.VERSION_CURRENT);\n    meta.writeLong(data.getFilePointer());\n    meta.writeVLong(count);\n    meta.writeVInt(BLOCK_SIZE);\n\n    switch (format) {\n      case GCD_COMPRESSED:\n        meta.writeLong(minValue);\n        meta.writeLong(gcd);\n        final BlockPackedWriter quotientWriter = new BlockPackedWriter(data, BLOCK_SIZE);\n        for (Number nv : values) {\n          quotientWriter.add((nv.longValue() - minValue) / gcd);\n        }\n        quotientWriter.finish();\n        break;\n      case DELTA_COMPRESSED:\n        final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n        for (Number nv : values) {\n          writer.add(nv.longValue());\n        }\n        writer.finish();\n        break;\n      case TABLE_COMPRESSED:\n        final Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n        final HashMap<Long,Integer> encode = new HashMap<Long,Integer>();\n        meta.writeVInt(decode.length);\n        for (int i = 0; i < decode.length; i++) {\n          meta.writeLong(decode[i]);\n          encode.put(decode[i], i);\n        }\n        final int bitsRequired = PackedInts.bitsRequired(uniqueValues.size() - 1);\n        final PackedInts.Writer ordsWriter = PackedInts.getWriterNoHeader(data, PackedInts.Format.PACKED, (int) count, bitsRequired, PackedInts.DEFAULT_BUFFER_SIZE);\n        for (Number nv : values) {\n          ordsWriter.add(encode.get(nv.longValue()));\n        }\n        ordsWriter.finish();\n        break;\n      default:\n        throw new AssertionError();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":4,"author":"Han Jiang","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesConsumer#addNumericField(FieldInfo,Iterable[Number],boolean).mjava","sourceNew":null,"sourceOld":"  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {\n    long count = 0;\n    long minValue = Long.MAX_VALUE;\n    long maxValue = Long.MIN_VALUE;\n    long gcd = 0;\n    // TODO: more efficient?\n    HashSet<Long> uniqueValues = null;\n    if (optimizeStorage) {\n      uniqueValues = new HashSet<>();\n\n      for (Number nv : values) {\n        final long v = nv.longValue();\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (count != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minValue);\n          }\n        }\n\n        minValue = Math.min(minValue, v);\n        maxValue = Math.max(maxValue, v);\n\n        if (uniqueValues != null) {\n          if (uniqueValues.add(v)) {\n            if (uniqueValues.size() > 256) {\n              uniqueValues = null;\n            }\n          }\n        }\n\n        ++count;\n      }\n    } else {\n      for (@SuppressWarnings(\"unused\") Number nv : values) {\n        ++count;\n      }\n    }\n    \n    final long delta = maxValue - minValue;\n\n    final int format;\n    if (uniqueValues != null\n        && (delta < 0L || PackedInts.bitsRequired(uniqueValues.size() - 1) < PackedInts.bitsRequired(delta))\n        && count <= Integer.MAX_VALUE) {\n      format = TABLE_COMPRESSED;\n    } else if (gcd != 0 && gcd != 1) {\n      format = GCD_COMPRESSED;\n    } else {\n      format = DELTA_COMPRESSED;\n    }\n    meta.writeVInt(field.number);\n    meta.writeByte(DiskDocValuesFormat.NUMERIC);\n    meta.writeVInt(format);\n    meta.writeVInt(PackedInts.VERSION_CURRENT);\n    meta.writeLong(data.getFilePointer());\n    meta.writeVLong(count);\n    meta.writeVInt(BLOCK_SIZE);\n\n    switch (format) {\n      case GCD_COMPRESSED:\n        meta.writeLong(minValue);\n        meta.writeLong(gcd);\n        final BlockPackedWriter quotientWriter = new BlockPackedWriter(data, BLOCK_SIZE);\n        for (Number nv : values) {\n          quotientWriter.add((nv.longValue() - minValue) / gcd);\n        }\n        quotientWriter.finish();\n        break;\n      case DELTA_COMPRESSED:\n        final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);\n        for (Number nv : values) {\n          writer.add(nv.longValue());\n        }\n        writer.finish();\n        break;\n      case TABLE_COMPRESSED:\n        final Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);\n        final HashMap<Long,Integer> encode = new HashMap<Long,Integer>();\n        meta.writeVInt(decode.length);\n        for (int i = 0; i < decode.length; i++) {\n          meta.writeLong(decode[i]);\n          encode.put(decode[i], i);\n        }\n        final int bitsRequired = PackedInts.bitsRequired(uniqueValues.size() - 1);\n        final PackedInts.Writer ordsWriter = PackedInts.getWriterNoHeader(data, PackedInts.Format.PACKED, (int) count, bitsRequired, PackedInts.DEFAULT_BUFFER_SIZE);\n        for (Number nv : values) {\n          ordsWriter.add(encode.get(nv.longValue()));\n        }\n        ordsWriter.finish();\n        break;\n      default:\n        throw new AssertionError();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"11a746437bc5c0a0b3df0337ed249c387c812871":["9faa42f41b6adb98daf009cf99a4ee239189e469"],"9faa42f41b6adb98daf009cf99a4ee239189e469":["bf317f41f8247db62a955791ebb8a5ab3e7c8d47"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["bf317f41f8247db62a955791ebb8a5ab3e7c8d47","e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"bf317f41f8247db62a955791ebb8a5ab3e7c8d47":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff":["bf317f41f8247db62a955791ebb8a5ab3e7c8d47","11a746437bc5c0a0b3df0337ed249c387c812871"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff"]},"commit2Childs":{"11a746437bc5c0a0b3df0337ed249c387c812871":["e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff"],"9faa42f41b6adb98daf009cf99a4ee239189e469":["11a746437bc5c0a0b3df0337ed249c387c812871"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["bf317f41f8247db62a955791ebb8a5ab3e7c8d47"],"bf317f41f8247db62a955791ebb8a5ab3e7c8d47":["9faa42f41b6adb98daf009cf99a4ee239189e469","3dffec77fb8f7d0e9ca4869dddd6af94528b4576","e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff"],"e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}