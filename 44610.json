{"path":"solr/core/src/java/org/apache/solr/search/DocSetUtil#createDocSet(SolrIndexSearcher,Term).mjava","commits":[{"id":"6727dd701b30630840235b6788bb5c728d20bbfd","date":1439421226,"type":0,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/DocSetUtil#createDocSet(SolrIndexSearcher,Term).mjava","pathOld":"/dev/null","sourceNew":"  public static DocSet createDocSet(SolrIndexSearcher searcher, Term term) throws IOException {\n    DirectoryReader reader = searcher.getRawReader();  // raw reader to avoid extra wrapping overhead\n    int maxDoc = searcher.getIndexReader().maxDoc();\n    int smallSetSize = (maxDoc >> 6) + 5;\n\n\n    String field = term.field();\n    BytesRef termVal = term.bytes();\n\n    int maxCount = 0;\n    int firstReader = -1;\n    List<LeafReaderContext> leaves = reader.leaves();\n    PostingsEnum[] postList = new PostingsEnum[leaves.size()]; // use array for slightly higher scanning cost, but fewer memory allocations\n    for (LeafReaderContext ctx : leaves) {\n      assert leaves.get(ctx.ord) == ctx;\n      LeafReader r = ctx.reader();\n      Fields f = r.fields();\n      Terms t = f.terms(field);\n      if (t == null) continue;  // field is missing\n      TermsEnum te = t.iterator();\n      if (te.seekExact(termVal)) {\n        maxCount += te.docFreq();\n        postList[ctx.ord] = te.postings(null, PostingsEnum.NONE);\n        if (firstReader < 0) firstReader = ctx.ord;\n      }\n    }\n\n    if (maxCount == 0) {\n      return DocSet.EMPTY;\n    }\n\n    if (maxCount <= smallSetSize) {\n      return createSmallSet(leaves, postList, maxCount, firstReader);\n    }\n\n    return createBigSet(leaves, postList, maxDoc, firstReader);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ca0cc2f173b07ff75ca951e017f5dd1f319fdad0","date":1442027674,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/DocSetUtil#createDocSet(SolrIndexSearcher,Term).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/DocSetUtil#createDocSet(SolrIndexSearcher,Term).mjava","sourceNew":"  public static DocSet createDocSet(SolrIndexSearcher searcher, Term term) throws IOException {\n    DirectoryReader reader = searcher.getRawReader();  // raw reader to avoid extra wrapping overhead\n    int maxDoc = searcher.getIndexReader().maxDoc();\n    int smallSetSize = smallSetSize(maxDoc);\n\n\n    String field = term.field();\n    BytesRef termVal = term.bytes();\n\n    int maxCount = 0;\n    int firstReader = -1;\n    List<LeafReaderContext> leaves = reader.leaves();\n    PostingsEnum[] postList = new PostingsEnum[leaves.size()]; // use array for slightly higher scanning cost, but fewer memory allocations\n    for (LeafReaderContext ctx : leaves) {\n      assert leaves.get(ctx.ord) == ctx;\n      LeafReader r = ctx.reader();\n      Fields f = r.fields();\n      Terms t = f.terms(field);\n      if (t == null) continue;  // field is missing\n      TermsEnum te = t.iterator();\n      if (te.seekExact(termVal)) {\n        maxCount += te.docFreq();\n        postList[ctx.ord] = te.postings(null, PostingsEnum.NONE);\n        if (firstReader < 0) firstReader = ctx.ord;\n      }\n    }\n\n    if (maxCount == 0) {\n      return DocSet.EMPTY;\n    }\n\n    if (maxCount <= smallSetSize) {\n      return createSmallSet(leaves, postList, maxCount, firstReader);\n    }\n\n    return createBigSet(leaves, postList, maxDoc, firstReader);\n  }\n\n","sourceOld":"  public static DocSet createDocSet(SolrIndexSearcher searcher, Term term) throws IOException {\n    DirectoryReader reader = searcher.getRawReader();  // raw reader to avoid extra wrapping overhead\n    int maxDoc = searcher.getIndexReader().maxDoc();\n    int smallSetSize = (maxDoc >> 6) + 5;\n\n\n    String field = term.field();\n    BytesRef termVal = term.bytes();\n\n    int maxCount = 0;\n    int firstReader = -1;\n    List<LeafReaderContext> leaves = reader.leaves();\n    PostingsEnum[] postList = new PostingsEnum[leaves.size()]; // use array for slightly higher scanning cost, but fewer memory allocations\n    for (LeafReaderContext ctx : leaves) {\n      assert leaves.get(ctx.ord) == ctx;\n      LeafReader r = ctx.reader();\n      Fields f = r.fields();\n      Terms t = f.terms(field);\n      if (t == null) continue;  // field is missing\n      TermsEnum te = t.iterator();\n      if (te.seekExact(termVal)) {\n        maxCount += te.docFreq();\n        postList[ctx.ord] = te.postings(null, PostingsEnum.NONE);\n        if (firstReader < 0) firstReader = ctx.ord;\n      }\n    }\n\n    if (maxCount == 0) {\n      return DocSet.EMPTY;\n    }\n\n    if (maxCount <= smallSetSize) {\n      return createSmallSet(leaves, postList, maxCount, firstReader);\n    }\n\n    return createBigSet(leaves, postList, maxDoc, firstReader);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0cb5245d4a5c7ba59bbfdedb7ace8a9fbd36ada3","date":1485881524,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/DocSetUtil#createDocSet(SolrIndexSearcher,Term).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/DocSetUtil#createDocSet(SolrIndexSearcher,Term).mjava","sourceNew":"  public static DocSet createDocSet(SolrIndexSearcher searcher, Term term) throws IOException {\n    DirectoryReader reader = searcher.getRawReader();  // raw reader to avoid extra wrapping overhead\n    int maxDoc = searcher.getIndexReader().maxDoc();\n    int smallSetSize = smallSetSize(maxDoc);\n\n    String field = term.field();\n    BytesRef termVal = term.bytes();\n\n    int maxCount = 0;\n    int firstReader = -1;\n    List<LeafReaderContext> leaves = reader.leaves();\n    PostingsEnum[] postList = new PostingsEnum[leaves.size()]; // use array for slightly higher scanning cost, but fewer memory allocations\n    for (LeafReaderContext ctx : leaves) {\n      assert leaves.get(ctx.ord) == ctx;\n      LeafReader r = ctx.reader();\n      Fields f = r.fields();\n      Terms t = f.terms(field);\n      if (t == null) continue;  // field is missing\n      TermsEnum te = t.iterator();\n      if (te.seekExact(termVal)) {\n        maxCount += te.docFreq();\n        postList[ctx.ord] = te.postings(null, PostingsEnum.NONE);\n        if (firstReader < 0) firstReader = ctx.ord;\n      }\n    }\n\n    DocSet answer = null;\n    if (maxCount == 0) {\n      answer = DocSet.EMPTY;\n    } else if (maxCount <= smallSetSize) {\n      answer = createSmallSet(leaves, postList, maxCount, firstReader);\n    } else {\n      answer = createBigSet(leaves, postList, maxDoc, firstReader);\n    }\n\n    return DocSetUtil.getDocSet( answer, searcher );\n  }\n\n","sourceOld":"  public static DocSet createDocSet(SolrIndexSearcher searcher, Term term) throws IOException {\n    DirectoryReader reader = searcher.getRawReader();  // raw reader to avoid extra wrapping overhead\n    int maxDoc = searcher.getIndexReader().maxDoc();\n    int smallSetSize = smallSetSize(maxDoc);\n\n\n    String field = term.field();\n    BytesRef termVal = term.bytes();\n\n    int maxCount = 0;\n    int firstReader = -1;\n    List<LeafReaderContext> leaves = reader.leaves();\n    PostingsEnum[] postList = new PostingsEnum[leaves.size()]; // use array for slightly higher scanning cost, but fewer memory allocations\n    for (LeafReaderContext ctx : leaves) {\n      assert leaves.get(ctx.ord) == ctx;\n      LeafReader r = ctx.reader();\n      Fields f = r.fields();\n      Terms t = f.terms(field);\n      if (t == null) continue;  // field is missing\n      TermsEnum te = t.iterator();\n      if (te.seekExact(termVal)) {\n        maxCount += te.docFreq();\n        postList[ctx.ord] = te.postings(null, PostingsEnum.NONE);\n        if (firstReader < 0) firstReader = ctx.ord;\n      }\n    }\n\n    if (maxCount == 0) {\n      return DocSet.EMPTY;\n    }\n\n    if (maxCount <= smallSetSize) {\n      return createSmallSet(leaves, postList, maxCount, firstReader);\n    }\n\n    return createBigSet(leaves, postList, maxDoc, firstReader);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c3523a0ab04c3002eee3896c75ea5f10f388bcc","date":1485968422,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/DocSetUtil#createDocSet(SolrIndexSearcher,Term).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/DocSetUtil#createDocSet(SolrIndexSearcher,Term).mjava","sourceNew":"  public static DocSet createDocSet(SolrIndexSearcher searcher, Term term) throws IOException {\n    DirectoryReader reader = searcher.getRawReader();  // raw reader to avoid extra wrapping overhead\n    int maxDoc = searcher.getIndexReader().maxDoc();\n    int smallSetSize = smallSetSize(maxDoc);\n\n    String field = term.field();\n    BytesRef termVal = term.bytes();\n\n    int maxCount = 0;\n    int firstReader = -1;\n    List<LeafReaderContext> leaves = reader.leaves();\n    PostingsEnum[] postList = new PostingsEnum[leaves.size()]; // use array for slightly higher scanning cost, but fewer memory allocations\n    for (LeafReaderContext ctx : leaves) {\n      assert leaves.get(ctx.ord) == ctx;\n      LeafReader r = ctx.reader();\n      Fields f = r.fields();\n      Terms t = f.terms(field);\n      if (t == null) continue;  // field is missing\n      TermsEnum te = t.iterator();\n      if (te.seekExact(termVal)) {\n        maxCount += te.docFreq();\n        postList[ctx.ord] = te.postings(null, PostingsEnum.NONE);\n        if (firstReader < 0) firstReader = ctx.ord;\n      }\n    }\n\n    DocSet answer = null;\n    if (maxCount == 0) {\n      answer = DocSet.EMPTY;\n    } else if (maxCount <= smallSetSize) {\n      answer = createSmallSet(leaves, postList, maxCount, firstReader);\n    } else {\n      answer = createBigSet(leaves, postList, maxDoc, firstReader);\n    }\n\n    return DocSetUtil.getDocSet( answer, searcher );\n  }\n\n","sourceOld":"  public static DocSet createDocSet(SolrIndexSearcher searcher, Term term) throws IOException {\n    DirectoryReader reader = searcher.getRawReader();  // raw reader to avoid extra wrapping overhead\n    int maxDoc = searcher.getIndexReader().maxDoc();\n    int smallSetSize = smallSetSize(maxDoc);\n\n\n    String field = term.field();\n    BytesRef termVal = term.bytes();\n\n    int maxCount = 0;\n    int firstReader = -1;\n    List<LeafReaderContext> leaves = reader.leaves();\n    PostingsEnum[] postList = new PostingsEnum[leaves.size()]; // use array for slightly higher scanning cost, but fewer memory allocations\n    for (LeafReaderContext ctx : leaves) {\n      assert leaves.get(ctx.ord) == ctx;\n      LeafReader r = ctx.reader();\n      Fields f = r.fields();\n      Terms t = f.terms(field);\n      if (t == null) continue;  // field is missing\n      TermsEnum te = t.iterator();\n      if (te.seekExact(termVal)) {\n        maxCount += te.docFreq();\n        postList[ctx.ord] = te.postings(null, PostingsEnum.NONE);\n        if (firstReader < 0) firstReader = ctx.ord;\n      }\n    }\n\n    if (maxCount == 0) {\n      return DocSet.EMPTY;\n    }\n\n    if (maxCount <= smallSetSize) {\n      return createSmallSet(leaves, postList, maxCount, firstReader);\n    }\n\n    return createBigSet(leaves, postList, maxDoc, firstReader);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b","date":1497408244,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/DocSetUtil#createDocSet(SolrIndexSearcher,Term).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/DocSetUtil#createDocSet(SolrIndexSearcher,Term).mjava","sourceNew":"  public static DocSet createDocSet(SolrIndexSearcher searcher, Term term) throws IOException {\n    DirectoryReader reader = searcher.getRawReader();  // raw reader to avoid extra wrapping overhead\n    int maxDoc = searcher.getIndexReader().maxDoc();\n    int smallSetSize = smallSetSize(maxDoc);\n\n    String field = term.field();\n    BytesRef termVal = term.bytes();\n\n    int maxCount = 0;\n    int firstReader = -1;\n    List<LeafReaderContext> leaves = reader.leaves();\n    PostingsEnum[] postList = new PostingsEnum[leaves.size()]; // use array for slightly higher scanning cost, but fewer memory allocations\n    for (LeafReaderContext ctx : leaves) {\n      assert leaves.get(ctx.ord) == ctx;\n      LeafReader r = ctx.reader();\n      Terms t = r.terms(field);\n      if (t == null) continue;  // field is missing\n      TermsEnum te = t.iterator();\n      if (te.seekExact(termVal)) {\n        maxCount += te.docFreq();\n        postList[ctx.ord] = te.postings(null, PostingsEnum.NONE);\n        if (firstReader < 0) firstReader = ctx.ord;\n      }\n    }\n\n    DocSet answer = null;\n    if (maxCount == 0) {\n      answer = DocSet.EMPTY;\n    } else if (maxCount <= smallSetSize) {\n      answer = createSmallSet(leaves, postList, maxCount, firstReader);\n    } else {\n      answer = createBigSet(leaves, postList, maxDoc, firstReader);\n    }\n\n    return DocSetUtil.getDocSet( answer, searcher );\n  }\n\n","sourceOld":"  public static DocSet createDocSet(SolrIndexSearcher searcher, Term term) throws IOException {\n    DirectoryReader reader = searcher.getRawReader();  // raw reader to avoid extra wrapping overhead\n    int maxDoc = searcher.getIndexReader().maxDoc();\n    int smallSetSize = smallSetSize(maxDoc);\n\n    String field = term.field();\n    BytesRef termVal = term.bytes();\n\n    int maxCount = 0;\n    int firstReader = -1;\n    List<LeafReaderContext> leaves = reader.leaves();\n    PostingsEnum[] postList = new PostingsEnum[leaves.size()]; // use array for slightly higher scanning cost, but fewer memory allocations\n    for (LeafReaderContext ctx : leaves) {\n      assert leaves.get(ctx.ord) == ctx;\n      LeafReader r = ctx.reader();\n      Fields f = r.fields();\n      Terms t = f.terms(field);\n      if (t == null) continue;  // field is missing\n      TermsEnum te = t.iterator();\n      if (te.seekExact(termVal)) {\n        maxCount += te.docFreq();\n        postList[ctx.ord] = te.postings(null, PostingsEnum.NONE);\n        if (firstReader < 0) firstReader = ctx.ord;\n      }\n    }\n\n    DocSet answer = null;\n    if (maxCount == 0) {\n      answer = DocSet.EMPTY;\n    } else if (maxCount <= smallSetSize) {\n      answer = createSmallSet(leaves, postList, maxCount, firstReader);\n    } else {\n      answer = createBigSet(leaves, postList, maxDoc, firstReader);\n    }\n\n    return DocSetUtil.getDocSet( answer, searcher );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","date":1498028748,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/DocSetUtil#createDocSet(SolrIndexSearcher,Term).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/DocSetUtil#createDocSet(SolrIndexSearcher,Term).mjava","sourceNew":"  public static DocSet createDocSet(SolrIndexSearcher searcher, Term term) throws IOException {\n    DirectoryReader reader = searcher.getRawReader();  // raw reader to avoid extra wrapping overhead\n    int maxDoc = searcher.getIndexReader().maxDoc();\n    int smallSetSize = smallSetSize(maxDoc);\n\n    String field = term.field();\n    BytesRef termVal = term.bytes();\n\n    int maxCount = 0;\n    int firstReader = -1;\n    List<LeafReaderContext> leaves = reader.leaves();\n    PostingsEnum[] postList = new PostingsEnum[leaves.size()]; // use array for slightly higher scanning cost, but fewer memory allocations\n    for (LeafReaderContext ctx : leaves) {\n      assert leaves.get(ctx.ord) == ctx;\n      LeafReader r = ctx.reader();\n      Terms t = r.terms(field);\n      if (t == null) continue;  // field is missing\n      TermsEnum te = t.iterator();\n      if (te.seekExact(termVal)) {\n        maxCount += te.docFreq();\n        postList[ctx.ord] = te.postings(null, PostingsEnum.NONE);\n        if (firstReader < 0) firstReader = ctx.ord;\n      }\n    }\n\n    DocSet answer = null;\n    if (maxCount == 0) {\n      answer = DocSet.EMPTY;\n    } else if (maxCount <= smallSetSize) {\n      answer = createSmallSet(leaves, postList, maxCount, firstReader);\n    } else {\n      answer = createBigSet(leaves, postList, maxDoc, firstReader);\n    }\n\n    return DocSetUtil.getDocSet( answer, searcher );\n  }\n\n","sourceOld":"  public static DocSet createDocSet(SolrIndexSearcher searcher, Term term) throws IOException {\n    DirectoryReader reader = searcher.getRawReader();  // raw reader to avoid extra wrapping overhead\n    int maxDoc = searcher.getIndexReader().maxDoc();\n    int smallSetSize = smallSetSize(maxDoc);\n\n    String field = term.field();\n    BytesRef termVal = term.bytes();\n\n    int maxCount = 0;\n    int firstReader = -1;\n    List<LeafReaderContext> leaves = reader.leaves();\n    PostingsEnum[] postList = new PostingsEnum[leaves.size()]; // use array for slightly higher scanning cost, but fewer memory allocations\n    for (LeafReaderContext ctx : leaves) {\n      assert leaves.get(ctx.ord) == ctx;\n      LeafReader r = ctx.reader();\n      Fields f = r.fields();\n      Terms t = f.terms(field);\n      if (t == null) continue;  // field is missing\n      TermsEnum te = t.iterator();\n      if (te.seekExact(termVal)) {\n        maxCount += te.docFreq();\n        postList[ctx.ord] = te.postings(null, PostingsEnum.NONE);\n        if (firstReader < 0) firstReader = ctx.ord;\n      }\n    }\n\n    DocSet answer = null;\n    if (maxCount == 0) {\n      answer = DocSet.EMPTY;\n    } else if (maxCount <= smallSetSize) {\n      answer = createSmallSet(leaves, postList, maxCount, firstReader);\n    } else {\n      answer = createBigSet(leaves, postList, maxDoc, firstReader);\n    }\n\n    return DocSetUtil.getDocSet( answer, searcher );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/DocSetUtil#createDocSet(SolrIndexSearcher,Term).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/DocSetUtil#createDocSet(SolrIndexSearcher,Term).mjava","sourceNew":"  public static DocSet createDocSet(SolrIndexSearcher searcher, Term term) throws IOException {\n    DirectoryReader reader = searcher.getRawReader();  // raw reader to avoid extra wrapping overhead\n    int maxDoc = searcher.getIndexReader().maxDoc();\n    int smallSetSize = smallSetSize(maxDoc);\n\n    String field = term.field();\n    BytesRef termVal = term.bytes();\n\n    int maxCount = 0;\n    int firstReader = -1;\n    List<LeafReaderContext> leaves = reader.leaves();\n    PostingsEnum[] postList = new PostingsEnum[leaves.size()]; // use array for slightly higher scanning cost, but fewer memory allocations\n    for (LeafReaderContext ctx : leaves) {\n      assert leaves.get(ctx.ord) == ctx;\n      LeafReader r = ctx.reader();\n      Terms t = r.terms(field);\n      if (t == null) continue;  // field is missing\n      TermsEnum te = t.iterator();\n      if (te.seekExact(termVal)) {\n        maxCount += te.docFreq();\n        postList[ctx.ord] = te.postings(null, PostingsEnum.NONE);\n        if (firstReader < 0) firstReader = ctx.ord;\n      }\n    }\n\n    DocSet answer = null;\n    if (maxCount == 0) {\n      answer = DocSet.EMPTY;\n    } else if (maxCount <= smallSetSize) {\n      answer = createSmallSet(leaves, postList, maxCount, firstReader);\n    } else {\n      answer = createBigSet(leaves, postList, maxDoc, firstReader);\n    }\n\n    return DocSetUtil.getDocSet( answer, searcher );\n  }\n\n","sourceOld":"  public static DocSet createDocSet(SolrIndexSearcher searcher, Term term) throws IOException {\n    DirectoryReader reader = searcher.getRawReader();  // raw reader to avoid extra wrapping overhead\n    int maxDoc = searcher.getIndexReader().maxDoc();\n    int smallSetSize = smallSetSize(maxDoc);\n\n    String field = term.field();\n    BytesRef termVal = term.bytes();\n\n    int maxCount = 0;\n    int firstReader = -1;\n    List<LeafReaderContext> leaves = reader.leaves();\n    PostingsEnum[] postList = new PostingsEnum[leaves.size()]; // use array for slightly higher scanning cost, but fewer memory allocations\n    for (LeafReaderContext ctx : leaves) {\n      assert leaves.get(ctx.ord) == ctx;\n      LeafReader r = ctx.reader();\n      Fields f = r.fields();\n      Terms t = f.terms(field);\n      if (t == null) continue;  // field is missing\n      TermsEnum te = t.iterator();\n      if (te.seekExact(termVal)) {\n        maxCount += te.docFreq();\n        postList[ctx.ord] = te.postings(null, PostingsEnum.NONE);\n        if (firstReader < 0) firstReader = ctx.ord;\n      }\n    }\n\n    DocSet answer = null;\n    if (maxCount == 0) {\n      answer = DocSet.EMPTY;\n    } else if (maxCount <= smallSetSize) {\n      answer = createSmallSet(leaves, postList, maxCount, firstReader);\n    } else {\n      answer = createBigSet(leaves, postList, maxDoc, firstReader);\n    }\n\n    return DocSetUtil.getDocSet( answer, searcher );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d67f43c293f81a92c10131e75761f4e4e968b06c","date":1584534689,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/DocSetUtil#createDocSet(SolrIndexSearcher,Term).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/DocSetUtil#createDocSet(SolrIndexSearcher,Term).mjava","sourceNew":"  public static DocSet createDocSet(SolrIndexSearcher searcher, Term term) throws IOException {\n    DirectoryReader reader = searcher.getRawReader();  // raw reader to avoid extra wrapping overhead\n    int maxDoc = searcher.getIndexReader().maxDoc();\n    int smallSetSize = smallSetSize(maxDoc);\n\n    String field = term.field();\n    BytesRef termVal = term.bytes();\n\n    int maxCount = 0;\n    int firstReader = -1;\n    List<LeafReaderContext> leaves = reader.leaves();\n    PostingsEnum[] postList = new PostingsEnum[leaves.size()]; // use array for slightly higher scanning cost, but fewer memory allocations\n    for (LeafReaderContext ctx : leaves) {\n      assert leaves.get(ctx.ord) == ctx;\n      LeafReader r = ctx.reader();\n      Terms t = r.terms(field);\n      if (t == null) continue;  // field is missing\n      TermsEnum te = t.iterator();\n      if (te.seekExact(termVal)) {\n        maxCount += te.docFreq();\n        postList[ctx.ord] = te.postings(null, PostingsEnum.NONE);\n        if (firstReader < 0) firstReader = ctx.ord;\n      }\n    }\n\n    DocSet answer = null;\n    if (maxCount == 0) {\n      answer = DocSet.empty();\n    } else if (maxCount <= smallSetSize) {\n      answer = createSmallSet(leaves, postList, maxCount, firstReader);\n    } else {\n      answer = createBigSet(leaves, postList, maxDoc, firstReader);\n    }\n\n    return DocSetUtil.getDocSet( answer, searcher );\n  }\n\n","sourceOld":"  public static DocSet createDocSet(SolrIndexSearcher searcher, Term term) throws IOException {\n    DirectoryReader reader = searcher.getRawReader();  // raw reader to avoid extra wrapping overhead\n    int maxDoc = searcher.getIndexReader().maxDoc();\n    int smallSetSize = smallSetSize(maxDoc);\n\n    String field = term.field();\n    BytesRef termVal = term.bytes();\n\n    int maxCount = 0;\n    int firstReader = -1;\n    List<LeafReaderContext> leaves = reader.leaves();\n    PostingsEnum[] postList = new PostingsEnum[leaves.size()]; // use array for slightly higher scanning cost, but fewer memory allocations\n    for (LeafReaderContext ctx : leaves) {\n      assert leaves.get(ctx.ord) == ctx;\n      LeafReader r = ctx.reader();\n      Terms t = r.terms(field);\n      if (t == null) continue;  // field is missing\n      TermsEnum te = t.iterator();\n      if (te.seekExact(termVal)) {\n        maxCount += te.docFreq();\n        postList[ctx.ord] = te.postings(null, PostingsEnum.NONE);\n        if (firstReader < 0) firstReader = ctx.ord;\n      }\n    }\n\n    DocSet answer = null;\n    if (maxCount == 0) {\n      answer = DocSet.EMPTY;\n    } else if (maxCount <= smallSetSize) {\n      answer = createSmallSet(leaves, postList, maxCount, firstReader);\n    } else {\n      answer = createBigSet(leaves, postList, maxDoc, firstReader);\n    }\n\n    return DocSetUtil.getDocSet( answer, searcher );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b":["0cb5245d4a5c7ba59bbfdedb7ace8a9fbd36ada3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"7c3523a0ab04c3002eee3896c75ea5f10f388bcc":["ca0cc2f173b07ff75ca951e017f5dd1f319fdad0","0cb5245d4a5c7ba59bbfdedb7ace8a9fbd36ada3"],"ca0cc2f173b07ff75ca951e017f5dd1f319fdad0":["6727dd701b30630840235b6788bb5c728d20bbfd"],"0cb5245d4a5c7ba59bbfdedb7ace8a9fbd36ada3":["ca0cc2f173b07ff75ca951e017f5dd1f319fdad0"],"28288370235ed02234a64753cdbf0c6ec096304a":["0cb5245d4a5c7ba59bbfdedb7ace8a9fbd36ada3","e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":["0cb5245d4a5c7ba59bbfdedb7ace8a9fbd36ada3","e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d67f43c293f81a92c10131e75761f4e4e968b06c"],"d67f43c293f81a92c10131e75761f4e4e968b06c":["28288370235ed02234a64753cdbf0c6ec096304a"],"6727dd701b30630840235b6788bb5c728d20bbfd":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b":["28288370235ed02234a64753cdbf0c6ec096304a","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["6727dd701b30630840235b6788bb5c728d20bbfd"],"7c3523a0ab04c3002eee3896c75ea5f10f388bcc":[],"ca0cc2f173b07ff75ca951e017f5dd1f319fdad0":["7c3523a0ab04c3002eee3896c75ea5f10f388bcc","0cb5245d4a5c7ba59bbfdedb7ace8a9fbd36ada3"],"0cb5245d4a5c7ba59bbfdedb7ace8a9fbd36ada3":["e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b","7c3523a0ab04c3002eee3896c75ea5f10f388bcc","28288370235ed02234a64753cdbf0c6ec096304a","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"28288370235ed02234a64753cdbf0c6ec096304a":["d67f43c293f81a92c10131e75761f4e4e968b06c"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":[],"6727dd701b30630840235b6788bb5c728d20bbfd":["ca0cc2f173b07ff75ca951e017f5dd1f319fdad0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"d67f43c293f81a92c10131e75761f4e4e968b06c":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["7c3523a0ab04c3002eee3896c75ea5f10f388bcc","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}