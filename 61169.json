{"path":"lucene/core/src/test/org/apache/lucene/index/Test2BDocs#test2BDocs().mjava","commits":[{"id":"af012165196cf308c4c905cbe866039b6fcd2233","date":1473537504,"type":0,"author":"yonik","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/Test2BDocs#test2BDocs().mjava","pathOld":"/dev/null","sourceNew":"  // indexes Integer.MAX_VALUE docs with indexed field(s)\n  public void test2BDocs() throws Exception {\n    BaseDirectoryWrapper dir = newFSDirectory(createTempDir(\"2BDocs\"));\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    }\n    \n    IndexWriter w = new IndexWriter(dir,\n        new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n        .setRAMBufferSizeMB(256.0)\n        .setMergeScheduler(new ConcurrentMergeScheduler())\n        .setMergePolicy(newLogMergePolicy(false, 10))\n        .setOpenMode(IndexWriterConfig.OpenMode.CREATE)\n        .setCodec(TestUtil.getDefaultCodec()));\n\n    Document doc = new Document();\n    Field field = new Field(\"f1\", \"a\", StringField.TYPE_NOT_STORED);\n    doc.add(field);\n    \n    for (int i = 0; i < IndexWriter.MAX_DOCS; i++) {\n      w.addDocument(doc);\n      if (i % (10*1000*1000) == 0) {\n        System.out.println(\"indexed: \" + i);\n        System.out.flush();\n      }\n    }\n    \n    w.forceMerge(1);\n    w.close();\n    \n    System.out.println(\"verifying...\");\n    System.out.flush();\n    \n    DirectoryReader r = DirectoryReader.open(dir);\n\n    BytesRef term = new BytesRef(1);\n    term.bytes[0] = (byte)'a';\n    term.length = 1;\n\n    long skips = 0;\n\n    Random rnd = random();\n\n    long start = System.nanoTime();\n\n    for (LeafReaderContext context : r.leaves()) {\n      LeafReader reader = context.reader();\n      int lim = context.reader().maxDoc();\n\n      Terms terms = reader.fields().terms(\"f1\");\n      for (int i=0; i<10000; i++) {\n        TermsEnum te = terms.iterator();\n        assertTrue( te.seekExact(term) );\n        PostingsEnum docs = te.postings(null);\n\n        // skip randomly through the term\n        for (int target = -1;;)\n        {\n          int maxSkipSize = lim - target + 1;\n          // do a smaller skip half of the time\n          if (rnd.nextBoolean()) {\n            maxSkipSize = Math.min(256, maxSkipSize);\n          }\n          int newTarget = target + rnd.nextInt(maxSkipSize) + 1;\n          if (newTarget >= lim) {\n            if (target+1 >= lim) break; // we already skipped to end, so break.\n            newTarget = lim-1;  // skip to end\n          }\n          target = newTarget;\n\n          int res = docs.advance(target);\n          if (res == PostingsEnum.NO_MORE_DOCS) break;\n\n          assertTrue( res >= target );\n\n          skips++;\n          target = res;\n        }\n      }\n    }\n    \n    r.close();\n    dir.close();\n\n    long end = System.nanoTime();\n\n    System.out.println(\"Skip count=\" + skips + \" seconds=\" + TimeUnit.NANOSECONDS.toSeconds(end-start));\n    assert skips > 0;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"89424def13674ea17829b41c5883c54ecc31a132","date":1473767373,"type":0,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/Test2BDocs#test2BDocs().mjava","pathOld":"/dev/null","sourceNew":"  // indexes Integer.MAX_VALUE docs with indexed field(s)\n  public void test2BDocs() throws Exception {\n    BaseDirectoryWrapper dir = newFSDirectory(createTempDir(\"2BDocs\"));\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    }\n    \n    IndexWriter w = new IndexWriter(dir,\n        new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n        .setRAMBufferSizeMB(256.0)\n        .setMergeScheduler(new ConcurrentMergeScheduler())\n        .setMergePolicy(newLogMergePolicy(false, 10))\n        .setOpenMode(IndexWriterConfig.OpenMode.CREATE)\n        .setCodec(TestUtil.getDefaultCodec()));\n\n    Document doc = new Document();\n    Field field = new Field(\"f1\", \"a\", StringField.TYPE_NOT_STORED);\n    doc.add(field);\n    \n    for (int i = 0; i < IndexWriter.MAX_DOCS; i++) {\n      w.addDocument(doc);\n      if (i % (10*1000*1000) == 0) {\n        System.out.println(\"indexed: \" + i);\n        System.out.flush();\n      }\n    }\n    \n    w.forceMerge(1);\n    w.close();\n    \n    System.out.println(\"verifying...\");\n    System.out.flush();\n    \n    DirectoryReader r = DirectoryReader.open(dir);\n\n    BytesRef term = new BytesRef(1);\n    term.bytes[0] = (byte)'a';\n    term.length = 1;\n\n    long skips = 0;\n\n    Random rnd = random();\n\n    long start = System.nanoTime();\n\n    for (LeafReaderContext context : r.leaves()) {\n      LeafReader reader = context.reader();\n      int lim = context.reader().maxDoc();\n\n      Terms terms = reader.fields().terms(\"f1\");\n      for (int i=0; i<10000; i++) {\n        TermsEnum te = terms.iterator();\n        assertTrue( te.seekExact(term) );\n        PostingsEnum docs = te.postings(null);\n\n        // skip randomly through the term\n        for (int target = -1;;)\n        {\n          int maxSkipSize = lim - target + 1;\n          // do a smaller skip half of the time\n          if (rnd.nextBoolean()) {\n            maxSkipSize = Math.min(256, maxSkipSize);\n          }\n          int newTarget = target + rnd.nextInt(maxSkipSize) + 1;\n          if (newTarget >= lim) {\n            if (target+1 >= lim) break; // we already skipped to end, so break.\n            newTarget = lim-1;  // skip to end\n          }\n          target = newTarget;\n\n          int res = docs.advance(target);\n          if (res == PostingsEnum.NO_MORE_DOCS) break;\n\n          assertTrue( res >= target );\n\n          skips++;\n          target = res;\n        }\n      }\n    }\n    \n    r.close();\n    dir.close();\n\n    long end = System.nanoTime();\n\n    System.out.println(\"Skip count=\" + skips + \" seconds=\" + TimeUnit.NANOSECONDS.toSeconds(end-start));\n    assert skips > 0;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/Test2BDocs#test2BDocs().mjava","pathOld":"/dev/null","sourceNew":"  // indexes Integer.MAX_VALUE docs with indexed field(s)\n  public void test2BDocs() throws Exception {\n    BaseDirectoryWrapper dir = newFSDirectory(createTempDir(\"2BDocs\"));\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    }\n    \n    IndexWriter w = new IndexWriter(dir,\n        new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n        .setRAMBufferSizeMB(256.0)\n        .setMergeScheduler(new ConcurrentMergeScheduler())\n        .setMergePolicy(newLogMergePolicy(false, 10))\n        .setOpenMode(IndexWriterConfig.OpenMode.CREATE)\n        .setCodec(TestUtil.getDefaultCodec()));\n\n    Document doc = new Document();\n    Field field = new Field(\"f1\", \"a\", StringField.TYPE_NOT_STORED);\n    doc.add(field);\n    \n    for (int i = 0; i < IndexWriter.MAX_DOCS; i++) {\n      w.addDocument(doc);\n      if (i % (10*1000*1000) == 0) {\n        System.out.println(\"indexed: \" + i);\n        System.out.flush();\n      }\n    }\n    \n    w.forceMerge(1);\n    w.close();\n    \n    System.out.println(\"verifying...\");\n    System.out.flush();\n    \n    DirectoryReader r = DirectoryReader.open(dir);\n\n    BytesRef term = new BytesRef(1);\n    term.bytes[0] = (byte)'a';\n    term.length = 1;\n\n    long skips = 0;\n\n    Random rnd = random();\n\n    long start = System.nanoTime();\n\n    for (LeafReaderContext context : r.leaves()) {\n      LeafReader reader = context.reader();\n      int lim = context.reader().maxDoc();\n\n      Terms terms = reader.fields().terms(\"f1\");\n      for (int i=0; i<10000; i++) {\n        TermsEnum te = terms.iterator();\n        assertTrue( te.seekExact(term) );\n        PostingsEnum docs = te.postings(null);\n\n        // skip randomly through the term\n        for (int target = -1;;)\n        {\n          int maxSkipSize = lim - target + 1;\n          // do a smaller skip half of the time\n          if (rnd.nextBoolean()) {\n            maxSkipSize = Math.min(256, maxSkipSize);\n          }\n          int newTarget = target + rnd.nextInt(maxSkipSize) + 1;\n          if (newTarget >= lim) {\n            if (target+1 >= lim) break; // we already skipped to end, so break.\n            newTarget = lim-1;  // skip to end\n          }\n          target = newTarget;\n\n          int res = docs.advance(target);\n          if (res == PostingsEnum.NO_MORE_DOCS) break;\n\n          assertTrue( res >= target );\n\n          skips++;\n          target = res;\n        }\n      }\n    }\n    \n    r.close();\n    dir.close();\n\n    long end = System.nanoTime();\n\n    System.out.println(\"Skip count=\" + skips + \" seconds=\" + TimeUnit.NANOSECONDS.toSeconds(end-start));\n    assert skips > 0;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/Test2BDocs#test2BDocs().mjava","pathOld":"/dev/null","sourceNew":"  // indexes Integer.MAX_VALUE docs with indexed field(s)\n  public void test2BDocs() throws Exception {\n    BaseDirectoryWrapper dir = newFSDirectory(createTempDir(\"2BDocs\"));\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    }\n    \n    IndexWriter w = new IndexWriter(dir,\n        new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n        .setRAMBufferSizeMB(256.0)\n        .setMergeScheduler(new ConcurrentMergeScheduler())\n        .setMergePolicy(newLogMergePolicy(false, 10))\n        .setOpenMode(IndexWriterConfig.OpenMode.CREATE)\n        .setCodec(TestUtil.getDefaultCodec()));\n\n    Document doc = new Document();\n    Field field = new Field(\"f1\", \"a\", StringField.TYPE_NOT_STORED);\n    doc.add(field);\n    \n    for (int i = 0; i < IndexWriter.MAX_DOCS; i++) {\n      w.addDocument(doc);\n      if (i % (10*1000*1000) == 0) {\n        System.out.println(\"indexed: \" + i);\n        System.out.flush();\n      }\n    }\n    \n    w.forceMerge(1);\n    w.close();\n    \n    System.out.println(\"verifying...\");\n    System.out.flush();\n    \n    DirectoryReader r = DirectoryReader.open(dir);\n\n    BytesRef term = new BytesRef(1);\n    term.bytes[0] = (byte)'a';\n    term.length = 1;\n\n    long skips = 0;\n\n    Random rnd = random();\n\n    long start = System.nanoTime();\n\n    for (LeafReaderContext context : r.leaves()) {\n      LeafReader reader = context.reader();\n      int lim = context.reader().maxDoc();\n\n      Terms terms = reader.fields().terms(\"f1\");\n      for (int i=0; i<10000; i++) {\n        TermsEnum te = terms.iterator();\n        assertTrue( te.seekExact(term) );\n        PostingsEnum docs = te.postings(null);\n\n        // skip randomly through the term\n        for (int target = -1;;)\n        {\n          int maxSkipSize = lim - target + 1;\n          // do a smaller skip half of the time\n          if (rnd.nextBoolean()) {\n            maxSkipSize = Math.min(256, maxSkipSize);\n          }\n          int newTarget = target + rnd.nextInt(maxSkipSize) + 1;\n          if (newTarget >= lim) {\n            if (target+1 >= lim) break; // we already skipped to end, so break.\n            newTarget = lim-1;  // skip to end\n          }\n          target = newTarget;\n\n          int res = docs.advance(target);\n          if (res == PostingsEnum.NO_MORE_DOCS) break;\n\n          assertTrue( res >= target );\n\n          skips++;\n          target = res;\n        }\n      }\n    }\n    \n    r.close();\n    dir.close();\n\n    long end = System.nanoTime();\n\n    System.out.println(\"Skip count=\" + skips + \" seconds=\" + TimeUnit.NANOSECONDS.toSeconds(end-start));\n    assert skips > 0;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b","date":1497408244,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/Test2BDocs#test2BDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/Test2BDocs#test2BDocs().mjava","sourceNew":"  // indexes Integer.MAX_VALUE docs with indexed field(s)\n  public void test2BDocs() throws Exception {\n    BaseDirectoryWrapper dir = newFSDirectory(createTempDir(\"2BDocs\"));\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    }\n    \n    IndexWriter w = new IndexWriter(dir,\n        new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n        .setRAMBufferSizeMB(256.0)\n        .setMergeScheduler(new ConcurrentMergeScheduler())\n        .setMergePolicy(newLogMergePolicy(false, 10))\n        .setOpenMode(IndexWriterConfig.OpenMode.CREATE)\n        .setCodec(TestUtil.getDefaultCodec()));\n\n    Document doc = new Document();\n    Field field = new Field(\"f1\", \"a\", StringField.TYPE_NOT_STORED);\n    doc.add(field);\n    \n    for (int i = 0; i < IndexWriter.MAX_DOCS; i++) {\n      w.addDocument(doc);\n      if (i % (10*1000*1000) == 0) {\n        System.out.println(\"indexed: \" + i);\n        System.out.flush();\n      }\n    }\n    \n    w.forceMerge(1);\n    w.close();\n    \n    System.out.println(\"verifying...\");\n    System.out.flush();\n    \n    DirectoryReader r = DirectoryReader.open(dir);\n\n    BytesRef term = new BytesRef(1);\n    term.bytes[0] = (byte)'a';\n    term.length = 1;\n\n    long skips = 0;\n\n    Random rnd = random();\n\n    long start = System.nanoTime();\n\n    for (LeafReaderContext context : r.leaves()) {\n      LeafReader reader = context.reader();\n      int lim = context.reader().maxDoc();\n\n      Terms terms = reader.terms(\"f1\");\n      for (int i=0; i<10000; i++) {\n        TermsEnum te = terms.iterator();\n        assertTrue( te.seekExact(term) );\n        PostingsEnum docs = te.postings(null);\n\n        // skip randomly through the term\n        for (int target = -1;;)\n        {\n          int maxSkipSize = lim - target + 1;\n          // do a smaller skip half of the time\n          if (rnd.nextBoolean()) {\n            maxSkipSize = Math.min(256, maxSkipSize);\n          }\n          int newTarget = target + rnd.nextInt(maxSkipSize) + 1;\n          if (newTarget >= lim) {\n            if (target+1 >= lim) break; // we already skipped to end, so break.\n            newTarget = lim-1;  // skip to end\n          }\n          target = newTarget;\n\n          int res = docs.advance(target);\n          if (res == PostingsEnum.NO_MORE_DOCS) break;\n\n          assertTrue( res >= target );\n\n          skips++;\n          target = res;\n        }\n      }\n    }\n    \n    r.close();\n    dir.close();\n\n    long end = System.nanoTime();\n\n    System.out.println(\"Skip count=\" + skips + \" seconds=\" + TimeUnit.NANOSECONDS.toSeconds(end-start));\n    assert skips > 0;\n  }\n\n","sourceOld":"  // indexes Integer.MAX_VALUE docs with indexed field(s)\n  public void test2BDocs() throws Exception {\n    BaseDirectoryWrapper dir = newFSDirectory(createTempDir(\"2BDocs\"));\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    }\n    \n    IndexWriter w = new IndexWriter(dir,\n        new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n        .setRAMBufferSizeMB(256.0)\n        .setMergeScheduler(new ConcurrentMergeScheduler())\n        .setMergePolicy(newLogMergePolicy(false, 10))\n        .setOpenMode(IndexWriterConfig.OpenMode.CREATE)\n        .setCodec(TestUtil.getDefaultCodec()));\n\n    Document doc = new Document();\n    Field field = new Field(\"f1\", \"a\", StringField.TYPE_NOT_STORED);\n    doc.add(field);\n    \n    for (int i = 0; i < IndexWriter.MAX_DOCS; i++) {\n      w.addDocument(doc);\n      if (i % (10*1000*1000) == 0) {\n        System.out.println(\"indexed: \" + i);\n        System.out.flush();\n      }\n    }\n    \n    w.forceMerge(1);\n    w.close();\n    \n    System.out.println(\"verifying...\");\n    System.out.flush();\n    \n    DirectoryReader r = DirectoryReader.open(dir);\n\n    BytesRef term = new BytesRef(1);\n    term.bytes[0] = (byte)'a';\n    term.length = 1;\n\n    long skips = 0;\n\n    Random rnd = random();\n\n    long start = System.nanoTime();\n\n    for (LeafReaderContext context : r.leaves()) {\n      LeafReader reader = context.reader();\n      int lim = context.reader().maxDoc();\n\n      Terms terms = reader.fields().terms(\"f1\");\n      for (int i=0; i<10000; i++) {\n        TermsEnum te = terms.iterator();\n        assertTrue( te.seekExact(term) );\n        PostingsEnum docs = te.postings(null);\n\n        // skip randomly through the term\n        for (int target = -1;;)\n        {\n          int maxSkipSize = lim - target + 1;\n          // do a smaller skip half of the time\n          if (rnd.nextBoolean()) {\n            maxSkipSize = Math.min(256, maxSkipSize);\n          }\n          int newTarget = target + rnd.nextInt(maxSkipSize) + 1;\n          if (newTarget >= lim) {\n            if (target+1 >= lim) break; // we already skipped to end, so break.\n            newTarget = lim-1;  // skip to end\n          }\n          target = newTarget;\n\n          int res = docs.advance(target);\n          if (res == PostingsEnum.NO_MORE_DOCS) break;\n\n          assertTrue( res >= target );\n\n          skips++;\n          target = res;\n        }\n      }\n    }\n    \n    r.close();\n    dir.close();\n\n    long end = System.nanoTime();\n\n    System.out.println(\"Skip count=\" + skips + \" seconds=\" + TimeUnit.NANOSECONDS.toSeconds(end-start));\n    assert skips > 0;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","date":1498028748,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/Test2BDocs#test2BDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/Test2BDocs#test2BDocs().mjava","sourceNew":"  // indexes Integer.MAX_VALUE docs with indexed field(s)\n  public void test2BDocs() throws Exception {\n    BaseDirectoryWrapper dir = newFSDirectory(createTempDir(\"2BDocs\"));\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    }\n    \n    IndexWriter w = new IndexWriter(dir,\n        new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n        .setRAMBufferSizeMB(256.0)\n        .setMergeScheduler(new ConcurrentMergeScheduler())\n        .setMergePolicy(newLogMergePolicy(false, 10))\n        .setOpenMode(IndexWriterConfig.OpenMode.CREATE)\n        .setCodec(TestUtil.getDefaultCodec()));\n\n    Document doc = new Document();\n    Field field = new Field(\"f1\", \"a\", StringField.TYPE_NOT_STORED);\n    doc.add(field);\n    \n    for (int i = 0; i < IndexWriter.MAX_DOCS; i++) {\n      w.addDocument(doc);\n      if (i % (10*1000*1000) == 0) {\n        System.out.println(\"indexed: \" + i);\n        System.out.flush();\n      }\n    }\n    \n    w.forceMerge(1);\n    w.close();\n    \n    System.out.println(\"verifying...\");\n    System.out.flush();\n    \n    DirectoryReader r = DirectoryReader.open(dir);\n\n    BytesRef term = new BytesRef(1);\n    term.bytes[0] = (byte)'a';\n    term.length = 1;\n\n    long skips = 0;\n\n    Random rnd = random();\n\n    long start = System.nanoTime();\n\n    for (LeafReaderContext context : r.leaves()) {\n      LeafReader reader = context.reader();\n      int lim = context.reader().maxDoc();\n\n      Terms terms = reader.terms(\"f1\");\n      for (int i=0; i<10000; i++) {\n        TermsEnum te = terms.iterator();\n        assertTrue( te.seekExact(term) );\n        PostingsEnum docs = te.postings(null);\n\n        // skip randomly through the term\n        for (int target = -1;;)\n        {\n          int maxSkipSize = lim - target + 1;\n          // do a smaller skip half of the time\n          if (rnd.nextBoolean()) {\n            maxSkipSize = Math.min(256, maxSkipSize);\n          }\n          int newTarget = target + rnd.nextInt(maxSkipSize) + 1;\n          if (newTarget >= lim) {\n            if (target+1 >= lim) break; // we already skipped to end, so break.\n            newTarget = lim-1;  // skip to end\n          }\n          target = newTarget;\n\n          int res = docs.advance(target);\n          if (res == PostingsEnum.NO_MORE_DOCS) break;\n\n          assertTrue( res >= target );\n\n          skips++;\n          target = res;\n        }\n      }\n    }\n    \n    r.close();\n    dir.close();\n\n    long end = System.nanoTime();\n\n    System.out.println(\"Skip count=\" + skips + \" seconds=\" + TimeUnit.NANOSECONDS.toSeconds(end-start));\n    assert skips > 0;\n  }\n\n","sourceOld":"  // indexes Integer.MAX_VALUE docs with indexed field(s)\n  public void test2BDocs() throws Exception {\n    BaseDirectoryWrapper dir = newFSDirectory(createTempDir(\"2BDocs\"));\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    }\n    \n    IndexWriter w = new IndexWriter(dir,\n        new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n        .setRAMBufferSizeMB(256.0)\n        .setMergeScheduler(new ConcurrentMergeScheduler())\n        .setMergePolicy(newLogMergePolicy(false, 10))\n        .setOpenMode(IndexWriterConfig.OpenMode.CREATE)\n        .setCodec(TestUtil.getDefaultCodec()));\n\n    Document doc = new Document();\n    Field field = new Field(\"f1\", \"a\", StringField.TYPE_NOT_STORED);\n    doc.add(field);\n    \n    for (int i = 0; i < IndexWriter.MAX_DOCS; i++) {\n      w.addDocument(doc);\n      if (i % (10*1000*1000) == 0) {\n        System.out.println(\"indexed: \" + i);\n        System.out.flush();\n      }\n    }\n    \n    w.forceMerge(1);\n    w.close();\n    \n    System.out.println(\"verifying...\");\n    System.out.flush();\n    \n    DirectoryReader r = DirectoryReader.open(dir);\n\n    BytesRef term = new BytesRef(1);\n    term.bytes[0] = (byte)'a';\n    term.length = 1;\n\n    long skips = 0;\n\n    Random rnd = random();\n\n    long start = System.nanoTime();\n\n    for (LeafReaderContext context : r.leaves()) {\n      LeafReader reader = context.reader();\n      int lim = context.reader().maxDoc();\n\n      Terms terms = reader.fields().terms(\"f1\");\n      for (int i=0; i<10000; i++) {\n        TermsEnum te = terms.iterator();\n        assertTrue( te.seekExact(term) );\n        PostingsEnum docs = te.postings(null);\n\n        // skip randomly through the term\n        for (int target = -1;;)\n        {\n          int maxSkipSize = lim - target + 1;\n          // do a smaller skip half of the time\n          if (rnd.nextBoolean()) {\n            maxSkipSize = Math.min(256, maxSkipSize);\n          }\n          int newTarget = target + rnd.nextInt(maxSkipSize) + 1;\n          if (newTarget >= lim) {\n            if (target+1 >= lim) break; // we already skipped to end, so break.\n            newTarget = lim-1;  // skip to end\n          }\n          target = newTarget;\n\n          int res = docs.advance(target);\n          if (res == PostingsEnum.NO_MORE_DOCS) break;\n\n          assertTrue( res >= target );\n\n          skips++;\n          target = res;\n        }\n      }\n    }\n    \n    r.close();\n    dir.close();\n\n    long end = System.nanoTime();\n\n    System.out.println(\"Skip count=\" + skips + \" seconds=\" + TimeUnit.NANOSECONDS.toSeconds(end-start));\n    assert skips > 0;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/Test2BDocs#test2BDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/Test2BDocs#test2BDocs().mjava","sourceNew":"  // indexes Integer.MAX_VALUE docs with indexed field(s)\n  public void test2BDocs() throws Exception {\n    BaseDirectoryWrapper dir = newFSDirectory(createTempDir(\"2BDocs\"));\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    }\n    \n    IndexWriter w = new IndexWriter(dir,\n        new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n        .setRAMBufferSizeMB(256.0)\n        .setMergeScheduler(new ConcurrentMergeScheduler())\n        .setMergePolicy(newLogMergePolicy(false, 10))\n        .setOpenMode(IndexWriterConfig.OpenMode.CREATE)\n        .setCodec(TestUtil.getDefaultCodec()));\n\n    Document doc = new Document();\n    Field field = new Field(\"f1\", \"a\", StringField.TYPE_NOT_STORED);\n    doc.add(field);\n    \n    for (int i = 0; i < IndexWriter.MAX_DOCS; i++) {\n      w.addDocument(doc);\n      if (i % (10*1000*1000) == 0) {\n        System.out.println(\"indexed: \" + i);\n        System.out.flush();\n      }\n    }\n    \n    w.forceMerge(1);\n    w.close();\n    \n    System.out.println(\"verifying...\");\n    System.out.flush();\n    \n    DirectoryReader r = DirectoryReader.open(dir);\n\n    BytesRef term = new BytesRef(1);\n    term.bytes[0] = (byte)'a';\n    term.length = 1;\n\n    long skips = 0;\n\n    Random rnd = random();\n\n    long start = System.nanoTime();\n\n    for (LeafReaderContext context : r.leaves()) {\n      LeafReader reader = context.reader();\n      int lim = context.reader().maxDoc();\n\n      Terms terms = reader.terms(\"f1\");\n      for (int i=0; i<10000; i++) {\n        TermsEnum te = terms.iterator();\n        assertTrue( te.seekExact(term) );\n        PostingsEnum docs = te.postings(null);\n\n        // skip randomly through the term\n        for (int target = -1;;)\n        {\n          int maxSkipSize = lim - target + 1;\n          // do a smaller skip half of the time\n          if (rnd.nextBoolean()) {\n            maxSkipSize = Math.min(256, maxSkipSize);\n          }\n          int newTarget = target + rnd.nextInt(maxSkipSize) + 1;\n          if (newTarget >= lim) {\n            if (target+1 >= lim) break; // we already skipped to end, so break.\n            newTarget = lim-1;  // skip to end\n          }\n          target = newTarget;\n\n          int res = docs.advance(target);\n          if (res == PostingsEnum.NO_MORE_DOCS) break;\n\n          assertTrue( res >= target );\n\n          skips++;\n          target = res;\n        }\n      }\n    }\n    \n    r.close();\n    dir.close();\n\n    long end = System.nanoTime();\n\n    System.out.println(\"Skip count=\" + skips + \" seconds=\" + TimeUnit.NANOSECONDS.toSeconds(end-start));\n    assert skips > 0;\n  }\n\n","sourceOld":"  // indexes Integer.MAX_VALUE docs with indexed field(s)\n  public void test2BDocs() throws Exception {\n    BaseDirectoryWrapper dir = newFSDirectory(createTempDir(\"2BDocs\"));\n    if (dir instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper)dir).setThrottling(MockDirectoryWrapper.Throttling.NEVER);\n    }\n    \n    IndexWriter w = new IndexWriter(dir,\n        new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n        .setRAMBufferSizeMB(256.0)\n        .setMergeScheduler(new ConcurrentMergeScheduler())\n        .setMergePolicy(newLogMergePolicy(false, 10))\n        .setOpenMode(IndexWriterConfig.OpenMode.CREATE)\n        .setCodec(TestUtil.getDefaultCodec()));\n\n    Document doc = new Document();\n    Field field = new Field(\"f1\", \"a\", StringField.TYPE_NOT_STORED);\n    doc.add(field);\n    \n    for (int i = 0; i < IndexWriter.MAX_DOCS; i++) {\n      w.addDocument(doc);\n      if (i % (10*1000*1000) == 0) {\n        System.out.println(\"indexed: \" + i);\n        System.out.flush();\n      }\n    }\n    \n    w.forceMerge(1);\n    w.close();\n    \n    System.out.println(\"verifying...\");\n    System.out.flush();\n    \n    DirectoryReader r = DirectoryReader.open(dir);\n\n    BytesRef term = new BytesRef(1);\n    term.bytes[0] = (byte)'a';\n    term.length = 1;\n\n    long skips = 0;\n\n    Random rnd = random();\n\n    long start = System.nanoTime();\n\n    for (LeafReaderContext context : r.leaves()) {\n      LeafReader reader = context.reader();\n      int lim = context.reader().maxDoc();\n\n      Terms terms = reader.fields().terms(\"f1\");\n      for (int i=0; i<10000; i++) {\n        TermsEnum te = terms.iterator();\n        assertTrue( te.seekExact(term) );\n        PostingsEnum docs = te.postings(null);\n\n        // skip randomly through the term\n        for (int target = -1;;)\n        {\n          int maxSkipSize = lim - target + 1;\n          // do a smaller skip half of the time\n          if (rnd.nextBoolean()) {\n            maxSkipSize = Math.min(256, maxSkipSize);\n          }\n          int newTarget = target + rnd.nextInt(maxSkipSize) + 1;\n          if (newTarget >= lim) {\n            if (target+1 >= lim) break; // we already skipped to end, so break.\n            newTarget = lim-1;  // skip to end\n          }\n          target = newTarget;\n\n          int res = docs.advance(target);\n          if (res == PostingsEnum.NO_MORE_DOCS) break;\n\n          assertTrue( res >= target );\n\n          skips++;\n          target = res;\n        }\n      }\n    }\n    \n    r.close();\n    dir.close();\n\n    long end = System.nanoTime();\n\n    System.out.println(\"Skip count=\" + skips + \" seconds=\" + TimeUnit.NANOSECONDS.toSeconds(end-start));\n    assert skips > 0;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","89424def13674ea17829b41c5883c54ecc31a132"],"28288370235ed02234a64753cdbf0c6ec096304a":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["28288370235ed02234a64753cdbf0c6ec096304a"],"89424def13674ea17829b41c5883c54ecc31a132":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","af012165196cf308c4c905cbe866039b6fcd2233"],"af012165196cf308c4c905cbe866039b6fcd2233":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b":["28288370235ed02234a64753cdbf0c6ec096304a","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b","28288370235ed02234a64753cdbf0c6ec096304a","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","89424def13674ea17829b41c5883c54ecc31a132","af012165196cf308c4c905cbe866039b6fcd2233"],"28288370235ed02234a64753cdbf0c6ec096304a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":[],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"89424def13674ea17829b41c5883c54ecc31a132":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"af012165196cf308c4c905cbe866039b6fcd2233":["89424def13674ea17829b41c5883c54ecc31a132"]},"heads":["2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}