{"path":"lucene/src/java/org/apache/lucene/search/cache/DocTermsIndexCreator#create(IndexReader).mjava","commits":[{"id":"eee9156bf08d7eaae5e8d8ab5f855ba61012e257","date":1285443157,"type":0,"author":"Ryan McKinley","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/cache/DocTermsIndexCreator#create(IndexReader).mjava","pathOld":"/dev/null","sourceNew":"  @Override\r\n  public T create(IndexReader reader) throws IOException\r\n  {\r\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\r\n    Terms terms = MultiFields.getTerms(reader, field);\r\n\r\n    final boolean fasterButMoreRAM = hasOption(FASTER_BUT_MORE_RAM);\r\n\r\n    final PagedBytes bytes = new PagedBytes(15);\r\n\r\n    int startBytesBPV;\r\n    int startTermsBPV;\r\n    int startNumUniqueTerms;\r\n\r\n    int maxDoc = reader.maxDoc();\r\n    final int termCountHardLimit;\r\n    if (maxDoc == Integer.MAX_VALUE) {\r\n      termCountHardLimit = Integer.MAX_VALUE;\r\n    } else {\r\n      termCountHardLimit = maxDoc+1;\r\n    }\r\n\r\n    if (terms != null) {\r\n      // Try for coarse estimate for number of bits; this\r\n      // should be an underestimate most of the time, which\r\n      // is fine -- GrowableWriter will reallocate as needed\r\n      long numUniqueTerms = 0;\r\n      try {\r\n        numUniqueTerms = terms.getUniqueTermCount();\r\n      } catch (UnsupportedOperationException uoe) {\r\n        numUniqueTerms = -1;\r\n      }\r\n      if (numUniqueTerms != -1) {\r\n\r\n        if (numUniqueTerms > termCountHardLimit) {\r\n          // app is misusing the API (there is more than\r\n          // one term per doc); in this case we make best\r\n          // effort to load what we can (see LUCENE-2142)\r\n          numUniqueTerms = termCountHardLimit;\r\n        }\r\n\r\n        startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\r\n        startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\r\n\r\n        startNumUniqueTerms = (int) numUniqueTerms;\r\n      } else {\r\n        startBytesBPV = 1;\r\n        startTermsBPV = 1;\r\n        startNumUniqueTerms = 1;\r\n      }\r\n    } else {\r\n      startBytesBPV = 1;\r\n      startTermsBPV = 1;\r\n      startNumUniqueTerms = 1;\r\n    }\r\n\r\n    GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\r\n    final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\r\n\r\n    // 0 is reserved for \"unset\"\r\n    bytes.copyUsingLengthPrefix(new BytesRef());\r\n    int termOrd = 1;\r\n\r\n    if (terms != null) {\r\n      final TermsEnum termsEnum = terms.iterator();\r\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\r\n      DocsEnum docs = null;\r\n\r\n      while(true) {\r\n        final BytesRef term = termsEnum.next();\r\n        if (term == null) {\r\n          break;\r\n        }\r\n        if (termOrd >= termCountHardLimit) {\r\n          break;\r\n        }\r\n\r\n        if (termOrd == termOrdToBytesOffset.size()) {\r\n          // NOTE: this code only runs if the incoming\r\n          // reader impl doesn't implement\r\n          // getUniqueTermCount (which should be uncommon)\r\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\r\n        }\r\n        termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\r\n        docs = termsEnum.docs(delDocs, docs);\r\n        while (true) {\r\n          final int docID = docs.nextDoc();\r\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\r\n            break;\r\n          }\r\n          docToTermOrd.set(docID, termOrd);\r\n        }\r\n        termOrd++;\r\n      }\r\n\r\n      if (termOrdToBytesOffset.size() > termOrd) {\r\n        termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\r\n      }\r\n    }\r\n\r\n    // maybe an int-only impl?\r\n    return (T)new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\r\n  }\r\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5637938a7dc36e7ff09a5d9398957bd46b15129a","date":1285538458,"type":3,"author":"Ryan McKinley","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/cache/DocTermsIndexCreator#create(IndexReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/cache/DocTermsIndexCreator#create(IndexReader).mjava","sourceNew":"  @Override\n  public T create(IndexReader reader) throws IOException\n  {\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption(FASTER_BUT_MORE_RAM);\n\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBytesBPV;\n    int startTermsBPV;\n    int startNumUniqueTerms;\n\n    int maxDoc = reader.maxDoc();\n    final int termCountHardLimit;\n    if (maxDoc == Integer.MAX_VALUE) {\n      termCountHardLimit = Integer.MAX_VALUE;\n    } else {\n      termCountHardLimit = maxDoc+1;\n    }\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n\n        if (numUniqueTerms > termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          numUniqueTerms = termCountHardLimit;\n        }\n\n        startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n        startNumUniqueTerms = (int) numUniqueTerms;\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n    } else {\n      startBytesBPV = 1;\n      startTermsBPV = 1;\n      startNumUniqueTerms = 1;\n    }\n\n    GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n    final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // 0 is reserved for \"unset\"\n    bytes.copyUsingLengthPrefix(new BytesRef());\n    int termOrd = 1;\n\n    if (terms != null) {\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\n      DocsEnum docs = null;\n\n      while(true) {\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        if (termOrd >= termCountHardLimit) {\n          break;\n        }\n\n        if (termOrd == termOrdToBytesOffset.size()) {\n          // NOTE: this code only runs if the incoming\n          // reader impl doesn't implement\n          // getUniqueTermCount (which should be uncommon)\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n        }\n        termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n        docs = termsEnum.docs(delDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToTermOrd.set(docID, termOrd);\n        }\n        termOrd++;\n      }\n\n      if (termOrdToBytesOffset.size() > termOrd) {\n        termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n      }\n    }\n\n    // maybe an int-only impl?\n    return (T)new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n  }\n\n","sourceOld":"  @Override\r\n  public T create(IndexReader reader) throws IOException\r\n  {\r\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\r\n    Terms terms = MultiFields.getTerms(reader, field);\r\n\r\n    final boolean fasterButMoreRAM = hasOption(FASTER_BUT_MORE_RAM);\r\n\r\n    final PagedBytes bytes = new PagedBytes(15);\r\n\r\n    int startBytesBPV;\r\n    int startTermsBPV;\r\n    int startNumUniqueTerms;\r\n\r\n    int maxDoc = reader.maxDoc();\r\n    final int termCountHardLimit;\r\n    if (maxDoc == Integer.MAX_VALUE) {\r\n      termCountHardLimit = Integer.MAX_VALUE;\r\n    } else {\r\n      termCountHardLimit = maxDoc+1;\r\n    }\r\n\r\n    if (terms != null) {\r\n      // Try for coarse estimate for number of bits; this\r\n      // should be an underestimate most of the time, which\r\n      // is fine -- GrowableWriter will reallocate as needed\r\n      long numUniqueTerms = 0;\r\n      try {\r\n        numUniqueTerms = terms.getUniqueTermCount();\r\n      } catch (UnsupportedOperationException uoe) {\r\n        numUniqueTerms = -1;\r\n      }\r\n      if (numUniqueTerms != -1) {\r\n\r\n        if (numUniqueTerms > termCountHardLimit) {\r\n          // app is misusing the API (there is more than\r\n          // one term per doc); in this case we make best\r\n          // effort to load what we can (see LUCENE-2142)\r\n          numUniqueTerms = termCountHardLimit;\r\n        }\r\n\r\n        startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\r\n        startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\r\n\r\n        startNumUniqueTerms = (int) numUniqueTerms;\r\n      } else {\r\n        startBytesBPV = 1;\r\n        startTermsBPV = 1;\r\n        startNumUniqueTerms = 1;\r\n      }\r\n    } else {\r\n      startBytesBPV = 1;\r\n      startTermsBPV = 1;\r\n      startNumUniqueTerms = 1;\r\n    }\r\n\r\n    GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\r\n    final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\r\n\r\n    // 0 is reserved for \"unset\"\r\n    bytes.copyUsingLengthPrefix(new BytesRef());\r\n    int termOrd = 1;\r\n\r\n    if (terms != null) {\r\n      final TermsEnum termsEnum = terms.iterator();\r\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\r\n      DocsEnum docs = null;\r\n\r\n      while(true) {\r\n        final BytesRef term = termsEnum.next();\r\n        if (term == null) {\r\n          break;\r\n        }\r\n        if (termOrd >= termCountHardLimit) {\r\n          break;\r\n        }\r\n\r\n        if (termOrd == termOrdToBytesOffset.size()) {\r\n          // NOTE: this code only runs if the incoming\r\n          // reader impl doesn't implement\r\n          // getUniqueTermCount (which should be uncommon)\r\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\r\n        }\r\n        termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\r\n        docs = termsEnum.docs(delDocs, docs);\r\n        while (true) {\r\n          final int docID = docs.nextDoc();\r\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\r\n            break;\r\n          }\r\n          docToTermOrd.set(docID, termOrd);\r\n        }\r\n        termOrd++;\r\n      }\r\n\r\n      if (termOrdToBytesOffset.size() > termOrd) {\r\n        termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\r\n      }\r\n    }\r\n\r\n    // maybe an int-only impl?\r\n    return (T)new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\r\n  }\r\n\n","bugFix":null,"bugIntro":["eee24cbec95372b8e747bf9146a5ef33734029a6"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"eda01daf042abfe36d82ad9ecad499220a1f3ecf","date":1287094261,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/cache/DocTermsIndexCreator#create(IndexReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/cache/DocTermsIndexCreator#create(IndexReader).mjava","sourceNew":"  @Override\n  public T create(IndexReader reader) throws IOException\n  {\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption(FASTER_BUT_MORE_RAM);\n\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBytesBPV;\n    int startTermsBPV;\n    int startNumUniqueTerms;\n\n    int maxDoc = reader.maxDoc();\n    final int termCountHardLimit;\n    if (maxDoc == Integer.MAX_VALUE) {\n      termCountHardLimit = Integer.MAX_VALUE;\n    } else {\n      termCountHardLimit = maxDoc+1;\n    }\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n\n        if (numUniqueTerms > termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          numUniqueTerms = termCountHardLimit;\n        }\n\n        startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n        startNumUniqueTerms = (int) numUniqueTerms;\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n    } else {\n      startBytesBPV = 1;\n      startTermsBPV = 1;\n      startNumUniqueTerms = 1;\n    }\n\n    GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n    final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // 0 is reserved for \"unset\"\n    bytes.copyUsingLengthPrefix(new BytesRef());\n    int termOrd = 1;\n\n    if (terms != null) {\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\n      DocsEnum docs = null;\n\n      while(true) {\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        if (termOrd >= termCountHardLimit) {\n          break;\n        }\n\n        if (termOrd == termOrdToBytesOffset.size()) {\n          // NOTE: this code only runs if the incoming\n          // reader impl doesn't implement\n          // getUniqueTermCount (which should be uncommon)\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n        }\n        termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n        docs = termsEnum.docs(delDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToTermOrd.set(docID, termOrd);\n        }\n        termOrd++;\n      }\n\n      if (termOrdToBytesOffset.size() > termOrd) {\n        termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n      }\n    }\n\n    // maybe an int-only impl?\n    @SuppressWarnings(\"unchecked\") final T t =\n      (T)new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    return t;\n  }\n\n","sourceOld":"  @Override\n  public T create(IndexReader reader) throws IOException\n  {\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption(FASTER_BUT_MORE_RAM);\n\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBytesBPV;\n    int startTermsBPV;\n    int startNumUniqueTerms;\n\n    int maxDoc = reader.maxDoc();\n    final int termCountHardLimit;\n    if (maxDoc == Integer.MAX_VALUE) {\n      termCountHardLimit = Integer.MAX_VALUE;\n    } else {\n      termCountHardLimit = maxDoc+1;\n    }\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n\n        if (numUniqueTerms > termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          numUniqueTerms = termCountHardLimit;\n        }\n\n        startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n        startNumUniqueTerms = (int) numUniqueTerms;\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n    } else {\n      startBytesBPV = 1;\n      startTermsBPV = 1;\n      startNumUniqueTerms = 1;\n    }\n\n    GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n    final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // 0 is reserved for \"unset\"\n    bytes.copyUsingLengthPrefix(new BytesRef());\n    int termOrd = 1;\n\n    if (terms != null) {\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\n      DocsEnum docs = null;\n\n      while(true) {\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        if (termOrd >= termCountHardLimit) {\n          break;\n        }\n\n        if (termOrd == termOrdToBytesOffset.size()) {\n          // NOTE: this code only runs if the incoming\n          // reader impl doesn't implement\n          // getUniqueTermCount (which should be uncommon)\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n        }\n        termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n        docs = termsEnum.docs(delDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToTermOrd.set(docID, termOrd);\n        }\n        termOrd++;\n      }\n\n      if (termOrdToBytesOffset.size() > termOrd) {\n        termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n      }\n    }\n\n    // maybe an int-only impl?\n    return (T)new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8f684ff0d559fa047f3bfa01bcbc78fc826a7817","date":1287264459,"type":3,"author":"Ryan McKinley","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/cache/DocTermsIndexCreator#create(IndexReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/cache/DocTermsIndexCreator#create(IndexReader).mjava","sourceNew":"  @Override\n  public DocTermsIndex create(IndexReader reader) throws IOException\n  {\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption(FASTER_BUT_MORE_RAM);\n\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBytesBPV;\n    int startTermsBPV;\n    int startNumUniqueTerms;\n\n    int maxDoc = reader.maxDoc();\n    final int termCountHardLimit;\n    if (maxDoc == Integer.MAX_VALUE) {\n      termCountHardLimit = Integer.MAX_VALUE;\n    } else {\n      termCountHardLimit = maxDoc+1;\n    }\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n\n        if (numUniqueTerms > termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          numUniqueTerms = termCountHardLimit;\n        }\n\n        startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n        startNumUniqueTerms = (int) numUniqueTerms;\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n    } else {\n      startBytesBPV = 1;\n      startTermsBPV = 1;\n      startNumUniqueTerms = 1;\n    }\n\n    GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n    final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // 0 is reserved for \"unset\"\n    bytes.copyUsingLengthPrefix(new BytesRef());\n    int termOrd = 1;\n\n    if (terms != null) {\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\n      DocsEnum docs = null;\n\n      while(true) {\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        if (termOrd >= termCountHardLimit) {\n          break;\n        }\n\n        if (termOrd == termOrdToBytesOffset.size()) {\n          // NOTE: this code only runs if the incoming\n          // reader impl doesn't implement\n          // getUniqueTermCount (which should be uncommon)\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n        }\n        termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n        docs = termsEnum.docs(delDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToTermOrd.set(docID, termOrd);\n        }\n        termOrd++;\n      }\n\n      if (termOrdToBytesOffset.size() > termOrd) {\n        termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n  }\n\n","sourceOld":"  @Override\n  public T create(IndexReader reader) throws IOException\n  {\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption(FASTER_BUT_MORE_RAM);\n\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBytesBPV;\n    int startTermsBPV;\n    int startNumUniqueTerms;\n\n    int maxDoc = reader.maxDoc();\n    final int termCountHardLimit;\n    if (maxDoc == Integer.MAX_VALUE) {\n      termCountHardLimit = Integer.MAX_VALUE;\n    } else {\n      termCountHardLimit = maxDoc+1;\n    }\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n\n        if (numUniqueTerms > termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          numUniqueTerms = termCountHardLimit;\n        }\n\n        startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n        startNumUniqueTerms = (int) numUniqueTerms;\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n    } else {\n      startBytesBPV = 1;\n      startTermsBPV = 1;\n      startNumUniqueTerms = 1;\n    }\n\n    GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n    final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // 0 is reserved for \"unset\"\n    bytes.copyUsingLengthPrefix(new BytesRef());\n    int termOrd = 1;\n\n    if (terms != null) {\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\n      DocsEnum docs = null;\n\n      while(true) {\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        if (termOrd >= termCountHardLimit) {\n          break;\n        }\n\n        if (termOrd == termOrdToBytesOffset.size()) {\n          // NOTE: this code only runs if the incoming\n          // reader impl doesn't implement\n          // getUniqueTermCount (which should be uncommon)\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n        }\n        termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n        docs = termsEnum.docs(delDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToTermOrd.set(docID, termOrd);\n        }\n        termOrd++;\n      }\n\n      if (termOrdToBytesOffset.size() > termOrd) {\n        termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n      }\n    }\n\n    // maybe an int-only impl?\n    @SuppressWarnings(\"unchecked\") final T t =\n      (T)new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    return t;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4ecea1664e8617d82eca3b8055a3c37cb4da8511","date":1287578668,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/search/cache/DocTermsIndexCreator#create(IndexReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/cache/DocTermsIndexCreator#create(IndexReader).mjava","sourceNew":"  @Override\n  public DocTermsIndex create(IndexReader reader) throws IOException\n  {\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption(FASTER_BUT_MORE_RAM);\n\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBytesBPV;\n    int startTermsBPV;\n    int startNumUniqueTerms;\n\n    int maxDoc = reader.maxDoc();\n    final int termCountHardLimit;\n    if (maxDoc == Integer.MAX_VALUE) {\n      termCountHardLimit = Integer.MAX_VALUE;\n    } else {\n      termCountHardLimit = maxDoc+1;\n    }\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n\n        if (numUniqueTerms > termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          numUniqueTerms = termCountHardLimit;\n        }\n\n        startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n        startNumUniqueTerms = (int) numUniqueTerms;\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n    } else {\n      startBytesBPV = 1;\n      startTermsBPV = 1;\n      startNumUniqueTerms = 1;\n    }\n\n    GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n    final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // 0 is reserved for \"unset\"\n    bytes.copyUsingLengthPrefix(new BytesRef());\n    int termOrd = 1;\n\n    if (terms != null) {\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\n      DocsEnum docs = null;\n\n      while(true) {\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        if (termOrd >= termCountHardLimit) {\n          break;\n        }\n\n        if (termOrd == termOrdToBytesOffset.size()) {\n          // NOTE: this code only runs if the incoming\n          // reader impl doesn't implement\n          // getUniqueTermCount (which should be uncommon)\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n        }\n        termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n        docs = termsEnum.docs(delDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToTermOrd.set(docID, termOrd);\n        }\n        termOrd++;\n      }\n\n      if (termOrdToBytesOffset.size() > termOrd) {\n        termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n  }\n\n","sourceOld":"  @Override\n  public T create(IndexReader reader) throws IOException\n  {\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption(FASTER_BUT_MORE_RAM);\n\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBytesBPV;\n    int startTermsBPV;\n    int startNumUniqueTerms;\n\n    int maxDoc = reader.maxDoc();\n    final int termCountHardLimit;\n    if (maxDoc == Integer.MAX_VALUE) {\n      termCountHardLimit = Integer.MAX_VALUE;\n    } else {\n      termCountHardLimit = maxDoc+1;\n    }\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n\n        if (numUniqueTerms > termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          numUniqueTerms = termCountHardLimit;\n        }\n\n        startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n        startNumUniqueTerms = (int) numUniqueTerms;\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n    } else {\n      startBytesBPV = 1;\n      startTermsBPV = 1;\n      startNumUniqueTerms = 1;\n    }\n\n    GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n    final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // 0 is reserved for \"unset\"\n    bytes.copyUsingLengthPrefix(new BytesRef());\n    int termOrd = 1;\n\n    if (terms != null) {\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\n      DocsEnum docs = null;\n\n      while(true) {\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        if (termOrd >= termCountHardLimit) {\n          break;\n        }\n\n        if (termOrd == termOrdToBytesOffset.size()) {\n          // NOTE: this code only runs if the incoming\n          // reader impl doesn't implement\n          // getUniqueTermCount (which should be uncommon)\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n        }\n        termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n        docs = termsEnum.docs(delDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToTermOrd.set(docID, termOrd);\n        }\n        termOrd++;\n      }\n\n      if (termOrdToBytesOffset.size() > termOrd) {\n        termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n      }\n    }\n\n    // maybe an int-only impl?\n    return (T)new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eee24cbec95372b8e747bf9146a5ef33734029a6","date":1291686380,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/cache/DocTermsIndexCreator#create(IndexReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/cache/DocTermsIndexCreator#create(IndexReader).mjava","sourceNew":"  @Override\n  public DocTermsIndex create(IndexReader reader) throws IOException\n  {\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption(FASTER_BUT_MORE_RAM);\n\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBytesBPV;\n    int startTermsBPV;\n    int startNumUniqueTerms;\n\n    int maxDoc = reader.maxDoc();\n    final int termCountHardLimit;\n    if (maxDoc == Integer.MAX_VALUE) {\n      termCountHardLimit = Integer.MAX_VALUE;\n    } else {\n      termCountHardLimit = maxDoc+1;\n    }\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n\n        if (numUniqueTerms > termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          numUniqueTerms = termCountHardLimit;\n        }\n\n        startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n        startNumUniqueTerms = (int) numUniqueTerms;\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n    } else {\n      startBytesBPV = 1;\n      startTermsBPV = 1;\n      startNumUniqueTerms = 1;\n    }\n\n    GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n    final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // 0 is reserved for \"unset\"\n    bytes.copyUsingLengthPrefix(new BytesRef());\n    int termOrd = 1;\n\n    if (terms != null) {\n      final TermsEnum termsEnum = terms.iterator();\n      DocsEnum docs = null;\n\n      while(true) {\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        if (termOrd >= termCountHardLimit) {\n          break;\n        }\n\n        if (termOrd == termOrdToBytesOffset.size()) {\n          // NOTE: this code only runs if the incoming\n          // reader impl doesn't implement\n          // getUniqueTermCount (which should be uncommon)\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n        }\n        termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n        docs = termsEnum.docs(null, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToTermOrd.set(docID, termOrd);\n        }\n        termOrd++;\n      }\n\n      if (termOrdToBytesOffset.size() > termOrd) {\n        termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n  }\n\n","sourceOld":"  @Override\n  public DocTermsIndex create(IndexReader reader) throws IOException\n  {\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption(FASTER_BUT_MORE_RAM);\n\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBytesBPV;\n    int startTermsBPV;\n    int startNumUniqueTerms;\n\n    int maxDoc = reader.maxDoc();\n    final int termCountHardLimit;\n    if (maxDoc == Integer.MAX_VALUE) {\n      termCountHardLimit = Integer.MAX_VALUE;\n    } else {\n      termCountHardLimit = maxDoc+1;\n    }\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n\n        if (numUniqueTerms > termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          numUniqueTerms = termCountHardLimit;\n        }\n\n        startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n        startNumUniqueTerms = (int) numUniqueTerms;\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n    } else {\n      startBytesBPV = 1;\n      startTermsBPV = 1;\n      startNumUniqueTerms = 1;\n    }\n\n    GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n    final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // 0 is reserved for \"unset\"\n    bytes.copyUsingLengthPrefix(new BytesRef());\n    int termOrd = 1;\n\n    if (terms != null) {\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\n      DocsEnum docs = null;\n\n      while(true) {\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        if (termOrd >= termCountHardLimit) {\n          break;\n        }\n\n        if (termOrd == termOrdToBytesOffset.size()) {\n          // NOTE: this code only runs if the incoming\n          // reader impl doesn't implement\n          // getUniqueTermCount (which should be uncommon)\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n        }\n        termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n        docs = termsEnum.docs(delDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToTermOrd.set(docID, termOrd);\n        }\n        termOrd++;\n      }\n\n      if (termOrdToBytesOffset.size() > termOrd) {\n        termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n  }\n\n","bugFix":["5637938a7dc36e7ff09a5d9398957bd46b15129a"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4a69e5860d014751cc9329dfeb441a6d8fd1ed8e","date":1291833341,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/cache/DocTermsIndexCreator#create(IndexReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/cache/DocTermsIndexCreator#create(IndexReader).mjava","sourceNew":"  @Override\n  public DocTermsIndex create(IndexReader reader) throws IOException\n  {\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption(FASTER_BUT_MORE_RAM);\n\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBytesBPV;\n    int startTermsBPV;\n    int startNumUniqueTerms;\n\n    int maxDoc = reader.maxDoc();\n    final int termCountHardLimit;\n    if (maxDoc == Integer.MAX_VALUE) {\n      termCountHardLimit = Integer.MAX_VALUE;\n    } else {\n      termCountHardLimit = maxDoc+1;\n    }\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n\n        if (numUniqueTerms > termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          numUniqueTerms = termCountHardLimit;\n        }\n\n        startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n        startNumUniqueTerms = (int) numUniqueTerms;\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n    } else {\n      startBytesBPV = 1;\n      startTermsBPV = 1;\n      startNumUniqueTerms = 1;\n    }\n\n    GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n    final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // 0 is reserved for \"unset\"\n    bytes.copyUsingLengthPrefix(new BytesRef());\n    int termOrd = 1;\n\n    if (terms != null) {\n      final TermsEnum termsEnum = terms.iterator();\n      DocsEnum docs = null;\n\n      while(true) {\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        if (termOrd >= termCountHardLimit) {\n          break;\n        }\n\n        if (termOrd == termOrdToBytesOffset.size()) {\n          // NOTE: this code only runs if the incoming\n          // reader impl doesn't implement\n          // getUniqueTermCount (which should be uncommon)\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n        }\n        termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n        docs = termsEnum.docs(null, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToTermOrd.set(docID, termOrd);\n        }\n        termOrd++;\n      }\n\n      if (termOrdToBytesOffset.size() > termOrd) {\n        termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n  }\n\n","sourceOld":"  @Override\n  public DocTermsIndex create(IndexReader reader) throws IOException\n  {\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption(FASTER_BUT_MORE_RAM);\n\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBytesBPV;\n    int startTermsBPV;\n    int startNumUniqueTerms;\n\n    int maxDoc = reader.maxDoc();\n    final int termCountHardLimit;\n    if (maxDoc == Integer.MAX_VALUE) {\n      termCountHardLimit = Integer.MAX_VALUE;\n    } else {\n      termCountHardLimit = maxDoc+1;\n    }\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n\n        if (numUniqueTerms > termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          numUniqueTerms = termCountHardLimit;\n        }\n\n        startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n        startNumUniqueTerms = (int) numUniqueTerms;\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n    } else {\n      startBytesBPV = 1;\n      startTermsBPV = 1;\n      startNumUniqueTerms = 1;\n    }\n\n    GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n    final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // 0 is reserved for \"unset\"\n    bytes.copyUsingLengthPrefix(new BytesRef());\n    int termOrd = 1;\n\n    if (terms != null) {\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\n      DocsEnum docs = null;\n\n      while(true) {\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        if (termOrd >= termCountHardLimit) {\n          break;\n        }\n\n        if (termOrd == termOrdToBytesOffset.size()) {\n          // NOTE: this code only runs if the incoming\n          // reader impl doesn't implement\n          // getUniqueTermCount (which should be uncommon)\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n        }\n        termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n        docs = termsEnum.docs(delDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToTermOrd.set(docID, termOrd);\n        }\n        termOrd++;\n      }\n\n      if (termOrdToBytesOffset.size() > termOrd) {\n        termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":0,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/search/cache/DocTermsIndexCreator#create(IndexReader).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public DocTermsIndex create(IndexReader reader) throws IOException\n  {\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption(FASTER_BUT_MORE_RAM);\n\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBytesBPV;\n    int startTermsBPV;\n    int startNumUniqueTerms;\n\n    int maxDoc = reader.maxDoc();\n    final int termCountHardLimit;\n    if (maxDoc == Integer.MAX_VALUE) {\n      termCountHardLimit = Integer.MAX_VALUE;\n    } else {\n      termCountHardLimit = maxDoc+1;\n    }\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n\n        if (numUniqueTerms > termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          numUniqueTerms = termCountHardLimit;\n        }\n\n        startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n        startNumUniqueTerms = (int) numUniqueTerms;\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n    } else {\n      startBytesBPV = 1;\n      startTermsBPV = 1;\n      startNumUniqueTerms = 1;\n    }\n\n    GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n    final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // 0 is reserved for \"unset\"\n    bytes.copyUsingLengthPrefix(new BytesRef());\n    int termOrd = 1;\n\n    if (terms != null) {\n      final TermsEnum termsEnum = terms.iterator();\n      DocsEnum docs = null;\n\n      while(true) {\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        if (termOrd >= termCountHardLimit) {\n          break;\n        }\n\n        if (termOrd == termOrdToBytesOffset.size()) {\n          // NOTE: this code only runs if the incoming\n          // reader impl doesn't implement\n          // getUniqueTermCount (which should be uncommon)\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n        }\n        termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n        docs = termsEnum.docs(null, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToTermOrd.set(docID, termOrd);\n        }\n        termOrd++;\n      }\n\n      if (termOrdToBytesOffset.size() > termOrd) {\n        termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fafef7c83fe8e0b3ca9298d5d75d6b943dc28153","date":1308670974,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/cache/DocTermsIndexCreator#create(IndexReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/cache/DocTermsIndexCreator#create(IndexReader).mjava","sourceNew":"  @Override\n  public DocTermsIndex create(IndexReader reader) throws IOException\n  {\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption(FASTER_BUT_MORE_RAM);\n\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBytesBPV;\n    int startTermsBPV;\n    int startNumUniqueTerms;\n\n    int maxDoc = reader.maxDoc();\n    final int termCountHardLimit;\n    if (maxDoc == Integer.MAX_VALUE) {\n      termCountHardLimit = Integer.MAX_VALUE;\n    } else {\n      termCountHardLimit = maxDoc+1;\n    }\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n\n        if (numUniqueTerms > termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          numUniqueTerms = termCountHardLimit;\n        }\n\n        startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n        startNumUniqueTerms = (int) numUniqueTerms;\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n    } else {\n      startBytesBPV = 1;\n      startTermsBPV = 1;\n      startNumUniqueTerms = 1;\n    }\n\n    GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n    final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // 0 is reserved for \"unset\"\n    bytes.copyUsingLengthPrefix(new BytesRef());\n    int termOrd = 1;\n\n    if (terms != null) {\n      final TermsEnum termsEnum = terms.iterator();\n      DocsEnum docs = null;\n\n      while(true) {\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        if (termOrd >= termCountHardLimit) {\n          break;\n        }\n\n        if (termOrd == termOrdToBytesOffset.size()) {\n          // NOTE: this code only runs if the incoming\n          // reader impl doesn't implement\n          // getUniqueTermCount (which should be uncommon)\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n        }\n        termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n        docs = termsEnum.docs(null, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToTermOrd.set(docID, termOrd);\n        }\n        termOrd++;\n      }\n\n      if (termOrdToBytesOffset.size() > termOrd) {\n        termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n  }\n\n","sourceOld":"  @Override\n  public DocTermsIndex create(IndexReader reader) throws IOException\n  {\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption(FASTER_BUT_MORE_RAM);\n\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBytesBPV;\n    int startTermsBPV;\n    int startNumUniqueTerms;\n\n    int maxDoc = reader.maxDoc();\n    final int termCountHardLimit;\n    if (maxDoc == Integer.MAX_VALUE) {\n      termCountHardLimit = Integer.MAX_VALUE;\n    } else {\n      termCountHardLimit = maxDoc+1;\n    }\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n\n        if (numUniqueTerms > termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          numUniqueTerms = termCountHardLimit;\n        }\n\n        startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n        startNumUniqueTerms = (int) numUniqueTerms;\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n    } else {\n      startBytesBPV = 1;\n      startTermsBPV = 1;\n      startNumUniqueTerms = 1;\n    }\n\n    GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n    final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // 0 is reserved for \"unset\"\n    bytes.copyUsingLengthPrefix(new BytesRef());\n    int termOrd = 1;\n\n    if (terms != null) {\n      final TermsEnum termsEnum = terms.iterator();\n      DocsEnum docs = null;\n\n      while(true) {\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        if (termOrd >= termCountHardLimit) {\n          break;\n        }\n\n        if (termOrd == termOrdToBytesOffset.size()) {\n          // NOTE: this code only runs if the incoming\n          // reader impl doesn't implement\n          // getUniqueTermCount (which should be uncommon)\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n        }\n        termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n        docs = termsEnum.docs(null, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToTermOrd.set(docID, termOrd);\n        }\n        termOrd++;\n      }\n\n      if (termOrdToBytesOffset.size() > termOrd) {\n        termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2553b00f699380c64959ccb27991289aae87be2e","date":1309290151,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/search/cache/DocTermsIndexCreator#create(IndexReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/cache/DocTermsIndexCreator#create(IndexReader).mjava","sourceNew":"  @Override\n  public DocTermsIndex create(IndexReader reader) throws IOException\n  {\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption(FASTER_BUT_MORE_RAM);\n\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBytesBPV;\n    int startTermsBPV;\n    int startNumUniqueTerms;\n\n    int maxDoc = reader.maxDoc();\n    final int termCountHardLimit;\n    if (maxDoc == Integer.MAX_VALUE) {\n      termCountHardLimit = Integer.MAX_VALUE;\n    } else {\n      termCountHardLimit = maxDoc+1;\n    }\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n\n        if (numUniqueTerms > termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          numUniqueTerms = termCountHardLimit;\n        }\n\n        startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n        startNumUniqueTerms = (int) numUniqueTerms;\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n    } else {\n      startBytesBPV = 1;\n      startTermsBPV = 1;\n      startNumUniqueTerms = 1;\n    }\n\n    GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n    final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // 0 is reserved for \"unset\"\n    bytes.copyUsingLengthPrefix(new BytesRef());\n    int termOrd = 1;\n\n    if (terms != null) {\n      final TermsEnum termsEnum = terms.iterator();\n      DocsEnum docs = null;\n\n      while(true) {\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        if (termOrd >= termCountHardLimit) {\n          break;\n        }\n\n        if (termOrd == termOrdToBytesOffset.size()) {\n          // NOTE: this code only runs if the incoming\n          // reader impl doesn't implement\n          // getUniqueTermCount (which should be uncommon)\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n        }\n        termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n        docs = termsEnum.docs(null, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToTermOrd.set(docID, termOrd);\n        }\n        termOrd++;\n      }\n\n      if (termOrdToBytesOffset.size() > termOrd) {\n        termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n  }\n\n","sourceOld":"  @Override\n  public DocTermsIndex create(IndexReader reader) throws IOException\n  {\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption(FASTER_BUT_MORE_RAM);\n\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBytesBPV;\n    int startTermsBPV;\n    int startNumUniqueTerms;\n\n    int maxDoc = reader.maxDoc();\n    final int termCountHardLimit;\n    if (maxDoc == Integer.MAX_VALUE) {\n      termCountHardLimit = Integer.MAX_VALUE;\n    } else {\n      termCountHardLimit = maxDoc+1;\n    }\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n\n        if (numUniqueTerms > termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          numUniqueTerms = termCountHardLimit;\n        }\n\n        startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n        startNumUniqueTerms = (int) numUniqueTerms;\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n    } else {\n      startBytesBPV = 1;\n      startTermsBPV = 1;\n      startNumUniqueTerms = 1;\n    }\n\n    GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n    final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // 0 is reserved for \"unset\"\n    bytes.copyUsingLengthPrefix(new BytesRef());\n    int termOrd = 1;\n\n    if (terms != null) {\n      final TermsEnum termsEnum = terms.iterator();\n      DocsEnum docs = null;\n\n      while(true) {\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        if (termOrd >= termCountHardLimit) {\n          break;\n        }\n\n        if (termOrd == termOrdToBytesOffset.size()) {\n          // NOTE: this code only runs if the incoming\n          // reader impl doesn't implement\n          // getUniqueTermCount (which should be uncommon)\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n        }\n        termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n        docs = termsEnum.docs(null, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToTermOrd.set(docID, termOrd);\n        }\n        termOrd++;\n      }\n\n      if (termOrdToBytesOffset.size() > termOrd) {\n        termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/search/cache/DocTermsIndexCreator#create(IndexReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/cache/DocTermsIndexCreator#create(IndexReader).mjava","sourceNew":"  @Override\n  public DocTermsIndex create(IndexReader reader) throws IOException\n  {\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption(FASTER_BUT_MORE_RAM);\n\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBytesBPV;\n    int startTermsBPV;\n    int startNumUniqueTerms;\n\n    int maxDoc = reader.maxDoc();\n    final int termCountHardLimit;\n    if (maxDoc == Integer.MAX_VALUE) {\n      termCountHardLimit = Integer.MAX_VALUE;\n    } else {\n      termCountHardLimit = maxDoc+1;\n    }\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n\n        if (numUniqueTerms > termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          numUniqueTerms = termCountHardLimit;\n        }\n\n        startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n        startNumUniqueTerms = (int) numUniqueTerms;\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n    } else {\n      startBytesBPV = 1;\n      startTermsBPV = 1;\n      startNumUniqueTerms = 1;\n    }\n\n    GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n    final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // 0 is reserved for \"unset\"\n    bytes.copyUsingLengthPrefix(new BytesRef());\n    int termOrd = 1;\n\n    if (terms != null) {\n      final TermsEnum termsEnum = terms.iterator();\n      DocsEnum docs = null;\n\n      while(true) {\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        if (termOrd >= termCountHardLimit) {\n          break;\n        }\n\n        if (termOrd == termOrdToBytesOffset.size()) {\n          // NOTE: this code only runs if the incoming\n          // reader impl doesn't implement\n          // getUniqueTermCount (which should be uncommon)\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n        }\n        termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n        docs = termsEnum.docs(null, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToTermOrd.set(docID, termOrd);\n        }\n        termOrd++;\n      }\n\n      if (termOrdToBytesOffset.size() > termOrd) {\n        termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n  }\n\n","sourceOld":"  @Override\n  public DocTermsIndex create(IndexReader reader) throws IOException\n  {\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption(FASTER_BUT_MORE_RAM);\n\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBytesBPV;\n    int startTermsBPV;\n    int startNumUniqueTerms;\n\n    int maxDoc = reader.maxDoc();\n    final int termCountHardLimit;\n    if (maxDoc == Integer.MAX_VALUE) {\n      termCountHardLimit = Integer.MAX_VALUE;\n    } else {\n      termCountHardLimit = maxDoc+1;\n    }\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n\n        if (numUniqueTerms > termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          numUniqueTerms = termCountHardLimit;\n        }\n\n        startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n        startNumUniqueTerms = (int) numUniqueTerms;\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n    } else {\n      startBytesBPV = 1;\n      startTermsBPV = 1;\n      startNumUniqueTerms = 1;\n    }\n\n    GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n    final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // 0 is reserved for \"unset\"\n    bytes.copyUsingLengthPrefix(new BytesRef());\n    int termOrd = 1;\n\n    if (terms != null) {\n      final TermsEnum termsEnum = terms.iterator();\n      DocsEnum docs = null;\n\n      while(true) {\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        if (termOrd >= termCountHardLimit) {\n          break;\n        }\n\n        if (termOrd == termOrdToBytesOffset.size()) {\n          // NOTE: this code only runs if the incoming\n          // reader impl doesn't implement\n          // getUniqueTermCount (which should be uncommon)\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n        }\n        termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n        docs = termsEnum.docs(null, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToTermOrd.set(docID, termOrd);\n        }\n        termOrd++;\n      }\n\n      if (termOrdToBytesOffset.size() > termOrd) {\n        termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"382fe3a6ca9745891afebda9b9a57cc158305545","date":1320952430,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/search/cache/DocTermsIndexCreator#create(IndexReader).mjava","sourceNew":null,"sourceOld":"  @Override\n  public DocTermsIndex create(IndexReader reader) throws IOException\n  {\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption(FASTER_BUT_MORE_RAM);\n\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBytesBPV;\n    int startTermsBPV;\n    int startNumUniqueTerms;\n\n    int maxDoc = reader.maxDoc();\n    final int termCountHardLimit;\n    if (maxDoc == Integer.MAX_VALUE) {\n      termCountHardLimit = Integer.MAX_VALUE;\n    } else {\n      termCountHardLimit = maxDoc+1;\n    }\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n\n        if (numUniqueTerms > termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          numUniqueTerms = termCountHardLimit;\n        }\n\n        startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n        startNumUniqueTerms = (int) numUniqueTerms;\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n    } else {\n      startBytesBPV = 1;\n      startTermsBPV = 1;\n      startNumUniqueTerms = 1;\n    }\n\n    GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n    final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // 0 is reserved for \"unset\"\n    bytes.copyUsingLengthPrefix(new BytesRef());\n    int termOrd = 1;\n\n    if (terms != null) {\n      final TermsEnum termsEnum = terms.iterator();\n      DocsEnum docs = null;\n\n      while(true) {\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        if (termOrd >= termCountHardLimit) {\n          break;\n        }\n\n        if (termOrd == termOrdToBytesOffset.size()) {\n          // NOTE: this code only runs if the incoming\n          // reader impl doesn't implement\n          // getUniqueTermCount (which should be uncommon)\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n        }\n        termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n        docs = termsEnum.docs(null, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToTermOrd.set(docID, termOrd);\n        }\n        termOrd++;\n      }\n\n      if (termOrdToBytesOffset.size() > termOrd) {\n        termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"eee9156bf08d7eaae5e8d8ab5f855ba61012e257":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"fafef7c83fe8e0b3ca9298d5d75d6b943dc28153":["eee24cbec95372b8e747bf9146a5ef33734029a6"],"4a69e5860d014751cc9329dfeb441a6d8fd1ed8e":["4ecea1664e8617d82eca3b8055a3c37cb4da8511"],"2553b00f699380c64959ccb27991289aae87be2e":["eee24cbec95372b8e747bf9146a5ef33734029a6","fafef7c83fe8e0b3ca9298d5d75d6b943dc28153"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","eee24cbec95372b8e747bf9146a5ef33734029a6"],"382fe3a6ca9745891afebda9b9a57cc158305545":["fafef7c83fe8e0b3ca9298d5d75d6b943dc28153"],"eee24cbec95372b8e747bf9146a5ef33734029a6":["8f684ff0d559fa047f3bfa01bcbc78fc826a7817"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["eee24cbec95372b8e747bf9146a5ef33734029a6","fafef7c83fe8e0b3ca9298d5d75d6b943dc28153"],"8f684ff0d559fa047f3bfa01bcbc78fc826a7817":["eda01daf042abfe36d82ad9ecad499220a1f3ecf"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"eda01daf042abfe36d82ad9ecad499220a1f3ecf":["5637938a7dc36e7ff09a5d9398957bd46b15129a"],"5637938a7dc36e7ff09a5d9398957bd46b15129a":["eee9156bf08d7eaae5e8d8ab5f855ba61012e257"],"4ecea1664e8617d82eca3b8055a3c37cb4da8511":["5637938a7dc36e7ff09a5d9398957bd46b15129a","8f684ff0d559fa047f3bfa01bcbc78fc826a7817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["382fe3a6ca9745891afebda9b9a57cc158305545"]},"commit2Childs":{"eee9156bf08d7eaae5e8d8ab5f855ba61012e257":["5637938a7dc36e7ff09a5d9398957bd46b15129a"],"fafef7c83fe8e0b3ca9298d5d75d6b943dc28153":["2553b00f699380c64959ccb27991289aae87be2e","382fe3a6ca9745891afebda9b9a57cc158305545","d083e83f225b11e5fdd900e83d26ddb385b6955c"],"4a69e5860d014751cc9329dfeb441a6d8fd1ed8e":[],"2553b00f699380c64959ccb27991289aae87be2e":[],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":[],"382fe3a6ca9745891afebda9b9a57cc158305545":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"eee24cbec95372b8e747bf9146a5ef33734029a6":["fafef7c83fe8e0b3ca9298d5d75d6b943dc28153","2553b00f699380c64959ccb27991289aae87be2e","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","d083e83f225b11e5fdd900e83d26ddb385b6955c"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"8f684ff0d559fa047f3bfa01bcbc78fc826a7817":["eee24cbec95372b8e747bf9146a5ef33734029a6","4ecea1664e8617d82eca3b8055a3c37cb4da8511"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["eee9156bf08d7eaae5e8d8ab5f855ba61012e257","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"eda01daf042abfe36d82ad9ecad499220a1f3ecf":["8f684ff0d559fa047f3bfa01bcbc78fc826a7817"],"4ecea1664e8617d82eca3b8055a3c37cb4da8511":["4a69e5860d014751cc9329dfeb441a6d8fd1ed8e"],"5637938a7dc36e7ff09a5d9398957bd46b15129a":["eda01daf042abfe36d82ad9ecad499220a1f3ecf","4ecea1664e8617d82eca3b8055a3c37cb4da8511"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4a69e5860d014751cc9329dfeb441a6d8fd1ed8e","2553b00f699380c64959ccb27991289aae87be2e","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","d083e83f225b11e5fdd900e83d26ddb385b6955c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}