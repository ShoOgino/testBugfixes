{"path":"lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/PostingsHighlighter#highlight(Query,IndexSearcher,TopDocs,int).mjava","commits":[{"id":"ffabe030a2b84ad50adb7265da07ee78f1c58f6a","date":1355239263,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/PostingsHighlighter#highlight(Query,IndexSearcher,TopDocs,int).mjava","pathOld":"/dev/null","sourceNew":"  public String[] highlight(Query query, IndexSearcher searcher, TopDocs topDocs, int maxPassages) throws IOException {\n    final IndexReader reader = searcher.getIndexReader();\n    final ScoreDoc scoreDocs[] = topDocs.scoreDocs;\n    query = rewrite(query);\n    SortedSet<Term> terms = new TreeSet<Term>();\n    query.extractTerms(terms);\n    terms = terms.subSet(floor, ceiling);\n    // TODO: should we have some reasonable defaults for term pruning? (e.g. stopwords)\n\n    int docids[] = new int[scoreDocs.length];\n    for (int i = 0; i < docids.length; i++) {\n      docids[i] = scoreDocs[i].doc;\n    }\n    IndexReaderContext readerContext = reader.getContext();\n    List<AtomicReaderContext> leaves = readerContext.leaves();\n    \n    // sort for sequential io\n    Arrays.sort(docids);\n    \n    // pull stored data\n    LimitedStoredFieldVisitor visitor = new LimitedStoredFieldVisitor(field, maxLength);\n    String contents[] = new String[docids.length];\n    for (int i = 0; i < contents.length; i++) {\n      reader.document(docids[i], visitor);\n      contents[i] = visitor.getValue();\n      visitor.reset();\n    }\n    \n    // now pull index stats: TODO: we should probably pull this from the reader instead?\n    // this could be a distributed call, which is crazy\n    CollectionStatistics collectionStats = searcher.collectionStatistics(field);\n    TermContext termContexts[] = new TermContext[terms.size()];\n    Term termTexts[] = new Term[terms.size()]; // needed for seekExact\n    float weights[] = new float[terms.size()];\n    int upto = 0;\n    for (Term term : terms) {\n      termTexts[upto] = term;\n      TermContext context = TermContext.build(readerContext, term, true);\n      termContexts[upto] = context;\n      TermStatistics termStats = searcher.termStatistics(term, context);\n      weights[upto] = scorer.weight(collectionStats, termStats);\n      upto++;\n      // TODO: should we instead score all the documents term-at-a-time here?\n      // the i/o would be better, but more transient ram\n    }\n    \n    BreakIterator bi = (BreakIterator)breakIterator.clone();\n    \n    Map<Integer,String> highlights = new HashMap<Integer,String>();\n    \n    // reuse in the real sense... for docs in same segment we just advance our old enum\n    DocsAndPositionsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      AtomicReaderContext subContext = leaves.get(leaf);\n      AtomicReader r = subContext.reader();\n      Terms t = r.terms(field);\n      if (t == null) {\n        continue; // nothing to do\n      }\n      if (leaf != lastLeaf) {\n        termsEnum = t.iterator(null);\n        postings = new DocsAndPositionsEnum[terms.size()];\n      };\n      Passage passages[] = highlightDoc(termTexts, termContexts, subContext.ord, weights, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      if (passages.length > 0) {\n        // otherwise a null snippet\n        highlights.put(doc, formatter.format(passages, content));\n      }\n      lastLeaf = leaf;\n    }\n    \n    String[] result = new String[scoreDocs.length];\n    for (int i = 0; i < scoreDocs.length; i++) {\n      result[i] = highlights.get(scoreDocs[i].doc);\n    }\n    return result;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9882ec7961965d6ac6203049795d94c122015950","date":1355244469,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/PostingsHighlighter#highlight(Query,IndexSearcher,TopDocs,int).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/PostingsHighlighter#highlight(Query,IndexSearcher,TopDocs,int).mjava","sourceNew":"  public String[] highlight(Query query, IndexSearcher searcher, TopDocs topDocs, int maxPassages) throws IOException {\n    final IndexReader reader = searcher.getIndexReader();\n    final ScoreDoc scoreDocs[] = topDocs.scoreDocs;\n    query = rewrite(query);\n    SortedSet<Term> terms = new TreeSet<Term>();\n    query.extractTerms(terms);\n    terms = terms.subSet(floor, ceiling);\n    // TODO: should we have some reasonable defaults for term pruning? (e.g. stopwords)\n\n    int docids[] = new int[scoreDocs.length];\n    for (int i = 0; i < docids.length; i++) {\n      docids[i] = scoreDocs[i].doc;\n    }\n    IndexReaderContext readerContext = reader.getContext();\n    List<AtomicReaderContext> leaves = readerContext.leaves();\n    \n    // sort for sequential io\n    Arrays.sort(docids);\n    \n    // pull stored data\n    LimitedStoredFieldVisitor visitor = new LimitedStoredFieldVisitor(field, maxLength);\n    String contents[] = new String[docids.length];\n    for (int i = 0; i < contents.length; i++) {\n      reader.document(docids[i], visitor);\n      contents[i] = visitor.getValue();\n      visitor.reset();\n    }\n    \n    // now pull index stats: TODO: we should probably pull this from the reader instead?\n    // this could be a distributed call, which is crazy\n    CollectionStatistics collectionStats = searcher.collectionStatistics(field);\n    TermContext termContexts[] = new TermContext[terms.size()];\n    Term termTexts[] = new Term[terms.size()]; // needed for seekExact\n    float weights[] = new float[terms.size()];\n    int upto = 0;\n    for (Term term : terms) {\n      termTexts[upto] = term;\n      TermContext context = TermContext.build(readerContext, term, true);\n      termContexts[upto] = context;\n      TermStatistics termStats = searcher.termStatistics(term, context);\n      weights[upto] = scorer.weight(collectionStats, termStats);\n      upto++;\n      // TODO: should we instead score all the documents term-at-a-time here?\n      // the i/o would be better, but more transient ram\n    }\n    \n    BreakIterator bi = (BreakIterator)breakIterator.clone();\n    \n    Map<Integer,String> highlights = new HashMap<Integer,String>();\n    \n    // reuse in the real sense... for docs in same segment we just advance our old enum\n    DocsAndPositionsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      AtomicReaderContext subContext = leaves.get(leaf);\n      AtomicReader r = subContext.reader();\n      Terms t = r.terms(field);\n      if (t == null) {\n        continue; // nothing to do\n      }\n      if (leaf != lastLeaf) {\n        termsEnum = t.iterator(null);\n        postings = new DocsAndPositionsEnum[terms.size()];\n      }\n      Passage passages[] = highlightDoc(termTexts, termContexts, subContext.ord, weights, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      if (passages.length > 0) {\n        // otherwise a null snippet\n        highlights.put(doc, formatter.format(passages, content));\n      }\n      lastLeaf = leaf;\n    }\n    \n    String[] result = new String[scoreDocs.length];\n    for (int i = 0; i < scoreDocs.length; i++) {\n      result[i] = highlights.get(scoreDocs[i].doc);\n    }\n    return result;\n  }\n\n","sourceOld":"  public String[] highlight(Query query, IndexSearcher searcher, TopDocs topDocs, int maxPassages) throws IOException {\n    final IndexReader reader = searcher.getIndexReader();\n    final ScoreDoc scoreDocs[] = topDocs.scoreDocs;\n    query = rewrite(query);\n    SortedSet<Term> terms = new TreeSet<Term>();\n    query.extractTerms(terms);\n    terms = terms.subSet(floor, ceiling);\n    // TODO: should we have some reasonable defaults for term pruning? (e.g. stopwords)\n\n    int docids[] = new int[scoreDocs.length];\n    for (int i = 0; i < docids.length; i++) {\n      docids[i] = scoreDocs[i].doc;\n    }\n    IndexReaderContext readerContext = reader.getContext();\n    List<AtomicReaderContext> leaves = readerContext.leaves();\n    \n    // sort for sequential io\n    Arrays.sort(docids);\n    \n    // pull stored data\n    LimitedStoredFieldVisitor visitor = new LimitedStoredFieldVisitor(field, maxLength);\n    String contents[] = new String[docids.length];\n    for (int i = 0; i < contents.length; i++) {\n      reader.document(docids[i], visitor);\n      contents[i] = visitor.getValue();\n      visitor.reset();\n    }\n    \n    // now pull index stats: TODO: we should probably pull this from the reader instead?\n    // this could be a distributed call, which is crazy\n    CollectionStatistics collectionStats = searcher.collectionStatistics(field);\n    TermContext termContexts[] = new TermContext[terms.size()];\n    Term termTexts[] = new Term[terms.size()]; // needed for seekExact\n    float weights[] = new float[terms.size()];\n    int upto = 0;\n    for (Term term : terms) {\n      termTexts[upto] = term;\n      TermContext context = TermContext.build(readerContext, term, true);\n      termContexts[upto] = context;\n      TermStatistics termStats = searcher.termStatistics(term, context);\n      weights[upto] = scorer.weight(collectionStats, termStats);\n      upto++;\n      // TODO: should we instead score all the documents term-at-a-time here?\n      // the i/o would be better, but more transient ram\n    }\n    \n    BreakIterator bi = (BreakIterator)breakIterator.clone();\n    \n    Map<Integer,String> highlights = new HashMap<Integer,String>();\n    \n    // reuse in the real sense... for docs in same segment we just advance our old enum\n    DocsAndPositionsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      AtomicReaderContext subContext = leaves.get(leaf);\n      AtomicReader r = subContext.reader();\n      Terms t = r.terms(field);\n      if (t == null) {\n        continue; // nothing to do\n      }\n      if (leaf != lastLeaf) {\n        termsEnum = t.iterator(null);\n        postings = new DocsAndPositionsEnum[terms.size()];\n      };\n      Passage passages[] = highlightDoc(termTexts, termContexts, subContext.ord, weights, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      if (passages.length > 0) {\n        // otherwise a null snippet\n        highlights.put(doc, formatter.format(passages, content));\n      }\n      lastLeaf = leaf;\n    }\n    \n    String[] result = new String[scoreDocs.length];\n    for (int i = 0; i < scoreDocs.length; i++) {\n      result[i] = highlights.get(scoreDocs[i].doc);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b2b02091dbb6637db0db1d7b5151f8ced718f552","date":1356549268,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/PostingsHighlighter#highlight(Query,IndexSearcher,TopDocs,int).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/PostingsHighlighter#highlight(Query,IndexSearcher,TopDocs,int).mjava","sourceNew":"  public String[] highlight(Query query, IndexSearcher searcher, TopDocs topDocs, int maxPassages) throws IOException {\n    final IndexReader reader = searcher.getIndexReader();\n    final ScoreDoc scoreDocs[] = topDocs.scoreDocs;\n    query = rewrite(query);\n    SortedSet<Term> terms = new TreeSet<Term>();\n    query.extractTerms(terms);\n    terms = terms.subSet(floor, ceiling);\n    Term termTexts[] = terms.toArray(new Term[terms.size()]);\n    // TODO: should we have some reasonable defaults for term pruning? (e.g. stopwords)\n\n    int docids[] = new int[scoreDocs.length];\n    for (int i = 0; i < docids.length; i++) {\n      docids[i] = scoreDocs[i].doc;\n    }\n    IndexReaderContext readerContext = reader.getContext();\n    List<AtomicReaderContext> leaves = readerContext.leaves();\n    \n    // sort for sequential io\n    Arrays.sort(docids);\n    \n    // pull stored data\n    LimitedStoredFieldVisitor visitor = new LimitedStoredFieldVisitor(field, maxLength);\n    String contents[] = new String[docids.length];\n    for (int i = 0; i < contents.length; i++) {\n      reader.document(docids[i], visitor);\n      contents[i] = visitor.getValue();\n      visitor.reset();\n    }\n    \n    BreakIterator bi = (BreakIterator)breakIterator.clone();\n    \n    Map<Integer,String> highlights = new HashMap<Integer,String>();\n    \n    // reuse in the real sense... for docs in same segment we just advance our old enum\n    DocsAndPositionsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      AtomicReaderContext subContext = leaves.get(leaf);\n      AtomicReader r = subContext.reader();\n      Terms t = r.terms(field);\n      if (t == null) {\n        continue; // nothing to do\n      }\n      if (leaf != lastLeaf) {\n        termsEnum = t.iterator(null);\n        postings = new DocsAndPositionsEnum[terms.size()];\n      }\n      Passage passages[] = highlightDoc(termTexts, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      if (passages.length > 0) {\n        // otherwise a null snippet\n        highlights.put(doc, formatter.format(passages, content));\n      }\n      lastLeaf = leaf;\n    }\n    \n    String[] result = new String[scoreDocs.length];\n    for (int i = 0; i < scoreDocs.length; i++) {\n      result[i] = highlights.get(scoreDocs[i].doc);\n    }\n    return result;\n  }\n\n","sourceOld":"  public String[] highlight(Query query, IndexSearcher searcher, TopDocs topDocs, int maxPassages) throws IOException {\n    final IndexReader reader = searcher.getIndexReader();\n    final ScoreDoc scoreDocs[] = topDocs.scoreDocs;\n    query = rewrite(query);\n    SortedSet<Term> terms = new TreeSet<Term>();\n    query.extractTerms(terms);\n    terms = terms.subSet(floor, ceiling);\n    // TODO: should we have some reasonable defaults for term pruning? (e.g. stopwords)\n\n    int docids[] = new int[scoreDocs.length];\n    for (int i = 0; i < docids.length; i++) {\n      docids[i] = scoreDocs[i].doc;\n    }\n    IndexReaderContext readerContext = reader.getContext();\n    List<AtomicReaderContext> leaves = readerContext.leaves();\n    \n    // sort for sequential io\n    Arrays.sort(docids);\n    \n    // pull stored data\n    LimitedStoredFieldVisitor visitor = new LimitedStoredFieldVisitor(field, maxLength);\n    String contents[] = new String[docids.length];\n    for (int i = 0; i < contents.length; i++) {\n      reader.document(docids[i], visitor);\n      contents[i] = visitor.getValue();\n      visitor.reset();\n    }\n    \n    // now pull index stats: TODO: we should probably pull this from the reader instead?\n    // this could be a distributed call, which is crazy\n    CollectionStatistics collectionStats = searcher.collectionStatistics(field);\n    TermContext termContexts[] = new TermContext[terms.size()];\n    Term termTexts[] = new Term[terms.size()]; // needed for seekExact\n    float weights[] = new float[terms.size()];\n    int upto = 0;\n    for (Term term : terms) {\n      termTexts[upto] = term;\n      TermContext context = TermContext.build(readerContext, term, true);\n      termContexts[upto] = context;\n      TermStatistics termStats = searcher.termStatistics(term, context);\n      weights[upto] = scorer.weight(collectionStats, termStats);\n      upto++;\n      // TODO: should we instead score all the documents term-at-a-time here?\n      // the i/o would be better, but more transient ram\n    }\n    \n    BreakIterator bi = (BreakIterator)breakIterator.clone();\n    \n    Map<Integer,String> highlights = new HashMap<Integer,String>();\n    \n    // reuse in the real sense... for docs in same segment we just advance our old enum\n    DocsAndPositionsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      AtomicReaderContext subContext = leaves.get(leaf);\n      AtomicReader r = subContext.reader();\n      Terms t = r.terms(field);\n      if (t == null) {\n        continue; // nothing to do\n      }\n      if (leaf != lastLeaf) {\n        termsEnum = t.iterator(null);\n        postings = new DocsAndPositionsEnum[terms.size()];\n      }\n      Passage passages[] = highlightDoc(termTexts, termContexts, subContext.ord, weights, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      if (passages.length > 0) {\n        // otherwise a null snippet\n        highlights.put(doc, formatter.format(passages, content));\n      }\n      lastLeaf = leaf;\n    }\n    \n    String[] result = new String[scoreDocs.length];\n    for (int i = 0; i < scoreDocs.length; i++) {\n      result[i] = highlights.get(scoreDocs[i].doc);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8bb94551de83b33535d086e8d4222fa6d0c4cfc0","date":1357181547,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,Term[],int[],List[AtomicReaderContext],int).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/PostingsHighlighter#highlight(Query,IndexSearcher,TopDocs,int).mjava","sourceNew":"  private Map<Integer,String> highlightField(String field, String contents[], BreakIterator bi, Term terms[], int[] docids, List<AtomicReaderContext> leaves, int maxPassages) throws IOException {  \n    Map<Integer,String> highlights = new HashMap<Integer,String>();\n    \n    // reuse in the real sense... for docs in same segment we just advance our old enum\n    DocsAndPositionsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      AtomicReaderContext subContext = leaves.get(leaf);\n      AtomicReader r = subContext.reader();\n      Terms t = r.terms(field);\n      if (t == null) {\n        continue; // nothing to do\n      }\n      if (leaf != lastLeaf) {\n        termsEnum = t.iterator(null);\n        postings = new DocsAndPositionsEnum[terms.length];\n      }\n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      if (passages.length > 0) {\n        // otherwise a null snippet\n        highlights.put(doc, formatter.format(passages, content));\n      }\n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","sourceOld":"  public String[] highlight(Query query, IndexSearcher searcher, TopDocs topDocs, int maxPassages) throws IOException {\n    final IndexReader reader = searcher.getIndexReader();\n    final ScoreDoc scoreDocs[] = topDocs.scoreDocs;\n    query = rewrite(query);\n    SortedSet<Term> terms = new TreeSet<Term>();\n    query.extractTerms(terms);\n    terms = terms.subSet(floor, ceiling);\n    Term termTexts[] = terms.toArray(new Term[terms.size()]);\n    // TODO: should we have some reasonable defaults for term pruning? (e.g. stopwords)\n\n    int docids[] = new int[scoreDocs.length];\n    for (int i = 0; i < docids.length; i++) {\n      docids[i] = scoreDocs[i].doc;\n    }\n    IndexReaderContext readerContext = reader.getContext();\n    List<AtomicReaderContext> leaves = readerContext.leaves();\n    \n    // sort for sequential io\n    Arrays.sort(docids);\n    \n    // pull stored data\n    LimitedStoredFieldVisitor visitor = new LimitedStoredFieldVisitor(field, maxLength);\n    String contents[] = new String[docids.length];\n    for (int i = 0; i < contents.length; i++) {\n      reader.document(docids[i], visitor);\n      contents[i] = visitor.getValue();\n      visitor.reset();\n    }\n    \n    BreakIterator bi = (BreakIterator)breakIterator.clone();\n    \n    Map<Integer,String> highlights = new HashMap<Integer,String>();\n    \n    // reuse in the real sense... for docs in same segment we just advance our old enum\n    DocsAndPositionsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      AtomicReaderContext subContext = leaves.get(leaf);\n      AtomicReader r = subContext.reader();\n      Terms t = r.terms(field);\n      if (t == null) {\n        continue; // nothing to do\n      }\n      if (leaf != lastLeaf) {\n        termsEnum = t.iterator(null);\n        postings = new DocsAndPositionsEnum[terms.size()];\n      }\n      Passage passages[] = highlightDoc(termTexts, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      if (passages.length > 0) {\n        // otherwise a null snippet\n        highlights.put(doc, formatter.format(passages, content));\n      }\n      lastLeaf = leaf;\n    }\n    \n    String[] result = new String[scoreDocs.length];\n    for (int i = 0; i < scoreDocs.length; i++) {\n      result[i] = highlights.get(scoreDocs[i].doc);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"9882ec7961965d6ac6203049795d94c122015950":["ffabe030a2b84ad50adb7265da07ee78f1c58f6a"],"b2b02091dbb6637db0db1d7b5151f8ced718f552":["9882ec7961965d6ac6203049795d94c122015950"],"ffabe030a2b84ad50adb7265da07ee78f1c58f6a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8bb94551de83b33535d086e8d4222fa6d0c4cfc0":["b2b02091dbb6637db0db1d7b5151f8ced718f552"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["8bb94551de83b33535d086e8d4222fa6d0c4cfc0"]},"commit2Childs":{"9882ec7961965d6ac6203049795d94c122015950":["b2b02091dbb6637db0db1d7b5151f8ced718f552"],"ffabe030a2b84ad50adb7265da07ee78f1c58f6a":["9882ec7961965d6ac6203049795d94c122015950"],"b2b02091dbb6637db0db1d7b5151f8ced718f552":["8bb94551de83b33535d086e8d4222fa6d0c4cfc0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["ffabe030a2b84ad50adb7265da07ee78f1c58f6a"],"8bb94551de83b33535d086e8d4222fa6d0c4cfc0":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}