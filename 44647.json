{"path":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#testUpgradeOldSingleSegmentIndexWithAdditions().mjava","commits":[{"id":"d14e8d18c0e3970c20354dbeeb49da11bd587fbd","date":1321041051,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#testUpgradeOldSingleSegmentIndexWithAdditions().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#testUpgradeOldOptimizedIndexWithAdditions().mjava","sourceNew":"  public void testUpgradeOldSingleSegmentIndexWithAdditions() throws Exception {\n    for (String name : oldSingleSegmentNames) {\n      if (VERBOSE) {\n        System.out.println(\"testUpgradeOldSingleSegmentIndexWithAdditions: index=\" +name);\n      }\n      File oldIndxeDir = _TestUtil.getTempDir(name);\n      _TestUtil.unzip(getDataFile(\"index.\" + name + \".zip\"), oldIndxeDir);\n      Directory dir = newFSDirectory(oldIndxeDir);\n\n      assertEquals(\"Original index must be single segment\", 1, getNumberOfSegments(dir));\n\n      // create a bunch of dummy segments\n      int id = 40;\n      RAMDirectory ramDir = new RAMDirectory();\n      for (int i = 0; i < 3; i++) {\n        // only use Log- or TieredMergePolicy, to make document addition predictable and not suddenly merge:\n        MergePolicy mp = random.nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();\n        IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n          .setMergePolicy(mp);\n        IndexWriter w = new IndexWriter(ramDir, iwc);\n        // add few more docs:\n        for(int j = 0; j < RANDOM_MULTIPLIER * random.nextInt(30); j++) {\n          addDoc(w, id++);\n        }\n        w.close(false);\n      }\n      \n      // add dummy segments (which are all in current\n      // version) to single segment index\n      MergePolicy mp = random.nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();\n      IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, null)\n        .setMergePolicy(mp);\n      IndexWriter w = new IndexWriter(dir, iwc);\n      w.addIndexes(ramDir);\n      w.close(false);\n      \n      // determine count of segments in modified index\n      final int origSegCount = getNumberOfSegments(dir);\n      \n      new IndexUpgrader(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, null), false)\n        .upgrade();\n\n      final int segCount = checkAllSegmentsUpgraded(dir);\n      assertEquals(\"Index must still contain the same number of segments, as only one segment was upgraded and nothing else merged\",\n        origSegCount, segCount);\n      \n      dir.close();\n      _TestUtil.rmDir(oldIndxeDir);\n    }\n  }\n\n","sourceOld":"  public void testUpgradeOldOptimizedIndexWithAdditions() throws Exception {\n    for (String name : oldOptimizedNames) {\n      if (VERBOSE) {\n        System.out.println(\"testUpgradeOldOptimizedIndexWithAdditions: index=\" +name);\n      }\n      File oldIndxeDir = _TestUtil.getTempDir(name);\n      _TestUtil.unzip(getDataFile(\"index.\" + name + \".zip\"), oldIndxeDir);\n      Directory dir = newFSDirectory(oldIndxeDir);\n\n      assertEquals(\"Original index must be optimized\", 1, getNumberOfSegments(dir));\n\n      // create a bunch of dummy segments\n      int id = 40;\n      RAMDirectory ramDir = new RAMDirectory();\n      for (int i = 0; i < 3; i++) {\n        // only use Log- or TieredMergePolicy, to make document addition predictable and not suddenly merge:\n        MergePolicy mp = random.nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();\n        IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n          .setMergePolicy(mp);\n        IndexWriter w = new IndexWriter(ramDir, iwc);\n        // add few more docs:\n        for(int j = 0; j < RANDOM_MULTIPLIER * random.nextInt(30); j++) {\n          addDoc(w, id++);\n        }\n        w.close(false);\n      }\n      \n      // add dummy segments (which are all in current version) to optimized index\n      MergePolicy mp = random.nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();\n      IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, null)\n        .setMergePolicy(mp);\n      IndexWriter w = new IndexWriter(dir, iwc);\n      w.addIndexes(ramDir);\n      w.close(false);\n      \n      // determine count of segments in modified index\n      final int origSegCount = getNumberOfSegments(dir);\n      \n      new IndexUpgrader(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, null), false)\n        .upgrade();\n\n      final int segCount = checkAllSegmentsUpgraded(dir);\n      assertEquals(\"Index must still contain the same number of segments, as only one segment was upgraded and nothing else merged\",\n        origSegCount, segCount);\n      \n      dir.close();\n      _TestUtil.rmDir(oldIndxeDir);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8be580b58bcc650d428f3f22de81cadcf51d650a","date":1325279655,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#testUpgradeOldSingleSegmentIndexWithAdditions().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#testUpgradeOldSingleSegmentIndexWithAdditions().mjava","sourceNew":"  public void testUpgradeOldSingleSegmentIndexWithAdditions() throws Exception {\n    for (String name : oldSingleSegmentNames) {\n      if (VERBOSE) {\n        System.out.println(\"testUpgradeOldSingleSegmentIndexWithAdditions: index=\" +name);\n      }\n      Directory dir = newDirectory(oldIndexDirs.get(name));\n\n      assertEquals(\"Original index must be single segment\", 1, getNumberOfSegments(dir));\n\n      // create a bunch of dummy segments\n      int id = 40;\n      RAMDirectory ramDir = new RAMDirectory();\n      for (int i = 0; i < 3; i++) {\n        // only use Log- or TieredMergePolicy, to make document addition predictable and not suddenly merge:\n        MergePolicy mp = random.nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();\n        IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n          .setMergePolicy(mp);\n        IndexWriter w = new IndexWriter(ramDir, iwc);\n        // add few more docs:\n        for(int j = 0; j < RANDOM_MULTIPLIER * random.nextInt(30); j++) {\n          addDoc(w, id++);\n        }\n        w.close(false);\n      }\n      \n      // add dummy segments (which are all in current\n      // version) to single segment index\n      MergePolicy mp = random.nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();\n      IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, null)\n        .setMergePolicy(mp);\n      IndexWriter w = new IndexWriter(dir, iwc);\n      w.addIndexes(ramDir);\n      w.close(false);\n      \n      // determine count of segments in modified index\n      final int origSegCount = getNumberOfSegments(dir);\n      \n      new IndexUpgrader(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, null), false)\n        .upgrade();\n\n      final int segCount = checkAllSegmentsUpgraded(dir);\n      assertEquals(\"Index must still contain the same number of segments, as only one segment was upgraded and nothing else merged\",\n        origSegCount, segCount);\n      \n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testUpgradeOldSingleSegmentIndexWithAdditions() throws Exception {\n    for (String name : oldSingleSegmentNames) {\n      if (VERBOSE) {\n        System.out.println(\"testUpgradeOldSingleSegmentIndexWithAdditions: index=\" +name);\n      }\n      File oldIndxeDir = _TestUtil.getTempDir(name);\n      _TestUtil.unzip(getDataFile(\"index.\" + name + \".zip\"), oldIndxeDir);\n      Directory dir = newFSDirectory(oldIndxeDir);\n\n      assertEquals(\"Original index must be single segment\", 1, getNumberOfSegments(dir));\n\n      // create a bunch of dummy segments\n      int id = 40;\n      RAMDirectory ramDir = new RAMDirectory();\n      for (int i = 0; i < 3; i++) {\n        // only use Log- or TieredMergePolicy, to make document addition predictable and not suddenly merge:\n        MergePolicy mp = random.nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();\n        IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n          .setMergePolicy(mp);\n        IndexWriter w = new IndexWriter(ramDir, iwc);\n        // add few more docs:\n        for(int j = 0; j < RANDOM_MULTIPLIER * random.nextInt(30); j++) {\n          addDoc(w, id++);\n        }\n        w.close(false);\n      }\n      \n      // add dummy segments (which are all in current\n      // version) to single segment index\n      MergePolicy mp = random.nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();\n      IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, null)\n        .setMergePolicy(mp);\n      IndexWriter w = new IndexWriter(dir, iwc);\n      w.addIndexes(ramDir);\n      w.close(false);\n      \n      // determine count of segments in modified index\n      final int origSegCount = getNumberOfSegments(dir);\n      \n      new IndexUpgrader(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, null), false)\n        .upgrade();\n\n      final int segCount = checkAllSegmentsUpgraded(dir);\n      assertEquals(\"Index must still contain the same number of segments, as only one segment was upgraded and nothing else merged\",\n        origSegCount, segCount);\n      \n      dir.close();\n      _TestUtil.rmDir(oldIndxeDir);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#testUpgradeOldSingleSegmentIndexWithAdditions().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#testUpgradeOldSingleSegmentIndexWithAdditions().mjava","sourceNew":"  public void testUpgradeOldSingleSegmentIndexWithAdditions() throws Exception {\n    for (String name : oldSingleSegmentNames) {\n      if (VERBOSE) {\n        System.out.println(\"testUpgradeOldSingleSegmentIndexWithAdditions: index=\" +name);\n      }\n      Directory dir = newDirectory(oldIndexDirs.get(name));\n\n      assertEquals(\"Original index must be single segment\", 1, getNumberOfSegments(dir));\n\n      // create a bunch of dummy segments\n      int id = 40;\n      RAMDirectory ramDir = new RAMDirectory();\n      for (int i = 0; i < 3; i++) {\n        // only use Log- or TieredMergePolicy, to make document addition predictable and not suddenly merge:\n        MergePolicy mp = random.nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();\n        IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n          .setMergePolicy(mp);\n        IndexWriter w = new IndexWriter(ramDir, iwc);\n        // add few more docs:\n        for(int j = 0; j < RANDOM_MULTIPLIER * random.nextInt(30); j++) {\n          addDoc(w, id++);\n        }\n        w.close(false);\n      }\n      \n      // add dummy segments (which are all in current\n      // version) to single segment index\n      MergePolicy mp = random.nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();\n      IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, null)\n        .setMergePolicy(mp);\n      IndexWriter w = new IndexWriter(dir, iwc);\n      w.addIndexes(ramDir);\n      w.close(false);\n      \n      // determine count of segments in modified index\n      final int origSegCount = getNumberOfSegments(dir);\n      \n      new IndexUpgrader(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, null), false)\n        .upgrade();\n\n      final int segCount = checkAllSegmentsUpgraded(dir);\n      assertEquals(\"Index must still contain the same number of segments, as only one segment was upgraded and nothing else merged\",\n        origSegCount, segCount);\n      \n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testUpgradeOldSingleSegmentIndexWithAdditions() throws Exception {\n    for (String name : oldSingleSegmentNames) {\n      if (VERBOSE) {\n        System.out.println(\"testUpgradeOldSingleSegmentIndexWithAdditions: index=\" +name);\n      }\n      Directory dir = newDirectory(oldIndexDirs.get(name));\n\n      assertEquals(\"Original index must be single segment\", 1, getNumberOfSegments(dir));\n\n      // create a bunch of dummy segments\n      int id = 40;\n      RAMDirectory ramDir = new RAMDirectory();\n      for (int i = 0; i < 3; i++) {\n        // only use Log- or TieredMergePolicy, to make document addition predictable and not suddenly merge:\n        MergePolicy mp = random.nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();\n        IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n          .setMergePolicy(mp);\n        IndexWriter w = new IndexWriter(ramDir, iwc);\n        // add few more docs:\n        for(int j = 0; j < RANDOM_MULTIPLIER * random.nextInt(30); j++) {\n          addDoc(w, id++);\n        }\n        w.close(false);\n      }\n      \n      // add dummy segments (which are all in current\n      // version) to single segment index\n      MergePolicy mp = random.nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();\n      IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, null)\n        .setMergePolicy(mp);\n      IndexWriter w = new IndexWriter(dir, iwc);\n      w.addIndexes(ramDir);\n      w.close(false);\n      \n      // determine count of segments in modified index\n      final int origSegCount = getNumberOfSegments(dir);\n      \n      new IndexUpgrader(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, null), false)\n        .upgrade();\n\n      final int segCount = checkAllSegmentsUpgraded(dir);\n      assertEquals(\"Index must still contain the same number of segments, as only one segment was upgraded and nothing else merged\",\n        origSegCount, segCount);\n      \n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["8be580b58bcc650d428f3f22de81cadcf51d650a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8be580b58bcc650d428f3f22de81cadcf51d650a":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"8be580b58bcc650d428f3f22de81cadcf51d650a":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["8be580b58bcc650d428f3f22de81cadcf51d650a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}