{"path":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = _TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory mmapDir = new MMapDirectory(path);\n    mmapDir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", StringField.TYPE_STORED);\n    Field junk = newField(\"junk\", \"\", StringField.TYPE_STORED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = _TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory mmapDir = new MMapDirectory(path);\n    mmapDir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", StringField.TYPE_STORED);\n    Field junk = newField(\"junk\", \"\", StringField.TYPE_STORED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a78a90fc9701e511308346ea29f4f5e548bb39fe","date":1329489995,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = _TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory mmapDir = new MMapDirectory(path);\n    mmapDir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", StringField.TYPE_STORED);\n    Field junk = newField(\"junk\", \"\", StringField.TYPE_STORED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setStringValue(\"\" + i);\n      junk.setStringValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = _TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory mmapDir = new MMapDirectory(path);\n    mmapDir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", StringField.TYPE_STORED);\n    Field junk = newField(\"junk\", \"\", StringField.TYPE_STORED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":["fdefa116bcbcf81f5fad6c30040fcc2d2a79f7f7"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = _TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory mmapDir = new MMapDirectory(path);\n    mmapDir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newStringField(\"docid\", \"0\", Field.Store.YES);\n    Field junk = newStringField(\"junk\", \"\", Field.Store.YES);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setStringValue(\"\" + i);\n      junk.setStringValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = _TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory mmapDir = new MMapDirectory(path);\n    mmapDir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", StringField.TYPE_STORED);\n    Field junk = newField(\"junk\", \"\", StringField.TYPE_STORED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setStringValue(\"\" + i);\n      junk.setStringValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4e5e734869d76c22acfc12bc53ecbfcc1606c2f5","date":1347072117,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = _TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory mmapDir = new MMapDirectory(path, null, chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newStringField(\"docid\", \"0\", Field.Store.YES);\n    Field junk = newStringField(\"junk\", \"\", Field.Store.YES);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setStringValue(\"\" + i);\n      junk.setStringValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = _TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory mmapDir = new MMapDirectory(path);\n    mmapDir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newStringField(\"docid\", \"0\", Field.Store.YES);\n    Field junk = newStringField(\"junk\", \"\", Field.Store.YES);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setStringValue(\"\" + i);\n      junk.setStringValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory mmapDir = new MMapDirectory(path, null, chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newStringField(\"docid\", \"0\", Field.Store.YES);\n    Field junk = newStringField(\"junk\", \"\", Field.Store.YES);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setStringValue(\"\" + i);\n      junk.setStringValue(TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = _TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory mmapDir = new MMapDirectory(path, null, chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newStringField(\"docid\", \"0\", Field.Store.YES);\n    Field junk = newStringField(\"junk\", \"\", Field.Store.YES);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setStringValue(\"\" + i);\n      junk.setStringValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a9a24bae1e63c3bb5ff2fb47b0119240d840ee7c","date":1396633078,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = createTempDir(\"mmap\" + chunkSize);\n    MMapDirectory mmapDir = new MMapDirectory(path, null, chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newStringField(\"docid\", \"0\", Field.Store.YES);\n    Field junk = newStringField(\"junk\", \"\", Field.Store.YES);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setStringValue(\"\" + i);\n      junk.setStringValue(TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory mmapDir = new MMapDirectory(path, null, chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newStringField(\"docid\", \"0\", Field.Store.YES);\n    Field junk = newStringField(\"junk\", \"\", Field.Store.YES);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setStringValue(\"\" + i);\n      junk.setStringValue(TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2a0f5bb79c600763ffe7b8141df59a3169d31e48","date":1396689440,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = createTempDir(\"mmap\" + chunkSize);\n    MMapDirectory mmapDir = new MMapDirectory(path, null, chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newStringField(\"docid\", \"0\", Field.Store.YES);\n    Field junk = newStringField(\"junk\", \"\", Field.Store.YES);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setStringValue(\"\" + i);\n      junk.setStringValue(TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory mmapDir = new MMapDirectory(path, null, chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newStringField(\"docid\", \"0\", Field.Store.YES);\n    Field junk = newStringField(\"junk\", \"\", Field.Store.YES);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setStringValue(\"\" + i);\n      junk.setStringValue(TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = createTempDir(\"mmap\" + chunkSize);\n    MMapDirectory mmapDir = new MMapDirectory(path, null, chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newStringField(\"docid\", \"0\", Field.Store.YES);\n    Field junk = newStringField(\"junk\", \"\", Field.Store.YES);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setStringValue(\"\" + i);\n      junk.setStringValue(TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.shutdown();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = createTempDir(\"mmap\" + chunkSize);\n    MMapDirectory mmapDir = new MMapDirectory(path, null, chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newStringField(\"docid\", \"0\", Field.Store.YES);\n    Field junk = newStringField(\"junk\", \"\", Field.Store.YES);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setStringValue(\"\" + i);\n      junk.setStringValue(TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f986320666d675a260eb4529a0c3c40595731441","date":1401729997,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = createTempDir(\"mmap\" + chunkSize);\n    MMapDirectory mmapDir = new MMapDirectory(path, null, chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newStringField(\"docid\", \"0\", Field.Store.YES);\n    Field junk = newStringField(\"junk\", \"\", Field.Store.YES);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setStringValue(\"\" + i);\n      junk.setStringValue(TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.shutdown();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    writer.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = createTempDir(\"mmap\" + chunkSize);\n    MMapDirectory mmapDir = new MMapDirectory(path, null, chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newStringField(\"docid\", \"0\", Field.Store.YES);\n    Field junk = newStringField(\"junk\", \"\", Field.Store.YES);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setStringValue(\"\" + i);\n      junk.setStringValue(TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.shutdown();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = createTempDir(\"mmap\" + chunkSize);\n    MMapDirectory mmapDir = new MMapDirectory(path, null, chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newStringField(\"docid\", \"0\", Field.Store.YES);\n    Field junk = newStringField(\"junk\", \"\", Field.Store.YES);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setStringValue(\"\" + i);\n      junk.setStringValue(TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.shutdown();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    writer.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = createTempDir(\"mmap\" + chunkSize);\n    MMapDirectory mmapDir = new MMapDirectory(path, null, chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newStringField(\"docid\", \"0\", Field.Store.YES);\n    Field junk = newStringField(\"junk\", \"\", Field.Store.YES);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setStringValue(\"\" + i);\n      junk.setStringValue(TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.shutdown();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    writer.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = createTempDir(\"mmap\" + chunkSize);\n    MMapDirectory mmapDir = new MMapDirectory(path, null, chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newStringField(\"docid\", \"0\", Field.Store.YES);\n    Field junk = newStringField(\"junk\", \"\", Field.Store.YES);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setStringValue(\"\" + i);\n      junk.setStringValue(TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = createTempDir(\"mmap\" + chunkSize);\n    MMapDirectory mmapDir = new MMapDirectory(path, null, chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newStringField(\"docid\", \"0\", Field.Store.YES);\n    Field junk = newStringField(\"junk\", \"\", Field.Store.YES);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setStringValue(\"\" + i);\n      junk.setStringValue(TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.shutdown();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    writer.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f4abec28b874149a7223e32cc7a01704c27790de","date":1410644789,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    Path path = createTempDir(\"mmap\" + chunkSize);\n    MMapDirectory mmapDir = new MMapDirectory(path, null, chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newStringField(\"docid\", \"0\", Field.Store.YES);\n    Field junk = newStringField(\"junk\", \"\", Field.Store.YES);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setStringValue(\"\" + i);\n      junk.setStringValue(TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = createTempDir(\"mmap\" + chunkSize);\n    MMapDirectory mmapDir = new MMapDirectory(path, null, chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newStringField(\"docid\", \"0\", Field.Store.YES);\n    Field junk = newStringField(\"junk\", \"\", Field.Store.YES);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setStringValue(\"\" + i);\n      junk.setStringValue(TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"285cdc737de75b7cc7c284a156b20214deb67bca","date":1415535483,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    Path path = createTempDir(\"mmap\" + chunkSize);\n    MMapDirectory mmapDir = new MMapDirectory(path, chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newStringField(\"docid\", \"0\", Field.Store.YES);\n    Field junk = newStringField(\"junk\", \"\", Field.Store.YES);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setStringValue(\"\" + i);\n      junk.setStringValue(TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    Path path = createTempDir(\"mmap\" + chunkSize);\n    MMapDirectory mmapDir = new MMapDirectory(path, null, chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newStringField(\"docid\", \"0\", Field.Store.YES);\n    Field junk = newStringField(\"junk\", \"\", Field.Store.YES);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setStringValue(\"\" + i);\n      junk.setStringValue(TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["f986320666d675a260eb4529a0c3c40595731441"],"4e5e734869d76c22acfc12bc53ecbfcc1606c2f5":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"2a0f5bb79c600763ffe7b8141df59a3169d31e48":["6613659748fe4411a7dcf85266e55db1f95f7315","a9a24bae1e63c3bb5ff2fb47b0119240d840ee7c"],"6613659748fe4411a7dcf85266e55db1f95f7315":["4e5e734869d76c22acfc12bc53ecbfcc1606c2f5"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"285cdc737de75b7cc7c284a156b20214deb67bca":["f4abec28b874149a7223e32cc7a01704c27790de"],"f986320666d675a260eb4529a0c3c40595731441":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"f4abec28b874149a7223e32cc7a01704c27790de":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"a78a90fc9701e511308346ea29f4f5e548bb39fe":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"a9a24bae1e63c3bb5ff2fb47b0119240d840ee7c":["6613659748fe4411a7dcf85266e55db1f95f7315"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["2a0f5bb79c600763ffe7b8141df59a3169d31e48"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["285cdc737de75b7cc7c284a156b20214deb67bca"]},"commit2Childs":{"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"4e5e734869d76c22acfc12bc53ecbfcc1606c2f5":["6613659748fe4411a7dcf85266e55db1f95f7315"],"2a0f5bb79c600763ffe7b8141df59a3169d31e48":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"6613659748fe4411a7dcf85266e55db1f95f7315":["2a0f5bb79c600763ffe7b8141df59a3169d31e48","a9a24bae1e63c3bb5ff2fb47b0119240d840ee7c"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"f986320666d675a260eb4529a0c3c40595731441":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"285cdc737de75b7cc7c284a156b20214deb67bca":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"f4abec28b874149a7223e32cc7a01704c27790de":["285cdc737de75b7cc7c284a156b20214deb67bca"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["4e5e734869d76c22acfc12bc53ecbfcc1606c2f5"],"a78a90fc9701e511308346ea29f4f5e548bb39fe":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"a9a24bae1e63c3bb5ff2fb47b0119240d840ee7c":["2a0f5bb79c600763ffe7b8141df59a3169d31e48"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["f4abec28b874149a7223e32cc7a01704c27790de"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["f986320666d675a260eb4529a0c3c40595731441"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}