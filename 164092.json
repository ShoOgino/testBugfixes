{"path":"lucene/core/src/java/org/apache/lucene/codecs/lucene70/Lucene70DocValuesConsumer#addTermsDict(FieldInfo,Iterable[BytesRef]).mjava","commits":[{"id":"084884d4602f4d1c7411eab29e897e349ce62675","date":1475571034,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene70/Lucene70DocValuesConsumer#addTermsDict(FieldInfo,Iterable[BytesRef]).mjava","pathOld":"/dev/null","sourceNew":"  /** expert: writes a value dictionary for a sorted/sortedset field */\n  private void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {\n    // first check if it's a \"fixed-length\" terms dict, and compressibility if so\n    int minLength = Integer.MAX_VALUE;\n    int maxLength = Integer.MIN_VALUE;\n    long numValues = 0;\n    BytesRefBuilder previousValue = new BytesRefBuilder();\n    long prefixSum = 0; // only valid for fixed-width data, as we have a choice there\n    for (BytesRef v : values) {\n      minLength = Math.min(minLength, v.length);\n      maxLength = Math.max(maxLength, v.length);\n      if (minLength == maxLength) {\n        int termPosition = (int) (numValues & INTERVAL_MASK);\n        if (termPosition == 0) {\n          // first term in block, save it away to compare against the last term later\n          previousValue.copyBytes(v);\n        } else if (termPosition == INTERVAL_COUNT - 1) {\n          // last term in block, accumulate shared prefix against first term\n          prefixSum += StringHelper.bytesDifference(previousValue.get(), v);\n        }\n      }\n      numValues++;\n    }\n    // for fixed width data, look at the avg(shared prefix) before deciding how to encode:\n    // prefix compression \"costs\" worst case 2 bytes per term because we must store suffix lengths.\n    // so if we share at least 3 bytes on average, always compress.\n    if (minLength == maxLength && prefixSum <= 3*(numValues >> INTERVAL_SHIFT)) {\n      // no index needed: not very compressible, direct addressing by mult\n      addBinaryField(field, values);\n    } else if (numValues < REVERSE_INTERVAL_COUNT) {\n      // low cardinality: waste a few KB of ram, but can't really use fancy index etc\n      addBinaryField(field, values);\n    } else {\n      assert numValues > 0; // we don't have to handle the empty case\n      // header\n      meta.writeVInt(field.number);\n      meta.writeByte(Lucene70DocValuesFormat.BINARY);\n      meta.writeVInt(BINARY_PREFIX_COMPRESSED);\n      meta.writeLong(-1L);\n      // now write the bytes: sharing prefixes within a block\n      final long startFP = data.getFilePointer();\n      // currently, we have to store the delta from expected for every 1/nth term\n      // we could avoid this, but it's not much and less overall RAM than the previous approach!\n      RAMOutputStream addressBuffer = new RAMOutputStream();\n      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, MONOTONIC_BLOCK_SIZE);\n      // buffers up 16 terms\n      RAMOutputStream bytesBuffer = new RAMOutputStream();\n      // buffers up block header\n      RAMOutputStream headerBuffer = new RAMOutputStream();\n      BytesRefBuilder lastTerm = new BytesRefBuilder();\n      lastTerm.grow(maxLength);\n      long count = 0;\n      int suffixDeltas[] = new int[INTERVAL_COUNT];\n      for (BytesRef v : values) {\n        int termPosition = (int) (count & INTERVAL_MASK);\n        if (termPosition == 0) {\n          termAddresses.add(data.getFilePointer() - startFP);\n          // abs-encode first term\n          headerBuffer.writeVInt(v.length);\n          headerBuffer.writeBytes(v.bytes, v.offset, v.length);\n          lastTerm.copyBytes(v);\n        } else {\n          // prefix-code: we only share at most 255 characters, to encode the length as a single\n          // byte and have random access. Larger terms just get less compression.\n          int sharedPrefix = Math.min(255, StringHelper.bytesDifference(lastTerm.get(), v));\n          bytesBuffer.writeByte((byte) sharedPrefix);\n          bytesBuffer.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);\n          // we can encode one smaller, because terms are unique.\n          suffixDeltas[termPosition] = v.length - sharedPrefix - 1;\n        }\n        \n        count++;\n        // flush block\n        if ((count & INTERVAL_MASK) == 0) {\n          flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n        }\n      }\n      // flush trailing crap\n      int leftover = (int) (count & INTERVAL_MASK);\n      if (leftover > 0) {\n        Arrays.fill(suffixDeltas, leftover, suffixDeltas.length, 0);\n        flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n      }\n      final long indexStartFP = data.getFilePointer();\n      // write addresses of indexed terms\n      termAddresses.finish();\n      addressBuffer.writeTo(data);\n      addressBuffer = null;\n      termAddresses = null;\n      meta.writeVInt(minLength);\n      meta.writeVInt(maxLength);\n      meta.writeVLong(count);\n      meta.writeLong(startFP);\n      meta.writeLong(indexStartFP);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeVInt(MONOTONIC_BLOCK_SIZE);\n      addReverseTermIndex(field, values, maxLength);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1967bed916cc89da82a1c2085f27976da6d08cbd","date":1475588750,"type":0,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene70/Lucene70DocValuesConsumer#addTermsDict(FieldInfo,Iterable[BytesRef]).mjava","pathOld":"/dev/null","sourceNew":"  /** expert: writes a value dictionary for a sorted/sortedset field */\n  private void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {\n    // first check if it's a \"fixed-length\" terms dict, and compressibility if so\n    int minLength = Integer.MAX_VALUE;\n    int maxLength = Integer.MIN_VALUE;\n    long numValues = 0;\n    BytesRefBuilder previousValue = new BytesRefBuilder();\n    long prefixSum = 0; // only valid for fixed-width data, as we have a choice there\n    for (BytesRef v : values) {\n      minLength = Math.min(minLength, v.length);\n      maxLength = Math.max(maxLength, v.length);\n      if (minLength == maxLength) {\n        int termPosition = (int) (numValues & INTERVAL_MASK);\n        if (termPosition == 0) {\n          // first term in block, save it away to compare against the last term later\n          previousValue.copyBytes(v);\n        } else if (termPosition == INTERVAL_COUNT - 1) {\n          // last term in block, accumulate shared prefix against first term\n          prefixSum += StringHelper.bytesDifference(previousValue.get(), v);\n        }\n      }\n      numValues++;\n    }\n    // for fixed width data, look at the avg(shared prefix) before deciding how to encode:\n    // prefix compression \"costs\" worst case 2 bytes per term because we must store suffix lengths.\n    // so if we share at least 3 bytes on average, always compress.\n    if (minLength == maxLength && prefixSum <= 3*(numValues >> INTERVAL_SHIFT)) {\n      // no index needed: not very compressible, direct addressing by mult\n      addBinaryField(field, values);\n    } else if (numValues < REVERSE_INTERVAL_COUNT) {\n      // low cardinality: waste a few KB of ram, but can't really use fancy index etc\n      addBinaryField(field, values);\n    } else {\n      assert numValues > 0; // we don't have to handle the empty case\n      // header\n      meta.writeVInt(field.number);\n      meta.writeByte(Lucene70DocValuesFormat.BINARY);\n      meta.writeVInt(BINARY_PREFIX_COMPRESSED);\n      meta.writeLong(-1L);\n      // now write the bytes: sharing prefixes within a block\n      final long startFP = data.getFilePointer();\n      // currently, we have to store the delta from expected for every 1/nth term\n      // we could avoid this, but it's not much and less overall RAM than the previous approach!\n      RAMOutputStream addressBuffer = new RAMOutputStream();\n      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, MONOTONIC_BLOCK_SIZE);\n      // buffers up 16 terms\n      RAMOutputStream bytesBuffer = new RAMOutputStream();\n      // buffers up block header\n      RAMOutputStream headerBuffer = new RAMOutputStream();\n      BytesRefBuilder lastTerm = new BytesRefBuilder();\n      lastTerm.grow(maxLength);\n      long count = 0;\n      int suffixDeltas[] = new int[INTERVAL_COUNT];\n      for (BytesRef v : values) {\n        int termPosition = (int) (count & INTERVAL_MASK);\n        if (termPosition == 0) {\n          termAddresses.add(data.getFilePointer() - startFP);\n          // abs-encode first term\n          headerBuffer.writeVInt(v.length);\n          headerBuffer.writeBytes(v.bytes, v.offset, v.length);\n          lastTerm.copyBytes(v);\n        } else {\n          // prefix-code: we only share at most 255 characters, to encode the length as a single\n          // byte and have random access. Larger terms just get less compression.\n          int sharedPrefix = Math.min(255, StringHelper.bytesDifference(lastTerm.get(), v));\n          bytesBuffer.writeByte((byte) sharedPrefix);\n          bytesBuffer.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);\n          // we can encode one smaller, because terms are unique.\n          suffixDeltas[termPosition] = v.length - sharedPrefix - 1;\n        }\n        \n        count++;\n        // flush block\n        if ((count & INTERVAL_MASK) == 0) {\n          flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n        }\n      }\n      // flush trailing crap\n      int leftover = (int) (count & INTERVAL_MASK);\n      if (leftover > 0) {\n        Arrays.fill(suffixDeltas, leftover, suffixDeltas.length, 0);\n        flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n      }\n      final long indexStartFP = data.getFilePointer();\n      // write addresses of indexed terms\n      termAddresses.finish();\n      addressBuffer.writeTo(data);\n      addressBuffer = null;\n      termAddresses = null;\n      meta.writeVInt(minLength);\n      meta.writeVInt(maxLength);\n      meta.writeVLong(count);\n      meta.writeLong(startFP);\n      meta.writeLong(indexStartFP);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeVInt(MONOTONIC_BLOCK_SIZE);\n      addReverseTermIndex(field, values, maxLength);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"23e44daeaa8b89694d10df5999956c8e14a7dd09","date":1476689300,"type":4,"author":"Adrien Grand","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene70/Lucene70DocValuesConsumer#addTermsDict(FieldInfo,Iterable[BytesRef]).mjava","sourceNew":null,"sourceOld":"  /** expert: writes a value dictionary for a sorted/sortedset field */\n  private void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {\n    // first check if it's a \"fixed-length\" terms dict, and compressibility if so\n    int minLength = Integer.MAX_VALUE;\n    int maxLength = Integer.MIN_VALUE;\n    long numValues = 0;\n    BytesRefBuilder previousValue = new BytesRefBuilder();\n    long prefixSum = 0; // only valid for fixed-width data, as we have a choice there\n    for (BytesRef v : values) {\n      minLength = Math.min(minLength, v.length);\n      maxLength = Math.max(maxLength, v.length);\n      if (minLength == maxLength) {\n        int termPosition = (int) (numValues & INTERVAL_MASK);\n        if (termPosition == 0) {\n          // first term in block, save it away to compare against the last term later\n          previousValue.copyBytes(v);\n        } else if (termPosition == INTERVAL_COUNT - 1) {\n          // last term in block, accumulate shared prefix against first term\n          prefixSum += StringHelper.bytesDifference(previousValue.get(), v);\n        }\n      }\n      numValues++;\n    }\n    // for fixed width data, look at the avg(shared prefix) before deciding how to encode:\n    // prefix compression \"costs\" worst case 2 bytes per term because we must store suffix lengths.\n    // so if we share at least 3 bytes on average, always compress.\n    if (minLength == maxLength && prefixSum <= 3*(numValues >> INTERVAL_SHIFT)) {\n      // no index needed: not very compressible, direct addressing by mult\n      addBinaryField(field, values);\n    } else if (numValues < REVERSE_INTERVAL_COUNT) {\n      // low cardinality: waste a few KB of ram, but can't really use fancy index etc\n      addBinaryField(field, values);\n    } else {\n      assert numValues > 0; // we don't have to handle the empty case\n      // header\n      meta.writeVInt(field.number);\n      meta.writeByte(Lucene70DocValuesFormat.BINARY);\n      meta.writeVInt(BINARY_PREFIX_COMPRESSED);\n      meta.writeLong(-1L);\n      // now write the bytes: sharing prefixes within a block\n      final long startFP = data.getFilePointer();\n      // currently, we have to store the delta from expected for every 1/nth term\n      // we could avoid this, but it's not much and less overall RAM than the previous approach!\n      RAMOutputStream addressBuffer = new RAMOutputStream();\n      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, MONOTONIC_BLOCK_SIZE);\n      // buffers up 16 terms\n      RAMOutputStream bytesBuffer = new RAMOutputStream();\n      // buffers up block header\n      RAMOutputStream headerBuffer = new RAMOutputStream();\n      BytesRefBuilder lastTerm = new BytesRefBuilder();\n      lastTerm.grow(maxLength);\n      long count = 0;\n      int suffixDeltas[] = new int[INTERVAL_COUNT];\n      for (BytesRef v : values) {\n        int termPosition = (int) (count & INTERVAL_MASK);\n        if (termPosition == 0) {\n          termAddresses.add(data.getFilePointer() - startFP);\n          // abs-encode first term\n          headerBuffer.writeVInt(v.length);\n          headerBuffer.writeBytes(v.bytes, v.offset, v.length);\n          lastTerm.copyBytes(v);\n        } else {\n          // prefix-code: we only share at most 255 characters, to encode the length as a single\n          // byte and have random access. Larger terms just get less compression.\n          int sharedPrefix = Math.min(255, StringHelper.bytesDifference(lastTerm.get(), v));\n          bytesBuffer.writeByte((byte) sharedPrefix);\n          bytesBuffer.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);\n          // we can encode one smaller, because terms are unique.\n          suffixDeltas[termPosition] = v.length - sharedPrefix - 1;\n        }\n        \n        count++;\n        // flush block\n        if ((count & INTERVAL_MASK) == 0) {\n          flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n        }\n      }\n      // flush trailing crap\n      int leftover = (int) (count & INTERVAL_MASK);\n      if (leftover > 0) {\n        Arrays.fill(suffixDeltas, leftover, suffixDeltas.length, 0);\n        flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n      }\n      final long indexStartFP = data.getFilePointer();\n      // write addresses of indexed terms\n      termAddresses.finish();\n      addressBuffer.writeTo(data);\n      addressBuffer = null;\n      termAddresses = null;\n      meta.writeVInt(minLength);\n      meta.writeVInt(maxLength);\n      meta.writeVLong(count);\n      meta.writeLong(startFP);\n      meta.writeLong(indexStartFP);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeVInt(MONOTONIC_BLOCK_SIZE);\n      addReverseTermIndex(field, values, maxLength);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"23e44daeaa8b89694d10df5999956c8e14a7dd09":["1967bed916cc89da82a1c2085f27976da6d08cbd"],"1967bed916cc89da82a1c2085f27976da6d08cbd":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","084884d4602f4d1c7411eab29e897e349ce62675"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["23e44daeaa8b89694d10df5999956c8e14a7dd09"],"084884d4602f4d1c7411eab29e897e349ce62675":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"23e44daeaa8b89694d10df5999956c8e14a7dd09":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"1967bed916cc89da82a1c2085f27976da6d08cbd":["23e44daeaa8b89694d10df5999956c8e14a7dd09"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1967bed916cc89da82a1c2085f27976da6d08cbd","084884d4602f4d1c7411eab29e897e349ce62675"],"084884d4602f4d1c7411eab29e897e349ce62675":["1967bed916cc89da82a1c2085f27976da6d08cbd"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}