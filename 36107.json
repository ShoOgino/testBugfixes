{"path":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segment == null) {\n      // this call is synchronized on IndexWriter.segmentInfos\n      segment = writer.newSegmentName();\n      assert numDocsInRAM == 0;\n      if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n        infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" init seg=\" + segment + \" delQueue=\" + deleteQueue);  \n      }\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segment);\n    }\n    int docCount = 0;\n    try {\n      for(Iterable<? extends IndexableField> doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segment == null) {\n      // this call is synchronized on IndexWriter.segmentInfos\n      segment = writer.newSegmentName();\n      assert numDocsInRAM == 0;\n      if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n        infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" init seg=\" + segment + \" delQueue=\" + deleteQueue);  \n      }\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segment);\n    }\n    int docCount = 0;\n    try {\n      for(Iterable<? extends IndexableField> doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"129c6e8ac0c0d9a110ba29e4b5f1889374f30076","date":1337725510,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segmentInfo == null) {\n      initSegmentInfo();\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    try {\n      for(Iterable<? extends IndexableField> doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segment == null) {\n      // this call is synchronized on IndexWriter.segmentInfos\n      segment = writer.newSegmentName();\n      assert numDocsInRAM == 0;\n      if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n        infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" init seg=\" + segment + \" delQueue=\" + deleteQueue);  \n      }\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segment);\n    }\n    int docCount = 0;\n    try {\n      for(Iterable<? extends IndexableField> doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"615ddbd81799980d0fdd95e0238e1c498b6f47b0","date":1338233290,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segmentInfo == null) {\n      initSegmentInfo();\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    try {\n      for(Iterable<? extends IndexableField> doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segment == null) {\n      // this call is synchronized on IndexWriter.segmentInfos\n      segment = writer.newSegmentName();\n      assert numDocsInRAM == 0;\n      if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n        infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" init seg=\" + segment + \" delQueue=\" + deleteQueue);  \n      }\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segment);\n    }\n    int docCount = 0;\n    try {\n      for(Iterable<? extends IndexableField> doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"33e9fa3b49f4a365a04fdfc8a32dbcd0df798f5a","date":1341524239,"type":5,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segmentInfo == null) {\n      initSegmentInfo();\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    try {\n      for(IndexDocument doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segmentInfo == null) {\n      initSegmentInfo();\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    try {\n      for(Iterable<? extends IndexableField> doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1d028314cced5858683a1bb4741423d0f934257b","date":1346596535,"type":5,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segmentInfo == null) {\n      initSegmentInfo();\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    try {\n      for(IndexDocument doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segmentInfo == null) {\n      initSegmentInfo();\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    try {\n      for(Iterable<? extends IndexableField> doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","date":1453060490,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(Iterable<? extends IndexableField> doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveOneDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveOneDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f492fe129985750df09c8dac738aecc503158bb3","date":1464099630,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(Iterable<? extends IndexableField> doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveOneDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n      // nocommit return seqNo here\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(Iterable<? extends IndexableField> doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveOneDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f6df47cbfd656ea50ca2996361f7954531ee18b","date":1464133540,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","sourceNew":"  public long updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(Iterable<? extends IndexableField> doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveOneDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      long seqNo;\n      if (delTerm != null) {\n        seqNo = deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n        return seqNo;\n      } else {\n        seqNo = deleteQueue.seqNo.get();\n      }\n\n      return seqNo;\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(Iterable<? extends IndexableField> doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveOneDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n      // nocommit return seqNo here\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"16ebfabc294f23b88b6a39722a02c9d39b353195","date":1464343867,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","sourceNew":"  public long updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(Iterable<? extends IndexableField> doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveOneDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      long seqNo;\n      if (delTerm != null) {\n        seqNo = deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n        return seqNo;\n      } else {\n        seqNo = deleteQueue.getNextSequenceNumber();\n      }\n\n      return seqNo;\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n  }\n\n","sourceOld":"  public long updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(Iterable<? extends IndexableField> doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveOneDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      long seqNo;\n      if (delTerm != null) {\n        seqNo = deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n        return seqNo;\n      } else {\n        seqNo = deleteQueue.seqNo.get();\n      }\n\n      return seqNo;\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"122251c49e5a9fa95f056ea257ae3ab452099fc7","date":1464820065,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","sourceNew":"  public long updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(Iterable<? extends IndexableField> doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveOneDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n\n        numDocsInRAM++;\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      long seqNo;\n      if (delTerm != null) {\n        seqNo = deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n        return seqNo;\n      } else {\n        seqNo = deleteQueue.updateSlice(deleteSlice);\n        if (seqNo < 0) {\n          seqNo = -seqNo;\n          deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n        } else {\n          deleteSlice.reset();\n        }\n      }\n\n      return seqNo;\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n  }\n\n","sourceOld":"  public long updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(Iterable<? extends IndexableField> doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveOneDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      long seqNo;\n      if (delTerm != null) {\n        seqNo = deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n        return seqNo;\n      } else {\n        seqNo = deleteQueue.getNextSequenceNumber();\n      }\n\n      return seqNo;\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b40b1a0adcc6bdcda63b0fbd75dfa2ddd8777e77","date":1464821470,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","sourceNew":"  public long updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(Iterable<? extends IndexableField> doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveOneDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n\n        numDocsInRAM++;\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      long seqNo;\n      if (delTerm != null) {\n        seqNo = deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n        return seqNo;\n      } else {\n        seqNo = deleteQueue.updateSlice(deleteSlice);\n        if (seqNo < 0) {\n          seqNo = -seqNo;\n          deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n        } else {\n          deleteSlice.reset();\n        }\n      }\n\n      return seqNo;\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n  }\n\n","sourceOld":"  public long updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(Iterable<? extends IndexableField> doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveOneDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      long seqNo;\n      if (delTerm != null) {\n        seqNo = deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n        return seqNo;\n      } else {\n        seqNo = deleteQueue.getNextSequenceNumber();\n      }\n\n      return seqNo;\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6483e4260c08168709c02238ae083a51519a28dd","date":1465117546,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","sourceNew":"  public long updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(Iterable<? extends IndexableField> doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveOneDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n\n        numDocsInRAM++;\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      long seqNo;\n      if (delTerm != null) {\n        seqNo = deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n        return seqNo;\n      } else {\n        seqNo = deleteQueue.updateSlice(deleteSlice);\n        if (seqNo < 0) {\n          seqNo = -seqNo;\n          deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n        } else {\n          deleteSlice.reset();\n        }\n      }\n\n      return seqNo;\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(Iterable<? extends IndexableField> doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveOneDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"191128ac5b85671b1671e2c857437694283b6ebf","date":1465297861,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","sourceNew":"  public long updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(Iterable<? extends IndexableField> doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveOneDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n\n        numDocsInRAM++;\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      long seqNo;\n      if (delTerm != null) {\n        seqNo = deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n        return seqNo;\n      } else {\n        seqNo = deleteQueue.updateSlice(deleteSlice);\n        if (seqNo < 0) {\n          seqNo = -seqNo;\n          deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n        } else {\n          deleteSlice.reset();\n        }\n      }\n\n      return seqNo;\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(Iterable<? extends IndexableField> doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveOneDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","sourceNew":"  public long updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(Iterable<? extends IndexableField> doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveOneDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n\n        numDocsInRAM++;\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      long seqNo;\n      if (delTerm != null) {\n        seqNo = deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n        return seqNo;\n      } else {\n        seqNo = deleteQueue.updateSlice(deleteSlice);\n        if (seqNo < 0) {\n          seqNo = -seqNo;\n          deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n        } else {\n          deleteSlice.reset();\n        }\n      }\n\n      return seqNo;\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(Iterable<? extends IndexableField> doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveOneDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"110125c995236a7f61057dd04b039ed2d267f3a1","date":1521014987,"type":5,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,DocumentsWriterDeleteQueue.Node[#]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","sourceNew":"  public long updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, DocumentsWriterDeleteQueue.Node<?> deleteNode) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delNode=\" + deleteNode + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(Iterable<? extends IndexableField> doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveOneDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n\n        numDocsInRAM++;\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      long seqNo;\n      if (deleteNode != null) {\n        seqNo = deleteQueue.add(deleteNode, deleteSlice);\n        assert deleteSlice.isTail(deleteNode) : \"expected the delete node as the tail\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n        return seqNo;\n      } else {\n        seqNo = deleteQueue.updateSlice(deleteSlice);\n        if (seqNo < 0) {\n          seqNo = -seqNo;\n          deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n        } else {\n          deleteSlice.reset();\n        }\n      }\n\n      return seqNo;\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n  }\n\n","sourceOld":"  public long updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(Iterable<? extends IndexableField> doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveOneDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n\n        numDocsInRAM++;\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      long seqNo;\n      if (delTerm != null) {\n        seqNo = deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n        return seqNo;\n      } else {\n        seqNo = deleteQueue.updateSlice(deleteSlice);\n        if (seqNo < 0) {\n          seqNo = -seqNo;\n          deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n        } else {\n          deleteSlice.reset();\n        }\n      }\n\n      return seqNo;\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"6483e4260c08168709c02238ae083a51519a28dd":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","b40b1a0adcc6bdcda63b0fbd75dfa2ddd8777e77"],"191128ac5b85671b1671e2c857437694283b6ebf":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","6483e4260c08168709c02238ae083a51519a28dd"],"110125c995236a7f61057dd04b039ed2d267f3a1":["191128ac5b85671b1671e2c857437694283b6ebf"],"16ebfabc294f23b88b6a39722a02c9d39b353195":["0f6df47cbfd656ea50ca2996361f7954531ee18b"],"b40b1a0adcc6bdcda63b0fbd75dfa2ddd8777e77":["16ebfabc294f23b88b6a39722a02c9d39b353195","122251c49e5a9fa95f056ea257ae3ab452099fc7"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","191128ac5b85671b1671e2c857437694283b6ebf"],"1d028314cced5858683a1bb4741423d0f934257b":["615ddbd81799980d0fdd95e0238e1c498b6f47b0","33e9fa3b49f4a365a04fdfc8a32dbcd0df798f5a"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","129c6e8ac0c0d9a110ba29e4b5f1889374f30076"],"129c6e8ac0c0d9a110ba29e4b5f1889374f30076":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"33e9fa3b49f4a365a04fdfc8a32dbcd0df798f5a":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"f492fe129985750df09c8dac738aecc503158bb3":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"0f6df47cbfd656ea50ca2996361f7954531ee18b":["f492fe129985750df09c8dac738aecc503158bb3"],"122251c49e5a9fa95f056ea257ae3ab452099fc7":["16ebfabc294f23b88b6a39722a02c9d39b353195"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["110125c995236a7f61057dd04b039ed2d267f3a1"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["1d028314cced5858683a1bb4741423d0f934257b"]},"commit2Childs":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["615ddbd81799980d0fdd95e0238e1c498b6f47b0","129c6e8ac0c0d9a110ba29e4b5f1889374f30076"],"6483e4260c08168709c02238ae083a51519a28dd":["191128ac5b85671b1671e2c857437694283b6ebf"],"191128ac5b85671b1671e2c857437694283b6ebf":["110125c995236a7f61057dd04b039ed2d267f3a1","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"110125c995236a7f61057dd04b039ed2d267f3a1":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"16ebfabc294f23b88b6a39722a02c9d39b353195":["b40b1a0adcc6bdcda63b0fbd75dfa2ddd8777e77","122251c49e5a9fa95f056ea257ae3ab452099fc7"],"b40b1a0adcc6bdcda63b0fbd75dfa2ddd8777e77":["6483e4260c08168709c02238ae083a51519a28dd"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"1d028314cced5858683a1bb4741423d0f934257b":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["1d028314cced5858683a1bb4741423d0f934257b","33e9fa3b49f4a365a04fdfc8a32dbcd0df798f5a"],"129c6e8ac0c0d9a110ba29e4b5f1889374f30076":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"33e9fa3b49f4a365a04fdfc8a32dbcd0df798f5a":["1d028314cced5858683a1bb4741423d0f934257b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"f492fe129985750df09c8dac738aecc503158bb3":["0f6df47cbfd656ea50ca2996361f7954531ee18b"],"0f6df47cbfd656ea50ca2996361f7954531ee18b":["16ebfabc294f23b88b6a39722a02c9d39b353195"],"122251c49e5a9fa95f056ea257ae3ab452099fc7":["b40b1a0adcc6bdcda63b0fbd75dfa2ddd8777e77"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["6483e4260c08168709c02238ae083a51519a28dd","191128ac5b85671b1671e2c857437694283b6ebf","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","f492fe129985750df09c8dac738aecc503158bb3"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}