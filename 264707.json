{"path":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String,String,boolean,boolean).mjava","commits":[{"id":"39342cb658ac11dfcbf4459807fb00eb9ada0218","date":1472876688,"type":1,"author":"Mikhail Khludnev","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String,String,boolean,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String,String,boolean,SolrParams).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, \n                                      String sort, String prefix, String contains, boolean ignoreCase, boolean intersectsCheck)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = global.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    LeafReader r = searcher.getLeafReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef prefixTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      prefixTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (prefixTermBytes != null) {\n        if (termsEnum.seekCeil(prefixTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    PostingsEnum postingsEnum = null;\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (prefixTermBytes != null && !StringHelper.startsWith(term, prefixTermBytes))\n          break;\n\n        if (contains == null || contains(term.utf8ToString(), contains, ignoreCase)) {\n          int df = termsEnum.docFreq();\n\n          // If we are sorting, we can use df>min (rather than >=) since we\n          // are going in index order.  For certain term distributions this can\n          // make a large difference (for example, many terms with df=1).\n          if (df > 0 && df > min) {\n            int c;\n\n            if (df >= minDfFilterCache) {\n              // use the filter cache\n\n              if (deState == null) {\n                deState = new SolrIndexSearcher.DocsEnumState();\n                deState.fieldName = field;\n                deState.liveDocs = r.getLiveDocs();\n                deState.termsEnum = termsEnum;\n                deState.postingsEnum = postingsEnum;\n              }\n\n              if (intersectsCheck) {\n                c = searcher.intersects(docs, deState) ? 1 : 0;\n              } else {\n                c = searcher.numDocs(docs, deState);\n              }\n\n              postingsEnum = deState.postingsEnum;\n            } else {\n              // iterate over TermDocs to calculate the intersection\n\n              // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n              // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n              // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n              postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n              c = 0;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                \n                SEGMENTS_LOOP:\n                for (int subindex = 0; subindex < numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    if (fastForRandomSet.exists(docid + base)) {\n                      c++;\n                      if (intersectsCheck) {\n                        assert c==1;\n                        break SEGMENTS_LOOP;\n                      }\n                    }\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid)) {\n                    c++;\n                    if (intersectsCheck) {\n                      assert c==1;\n                      break;\n                    }\n                  }\n                }\n              }\n\n            }\n\n            if (sortByCount) {\n              if (c > min) {\n                BytesRef termCopy = BytesRef.deepCopyOf(term);\n                queue.add(new CountPair<>(termCopy, c));\n                if (queue.size() >= maxsize) min = queue.last().val;\n              }\n            } else {\n              if (c >= mincount && --off < 0) {\n                if (--lim < 0) break;\n                ft.indexedToReadable(term, charsRef);\n                res.add(charsRef.toString(), c);\n              }\n            }\n          }\n        }\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix, String contains, boolean ignoreCase, SolrParams params)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = global.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    LeafReader r = searcher.getLeafReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef prefixTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      prefixTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (prefixTermBytes != null) {\n        if (termsEnum.seekCeil(prefixTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    PostingsEnum postingsEnum = null;\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (prefixTermBytes != null && !StringHelper.startsWith(term, prefixTermBytes))\n          break;\n\n        if (contains == null || contains(term.utf8ToString(), contains, ignoreCase)) {\n          int df = termsEnum.docFreq();\n\n          // If we are sorting, we can use df>min (rather than >=) since we\n          // are going in index order.  For certain term distributions this can\n          // make a large difference (for example, many terms with df=1).\n          if (df > 0 && df > min) {\n            int c;\n\n            if (df >= minDfFilterCache) {\n              // use the filter cache\n\n              if (deState == null) {\n                deState = new SolrIndexSearcher.DocsEnumState();\n                deState.fieldName = field;\n                deState.liveDocs = r.getLiveDocs();\n                deState.termsEnum = termsEnum;\n                deState.postingsEnum = postingsEnum;\n              }\n\n              c = searcher.numDocs(docs, deState);\n\n              postingsEnum = deState.postingsEnum;\n            } else {\n              // iterate over TermDocs to calculate the intersection\n\n              // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n              // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n              // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n              postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n              c = 0;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                for (int subindex = 0; subindex < numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    if (fastForRandomSet.exists(docid + base)) c++;\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid)) c++;\n                }\n              }\n\n            }\n\n            if (sortByCount) {\n              if (c > min) {\n                BytesRef termCopy = BytesRef.deepCopyOf(term);\n                queue.add(new CountPair<>(termCopy, c));\n                if (queue.size() >= maxsize) min = queue.last().val;\n              }\n            } else {\n              if (c >= mincount && --off < 0) {\n                if (--lim < 0) break;\n                ft.indexedToReadable(term, charsRef);\n                res.add(charsRef.toString(), c);\n              }\n            }\n          }\n        }\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bc8f206328a706450934717bec7ccc22ad166fc0","date":1473142172,"type":1,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String,String,boolean,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String,String,boolean,SolrParams).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, \n                                      String sort, String prefix, String contains, boolean ignoreCase, boolean intersectsCheck)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = global.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    LeafReader r = searcher.getLeafReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef prefixTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      prefixTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (prefixTermBytes != null) {\n        if (termsEnum.seekCeil(prefixTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    PostingsEnum postingsEnum = null;\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (prefixTermBytes != null && !StringHelper.startsWith(term, prefixTermBytes))\n          break;\n\n        if (contains == null || contains(term.utf8ToString(), contains, ignoreCase)) {\n          int df = termsEnum.docFreq();\n\n          // If we are sorting, we can use df>min (rather than >=) since we\n          // are going in index order.  For certain term distributions this can\n          // make a large difference (for example, many terms with df=1).\n          if (df > 0 && df > min) {\n            int c;\n\n            if (df >= minDfFilterCache) {\n              // use the filter cache\n\n              if (deState == null) {\n                deState = new SolrIndexSearcher.DocsEnumState();\n                deState.fieldName = field;\n                deState.liveDocs = r.getLiveDocs();\n                deState.termsEnum = termsEnum;\n                deState.postingsEnum = postingsEnum;\n              }\n\n              if (intersectsCheck) {\n                c = searcher.intersects(docs, deState) ? 1 : 0;\n              } else {\n                c = searcher.numDocs(docs, deState);\n              }\n\n              postingsEnum = deState.postingsEnum;\n            } else {\n              // iterate over TermDocs to calculate the intersection\n\n              // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n              // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n              // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n              postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n              c = 0;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                \n                SEGMENTS_LOOP:\n                for (int subindex = 0; subindex < numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    if (fastForRandomSet.exists(docid + base)) {\n                      c++;\n                      if (intersectsCheck) {\n                        assert c==1;\n                        break SEGMENTS_LOOP;\n                      }\n                    }\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid)) {\n                    c++;\n                    if (intersectsCheck) {\n                      assert c==1;\n                      break;\n                    }\n                  }\n                }\n              }\n\n            }\n\n            if (sortByCount) {\n              if (c > min) {\n                BytesRef termCopy = BytesRef.deepCopyOf(term);\n                queue.add(new CountPair<>(termCopy, c));\n                if (queue.size() >= maxsize) min = queue.last().val;\n              }\n            } else {\n              if (c >= mincount && --off < 0) {\n                if (--lim < 0) break;\n                ft.indexedToReadable(term, charsRef);\n                res.add(charsRef.toString(), c);\n              }\n            }\n          }\n        }\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix, String contains, boolean ignoreCase, SolrParams params)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = global.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    LeafReader r = searcher.getLeafReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef prefixTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      prefixTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (prefixTermBytes != null) {\n        if (termsEnum.seekCeil(prefixTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    PostingsEnum postingsEnum = null;\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (prefixTermBytes != null && !StringHelper.startsWith(term, prefixTermBytes))\n          break;\n\n        if (contains == null || contains(term.utf8ToString(), contains, ignoreCase)) {\n          int df = termsEnum.docFreq();\n\n          // If we are sorting, we can use df>min (rather than >=) since we\n          // are going in index order.  For certain term distributions this can\n          // make a large difference (for example, many terms with df=1).\n          if (df > 0 && df > min) {\n            int c;\n\n            if (df >= minDfFilterCache) {\n              // use the filter cache\n\n              if (deState == null) {\n                deState = new SolrIndexSearcher.DocsEnumState();\n                deState.fieldName = field;\n                deState.liveDocs = r.getLiveDocs();\n                deState.termsEnum = termsEnum;\n                deState.postingsEnum = postingsEnum;\n              }\n\n              c = searcher.numDocs(docs, deState);\n\n              postingsEnum = deState.postingsEnum;\n            } else {\n              // iterate over TermDocs to calculate the intersection\n\n              // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n              // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n              // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n              postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n              c = 0;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                for (int subindex = 0; subindex < numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    if (fastForRandomSet.exists(docid + base)) c++;\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid)) c++;\n                }\n              }\n\n            }\n\n            if (sortByCount) {\n              if (c > min) {\n                BytesRef termCopy = BytesRef.deepCopyOf(term);\n                queue.add(new CountPair<>(termCopy, c));\n                if (queue.size() >= maxsize) min = queue.last().val;\n              }\n            } else {\n              if (c >= mincount && --off < 0) {\n                if (--lim < 0) break;\n                ft.indexedToReadable(term, charsRef);\n                res.add(charsRef.toString(), c);\n              }\n            }\n          }\n        }\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"89424def13674ea17829b41c5883c54ecc31a132","date":1473767373,"type":1,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String,String,boolean,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String,String,boolean,SolrParams).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, \n                                      String sort, String prefix, String contains, boolean ignoreCase, boolean intersectsCheck)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = global.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    LeafReader r = searcher.getLeafReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef prefixTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      prefixTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (prefixTermBytes != null) {\n        if (termsEnum.seekCeil(prefixTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    PostingsEnum postingsEnum = null;\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (prefixTermBytes != null && !StringHelper.startsWith(term, prefixTermBytes))\n          break;\n\n        if (contains == null || contains(term.utf8ToString(), contains, ignoreCase)) {\n          int df = termsEnum.docFreq();\n\n          // If we are sorting, we can use df>min (rather than >=) since we\n          // are going in index order.  For certain term distributions this can\n          // make a large difference (for example, many terms with df=1).\n          if (df > 0 && df > min) {\n            int c;\n\n            if (df >= minDfFilterCache) {\n              // use the filter cache\n\n              if (deState == null) {\n                deState = new SolrIndexSearcher.DocsEnumState();\n                deState.fieldName = field;\n                deState.liveDocs = r.getLiveDocs();\n                deState.termsEnum = termsEnum;\n                deState.postingsEnum = postingsEnum;\n              }\n\n              if (intersectsCheck) {\n                c = searcher.intersects(docs, deState) ? 1 : 0;\n              } else {\n                c = searcher.numDocs(docs, deState);\n              }\n\n              postingsEnum = deState.postingsEnum;\n            } else {\n              // iterate over TermDocs to calculate the intersection\n\n              // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n              // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n              // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n              postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n              c = 0;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                \n                SEGMENTS_LOOP:\n                for (int subindex = 0; subindex < numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    if (fastForRandomSet.exists(docid + base)) {\n                      c++;\n                      if (intersectsCheck) {\n                        assert c==1;\n                        break SEGMENTS_LOOP;\n                      }\n                    }\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid)) {\n                    c++;\n                    if (intersectsCheck) {\n                      assert c==1;\n                      break;\n                    }\n                  }\n                }\n              }\n\n            }\n\n            if (sortByCount) {\n              if (c > min) {\n                BytesRef termCopy = BytesRef.deepCopyOf(term);\n                queue.add(new CountPair<>(termCopy, c));\n                if (queue.size() >= maxsize) min = queue.last().val;\n              }\n            } else {\n              if (c >= mincount && --off < 0) {\n                if (--lim < 0) break;\n                ft.indexedToReadable(term, charsRef);\n                res.add(charsRef.toString(), c);\n              }\n            }\n          }\n        }\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix, String contains, boolean ignoreCase, SolrParams params)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = global.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    LeafReader r = searcher.getLeafReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef prefixTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      prefixTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (prefixTermBytes != null) {\n        if (termsEnum.seekCeil(prefixTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    PostingsEnum postingsEnum = null;\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (prefixTermBytes != null && !StringHelper.startsWith(term, prefixTermBytes))\n          break;\n\n        if (contains == null || contains(term.utf8ToString(), contains, ignoreCase)) {\n          int df = termsEnum.docFreq();\n\n          // If we are sorting, we can use df>min (rather than >=) since we\n          // are going in index order.  For certain term distributions this can\n          // make a large difference (for example, many terms with df=1).\n          if (df > 0 && df > min) {\n            int c;\n\n            if (df >= minDfFilterCache) {\n              // use the filter cache\n\n              if (deState == null) {\n                deState = new SolrIndexSearcher.DocsEnumState();\n                deState.fieldName = field;\n                deState.liveDocs = r.getLiveDocs();\n                deState.termsEnum = termsEnum;\n                deState.postingsEnum = postingsEnum;\n              }\n\n              c = searcher.numDocs(docs, deState);\n\n              postingsEnum = deState.postingsEnum;\n            } else {\n              // iterate over TermDocs to calculate the intersection\n\n              // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n              // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n              // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n              postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n              c = 0;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                for (int subindex = 0; subindex < numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    if (fastForRandomSet.exists(docid + base)) c++;\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid)) c++;\n                }\n              }\n\n            }\n\n            if (sortByCount) {\n              if (c > min) {\n                BytesRef termCopy = BytesRef.deepCopyOf(term);\n                queue.add(new CountPair<>(termCopy, c));\n                if (queue.size() >= maxsize) min = queue.last().val;\n              }\n            } else {\n              if (c >= mincount && --off < 0) {\n                if (--lim < 0) break;\n                ft.indexedToReadable(term, charsRef);\n                res.add(charsRef.toString(), c);\n              }\n            }\n          }\n        }\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String,String,boolean,boolean).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, \n                                      String sort, String prefix, String contains, boolean ignoreCase, boolean intersectsCheck)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = global.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    LeafReader r = searcher.getLeafReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef prefixTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      prefixTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (prefixTermBytes != null) {\n        if (termsEnum.seekCeil(prefixTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    PostingsEnum postingsEnum = null;\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (prefixTermBytes != null && !StringHelper.startsWith(term, prefixTermBytes))\n          break;\n\n        if (contains == null || contains(term.utf8ToString(), contains, ignoreCase)) {\n          int df = termsEnum.docFreq();\n\n          // If we are sorting, we can use df>min (rather than >=) since we\n          // are going in index order.  For certain term distributions this can\n          // make a large difference (for example, many terms with df=1).\n          if (df > 0 && df > min) {\n            int c;\n\n            if (df >= minDfFilterCache) {\n              // use the filter cache\n\n              if (deState == null) {\n                deState = new SolrIndexSearcher.DocsEnumState();\n                deState.fieldName = field;\n                deState.liveDocs = r.getLiveDocs();\n                deState.termsEnum = termsEnum;\n                deState.postingsEnum = postingsEnum;\n              }\n\n              if (intersectsCheck) {\n                c = searcher.intersects(docs, deState) ? 1 : 0;\n              } else {\n                c = searcher.numDocs(docs, deState);\n              }\n\n              postingsEnum = deState.postingsEnum;\n            } else {\n              // iterate over TermDocs to calculate the intersection\n\n              // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n              // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n              // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n              postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n              c = 0;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                \n                SEGMENTS_LOOP:\n                for (int subindex = 0; subindex < numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    if (fastForRandomSet.exists(docid + base)) {\n                      c++;\n                      if (intersectsCheck) {\n                        assert c==1;\n                        break SEGMENTS_LOOP;\n                      }\n                    }\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid)) {\n                    c++;\n                    if (intersectsCheck) {\n                      assert c==1;\n                      break;\n                    }\n                  }\n                }\n              }\n\n            }\n\n            if (sortByCount) {\n              if (c > min) {\n                BytesRef termCopy = BytesRef.deepCopyOf(term);\n                queue.add(new CountPair<>(termCopy, c));\n                if (queue.size() >= maxsize) min = queue.last().val;\n              }\n            } else {\n              if (c >= mincount && --off < 0) {\n                if (--lim < 0) break;\n                ft.indexedToReadable(term, charsRef);\n                res.add(charsRef.toString(), c);\n              }\n            }\n          }\n        }\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e07c409cff8701e4dc3d45934b021a949a5a8822","date":1475694629,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String,String,boolean,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String,String,boolean,boolean).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, \n                                      String sort, String prefix, String contains, boolean ignoreCase, boolean intersectsCheck)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = global.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    LeafReader r = searcher.getSlowAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef prefixTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      prefixTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (prefixTermBytes != null) {\n        if (termsEnum.seekCeil(prefixTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    PostingsEnum postingsEnum = null;\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (prefixTermBytes != null && !StringHelper.startsWith(term, prefixTermBytes))\n          break;\n\n        if (contains == null || contains(term.utf8ToString(), contains, ignoreCase)) {\n          int df = termsEnum.docFreq();\n\n          // If we are sorting, we can use df>min (rather than >=) since we\n          // are going in index order.  For certain term distributions this can\n          // make a large difference (for example, many terms with df=1).\n          if (df > 0 && df > min) {\n            int c;\n\n            if (df >= minDfFilterCache) {\n              // use the filter cache\n\n              if (deState == null) {\n                deState = new SolrIndexSearcher.DocsEnumState();\n                deState.fieldName = field;\n                deState.liveDocs = r.getLiveDocs();\n                deState.termsEnum = termsEnum;\n                deState.postingsEnum = postingsEnum;\n              }\n\n              if (intersectsCheck) {\n                c = searcher.intersects(docs, deState) ? 1 : 0;\n              } else {\n                c = searcher.numDocs(docs, deState);\n              }\n\n              postingsEnum = deState.postingsEnum;\n            } else {\n              // iterate over TermDocs to calculate the intersection\n\n              // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n              // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n              // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n              postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n              c = 0;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                \n                SEGMENTS_LOOP:\n                for (int subindex = 0; subindex < numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    if (fastForRandomSet.exists(docid + base)) {\n                      c++;\n                      if (intersectsCheck) {\n                        assert c==1;\n                        break SEGMENTS_LOOP;\n                      }\n                    }\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid)) {\n                    c++;\n                    if (intersectsCheck) {\n                      assert c==1;\n                      break;\n                    }\n                  }\n                }\n              }\n\n            }\n\n            if (sortByCount) {\n              if (c > min) {\n                BytesRef termCopy = BytesRef.deepCopyOf(term);\n                queue.add(new CountPair<>(termCopy, c));\n                if (queue.size() >= maxsize) min = queue.last().val;\n              }\n            } else {\n              if (c >= mincount && --off < 0) {\n                if (--lim < 0) break;\n                ft.indexedToReadable(term, charsRef);\n                res.add(charsRef.toString(), c);\n              }\n            }\n          }\n        }\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, \n                                      String sort, String prefix, String contains, boolean ignoreCase, boolean intersectsCheck)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = global.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    LeafReader r = searcher.getLeafReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef prefixTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      prefixTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (prefixTermBytes != null) {\n        if (termsEnum.seekCeil(prefixTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    PostingsEnum postingsEnum = null;\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (prefixTermBytes != null && !StringHelper.startsWith(term, prefixTermBytes))\n          break;\n\n        if (contains == null || contains(term.utf8ToString(), contains, ignoreCase)) {\n          int df = termsEnum.docFreq();\n\n          // If we are sorting, we can use df>min (rather than >=) since we\n          // are going in index order.  For certain term distributions this can\n          // make a large difference (for example, many terms with df=1).\n          if (df > 0 && df > min) {\n            int c;\n\n            if (df >= minDfFilterCache) {\n              // use the filter cache\n\n              if (deState == null) {\n                deState = new SolrIndexSearcher.DocsEnumState();\n                deState.fieldName = field;\n                deState.liveDocs = r.getLiveDocs();\n                deState.termsEnum = termsEnum;\n                deState.postingsEnum = postingsEnum;\n              }\n\n              if (intersectsCheck) {\n                c = searcher.intersects(docs, deState) ? 1 : 0;\n              } else {\n                c = searcher.numDocs(docs, deState);\n              }\n\n              postingsEnum = deState.postingsEnum;\n            } else {\n              // iterate over TermDocs to calculate the intersection\n\n              // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n              // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n              // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n              postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n              c = 0;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                \n                SEGMENTS_LOOP:\n                for (int subindex = 0; subindex < numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    if (fastForRandomSet.exists(docid + base)) {\n                      c++;\n                      if (intersectsCheck) {\n                        assert c==1;\n                        break SEGMENTS_LOOP;\n                      }\n                    }\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid)) {\n                    c++;\n                    if (intersectsCheck) {\n                      assert c==1;\n                      break;\n                    }\n                  }\n                }\n              }\n\n            }\n\n            if (sortByCount) {\n              if (c > min) {\n                BytesRef termCopy = BytesRef.deepCopyOf(term);\n                queue.add(new CountPair<>(termCopy, c));\n                if (queue.size() >= maxsize) min = queue.last().val;\n              }\n            } else {\n              if (c >= mincount && --off < 0) {\n                if (--lim < 0) break;\n                ft.indexedToReadable(term, charsRef);\n                res.add(charsRef.toString(), c);\n              }\n            }\n          }\n        }\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String,String,boolean,boolean).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, \n                                      String sort, String prefix, String contains, boolean ignoreCase, boolean intersectsCheck)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = global.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    LeafReader r = searcher.getSlowAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef prefixTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      prefixTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (prefixTermBytes != null) {\n        if (termsEnum.seekCeil(prefixTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    PostingsEnum postingsEnum = null;\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (prefixTermBytes != null && !StringHelper.startsWith(term, prefixTermBytes))\n          break;\n\n        if (contains == null || contains(term.utf8ToString(), contains, ignoreCase)) {\n          int df = termsEnum.docFreq();\n\n          // If we are sorting, we can use df>min (rather than >=) since we\n          // are going in index order.  For certain term distributions this can\n          // make a large difference (for example, many terms with df=1).\n          if (df > 0 && df > min) {\n            int c;\n\n            if (df >= minDfFilterCache) {\n              // use the filter cache\n\n              if (deState == null) {\n                deState = new SolrIndexSearcher.DocsEnumState();\n                deState.fieldName = field;\n                deState.liveDocs = r.getLiveDocs();\n                deState.termsEnum = termsEnum;\n                deState.postingsEnum = postingsEnum;\n              }\n\n              if (intersectsCheck) {\n                c = searcher.intersects(docs, deState) ? 1 : 0;\n              } else {\n                c = searcher.numDocs(docs, deState);\n              }\n\n              postingsEnum = deState.postingsEnum;\n            } else {\n              // iterate over TermDocs to calculate the intersection\n\n              // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n              // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n              // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n              postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n              c = 0;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                \n                SEGMENTS_LOOP:\n                for (int subindex = 0; subindex < numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    if (fastForRandomSet.exists(docid + base)) {\n                      c++;\n                      if (intersectsCheck) {\n                        assert c==1;\n                        break SEGMENTS_LOOP;\n                      }\n                    }\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid)) {\n                    c++;\n                    if (intersectsCheck) {\n                      assert c==1;\n                      break;\n                    }\n                  }\n                }\n              }\n\n            }\n\n            if (sortByCount) {\n              if (c > min) {\n                BytesRef termCopy = BytesRef.deepCopyOf(term);\n                queue.add(new CountPair<>(termCopy, c));\n                if (queue.size() >= maxsize) min = queue.last().val;\n              }\n            } else {\n              if (c >= mincount && --off < 0) {\n                if (--lim < 0) break;\n                ft.indexedToReadable(term, charsRef);\n                res.add(charsRef.toString(), c);\n              }\n            }\n          }\n        }\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"46dc9ac8b3e748407baaef82453138ff3974480c","date":1484789241,"type":3,"author":"Tomas Fernandez Lobbe","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String,String,boolean,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String,String,boolean,boolean).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, \n                                      String sort, String prefix, String contains, boolean ignoreCase, boolean intersectsCheck)\n    throws IOException {\n    \n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = global.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n    IndexSchema schema = searcher.getSchema();\n    FieldType ft = schema.getFieldType(field);\n    assert !ft.isPointField(): \"Point Fields don't support enum method\";\n    \n    LeafReader r = searcher.getSlowAtomicReader();\n    \n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef prefixTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      prefixTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (prefixTermBytes != null) {\n        if (termsEnum.seekCeil(prefixTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    PostingsEnum postingsEnum = null;\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (prefixTermBytes != null && !StringHelper.startsWith(term, prefixTermBytes))\n          break;\n\n        if (contains == null || contains(term.utf8ToString(), contains, ignoreCase)) {\n          int df = termsEnum.docFreq();\n\n          // If we are sorting, we can use df>min (rather than >=) since we\n          // are going in index order.  For certain term distributions this can\n          // make a large difference (for example, many terms with df=1).\n          if (df > 0 && df > min) {\n            int c;\n\n            if (df >= minDfFilterCache) {\n              // use the filter cache\n\n              if (deState == null) {\n                deState = new SolrIndexSearcher.DocsEnumState();\n                deState.fieldName = field;\n                deState.liveDocs = r.getLiveDocs();\n                deState.termsEnum = termsEnum;\n                deState.postingsEnum = postingsEnum;\n              }\n\n              if (intersectsCheck) {\n                c = searcher.intersects(docs, deState) ? 1 : 0;\n              } else {\n                c = searcher.numDocs(docs, deState);\n              }\n\n              postingsEnum = deState.postingsEnum;\n            } else {\n              // iterate over TermDocs to calculate the intersection\n\n              // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n              // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n              // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n              postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n              c = 0;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                \n                SEGMENTS_LOOP:\n                for (int subindex = 0; subindex < numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    if (fastForRandomSet.exists(docid + base)) {\n                      c++;\n                      if (intersectsCheck) {\n                        assert c==1;\n                        break SEGMENTS_LOOP;\n                      }\n                    }\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid)) {\n                    c++;\n                    if (intersectsCheck) {\n                      assert c==1;\n                      break;\n                    }\n                  }\n                }\n              }\n\n            }\n\n            if (sortByCount) {\n              if (c > min) {\n                BytesRef termCopy = BytesRef.deepCopyOf(term);\n                queue.add(new CountPair<>(termCopy, c));\n                if (queue.size() >= maxsize) min = queue.last().val;\n              }\n            } else {\n              if (c >= mincount && --off < 0) {\n                if (--lim < 0) break;\n                ft.indexedToReadable(term, charsRef);\n                res.add(charsRef.toString(), c);\n              }\n            }\n          }\n        }\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, \n                                      String sort, String prefix, String contains, boolean ignoreCase, boolean intersectsCheck)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = global.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    LeafReader r = searcher.getSlowAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef prefixTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      prefixTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (prefixTermBytes != null) {\n        if (termsEnum.seekCeil(prefixTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    PostingsEnum postingsEnum = null;\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (prefixTermBytes != null && !StringHelper.startsWith(term, prefixTermBytes))\n          break;\n\n        if (contains == null || contains(term.utf8ToString(), contains, ignoreCase)) {\n          int df = termsEnum.docFreq();\n\n          // If we are sorting, we can use df>min (rather than >=) since we\n          // are going in index order.  For certain term distributions this can\n          // make a large difference (for example, many terms with df=1).\n          if (df > 0 && df > min) {\n            int c;\n\n            if (df >= minDfFilterCache) {\n              // use the filter cache\n\n              if (deState == null) {\n                deState = new SolrIndexSearcher.DocsEnumState();\n                deState.fieldName = field;\n                deState.liveDocs = r.getLiveDocs();\n                deState.termsEnum = termsEnum;\n                deState.postingsEnum = postingsEnum;\n              }\n\n              if (intersectsCheck) {\n                c = searcher.intersects(docs, deState) ? 1 : 0;\n              } else {\n                c = searcher.numDocs(docs, deState);\n              }\n\n              postingsEnum = deState.postingsEnum;\n            } else {\n              // iterate over TermDocs to calculate the intersection\n\n              // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n              // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n              // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n              postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n              c = 0;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                \n                SEGMENTS_LOOP:\n                for (int subindex = 0; subindex < numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    if (fastForRandomSet.exists(docid + base)) {\n                      c++;\n                      if (intersectsCheck) {\n                        assert c==1;\n                        break SEGMENTS_LOOP;\n                      }\n                    }\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid)) {\n                    c++;\n                    if (intersectsCheck) {\n                      assert c==1;\n                      break;\n                    }\n                  }\n                }\n              }\n\n            }\n\n            if (sortByCount) {\n              if (c > min) {\n                BytesRef termCopy = BytesRef.deepCopyOf(term);\n                queue.add(new CountPair<>(termCopy, c));\n                if (queue.size() >= maxsize) min = queue.last().val;\n              }\n            } else {\n              if (c >= mincount && --off < 0) {\n                if (--lim < 0) break;\n                ft.indexedToReadable(term, charsRef);\n                res.add(charsRef.toString(), c);\n              }\n            }\n          }\n        }\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"90a682dc1bfd188ef61cc28373c7f5d700b4ac75","date":1485186128,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String,String,boolean,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String,String,boolean,boolean).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, \n                                      String sort, String prefix, String contains, boolean ignoreCase, boolean intersectsCheck)\n    throws IOException {\n    \n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = global.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n    IndexSchema schema = searcher.getSchema();\n    FieldType ft = schema.getFieldType(field);\n    assert !ft.isPointField(): \"Point Fields don't support enum method\";\n    \n    LeafReader r = searcher.getSlowAtomicReader();\n    \n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef prefixTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      prefixTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (prefixTermBytes != null) {\n        if (termsEnum.seekCeil(prefixTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    PostingsEnum postingsEnum = null;\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (prefixTermBytes != null && !StringHelper.startsWith(term, prefixTermBytes))\n          break;\n\n        if (contains == null || contains(term.utf8ToString(), contains, ignoreCase)) {\n          int df = termsEnum.docFreq();\n\n          // If we are sorting, we can use df>min (rather than >=) since we\n          // are going in index order.  For certain term distributions this can\n          // make a large difference (for example, many terms with df=1).\n          if (df > 0 && df > min) {\n            int c;\n\n            if (df >= minDfFilterCache) {\n              // use the filter cache\n\n              if (deState == null) {\n                deState = new SolrIndexSearcher.DocsEnumState();\n                deState.fieldName = field;\n                deState.liveDocs = r.getLiveDocs();\n                deState.termsEnum = termsEnum;\n                deState.postingsEnum = postingsEnum;\n              }\n\n              if (intersectsCheck) {\n                c = searcher.intersects(docs, deState) ? 1 : 0;\n              } else {\n                c = searcher.numDocs(docs, deState);\n              }\n\n              postingsEnum = deState.postingsEnum;\n            } else {\n              // iterate over TermDocs to calculate the intersection\n\n              // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n              // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n              // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n              postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n              c = 0;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                \n                SEGMENTS_LOOP:\n                for (int subindex = 0; subindex < numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    if (fastForRandomSet.exists(docid + base)) {\n                      c++;\n                      if (intersectsCheck) {\n                        assert c==1;\n                        break SEGMENTS_LOOP;\n                      }\n                    }\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid)) {\n                    c++;\n                    if (intersectsCheck) {\n                      assert c==1;\n                      break;\n                    }\n                  }\n                }\n              }\n\n            }\n\n            if (sortByCount) {\n              if (c > min) {\n                BytesRef termCopy = BytesRef.deepCopyOf(term);\n                queue.add(new CountPair<>(termCopy, c));\n                if (queue.size() >= maxsize) min = queue.last().val;\n              }\n            } else {\n              if (c >= mincount && --off < 0) {\n                if (--lim < 0) break;\n                ft.indexedToReadable(term, charsRef);\n                res.add(charsRef.toString(), c);\n              }\n            }\n          }\n        }\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, \n                                      String sort, String prefix, String contains, boolean ignoreCase, boolean intersectsCheck)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = global.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n\n    IndexSchema schema = searcher.getSchema();\n    LeafReader r = searcher.getSlowAtomicReader();\n    FieldType ft = schema.getFieldType(field);\n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef prefixTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      prefixTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (prefixTermBytes != null) {\n        if (termsEnum.seekCeil(prefixTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    PostingsEnum postingsEnum = null;\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (prefixTermBytes != null && !StringHelper.startsWith(term, prefixTermBytes))\n          break;\n\n        if (contains == null || contains(term.utf8ToString(), contains, ignoreCase)) {\n          int df = termsEnum.docFreq();\n\n          // If we are sorting, we can use df>min (rather than >=) since we\n          // are going in index order.  For certain term distributions this can\n          // make a large difference (for example, many terms with df=1).\n          if (df > 0 && df > min) {\n            int c;\n\n            if (df >= minDfFilterCache) {\n              // use the filter cache\n\n              if (deState == null) {\n                deState = new SolrIndexSearcher.DocsEnumState();\n                deState.fieldName = field;\n                deState.liveDocs = r.getLiveDocs();\n                deState.termsEnum = termsEnum;\n                deState.postingsEnum = postingsEnum;\n              }\n\n              if (intersectsCheck) {\n                c = searcher.intersects(docs, deState) ? 1 : 0;\n              } else {\n                c = searcher.numDocs(docs, deState);\n              }\n\n              postingsEnum = deState.postingsEnum;\n            } else {\n              // iterate over TermDocs to calculate the intersection\n\n              // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n              // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n              // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n              postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n              c = 0;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                \n                SEGMENTS_LOOP:\n                for (int subindex = 0; subindex < numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    if (fastForRandomSet.exists(docid + base)) {\n                      c++;\n                      if (intersectsCheck) {\n                        assert c==1;\n                        break SEGMENTS_LOOP;\n                      }\n                    }\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid)) {\n                    c++;\n                    if (intersectsCheck) {\n                      assert c==1;\n                      break;\n                    }\n                  }\n                }\n              }\n\n            }\n\n            if (sortByCount) {\n              if (c > min) {\n                BytesRef termCopy = BytesRef.deepCopyOf(term);\n                queue.add(new CountPair<>(termCopy, c));\n                if (queue.size() >= maxsize) min = queue.last().val;\n              }\n            } else {\n              if (c >= mincount && --off < 0) {\n                if (--lim < 0) break;\n                ft.indexedToReadable(term, charsRef);\n                res.add(charsRef.toString(), c);\n              }\n            }\n          }\n        }\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"862ed062e72c1c01ecd8593b17804ac02b69cf0e","date":1486641184,"type":3,"author":"Christine Poerschke","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String,String,boolean,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String,String,boolean,boolean).mjava","sourceNew":"  /**\n   *  Works like {@link #getFacetTermEnumCounts(SolrIndexSearcher, DocSet, String, int, int, int, boolean, String, String, Predicate, boolean)}\n   *  but takes a substring directly for the contains check rather than a {@link Predicate} instance.\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing,\n                                                   String sort, String prefix, String contains, boolean ignoreCase, boolean intersectsCheck)\n    throws IOException {\n\n    final Predicate<BytesRef> termFilter = new SubstringBytesRefFilter(contains, ignoreCase);\n    return getFacetTermEnumCounts(searcher, docs, field, offset, limit, mincount, missing, sort, prefix, termFilter, intersectsCheck);\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList<Integer> getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, \n                                      String sort, String prefix, String contains, boolean ignoreCase, boolean intersectsCheck)\n    throws IOException {\n    \n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = global.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    // make sure we have a set that is fast for random access, if we will use it for that\n    DocSet fastForRandomSet = docs;\n    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {\n      SortedIntDocSet sset = (SortedIntDocSet)docs;\n      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n    }\n\n    IndexSchema schema = searcher.getSchema();\n    FieldType ft = schema.getFieldType(field);\n    assert !ft.isPointField(): \"Point Fields don't support enum method\";\n    \n    LeafReader r = searcher.getSlowAtomicReader();\n    \n\n    boolean sortByCount = sort.equals(\"count\") || sort.equals(\"true\");\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;\n    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;\n    final NamedList<Integer> res = new NamedList<>();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    BytesRef prefixTermBytes = null;\n    if (prefix != null) {\n      String indexedPrefix = ft.toInternal(prefix);\n      prefixTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = r.fields();\n    Terms terms = fields==null ? null : fields.terms(field);\n    TermsEnum termsEnum = null;\n    SolrIndexSearcher.DocsEnumState deState = null;\n    BytesRef term = null;\n    if (terms != null) {\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (prefixTermBytes != null) {\n        if (termsEnum.seekCeil(prefixTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    PostingsEnum postingsEnum = null;\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n\n    if (docs.size() >= mincount) {\n      while (term != null) {\n\n        if (prefixTermBytes != null && !StringHelper.startsWith(term, prefixTermBytes))\n          break;\n\n        if (contains == null || contains(term.utf8ToString(), contains, ignoreCase)) {\n          int df = termsEnum.docFreq();\n\n          // If we are sorting, we can use df>min (rather than >=) since we\n          // are going in index order.  For certain term distributions this can\n          // make a large difference (for example, many terms with df=1).\n          if (df > 0 && df > min) {\n            int c;\n\n            if (df >= minDfFilterCache) {\n              // use the filter cache\n\n              if (deState == null) {\n                deState = new SolrIndexSearcher.DocsEnumState();\n                deState.fieldName = field;\n                deState.liveDocs = r.getLiveDocs();\n                deState.termsEnum = termsEnum;\n                deState.postingsEnum = postingsEnum;\n              }\n\n              if (intersectsCheck) {\n                c = searcher.intersects(docs, deState) ? 1 : 0;\n              } else {\n                c = searcher.numDocs(docs, deState);\n              }\n\n              postingsEnum = deState.postingsEnum;\n            } else {\n              // iterate over TermDocs to calculate the intersection\n\n              // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?\n              // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)\n              // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?\n              postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n              c = 0;\n\n              if (postingsEnum instanceof MultiPostingsEnum) {\n                MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n                int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n                \n                SEGMENTS_LOOP:\n                for (int subindex = 0; subindex < numSubs; subindex++) {\n                  MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.postingsEnum == null) continue;\n                  int base = sub.slice.start;\n                  int docid;\n                  while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                    if (fastForRandomSet.exists(docid + base)) {\n                      c++;\n                      if (intersectsCheck) {\n                        assert c==1;\n                        break SEGMENTS_LOOP;\n                      }\n                    }\n                  }\n                }\n              } else {\n                int docid;\n                while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid)) {\n                    c++;\n                    if (intersectsCheck) {\n                      assert c==1;\n                      break;\n                    }\n                  }\n                }\n              }\n\n            }\n\n            if (sortByCount) {\n              if (c > min) {\n                BytesRef termCopy = BytesRef.deepCopyOf(term);\n                queue.add(new CountPair<>(termCopy, c));\n                if (queue.size() >= maxsize) min = queue.last().val;\n              }\n            } else {\n              if (c >= mincount && --off < 0) {\n                if (--lim < 0) break;\n                ft.indexedToReadable(term, charsRef);\n                res.add(charsRef.toString(), c);\n              }\n            }\n          }\n        }\n        term = termsEnum.next();\n      }\n    }\n\n    if (sortByCount) {\n      for (CountPair<BytesRef,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        ft.indexedToReadable(p.key, charsRef);\n        res.add(charsRef.toString(), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"862ed062e72c1c01ecd8593b17804ac02b69cf0e":["46dc9ac8b3e748407baaef82453138ff3974480c"],"46dc9ac8b3e748407baaef82453138ff3974480c":["e07c409cff8701e4dc3d45934b021a949a5a8822"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","89424def13674ea17829b41c5883c54ecc31a132"],"bc8f206328a706450934717bec7ccc22ad166fc0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","39342cb658ac11dfcbf4459807fb00eb9ada0218"],"e07c409cff8701e4dc3d45934b021a949a5a8822":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"39342cb658ac11dfcbf4459807fb00eb9ada0218":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","e07c409cff8701e4dc3d45934b021a949a5a8822"],"90a682dc1bfd188ef61cc28373c7f5d700b4ac75":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","46dc9ac8b3e748407baaef82453138ff3974480c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["862ed062e72c1c01ecd8593b17804ac02b69cf0e"],"89424def13674ea17829b41c5883c54ecc31a132":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","bc8f206328a706450934717bec7ccc22ad166fc0"]},"commit2Childs":{"862ed062e72c1c01ecd8593b17804ac02b69cf0e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"46dc9ac8b3e748407baaef82453138ff3974480c":["862ed062e72c1c01ecd8593b17804ac02b69cf0e","90a682dc1bfd188ef61cc28373c7f5d700b4ac75"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","bc8f206328a706450934717bec7ccc22ad166fc0","39342cb658ac11dfcbf4459807fb00eb9ada0218","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","89424def13674ea17829b41c5883c54ecc31a132"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["e07c409cff8701e4dc3d45934b021a949a5a8822"],"bc8f206328a706450934717bec7ccc22ad166fc0":["89424def13674ea17829b41c5883c54ecc31a132"],"e07c409cff8701e4dc3d45934b021a949a5a8822":["46dc9ac8b3e748407baaef82453138ff3974480c","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"39342cb658ac11dfcbf4459807fb00eb9ada0218":["bc8f206328a706450934717bec7ccc22ad166fc0"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["90a682dc1bfd188ef61cc28373c7f5d700b4ac75"],"90a682dc1bfd188ef61cc28373c7f5d700b4ac75":[],"89424def13674ea17829b41c5883c54ecc31a132":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["90a682dc1bfd188ef61cc28373c7f5d700b4ac75","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}