{"path":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","commits":[{"id":"d5cb41f0427fb7a75b0bfefe992561738c11fd80","date":1486576891,"type":1,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String).mjava","sourceNew":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs).toArray(new Metric[0]);\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(\"sort\", sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndOperation.class)\n        .withFunctionName(\"or\", OrOperation.class)\n        .withFunctionName(\"not\", NotOperation.class)\n        .withFunctionName(\"eq\", EqualsOperation.class)\n        .withFunctionName(\"gt\", GreaterThanOperation.class)\n        .withFunctionName(\"lt\", LessThanOperation.class)\n        .withFunctionName(\"lteq\", LessThanEqualToOperation.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToOperation.class);\n\n    if(havingPredicate != null) {\n      BooleanOperation booleanOperation = (BooleanOperation)factory.constructOperation(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","sourceOld":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit) throws IOException {\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs).toArray(new Metric[0]);\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(\"sort\", sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n      StreamFactory factory = new StreamFactory()\n          .withFunctionName(\"search\", CloudSolrStream.class)\n          .withFunctionName(\"parallel\", ParallelStream.class)\n          .withFunctionName(\"rollup\", RollupStream.class)\n          .withFunctionName(\"sum\", SumMetric.class)\n          .withFunctionName(\"min\", MinMetric.class)\n          .withFunctionName(\"max\", MaxMetric.class)\n          .withFunctionName(\"avg\", MeanMetric.class)\n          .withFunctionName(\"count\", CountMetric.class);\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3f20deebda1cf327549c84cb60464135abd31c71","date":1487004368,"type":3,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","sourceNew":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(\"sort\", sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndOperation.class)\n        .withFunctionName(\"or\", OrOperation.class)\n        .withFunctionName(\"not\", NotOperation.class)\n        .withFunctionName(\"eq\", EqualsOperation.class)\n        .withFunctionName(\"gt\", GreaterThanOperation.class)\n        .withFunctionName(\"lt\", LessThanOperation.class)\n        .withFunctionName(\"lteq\", LessThanEqualToOperation.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToOperation.class);\n\n    if(havingPredicate != null) {\n      BooleanOperation booleanOperation = (BooleanOperation)factory.constructOperation(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","sourceOld":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs).toArray(new Metric[0]);\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(\"sort\", sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndOperation.class)\n        .withFunctionName(\"or\", OrOperation.class)\n        .withFunctionName(\"not\", NotOperation.class)\n        .withFunctionName(\"eq\", EqualsOperation.class)\n        .withFunctionName(\"gt\", GreaterThanOperation.class)\n        .withFunctionName(\"lt\", LessThanOperation.class)\n        .withFunctionName(\"lteq\", LessThanEqualToOperation.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToOperation.class);\n\n    if(havingPredicate != null) {\n      BooleanOperation booleanOperation = (BooleanOperation)factory.constructOperation(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"116fdd6b9e04e18a6547a5650bc0afd3fda020aa","date":1487184909,"type":0,"author":"Joel Bernstein","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","pathOld":"/dev/null","sourceNew":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(\"sort\", sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndOperation.class)\n        .withFunctionName(\"or\", OrOperation.class)\n        .withFunctionName(\"not\", NotOperation.class)\n        .withFunctionName(\"eq\", EqualsOperation.class)\n        .withFunctionName(\"gt\", GreaterThanOperation.class)\n        .withFunctionName(\"lt\", LessThanOperation.class)\n        .withFunctionName(\"lteq\", LessThanEqualToOperation.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToOperation.class);\n\n    if(havingPredicate != null) {\n      BooleanOperation booleanOperation = (BooleanOperation)factory.constructOperation(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f48c9aed8d697923300603241ba117b0c19bf774","date":1487186484,"type":3,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","sourceNew":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(\"sort\", sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndEvaluator.class)\n        .withFunctionName(\"or\", OrEvaluator.class)\n        .withFunctionName(\"not\", NotEvaluator.class)\n        .withFunctionName(\"eq\", EqualsEvaluator.class)\n        .withFunctionName(\"gt\", GreaterThanEvaluator.class)\n        .withFunctionName(\"lt\", LessThanEvaluator.class)\n        .withFunctionName(\"val\", RawValueEvaluator.class)\n        .withFunctionName(\"lteq\", LessThanEqualToEvaluator.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToEvaluator.class);\n\n    if(havingPredicate != null) {\n      BooleanEvaluator booleanOperation = (BooleanEvaluator)factory.constructEvaluator(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","sourceOld":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(\"sort\", sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndOperation.class)\n        .withFunctionName(\"or\", OrOperation.class)\n        .withFunctionName(\"not\", NotOperation.class)\n        .withFunctionName(\"eq\", EqualsOperation.class)\n        .withFunctionName(\"gt\", GreaterThanOperation.class)\n        .withFunctionName(\"lt\", LessThanOperation.class)\n        .withFunctionName(\"lteq\", LessThanEqualToOperation.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToOperation.class);\n\n    if(havingPredicate != null) {\n      BooleanOperation booleanOperation = (BooleanOperation)factory.constructOperation(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b352170d31f8b9e7dc335bfa0c757a34b5ef0ce6","date":1489617105,"type":3,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","sourceNew":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    params.set(CommonParams.WT, CommonParams.JAVABIN);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(\"sort\", sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndEvaluator.class)\n        .withFunctionName(\"or\", OrEvaluator.class)\n        .withFunctionName(\"not\", NotEvaluator.class)\n        .withFunctionName(\"eq\", EqualsEvaluator.class)\n        .withFunctionName(\"gt\", GreaterThanEvaluator.class)\n        .withFunctionName(\"lt\", LessThanEvaluator.class)\n        .withFunctionName(\"val\", RawValueEvaluator.class)\n        .withFunctionName(\"lteq\", LessThanEqualToEvaluator.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToEvaluator.class);\n\n    if(havingPredicate != null) {\n      BooleanEvaluator booleanOperation = (BooleanEvaluator)factory.constructEvaluator(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","sourceOld":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(\"sort\", sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndEvaluator.class)\n        .withFunctionName(\"or\", OrEvaluator.class)\n        .withFunctionName(\"not\", NotEvaluator.class)\n        .withFunctionName(\"eq\", EqualsEvaluator.class)\n        .withFunctionName(\"gt\", GreaterThanEvaluator.class)\n        .withFunctionName(\"lt\", LessThanEvaluator.class)\n        .withFunctionName(\"val\", RawValueEvaluator.class)\n        .withFunctionName(\"lteq\", LessThanEqualToEvaluator.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToEvaluator.class);\n\n    if(havingPredicate != null) {\n      BooleanEvaluator booleanOperation = (BooleanEvaluator)factory.constructEvaluator(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"94179096945a2e59d9d3c224f780c1d79f2d4b8f","date":1489651910,"type":3,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","sourceNew":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    params.set(CommonParams.WT, CommonParams.JAVABIN);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(\"sort\", sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndEvaluator.class)\n        .withFunctionName(\"or\", OrEvaluator.class)\n        .withFunctionName(\"not\", NotEvaluator.class)\n        .withFunctionName(\"eq\", EqualsEvaluator.class)\n        .withFunctionName(\"gt\", GreaterThanEvaluator.class)\n        .withFunctionName(\"lt\", LessThanEvaluator.class)\n        .withFunctionName(\"val\", RawValueEvaluator.class)\n        .withFunctionName(\"lteq\", LessThanEqualToEvaluator.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToEvaluator.class);\n\n    if(havingPredicate != null) {\n      BooleanEvaluator booleanOperation = (BooleanEvaluator)factory.constructEvaluator(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","sourceOld":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(\"sort\", sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndEvaluator.class)\n        .withFunctionName(\"or\", OrEvaluator.class)\n        .withFunctionName(\"not\", NotEvaluator.class)\n        .withFunctionName(\"eq\", EqualsEvaluator.class)\n        .withFunctionName(\"gt\", GreaterThanEvaluator.class)\n        .withFunctionName(\"lt\", LessThanEvaluator.class)\n        .withFunctionName(\"val\", RawValueEvaluator.class)\n        .withFunctionName(\"lteq\", LessThanEqualToEvaluator.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToEvaluator.class);\n\n    if(havingPredicate != null) {\n      BooleanEvaluator booleanOperation = (BooleanEvaluator)factory.constructEvaluator(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab68488225b6a6c357dda72ed11dedca9914a192","date":1490013111,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","sourceNew":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    params.set(CommonParams.WT, CommonParams.JAVABIN);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(\"sort\", sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndEvaluator.class)\n        .withFunctionName(\"or\", OrEvaluator.class)\n        .withFunctionName(\"not\", NotEvaluator.class)\n        .withFunctionName(\"eq\", EqualsEvaluator.class)\n        .withFunctionName(\"gt\", GreaterThanEvaluator.class)\n        .withFunctionName(\"lt\", LessThanEvaluator.class)\n        .withFunctionName(\"val\", RawValueEvaluator.class)\n        .withFunctionName(\"lteq\", LessThanEqualToEvaluator.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToEvaluator.class);\n\n    if(havingPredicate != null) {\n      BooleanEvaluator booleanOperation = (BooleanEvaluator)factory.constructEvaluator(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","sourceOld":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(\"sort\", sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndEvaluator.class)\n        .withFunctionName(\"or\", OrEvaluator.class)\n        .withFunctionName(\"not\", NotEvaluator.class)\n        .withFunctionName(\"eq\", EqualsEvaluator.class)\n        .withFunctionName(\"gt\", GreaterThanEvaluator.class)\n        .withFunctionName(\"lt\", LessThanEvaluator.class)\n        .withFunctionName(\"val\", RawValueEvaluator.class)\n        .withFunctionName(\"lteq\", LessThanEqualToEvaluator.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToEvaluator.class);\n\n    if(havingPredicate != null) {\n      BooleanEvaluator booleanOperation = (BooleanEvaluator)factory.constructEvaluator(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"01624b85de12fb02335810bdf325124e59040772","date":1490254940,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","sourceNew":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    params.set(CommonParams.WT, CommonParams.JAVABIN);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(SORT, sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndEvaluator.class)\n        .withFunctionName(\"or\", OrEvaluator.class)\n        .withFunctionName(\"not\", NotEvaluator.class)\n        .withFunctionName(\"eq\", EqualsEvaluator.class)\n        .withFunctionName(\"gt\", GreaterThanEvaluator.class)\n        .withFunctionName(\"lt\", LessThanEvaluator.class)\n        .withFunctionName(\"val\", RawValueEvaluator.class)\n        .withFunctionName(\"lteq\", LessThanEqualToEvaluator.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToEvaluator.class);\n\n    if(havingPredicate != null) {\n      BooleanEvaluator booleanOperation = (BooleanEvaluator)factory.constructEvaluator(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","sourceOld":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    params.set(CommonParams.WT, CommonParams.JAVABIN);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(\"sort\", sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndEvaluator.class)\n        .withFunctionName(\"or\", OrEvaluator.class)\n        .withFunctionName(\"not\", NotEvaluator.class)\n        .withFunctionName(\"eq\", EqualsEvaluator.class)\n        .withFunctionName(\"gt\", GreaterThanEvaluator.class)\n        .withFunctionName(\"lt\", LessThanEvaluator.class)\n        .withFunctionName(\"val\", RawValueEvaluator.class)\n        .withFunctionName(\"lteq\", LessThanEqualToEvaluator.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToEvaluator.class);\n\n    if(havingPredicate != null) {\n      BooleanEvaluator booleanOperation = (BooleanEvaluator)factory.constructEvaluator(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6f4c5d3859373c3a74734e85efa122b17514e3e8","date":1490280013,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","sourceNew":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    params.set(CommonParams.WT, CommonParams.JAVABIN);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(SORT, sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndEvaluator.class)\n        .withFunctionName(\"or\", OrEvaluator.class)\n        .withFunctionName(\"not\", NotEvaluator.class)\n        .withFunctionName(\"eq\", EqualsEvaluator.class)\n        .withFunctionName(\"gt\", GreaterThanEvaluator.class)\n        .withFunctionName(\"lt\", LessThanEvaluator.class)\n        .withFunctionName(\"val\", RawValueEvaluator.class)\n        .withFunctionName(\"lteq\", LessThanEqualToEvaluator.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToEvaluator.class);\n\n    if(havingPredicate != null) {\n      BooleanEvaluator booleanOperation = (BooleanEvaluator)factory.constructEvaluator(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","sourceOld":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    params.set(CommonParams.WT, CommonParams.JAVABIN);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(\"sort\", sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndEvaluator.class)\n        .withFunctionName(\"or\", OrEvaluator.class)\n        .withFunctionName(\"not\", NotEvaluator.class)\n        .withFunctionName(\"eq\", EqualsEvaluator.class)\n        .withFunctionName(\"gt\", GreaterThanEvaluator.class)\n        .withFunctionName(\"lt\", LessThanEvaluator.class)\n        .withFunctionName(\"val\", RawValueEvaluator.class)\n        .withFunctionName(\"lteq\", LessThanEqualToEvaluator.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToEvaluator.class);\n\n    if(havingPredicate != null) {\n      BooleanEvaluator booleanOperation = (BooleanEvaluator)factory.constructEvaluator(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d9fe3175bbaa122d67f736a75c2984617c502cc0","date":1490722938,"type":3,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","sourceNew":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    Map<String, Class> fmap = new HashMap();\n    for(Map.Entry<String, Class> entry : fields) {\n      fmap.put(entry.getKey(), entry.getValue());\n    }\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    } else {\n      for(Metric metric : metrics) {\n        Class c = fmap.get(metric.getIdentifier());\n        if(Long.class.equals(c)) {\n          metric.outputLong = true;\n        }\n      }\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    params.set(CommonParams.WT, CommonParams.JAVABIN);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(SORT, sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndEvaluator.class)\n        .withFunctionName(\"or\", OrEvaluator.class)\n        .withFunctionName(\"not\", NotEvaluator.class)\n        .withFunctionName(\"eq\", EqualsEvaluator.class)\n        .withFunctionName(\"gt\", GreaterThanEvaluator.class)\n        .withFunctionName(\"lt\", LessThanEvaluator.class)\n        .withFunctionName(\"val\", RawValueEvaluator.class)\n        .withFunctionName(\"lteq\", LessThanEqualToEvaluator.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToEvaluator.class);\n\n    if(havingPredicate != null) {\n      BooleanEvaluator booleanOperation = (BooleanEvaluator)factory.constructEvaluator(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","sourceOld":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    params.set(CommonParams.WT, CommonParams.JAVABIN);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(SORT, sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndEvaluator.class)\n        .withFunctionName(\"or\", OrEvaluator.class)\n        .withFunctionName(\"not\", NotEvaluator.class)\n        .withFunctionName(\"eq\", EqualsEvaluator.class)\n        .withFunctionName(\"gt\", GreaterThanEvaluator.class)\n        .withFunctionName(\"lt\", LessThanEvaluator.class)\n        .withFunctionName(\"val\", RawValueEvaluator.class)\n        .withFunctionName(\"lteq\", LessThanEqualToEvaluator.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToEvaluator.class);\n\n    if(havingPredicate != null) {\n      BooleanEvaluator booleanOperation = (BooleanEvaluator)factory.constructEvaluator(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a9b741b54c163686a5fd077895a797daa9b0b95f","date":1490873946,"type":3,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","sourceNew":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    Map<String, Class> fmap = new HashMap();\n    for(Map.Entry<String, Class> entry : fields) {\n      fmap.put(entry.getKey(), entry.getValue());\n    }\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    } else {\n      for(Metric metric : metrics) {\n        Class c = fmap.get(metric.getIdentifier());\n        if(Long.class.equals(c)) {\n          metric.outputLong = true;\n        }\n      }\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    params.set(CommonParams.WT, CommonParams.JAVABIN);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(SORT, sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndEvaluator.class)\n        .withFunctionName(\"or\", OrEvaluator.class)\n        .withFunctionName(\"not\", NotEvaluator.class)\n        .withFunctionName(\"eq\", EqualsEvaluator.class)\n        .withFunctionName(\"gt\", GreaterThanEvaluator.class)\n        .withFunctionName(\"lt\", LessThanEvaluator.class)\n        .withFunctionName(\"val\", RawValueEvaluator.class)\n        .withFunctionName(\"lteq\", LessThanEqualToEvaluator.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToEvaluator.class);\n\n    if(havingPredicate != null) {\n      BooleanEvaluator booleanOperation = (BooleanEvaluator)factory.constructEvaluator(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","sourceOld":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    params.set(CommonParams.WT, CommonParams.JAVABIN);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(SORT, sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndEvaluator.class)\n        .withFunctionName(\"or\", OrEvaluator.class)\n        .withFunctionName(\"not\", NotEvaluator.class)\n        .withFunctionName(\"eq\", EqualsEvaluator.class)\n        .withFunctionName(\"gt\", GreaterThanEvaluator.class)\n        .withFunctionName(\"lt\", LessThanEvaluator.class)\n        .withFunctionName(\"val\", RawValueEvaluator.class)\n        .withFunctionName(\"lteq\", LessThanEqualToEvaluator.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToEvaluator.class);\n\n    if(havingPredicate != null) {\n      BooleanEvaluator booleanOperation = (BooleanEvaluator)factory.constructEvaluator(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ba1e7de64990e3928b90142a52fb92eeff556475","date":1503713257,"type":3,"author":"Dennis Gove","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","sourceNew":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    Map<String, Class> fmap = new HashMap();\n    for(Map.Entry<String, Class> entry : fields) {\n      fmap.put(entry.getKey(), entry.getValue());\n    }\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    } else {\n      for(Metric metric : metrics) {\n        Class c = fmap.get(metric.getIdentifier());\n        if(Long.class.equals(c)) {\n          metric.outputLong = true;\n        }\n      }\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    params.set(CommonParams.WT, CommonParams.JAVABIN);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(SORT, sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndEvaluator.class)\n        .withFunctionName(\"or\", OrEvaluator.class)\n        .withFunctionName(\"not\", NotEvaluator.class)\n        .withFunctionName(\"eq\", EqualToEvaluator.class)\n        .withFunctionName(\"gt\", GreaterThanEvaluator.class)\n        .withFunctionName(\"lt\", LessThanEvaluator.class)\n        .withFunctionName(\"val\", RawValueEvaluator.class)\n        .withFunctionName(\"lteq\", LessThanEqualToEvaluator.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToEvaluator.class);\n\n    if(havingPredicate != null) {\n      RecursiveBooleanEvaluator booleanOperation = (RecursiveBooleanEvaluator)factory.constructEvaluator(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","sourceOld":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    Map<String, Class> fmap = new HashMap();\n    for(Map.Entry<String, Class> entry : fields) {\n      fmap.put(entry.getKey(), entry.getValue());\n    }\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    } else {\n      for(Metric metric : metrics) {\n        Class c = fmap.get(metric.getIdentifier());\n        if(Long.class.equals(c)) {\n          metric.outputLong = true;\n        }\n      }\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    params.set(CommonParams.WT, CommonParams.JAVABIN);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(SORT, sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndEvaluator.class)\n        .withFunctionName(\"or\", OrEvaluator.class)\n        .withFunctionName(\"not\", NotEvaluator.class)\n        .withFunctionName(\"eq\", EqualsEvaluator.class)\n        .withFunctionName(\"gt\", GreaterThanEvaluator.class)\n        .withFunctionName(\"lt\", LessThanEvaluator.class)\n        .withFunctionName(\"val\", RawValueEvaluator.class)\n        .withFunctionName(\"lteq\", LessThanEqualToEvaluator.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToEvaluator.class);\n\n    if(havingPredicate != null) {\n      BooleanEvaluator booleanOperation = (BooleanEvaluator)factory.constructEvaluator(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"45c28dbb559366e1f4ef8077346552bfb3f7ecf6","date":1503738439,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","sourceNew":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    Map<String, Class> fmap = new HashMap();\n    for(Map.Entry<String, Class> entry : fields) {\n      fmap.put(entry.getKey(), entry.getValue());\n    }\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    } else {\n      for(Metric metric : metrics) {\n        Class c = fmap.get(metric.getIdentifier());\n        if(Long.class.equals(c)) {\n          metric.outputLong = true;\n        }\n      }\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    params.set(CommonParams.WT, CommonParams.JAVABIN);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(SORT, sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndEvaluator.class)\n        .withFunctionName(\"or\", OrEvaluator.class)\n        .withFunctionName(\"not\", NotEvaluator.class)\n        .withFunctionName(\"eq\", EqualToEvaluator.class)\n        .withFunctionName(\"gt\", GreaterThanEvaluator.class)\n        .withFunctionName(\"lt\", LessThanEvaluator.class)\n        .withFunctionName(\"val\", RawValueEvaluator.class)\n        .withFunctionName(\"lteq\", LessThanEqualToEvaluator.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToEvaluator.class);\n\n    if(havingPredicate != null) {\n      RecursiveBooleanEvaluator booleanOperation = (RecursiveBooleanEvaluator)factory.constructEvaluator(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","sourceOld":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    Map<String, Class> fmap = new HashMap();\n    for(Map.Entry<String, Class> entry : fields) {\n      fmap.put(entry.getKey(), entry.getValue());\n    }\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    } else {\n      for(Metric metric : metrics) {\n        Class c = fmap.get(metric.getIdentifier());\n        if(Long.class.equals(c)) {\n          metric.outputLong = true;\n        }\n      }\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    params.set(CommonParams.WT, CommonParams.JAVABIN);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(SORT, sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndEvaluator.class)\n        .withFunctionName(\"or\", OrEvaluator.class)\n        .withFunctionName(\"not\", NotEvaluator.class)\n        .withFunctionName(\"eq\", EqualsEvaluator.class)\n        .withFunctionName(\"gt\", GreaterThanEvaluator.class)\n        .withFunctionName(\"lt\", LessThanEvaluator.class)\n        .withFunctionName(\"val\", RawValueEvaluator.class)\n        .withFunctionName(\"lteq\", LessThanEqualToEvaluator.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToEvaluator.class);\n\n    if(havingPredicate != null) {\n      BooleanEvaluator booleanOperation = (BooleanEvaluator)factory.constructEvaluator(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a7809d1d753b67f48b1a706e17034bf8b624ea3","date":1504366927,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","sourceNew":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    Map<String, Class> fmap = new HashMap();\n    for(Map.Entry<String, Class> entry : fields) {\n      fmap.put(entry.getKey(), entry.getValue());\n    }\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    } else {\n      for(Metric metric : metrics) {\n        Class c = fmap.get(metric.getIdentifier());\n        if(Long.class.equals(c)) {\n          metric.outputLong = true;\n        }\n      }\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    params.set(CommonParams.WT, CommonParams.JAVABIN);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(SORT, sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndEvaluator.class)\n        .withFunctionName(\"or\", OrEvaluator.class)\n        .withFunctionName(\"not\", NotEvaluator.class)\n        .withFunctionName(\"eq\", EqualToEvaluator.class)\n        .withFunctionName(\"gt\", GreaterThanEvaluator.class)\n        .withFunctionName(\"lt\", LessThanEvaluator.class)\n        .withFunctionName(\"val\", RawValueEvaluator.class)\n        .withFunctionName(\"lteq\", LessThanEqualToEvaluator.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToEvaluator.class);\n\n    if(havingPredicate != null) {\n      RecursiveBooleanEvaluator booleanOperation = (RecursiveBooleanEvaluator)factory.constructEvaluator(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","sourceOld":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    Map<String, Class> fmap = new HashMap();\n    for(Map.Entry<String, Class> entry : fields) {\n      fmap.put(entry.getKey(), entry.getValue());\n    }\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    } else {\n      for(Metric metric : metrics) {\n        Class c = fmap.get(metric.getIdentifier());\n        if(Long.class.equals(c)) {\n          metric.outputLong = true;\n        }\n      }\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    params.set(CommonParams.WT, CommonParams.JAVABIN);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(SORT, sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndEvaluator.class)\n        .withFunctionName(\"or\", OrEvaluator.class)\n        .withFunctionName(\"not\", NotEvaluator.class)\n        .withFunctionName(\"eq\", EqualsEvaluator.class)\n        .withFunctionName(\"gt\", GreaterThanEvaluator.class)\n        .withFunctionName(\"lt\", LessThanEvaluator.class)\n        .withFunctionName(\"val\", RawValueEvaluator.class)\n        .withFunctionName(\"lteq\", LessThanEqualToEvaluator.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToEvaluator.class);\n\n    if(havingPredicate != null) {\n      BooleanEvaluator booleanOperation = (BooleanEvaluator)factory.constructEvaluator(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f15af35d55d70c34451f9df5edeaeff6b31f8cbe","date":1519625627,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","sourceNew":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    Map<String, Class> fmap = new HashMap();\n    for(Map.Entry<String, Class> entry : fields) {\n      fmap.put(entry.getKey(), entry.getValue());\n    }\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    } else {\n      for(Metric metric : metrics) {\n        Class c = fmap.get(metric.getIdentifier());\n        if(Long.class.equals(c)) {\n          metric.outputLong = true;\n        }\n      }\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    params.set(CommonParams.WT, CommonParams.JAVABIN);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(SORT, sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndEvaluator.class)\n        .withFunctionName(\"or\", OrEvaluator.class)\n        .withFunctionName(\"not\", NotEvaluator.class)\n        .withFunctionName(\"eq\", EqualToEvaluator.class)\n        .withFunctionName(\"gt\", GreaterThanEvaluator.class)\n        .withFunctionName(\"lt\", LessThanEvaluator.class)\n        .withFunctionName(\"val\", RawValueEvaluator.class)\n        .withFunctionName(\"lteq\", LessThanEqualToEvaluator.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToEvaluator.class);\n\n    if(havingPredicate != null) {\n      RecursiveBooleanEvaluator booleanOperation = (RecursiveBooleanEvaluator)factory.constructEvaluator(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      @SuppressWarnings(\"resource\")\n      final ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","sourceOld":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    Map<String, Class> fmap = new HashMap();\n    for(Map.Entry<String, Class> entry : fields) {\n      fmap.put(entry.getKey(), entry.getValue());\n    }\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    } else {\n      for(Metric metric : metrics) {\n        Class c = fmap.get(metric.getIdentifier());\n        if(Long.class.equals(c)) {\n          metric.outputLong = true;\n        }\n      }\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    params.set(CommonParams.WT, CommonParams.JAVABIN);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(SORT, sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndEvaluator.class)\n        .withFunctionName(\"or\", OrEvaluator.class)\n        .withFunctionName(\"not\", NotEvaluator.class)\n        .withFunctionName(\"eq\", EqualToEvaluator.class)\n        .withFunctionName(\"gt\", GreaterThanEvaluator.class)\n        .withFunctionName(\"lt\", LessThanEvaluator.class)\n        .withFunctionName(\"val\", RawValueEvaluator.class)\n        .withFunctionName(\"lteq\", LessThanEqualToEvaluator.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToEvaluator.class);\n\n    if(havingPredicate != null) {\n      RecursiveBooleanEvaluator booleanOperation = (RecursiveBooleanEvaluator)factory.constructEvaluator(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8582f07e9350eaeb33bf6c4617b8c9895d99c839","date":1591307386,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/sql/SolrTable#handleGroupByMapReduce(String,String,Properties,List[Map.Entry[String,Class]],String,List[Pair[String,String]],List[String],List[Pair[String,String]],String,String).mjava","sourceNew":"  @SuppressWarnings({\"rawtypes\"})\n  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    Map<String, Class> fmap = new HashMap<>();\n    for(Map.Entry<String, Class> entry : fields) {\n      fmap.put(entry.getKey(), entry.getValue());\n    }\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    } else {\n      for(Metric metric : metrics) {\n        Class c = fmap.get(metric.getIdentifier());\n        if(Long.class.equals(c)) {\n          metric.outputLong = true;\n        }\n      }\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    params.set(CommonParams.WT, CommonParams.JAVABIN);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(SORT, sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndEvaluator.class)\n        .withFunctionName(\"or\", OrEvaluator.class)\n        .withFunctionName(\"not\", NotEvaluator.class)\n        .withFunctionName(\"eq\", EqualToEvaluator.class)\n        .withFunctionName(\"gt\", GreaterThanEvaluator.class)\n        .withFunctionName(\"lt\", LessThanEvaluator.class)\n        .withFunctionName(\"val\", RawValueEvaluator.class)\n        .withFunctionName(\"lteq\", LessThanEqualToEvaluator.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToEvaluator.class);\n\n    if(havingPredicate != null) {\n      RecursiveBooleanEvaluator booleanOperation = (RecursiveBooleanEvaluator)factory.constructEvaluator(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      @SuppressWarnings(\"resource\")\n      final ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","sourceOld":"  private TupleStream handleGroupByMapReduce(String zk,\n                                             String collection,\n                                             Properties properties,\n                                             final List<Map.Entry<String, Class>> fields,\n                                             final String query,\n                                             final List<Pair<String, String>> orders,\n                                             final List<String> _buckets,\n                                             final List<Pair<String, String>> metricPairs,\n                                             final String limit,\n                                             final String havingPredicate) throws IOException {\n\n    Map<String, Class> fmap = new HashMap();\n    for(Map.Entry<String, Class> entry : fields) {\n      fmap.put(entry.getKey(), entry.getValue());\n    }\n\n    int numWorkers = Integer.parseInt(properties.getProperty(\"numWorkers\", \"1\"));\n\n    Bucket[] buckets = buildBuckets(_buckets, fields);\n    Metric[] metrics = buildMetrics(metricPairs, false).toArray(new Metric[0]);\n\n    if(metrics.length == 0) {\n      return handleSelectDistinctMapReduce(zk, collection, properties, fields, query, orders, buckets, limit);\n    } else {\n      for(Metric metric : metrics) {\n        Class c = fmap.get(metric.getIdentifier());\n        if(Long.class.equals(c)) {\n          metric.outputLong = true;\n        }\n      }\n    }\n\n    Set<String> fieldSet = getFieldSet(metrics, fields);\n\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = getFields(fieldSet);\n    String sortDirection = getSortDirection(orders);\n    String sort = bucketSort(buckets, sortDirection);\n\n    ModifiableSolrParams params = new ModifiableSolrParams();\n\n    params.set(CommonParams.FL, fl);\n    params.set(CommonParams.Q, query);\n    params.set(CommonParams.WT, CommonParams.JAVABIN);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.set(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.set(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.set(SORT, sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zk, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    StreamFactory factory = new StreamFactory()\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"rollup\", RollupStream.class)\n        .withFunctionName(\"sum\", SumMetric.class)\n        .withFunctionName(\"min\", MinMetric.class)\n        .withFunctionName(\"max\", MaxMetric.class)\n        .withFunctionName(\"avg\", MeanMetric.class)\n        .withFunctionName(\"count\", CountMetric.class)\n        .withFunctionName(\"and\", AndEvaluator.class)\n        .withFunctionName(\"or\", OrEvaluator.class)\n        .withFunctionName(\"not\", NotEvaluator.class)\n        .withFunctionName(\"eq\", EqualToEvaluator.class)\n        .withFunctionName(\"gt\", GreaterThanEvaluator.class)\n        .withFunctionName(\"lt\", LessThanEvaluator.class)\n        .withFunctionName(\"val\", RawValueEvaluator.class)\n        .withFunctionName(\"lteq\", LessThanEqualToEvaluator.class)\n        .withFunctionName(\"having\", HavingStream.class)\n        .withFunctionName(\"gteq\", GreaterThanEqualToEvaluator.class);\n\n    if(havingPredicate != null) {\n      RecursiveBooleanEvaluator booleanOperation = (RecursiveBooleanEvaluator)factory.constructEvaluator(StreamExpressionParser.parse(havingPredicate));\n      tupleStream = new HavingStream(tupleStream, booleanOperation);\n    }\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      @SuppressWarnings(\"resource\")\n      final ParallelStream parallelStream = new ParallelStream(zk, collection, tupleStream, numWorkers, comp);\n\n\n      parallelStream.setStreamFactory(factory);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: Currently we are not pushing down the having clause.\n    //      We need to push down the having clause to ensure that LIMIT does not cut off records prior to the having filter.\n\n    if(orders != null && orders.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, orders)) {\n        int lim = (limit == null) ? 100 : Integer.parseInt(limit);\n        StreamComparator comp = getComp(orders);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, lim, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(limit != null) {\n          tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n        }\n      }\n    } else {\n      //No order by, check for limit\n      if(limit != null) {\n        tupleStream = new LimitStream(tupleStream, Integer.parseInt(limit));\n      }\n    }\n\n    return tupleStream;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"d5cb41f0427fb7a75b0bfefe992561738c11fd80":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"6f4c5d3859373c3a74734e85efa122b17514e3e8":["ab68488225b6a6c357dda72ed11dedca9914a192"],"01624b85de12fb02335810bdf325124e59040772":["b352170d31f8b9e7dc335bfa0c757a34b5ef0ce6"],"8582f07e9350eaeb33bf6c4617b8c9895d99c839":["f15af35d55d70c34451f9df5edeaeff6b31f8cbe"],"a9b741b54c163686a5fd077895a797daa9b0b95f":["6f4c5d3859373c3a74734e85efa122b17514e3e8"],"b352170d31f8b9e7dc335bfa0c757a34b5ef0ce6":["f48c9aed8d697923300603241ba117b0c19bf774"],"116fdd6b9e04e18a6547a5650bc0afd3fda020aa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3f20deebda1cf327549c84cb60464135abd31c71"],"3a7809d1d753b67f48b1a706e17034bf8b624ea3":["a9b741b54c163686a5fd077895a797daa9b0b95f","45c28dbb559366e1f4ef8077346552bfb3f7ecf6"],"d9fe3175bbaa122d67f736a75c2984617c502cc0":["01624b85de12fb02335810bdf325124e59040772"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"94179096945a2e59d9d3c224f780c1d79f2d4b8f":["f48c9aed8d697923300603241ba117b0c19bf774"],"45c28dbb559366e1f4ef8077346552bfb3f7ecf6":["d9fe3175bbaa122d67f736a75c2984617c502cc0","ba1e7de64990e3928b90142a52fb92eeff556475"],"ba1e7de64990e3928b90142a52fb92eeff556475":["d9fe3175bbaa122d67f736a75c2984617c502cc0"],"3f20deebda1cf327549c84cb60464135abd31c71":["d5cb41f0427fb7a75b0bfefe992561738c11fd80"],"f15af35d55d70c34451f9df5edeaeff6b31f8cbe":["45c28dbb559366e1f4ef8077346552bfb3f7ecf6"],"ab68488225b6a6c357dda72ed11dedca9914a192":["f48c9aed8d697923300603241ba117b0c19bf774","94179096945a2e59d9d3c224f780c1d79f2d4b8f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["8582f07e9350eaeb33bf6c4617b8c9895d99c839"],"f48c9aed8d697923300603241ba117b0c19bf774":["116fdd6b9e04e18a6547a5650bc0afd3fda020aa"]},"commit2Childs":{"d5cb41f0427fb7a75b0bfefe992561738c11fd80":["3f20deebda1cf327549c84cb60464135abd31c71"],"6f4c5d3859373c3a74734e85efa122b17514e3e8":["a9b741b54c163686a5fd077895a797daa9b0b95f"],"01624b85de12fb02335810bdf325124e59040772":["d9fe3175bbaa122d67f736a75c2984617c502cc0"],"8582f07e9350eaeb33bf6c4617b8c9895d99c839":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a9b741b54c163686a5fd077895a797daa9b0b95f":["3a7809d1d753b67f48b1a706e17034bf8b624ea3"],"b352170d31f8b9e7dc335bfa0c757a34b5ef0ce6":["01624b85de12fb02335810bdf325124e59040772"],"116fdd6b9e04e18a6547a5650bc0afd3fda020aa":["f48c9aed8d697923300603241ba117b0c19bf774"],"3a7809d1d753b67f48b1a706e17034bf8b624ea3":[],"d9fe3175bbaa122d67f736a75c2984617c502cc0":["45c28dbb559366e1f4ef8077346552bfb3f7ecf6","ba1e7de64990e3928b90142a52fb92eeff556475"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d5cb41f0427fb7a75b0bfefe992561738c11fd80","116fdd6b9e04e18a6547a5650bc0afd3fda020aa"],"94179096945a2e59d9d3c224f780c1d79f2d4b8f":["ab68488225b6a6c357dda72ed11dedca9914a192"],"45c28dbb559366e1f4ef8077346552bfb3f7ecf6":["3a7809d1d753b67f48b1a706e17034bf8b624ea3","f15af35d55d70c34451f9df5edeaeff6b31f8cbe"],"ba1e7de64990e3928b90142a52fb92eeff556475":["45c28dbb559366e1f4ef8077346552bfb3f7ecf6"],"3f20deebda1cf327549c84cb60464135abd31c71":["116fdd6b9e04e18a6547a5650bc0afd3fda020aa"],"f15af35d55d70c34451f9df5edeaeff6b31f8cbe":["8582f07e9350eaeb33bf6c4617b8c9895d99c839"],"ab68488225b6a6c357dda72ed11dedca9914a192":["6f4c5d3859373c3a74734e85efa122b17514e3e8"],"f48c9aed8d697923300603241ba117b0c19bf774":["b352170d31f8b9e7dc335bfa0c757a34b5ef0ce6","94179096945a2e59d9d3c224f780c1d79f2d4b8f","ab68488225b6a6c357dda72ed11dedca9914a192"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["3a7809d1d753b67f48b1a706e17034bf8b624ea3","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}