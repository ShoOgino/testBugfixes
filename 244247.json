{"path":"lucene/test-framework/src/java/org/apache/lucene/search/similarities/BaseSimilarityTestCase#testRandomScoring().mjava","commits":[{"id":"ad1dc49b5314cfdb82a7ea40d2f92f07fe8cee46","date":1508899684,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/search/similarities/BaseSimilarityTestCase#testRandomScoring().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Tests scoring across a bunch of random terms/corpora/frequencies for each possible document length.\n   * It does the following checks:\n   * <ul>\n   *   <li>scores are non-negative and finite.\n   *   <li>score matches the explanation exactly.\n   *   <li>internal explanations calculations are sane (e.g. sum of: and so on actually compute sums)\n   *   <li>scores don't decrease as term frequencies increase: e.g. score(freq=N + 1) &gt;= score(freq=N)\n   *   <li>scores don't decrease as documents get shorter, e.g. score(len=M) &gt;= score(len=M+1)\n   *   <li>scores don't decrease as terms get rarer, e.g. score(term=N) &gt;= score(term=N+1)\n   *   <li>scoring works for floating point frequencies (e.g. sloppy phrase and span queries will work)\n   *   <li>scoring works for reasonably large 64-bit statistic values (e.g. distributed search will work)\n   *   <li>scoring works for reasonably large boost values (0 .. Integer.MAX_VALUE, e.g. query boosts will work)\n   *   <li>scoring works for parameters randomized within valid ranges (see {@link #getSimilarity(Random)})\n   * </ul>\n   */\n  public void testRandomScoring() throws Exception {\n    Random random = random();\n    final int iterations = atLeast(10);\n    for (int i = 0; i < iterations; i++) {\n      // pull a new similarity to switch up parameters\n      Similarity similarity = getSimilarity(random);\n      for (int j = 0; j < 10; j++) {\n        // for each norm value...\n        for (int k = 0; k < NORM_VALUES.size(); k++) {\n          CollectionStatistics corpus = newCorpus(random, k);\n          for (int l = 0; l < 10; l++) {\n            TermStatistics term = newTerm(random, corpus);\n            final float freq;\n            if (term.totalTermFreq() == -1) {\n              // omit TF\n              freq = 1;\n            } else if (term.docFreq() == 1) {\n              // only one document, all the instances must be here.\n              freq = Math.toIntExact(term.totalTermFreq());\n            } else {\n              // there is at least one other document, and those must have at least 1 instance each.\n              int upperBound = Math.toIntExact(Math.min(term.totalTermFreq() - term.docFreq() + 1, Integer.MAX_VALUE));\n              if (random.nextBoolean()) {\n                freq = TestUtil.nextInt(random, 1, upperBound);\n              } else {\n                float freqCandidate = upperBound * random.nextFloat();\n                // we need to be 2nd float value at a minimum, the pairwise test will check MIN_VALUE in this case.\n                // this avoids testing frequencies of 0 which seem wrong to allow (we should enforce computeSlopFactor etc)\n                if (freqCandidate <= Float.MIN_VALUE) {\n                  freqCandidate = Math.nextUp(Float.MIN_VALUE);\n                }\n                freq = freqCandidate;\n              }\n            }\n            // we just limit the test to \"reasonable\" boost values but don't enforce this anywhere.\n            // too big, and you are asking for overflow. that's hard for a sim to enforce (but definitely possible)\n            // for now, we just want to detect overflow where its a real bug/hazard in the computation with reasonable inputs.\n            final float boost;\n            switch (random.nextInt(5)) {\n              case 0:\n                // minimum value (not enforced)\n                boost = 0F;\n                break;\n              case 1:\n                // tiny value\n                boost = Float.MIN_VALUE;\n                break;\n              case 2:\n                // no-op value (sometimes treated special in explanations)\n                boost = 1F;\n                break;\n              case 3:\n                // maximum value (not enforceD)\n                boost = Integer.MAX_VALUE;\n                break;\n              default:\n                // random value\n                boost = random.nextFloat() * Integer.MAX_VALUE;\n                break;\n            }\n            doTestScoring(similarity, corpus, term, boost, freq, k);\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"086ffe31d8fba0110227db122974163709ecc1b4","date":1509678141,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/search/similarities/BaseSimilarityTestCase#testRandomScoring().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/search/similarities/BaseSimilarityTestCase#testRandomScoring().mjava","sourceNew":"  /**\n   * Tests scoring across a bunch of random terms/corpora/frequencies for each possible document length.\n   * It does the following checks:\n   * <ul>\n   *   <li>scores are non-negative and finite.\n   *   <li>score matches the explanation exactly.\n   *   <li>internal explanations calculations are sane (e.g. sum of: and so on actually compute sums)\n   *   <li>scores don't decrease as term frequencies increase: e.g. score(freq=N + 1) &gt;= score(freq=N)\n   *   <li>scores don't decrease as documents get shorter, e.g. score(len=M) &gt;= score(len=M+1)\n   *   <li>scores don't decrease as terms get rarer, e.g. score(term=N) &gt;= score(term=N+1)\n   *   <li>scoring works for floating point frequencies (e.g. sloppy phrase and span queries will work)\n   *   <li>scoring works for reasonably large 64-bit statistic values (e.g. distributed search will work)\n   *   <li>scoring works for reasonably large boost values (0 .. Integer.MAX_VALUE, e.g. query boosts will work)\n   *   <li>scoring works for parameters randomized within valid ranges (see {@link #getSimilarity(Random)})\n   * </ul>\n   */\n  public void testRandomScoring() throws Exception {\n    Random random = random();\n    final int iterations = atLeast(10);\n    for (int i = 0; i < iterations; i++) {\n      // pull a new similarity to switch up parameters\n      Similarity similarity = getSimilarity(random);\n      for (int j = 0; j < 10; j++) {\n        // for each norm value...\n        for (int k = 0; k < NORM_VALUES.size(); k++) {\n          CollectionStatistics corpus = newCorpus(random, k);\n          for (int l = 0; l < 10; l++) {\n            TermStatistics term = newTerm(random, corpus);\n            final float freq;\n            if (term.totalTermFreq() == term.docFreq()) {\n              // omit TF\n              freq = 1;\n            } else if (term.docFreq() == 1) {\n              // only one document, all the instances must be here.\n              freq = Math.toIntExact(term.totalTermFreq());\n            } else {\n              // there is at least one other document, and those must have at least 1 instance each.\n              int upperBound = Math.toIntExact(Math.min(term.totalTermFreq() - term.docFreq() + 1, Integer.MAX_VALUE));\n              if (random.nextBoolean()) {\n                freq = TestUtil.nextInt(random, 1, upperBound);\n              } else {\n                float freqCandidate = upperBound * random.nextFloat();\n                // we need to be 2nd float value at a minimum, the pairwise test will check MIN_VALUE in this case.\n                // this avoids testing frequencies of 0 which seem wrong to allow (we should enforce computeSlopFactor etc)\n                if (freqCandidate <= Float.MIN_VALUE) {\n                  freqCandidate = Math.nextUp(Float.MIN_VALUE);\n                }\n                freq = freqCandidate;\n              }\n            }\n            // we just limit the test to \"reasonable\" boost values but don't enforce this anywhere.\n            // too big, and you are asking for overflow. that's hard for a sim to enforce (but definitely possible)\n            // for now, we just want to detect overflow where its a real bug/hazard in the computation with reasonable inputs.\n            final float boost;\n            switch (random.nextInt(5)) {\n              case 0:\n                // minimum value (not enforced)\n                boost = 0F;\n                break;\n              case 1:\n                // tiny value\n                boost = Float.MIN_VALUE;\n                break;\n              case 2:\n                // no-op value (sometimes treated special in explanations)\n                boost = 1F;\n                break;\n              case 3:\n                // maximum value (not enforceD)\n                boost = Integer.MAX_VALUE;\n                break;\n              default:\n                // random value\n                boost = random.nextFloat() * Integer.MAX_VALUE;\n                break;\n            }\n            doTestScoring(similarity, corpus, term, boost, freq, k);\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Tests scoring across a bunch of random terms/corpora/frequencies for each possible document length.\n   * It does the following checks:\n   * <ul>\n   *   <li>scores are non-negative and finite.\n   *   <li>score matches the explanation exactly.\n   *   <li>internal explanations calculations are sane (e.g. sum of: and so on actually compute sums)\n   *   <li>scores don't decrease as term frequencies increase: e.g. score(freq=N + 1) &gt;= score(freq=N)\n   *   <li>scores don't decrease as documents get shorter, e.g. score(len=M) &gt;= score(len=M+1)\n   *   <li>scores don't decrease as terms get rarer, e.g. score(term=N) &gt;= score(term=N+1)\n   *   <li>scoring works for floating point frequencies (e.g. sloppy phrase and span queries will work)\n   *   <li>scoring works for reasonably large 64-bit statistic values (e.g. distributed search will work)\n   *   <li>scoring works for reasonably large boost values (0 .. Integer.MAX_VALUE, e.g. query boosts will work)\n   *   <li>scoring works for parameters randomized within valid ranges (see {@link #getSimilarity(Random)})\n   * </ul>\n   */\n  public void testRandomScoring() throws Exception {\n    Random random = random();\n    final int iterations = atLeast(10);\n    for (int i = 0; i < iterations; i++) {\n      // pull a new similarity to switch up parameters\n      Similarity similarity = getSimilarity(random);\n      for (int j = 0; j < 10; j++) {\n        // for each norm value...\n        for (int k = 0; k < NORM_VALUES.size(); k++) {\n          CollectionStatistics corpus = newCorpus(random, k);\n          for (int l = 0; l < 10; l++) {\n            TermStatistics term = newTerm(random, corpus);\n            final float freq;\n            if (term.totalTermFreq() == -1) {\n              // omit TF\n              freq = 1;\n            } else if (term.docFreq() == 1) {\n              // only one document, all the instances must be here.\n              freq = Math.toIntExact(term.totalTermFreq());\n            } else {\n              // there is at least one other document, and those must have at least 1 instance each.\n              int upperBound = Math.toIntExact(Math.min(term.totalTermFreq() - term.docFreq() + 1, Integer.MAX_VALUE));\n              if (random.nextBoolean()) {\n                freq = TestUtil.nextInt(random, 1, upperBound);\n              } else {\n                float freqCandidate = upperBound * random.nextFloat();\n                // we need to be 2nd float value at a minimum, the pairwise test will check MIN_VALUE in this case.\n                // this avoids testing frequencies of 0 which seem wrong to allow (we should enforce computeSlopFactor etc)\n                if (freqCandidate <= Float.MIN_VALUE) {\n                  freqCandidate = Math.nextUp(Float.MIN_VALUE);\n                }\n                freq = freqCandidate;\n              }\n            }\n            // we just limit the test to \"reasonable\" boost values but don't enforce this anywhere.\n            // too big, and you are asking for overflow. that's hard for a sim to enforce (but definitely possible)\n            // for now, we just want to detect overflow where its a real bug/hazard in the computation with reasonable inputs.\n            final float boost;\n            switch (random.nextInt(5)) {\n              case 0:\n                // minimum value (not enforced)\n                boost = 0F;\n                break;\n              case 1:\n                // tiny value\n                boost = Float.MIN_VALUE;\n                break;\n              case 2:\n                // no-op value (sometimes treated special in explanations)\n                boost = 1F;\n                break;\n              case 3:\n                // maximum value (not enforceD)\n                boost = Integer.MAX_VALUE;\n                break;\n              default:\n                // random value\n                boost = random.nextFloat() * Integer.MAX_VALUE;\n                break;\n            }\n            doTestScoring(similarity, corpus, term, boost, freq, k);\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d523b8189b211dd1630166aa77b8c88bb48b3fcc","date":1510144168,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/search/similarities/BaseSimilarityTestCase#testRandomScoring().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/search/similarities/BaseSimilarityTestCase#testRandomScoring().mjava","sourceNew":"  /**\n   * Tests scoring across a bunch of random terms/corpora/frequencies for each possible document length.\n   * It does the following checks:\n   * <ul>\n   *   <li>scores are non-negative and finite.\n   *   <li>score matches the explanation exactly.\n   *   <li>internal explanations calculations are sane (e.g. sum of: and so on actually compute sums)\n   *   <li>scores don't decrease as term frequencies increase: e.g. score(freq=N + 1) &gt;= score(freq=N)\n   *   <li>scores don't decrease as documents get shorter, e.g. score(len=M) &gt;= score(len=M+1)\n   *   <li>scores don't decrease as terms get rarer, e.g. score(term=N) &gt;= score(term=N+1)\n   *   <li>scoring works for floating point frequencies (e.g. sloppy phrase and span queries will work)\n   *   <li>scoring works for reasonably large 64-bit statistic values (e.g. distributed search will work)\n   *   <li>scoring works for reasonably large boost values (0 .. Integer.MAX_VALUE, e.g. query boosts will work)\n   *   <li>scoring works for parameters randomized within valid ranges (see {@link #getSimilarity(Random)})\n   * </ul>\n   */\n  public void testRandomScoring() throws Exception {\n    Random random = random();\n    final int iterations = atLeast(10);\n    for (int i = 0; i < iterations; i++) {\n      // pull a new similarity to switch up parameters\n      Similarity similarity = getSimilarity(random);\n      for (int j = 0; j < 10; j++) {\n        // for each norm value...\n        for (int k = 0; k < NORM_VALUES.size(); k++) {\n          CollectionStatistics corpus = newCorpus(random, k);\n          for (int l = 0; l < 10; l++) {\n            TermStatistics term = newTerm(random, corpus);\n            final float freq;\n            if (term.totalTermFreq() == term.docFreq()) {\n              // omit TF\n              freq = 1;\n            } else if (term.docFreq() == 1) {\n              // only one document, all the instances must be here.\n              freq = Math.toIntExact(term.totalTermFreq());\n            } else {\n              // there is at least one other document, and those must have at least 1 instance each.\n              int upperBound = Math.toIntExact(Math.min(term.totalTermFreq() - term.docFreq() + 1, Integer.MAX_VALUE));\n              if (random.nextBoolean()) {\n                freq = TestUtil.nextInt(random, 1, upperBound);\n              } else {\n                float freqCandidate = upperBound * random.nextFloat();\n                // we need to be 2nd float value at a minimum, the pairwise test will check MIN_VALUE in this case.\n                // this avoids testing frequencies of 0 which seem wrong to allow (we should enforce computeSlopFactor etc)\n                if (freqCandidate <= Float.MIN_VALUE) {\n                  freqCandidate = Math.nextUp(Float.MIN_VALUE);\n                }\n                freq = freqCandidate;\n              }\n            }\n            // we just limit the test to \"reasonable\" boost values but don't enforce this anywhere.\n            // too big, and you are asking for overflow. that's hard for a sim to enforce (but definitely possible)\n            // for now, we just want to detect overflow where its a real bug/hazard in the computation with reasonable inputs.\n            final float boost;\n            switch (random.nextInt(5)) {\n              case 0:\n                // minimum value (not enforced)\n                boost = 0F;\n                break;\n              case 1:\n                // tiny value\n                boost = Float.MIN_VALUE;\n                break;\n              case 2:\n                // no-op value (sometimes treated special in explanations)\n                boost = 1F;\n                break;\n              case 3:\n                // maximum value (not enforceD)\n                boost = Integer.MAX_VALUE;\n                break;\n              default:\n                // random value\n                boost = random.nextFloat() * Integer.MAX_VALUE;\n                break;\n            }\n            doTestScoring(similarity, corpus, term, boost, freq, k);\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Tests scoring across a bunch of random terms/corpora/frequencies for each possible document length.\n   * It does the following checks:\n   * <ul>\n   *   <li>scores are non-negative and finite.\n   *   <li>score matches the explanation exactly.\n   *   <li>internal explanations calculations are sane (e.g. sum of: and so on actually compute sums)\n   *   <li>scores don't decrease as term frequencies increase: e.g. score(freq=N + 1) &gt;= score(freq=N)\n   *   <li>scores don't decrease as documents get shorter, e.g. score(len=M) &gt;= score(len=M+1)\n   *   <li>scores don't decrease as terms get rarer, e.g. score(term=N) &gt;= score(term=N+1)\n   *   <li>scoring works for floating point frequencies (e.g. sloppy phrase and span queries will work)\n   *   <li>scoring works for reasonably large 64-bit statistic values (e.g. distributed search will work)\n   *   <li>scoring works for reasonably large boost values (0 .. Integer.MAX_VALUE, e.g. query boosts will work)\n   *   <li>scoring works for parameters randomized within valid ranges (see {@link #getSimilarity(Random)})\n   * </ul>\n   */\n  public void testRandomScoring() throws Exception {\n    Random random = random();\n    final int iterations = atLeast(10);\n    for (int i = 0; i < iterations; i++) {\n      // pull a new similarity to switch up parameters\n      Similarity similarity = getSimilarity(random);\n      for (int j = 0; j < 10; j++) {\n        // for each norm value...\n        for (int k = 0; k < NORM_VALUES.size(); k++) {\n          CollectionStatistics corpus = newCorpus(random, k);\n          for (int l = 0; l < 10; l++) {\n            TermStatistics term = newTerm(random, corpus);\n            final float freq;\n            if (term.totalTermFreq() == -1) {\n              // omit TF\n              freq = 1;\n            } else if (term.docFreq() == 1) {\n              // only one document, all the instances must be here.\n              freq = Math.toIntExact(term.totalTermFreq());\n            } else {\n              // there is at least one other document, and those must have at least 1 instance each.\n              int upperBound = Math.toIntExact(Math.min(term.totalTermFreq() - term.docFreq() + 1, Integer.MAX_VALUE));\n              if (random.nextBoolean()) {\n                freq = TestUtil.nextInt(random, 1, upperBound);\n              } else {\n                float freqCandidate = upperBound * random.nextFloat();\n                // we need to be 2nd float value at a minimum, the pairwise test will check MIN_VALUE in this case.\n                // this avoids testing frequencies of 0 which seem wrong to allow (we should enforce computeSlopFactor etc)\n                if (freqCandidate <= Float.MIN_VALUE) {\n                  freqCandidate = Math.nextUp(Float.MIN_VALUE);\n                }\n                freq = freqCandidate;\n              }\n            }\n            // we just limit the test to \"reasonable\" boost values but don't enforce this anywhere.\n            // too big, and you are asking for overflow. that's hard for a sim to enforce (but definitely possible)\n            // for now, we just want to detect overflow where its a real bug/hazard in the computation with reasonable inputs.\n            final float boost;\n            switch (random.nextInt(5)) {\n              case 0:\n                // minimum value (not enforced)\n                boost = 0F;\n                break;\n              case 1:\n                // tiny value\n                boost = Float.MIN_VALUE;\n                break;\n              case 2:\n                // no-op value (sometimes treated special in explanations)\n                boost = 1F;\n                break;\n              case 3:\n                // maximum value (not enforceD)\n                boost = Integer.MAX_VALUE;\n                break;\n              default:\n                // random value\n                boost = random.nextFloat() * Integer.MAX_VALUE;\n                break;\n            }\n            doTestScoring(similarity, corpus, term, boost, freq, k);\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"83d379038462cf6dcf64cc9e9a49053c4bb78011","date":1512580797,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/search/similarities/BaseSimilarityTestCase#testRandomScoring().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/search/similarities/BaseSimilarityTestCase#testRandomScoring().mjava","sourceNew":"  /**\n   * Tests scoring across a bunch of random terms/corpora/frequencies for each possible document length.\n   * It does the following checks:\n   * <ul>\n   *   <li>scores are non-negative and finite.\n   *   <li>score matches the explanation exactly.\n   *   <li>internal explanations calculations are sane (e.g. sum of: and so on actually compute sums)\n   *   <li>scores don't decrease as term frequencies increase: e.g. score(freq=N + 1) &gt;= score(freq=N)\n   *   <li>scores don't decrease as documents get shorter, e.g. score(len=M) &gt;= score(len=M+1)\n   *   <li>scores don't decrease as terms get rarer, e.g. score(term=N) &gt;= score(term=N+1)\n   *   <li>scoring works for floating point frequencies (e.g. sloppy phrase and span queries will work)\n   *   <li>scoring works for reasonably large 64-bit statistic values (e.g. distributed search will work)\n   *   <li>scoring works for reasonably large boost values (0 .. Integer.MAX_VALUE, e.g. query boosts will work)\n   *   <li>scoring works for parameters randomized within valid ranges (see {@link #getSimilarity(Random)})\n   * </ul>\n   */\n  public void testRandomScoring() throws Exception {\n    Random random = random();\n    final int iterations = atLeast(10);\n    for (int i = 0; i < iterations; i++) {\n      // pull a new similarity to switch up parameters\n      Similarity similarity = getSimilarity(random);\n      for (int j = 0; j < 10; j++) {\n        // for each norm value...\n        for (int k = 0; k < NORM_VALUES.size(); k++) {\n          CollectionStatistics corpus = newCorpus(random, k);\n          for (int l = 0; l < 10; l++) {\n            TermStatistics term = newTerm(random, corpus);\n            final float freq;\n            if (term.totalTermFreq() == term.docFreq()) {\n              // omit TF\n              freq = 1;\n            } else if (term.docFreq() == 1) {\n              // only one document, all the instances must be here.\n              freq = Math.toIntExact(term.totalTermFreq());\n            } else {\n              // there is at least one other document, and those must have at least 1 instance each.\n              int upperBound = Math.toIntExact(Math.min(term.totalTermFreq() - term.docFreq() + 1, Integer.MAX_VALUE));\n              if (random.nextBoolean()) {\n                // integer freq\n                switch (random.nextInt(3)) {\n                  case 0:\n                    // smallest freq\n                    freq = 1;\n                    break;\n                  case 1:\n                    // largest freq\n                    freq = upperBound;\n                    break;\n                  default:\n                    // random freq\n                    freq = TestUtil.nextInt(random, 1, upperBound);\n                    break;\n                }\n              } else {\n                // float freq\n                float freqCandidate;\n                switch (random.nextInt(2)) {\n                  case 0:\n                    // smallest freq\n                    freqCandidate = Float.MIN_VALUE;\n                    break;\n                  default:\n                    // random freq\n                    freqCandidate = upperBound * random.nextFloat();\n                    break;\n                }\n                // we need to be 2nd float value at a minimum, the pairwise test will check MIN_VALUE in this case.\n                // this avoids testing frequencies of 0 which seem wrong to allow (we should enforce computeSlopFactor etc)\n                if (freqCandidate <= Float.MIN_VALUE) {\n                  freqCandidate = Math.nextUp(Float.MIN_VALUE);\n                }\n                freq = freqCandidate;\n              }\n            }\n            // we just limit the test to \"reasonable\" boost values but don't enforce this anywhere.\n            // too big, and you are asking for overflow. that's hard for a sim to enforce (but definitely possible)\n            // for now, we just want to detect overflow where its a real bug/hazard in the computation with reasonable inputs.\n            final float boost;\n            switch (random.nextInt(5)) {\n              case 0:\n                // minimum value (not enforced)\n                boost = 0F;\n                break;\n              case 1:\n                // tiny value\n                boost = Float.MIN_VALUE;\n                break;\n              case 2:\n                // no-op value (sometimes treated special in explanations)\n                boost = 1F;\n                break;\n              case 3:\n                // maximum value (not enforceD)\n                boost = Integer.MAX_VALUE;\n                break;\n              default:\n                // random value\n                boost = random.nextFloat() * Integer.MAX_VALUE;\n                break;\n            }\n            doTestScoring(similarity, corpus, term, boost, freq, k);\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Tests scoring across a bunch of random terms/corpora/frequencies for each possible document length.\n   * It does the following checks:\n   * <ul>\n   *   <li>scores are non-negative and finite.\n   *   <li>score matches the explanation exactly.\n   *   <li>internal explanations calculations are sane (e.g. sum of: and so on actually compute sums)\n   *   <li>scores don't decrease as term frequencies increase: e.g. score(freq=N + 1) &gt;= score(freq=N)\n   *   <li>scores don't decrease as documents get shorter, e.g. score(len=M) &gt;= score(len=M+1)\n   *   <li>scores don't decrease as terms get rarer, e.g. score(term=N) &gt;= score(term=N+1)\n   *   <li>scoring works for floating point frequencies (e.g. sloppy phrase and span queries will work)\n   *   <li>scoring works for reasonably large 64-bit statistic values (e.g. distributed search will work)\n   *   <li>scoring works for reasonably large boost values (0 .. Integer.MAX_VALUE, e.g. query boosts will work)\n   *   <li>scoring works for parameters randomized within valid ranges (see {@link #getSimilarity(Random)})\n   * </ul>\n   */\n  public void testRandomScoring() throws Exception {\n    Random random = random();\n    final int iterations = atLeast(10);\n    for (int i = 0; i < iterations; i++) {\n      // pull a new similarity to switch up parameters\n      Similarity similarity = getSimilarity(random);\n      for (int j = 0; j < 10; j++) {\n        // for each norm value...\n        for (int k = 0; k < NORM_VALUES.size(); k++) {\n          CollectionStatistics corpus = newCorpus(random, k);\n          for (int l = 0; l < 10; l++) {\n            TermStatistics term = newTerm(random, corpus);\n            final float freq;\n            if (term.totalTermFreq() == term.docFreq()) {\n              // omit TF\n              freq = 1;\n            } else if (term.docFreq() == 1) {\n              // only one document, all the instances must be here.\n              freq = Math.toIntExact(term.totalTermFreq());\n            } else {\n              // there is at least one other document, and those must have at least 1 instance each.\n              int upperBound = Math.toIntExact(Math.min(term.totalTermFreq() - term.docFreq() + 1, Integer.MAX_VALUE));\n              if (random.nextBoolean()) {\n                freq = TestUtil.nextInt(random, 1, upperBound);\n              } else {\n                float freqCandidate = upperBound * random.nextFloat();\n                // we need to be 2nd float value at a minimum, the pairwise test will check MIN_VALUE in this case.\n                // this avoids testing frequencies of 0 which seem wrong to allow (we should enforce computeSlopFactor etc)\n                if (freqCandidate <= Float.MIN_VALUE) {\n                  freqCandidate = Math.nextUp(Float.MIN_VALUE);\n                }\n                freq = freqCandidate;\n              }\n            }\n            // we just limit the test to \"reasonable\" boost values but don't enforce this anywhere.\n            // too big, and you are asking for overflow. that's hard for a sim to enforce (but definitely possible)\n            // for now, we just want to detect overflow where its a real bug/hazard in the computation with reasonable inputs.\n            final float boost;\n            switch (random.nextInt(5)) {\n              case 0:\n                // minimum value (not enforced)\n                boost = 0F;\n                break;\n              case 1:\n                // tiny value\n                boost = Float.MIN_VALUE;\n                break;\n              case 2:\n                // no-op value (sometimes treated special in explanations)\n                boost = 1F;\n                break;\n              case 3:\n                // maximum value (not enforceD)\n                boost = Integer.MAX_VALUE;\n                break;\n              default:\n                // random value\n                boost = random.nextFloat() * Integer.MAX_VALUE;\n                break;\n            }\n            doTestScoring(similarity, corpus, term, boost, freq, k);\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"68d6cb7f0f019661a784bd0e5a21e85b5f812af6","date":1515075216,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/search/similarities/BaseSimilarityTestCase#testRandomScoring().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/search/similarities/BaseSimilarityTestCase#testRandomScoring().mjava","sourceNew":"  /**\n   * Tests scoring across a bunch of random terms/corpora/frequencies for each possible document length.\n   * It does the following checks:\n   * <ul>\n   *   <li>scores are non-negative and finite.\n   *   <li>score matches the explanation exactly.\n   *   <li>internal explanations calculations are sane (e.g. sum of: and so on actually compute sums)\n   *   <li>scores don't decrease as term frequencies increase: e.g. score(freq=N + 1) &gt;= score(freq=N)\n   *   <li>scores don't decrease as documents get shorter, e.g. score(len=M) &gt;= score(len=M+1)\n   *   <li>scores don't decrease as terms get rarer, e.g. score(term=N) &gt;= score(term=N+1)\n   *   <li>scoring works for floating point frequencies (e.g. sloppy phrase and span queries will work)\n   *   <li>scoring works for reasonably large 64-bit statistic values (e.g. distributed search will work)\n   *   <li>scoring works for reasonably large boost values (0 .. Integer.MAX_VALUE, e.g. query boosts will work)\n   *   <li>scoring works for parameters randomized within valid ranges (see {@link #getSimilarity(Random)})\n   * </ul>\n   */\n  public void testRandomScoring() throws Exception {\n    Random random = random();\n    final int iterations = atLeast(10);\n    for (int i = 0; i < iterations; i++) {\n      // pull a new similarity to switch up parameters\n      Similarity similarity = getSimilarity(random);\n      for (int j = 0; j < 10; j++) {\n        // for each norm value...\n        for (int k = 1; k < 256; k++) {\n          CollectionStatistics corpus = newCorpus(random, k);\n          for (int l = 0; l < 10; l++) {\n            TermStatistics term = newTerm(random, corpus);\n            final float freq;\n            if (term.totalTermFreq() == term.docFreq()) {\n              // omit TF\n              freq = 1;\n            } else if (term.docFreq() == 1) {\n              // only one document, all the instances must be here.\n              freq = Math.toIntExact(term.totalTermFreq());\n            } else {\n              // there is at least one other document, and those must have at least 1 instance each.\n              int upperBound = Math.toIntExact(Math.min(term.totalTermFreq() - term.docFreq() + 1, Integer.MAX_VALUE));\n              if (random.nextBoolean()) {\n                // integer freq\n                switch (random.nextInt(3)) {\n                  case 0:\n                    // smallest freq\n                    freq = 1;\n                    break;\n                  case 1:\n                    // largest freq\n                    freq = upperBound;\n                    break;\n                  default:\n                    // random freq\n                    freq = TestUtil.nextInt(random, 1, upperBound);\n                    break;\n                }\n              } else {\n                // float freq\n                float freqCandidate;\n                switch (random.nextInt(2)) {\n                  case 0:\n                    // smallest freq\n                    freqCandidate = Float.MIN_VALUE;\n                    break;\n                  default:\n                    // random freq\n                    freqCandidate = upperBound * random.nextFloat();\n                    break;\n                }\n                // we need to be 2nd float value at a minimum, the pairwise test will check MIN_VALUE in this case.\n                // this avoids testing frequencies of 0 which seem wrong to allow (we should enforce computeSlopFactor etc)\n                if (freqCandidate <= Float.MIN_VALUE) {\n                  freqCandidate = Math.nextUp(Float.MIN_VALUE);\n                }\n                freq = freqCandidate;\n              }\n            }\n            // we just limit the test to \"reasonable\" boost values but don't enforce this anywhere.\n            // too big, and you are asking for overflow. that's hard for a sim to enforce (but definitely possible)\n            // for now, we just want to detect overflow where its a real bug/hazard in the computation with reasonable inputs.\n            final float boost;\n            switch (random.nextInt(5)) {\n              case 0:\n                // minimum value (not enforced)\n                boost = 0F;\n                break;\n              case 1:\n                // tiny value\n                boost = Float.MIN_VALUE;\n                break;\n              case 2:\n                // no-op value (sometimes treated special in explanations)\n                boost = 1F;\n                break;\n              case 3:\n                // maximum value (not enforceD)\n                boost = Integer.MAX_VALUE;\n                break;\n              default:\n                // random value\n                boost = random.nextFloat() * Integer.MAX_VALUE;\n                break;\n            }\n            doTestScoring(similarity, corpus, term, boost, freq, k);\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Tests scoring across a bunch of random terms/corpora/frequencies for each possible document length.\n   * It does the following checks:\n   * <ul>\n   *   <li>scores are non-negative and finite.\n   *   <li>score matches the explanation exactly.\n   *   <li>internal explanations calculations are sane (e.g. sum of: and so on actually compute sums)\n   *   <li>scores don't decrease as term frequencies increase: e.g. score(freq=N + 1) &gt;= score(freq=N)\n   *   <li>scores don't decrease as documents get shorter, e.g. score(len=M) &gt;= score(len=M+1)\n   *   <li>scores don't decrease as terms get rarer, e.g. score(term=N) &gt;= score(term=N+1)\n   *   <li>scoring works for floating point frequencies (e.g. sloppy phrase and span queries will work)\n   *   <li>scoring works for reasonably large 64-bit statistic values (e.g. distributed search will work)\n   *   <li>scoring works for reasonably large boost values (0 .. Integer.MAX_VALUE, e.g. query boosts will work)\n   *   <li>scoring works for parameters randomized within valid ranges (see {@link #getSimilarity(Random)})\n   * </ul>\n   */\n  public void testRandomScoring() throws Exception {\n    Random random = random();\n    final int iterations = atLeast(10);\n    for (int i = 0; i < iterations; i++) {\n      // pull a new similarity to switch up parameters\n      Similarity similarity = getSimilarity(random);\n      for (int j = 0; j < 10; j++) {\n        // for each norm value...\n        for (int k = 0; k < NORM_VALUES.size(); k++) {\n          CollectionStatistics corpus = newCorpus(random, k);\n          for (int l = 0; l < 10; l++) {\n            TermStatistics term = newTerm(random, corpus);\n            final float freq;\n            if (term.totalTermFreq() == term.docFreq()) {\n              // omit TF\n              freq = 1;\n            } else if (term.docFreq() == 1) {\n              // only one document, all the instances must be here.\n              freq = Math.toIntExact(term.totalTermFreq());\n            } else {\n              // there is at least one other document, and those must have at least 1 instance each.\n              int upperBound = Math.toIntExact(Math.min(term.totalTermFreq() - term.docFreq() + 1, Integer.MAX_VALUE));\n              if (random.nextBoolean()) {\n                // integer freq\n                switch (random.nextInt(3)) {\n                  case 0:\n                    // smallest freq\n                    freq = 1;\n                    break;\n                  case 1:\n                    // largest freq\n                    freq = upperBound;\n                    break;\n                  default:\n                    // random freq\n                    freq = TestUtil.nextInt(random, 1, upperBound);\n                    break;\n                }\n              } else {\n                // float freq\n                float freqCandidate;\n                switch (random.nextInt(2)) {\n                  case 0:\n                    // smallest freq\n                    freqCandidate = Float.MIN_VALUE;\n                    break;\n                  default:\n                    // random freq\n                    freqCandidate = upperBound * random.nextFloat();\n                    break;\n                }\n                // we need to be 2nd float value at a minimum, the pairwise test will check MIN_VALUE in this case.\n                // this avoids testing frequencies of 0 which seem wrong to allow (we should enforce computeSlopFactor etc)\n                if (freqCandidate <= Float.MIN_VALUE) {\n                  freqCandidate = Math.nextUp(Float.MIN_VALUE);\n                }\n                freq = freqCandidate;\n              }\n            }\n            // we just limit the test to \"reasonable\" boost values but don't enforce this anywhere.\n            // too big, and you are asking for overflow. that's hard for a sim to enforce (but definitely possible)\n            // for now, we just want to detect overflow where its a real bug/hazard in the computation with reasonable inputs.\n            final float boost;\n            switch (random.nextInt(5)) {\n              case 0:\n                // minimum value (not enforced)\n                boost = 0F;\n                break;\n              case 1:\n                // tiny value\n                boost = Float.MIN_VALUE;\n                break;\n              case 2:\n                // no-op value (sometimes treated special in explanations)\n                boost = 1F;\n                break;\n              case 3:\n                // maximum value (not enforceD)\n                boost = Integer.MAX_VALUE;\n                break;\n              default:\n                // random value\n                boost = random.nextFloat() * Integer.MAX_VALUE;\n                break;\n            }\n            doTestScoring(similarity, corpus, term, boost, freq, k);\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b94236357aaa22b76c10629851fe4e376e0cea82","date":1516710914,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/search/similarities/BaseSimilarityTestCase#testRandomScoring().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/search/similarities/BaseSimilarityTestCase#testRandomScoring().mjava","sourceNew":"  /**\n   * Tests scoring across a bunch of random terms/corpora/frequencies for each possible document length.\n   * It does the following checks:\n   * <ul>\n   *   <li>scores are non-negative and finite.\n   *   <li>score matches the explanation exactly.\n   *   <li>internal explanations calculations are sane (e.g. sum of: and so on actually compute sums)\n   *   <li>scores don't decrease as term frequencies increase: e.g. score(freq=N + 1) &gt;= score(freq=N)\n   *   <li>scores don't decrease as documents get shorter, e.g. score(len=M) &gt;= score(len=M+1)\n   *   <li>scores don't decrease as terms get rarer, e.g. score(term=N) &gt;= score(term=N+1)\n   *   <li>scoring works for floating point frequencies (e.g. sloppy phrase and span queries will work)\n   *   <li>scoring works for reasonably large 64-bit statistic values (e.g. distributed search will work)\n   *   <li>scoring works for reasonably large boost values (0 .. Integer.MAX_VALUE, e.g. query boosts will work)\n   *   <li>scoring works for parameters randomized within valid ranges (see {@link #getSimilarity(Random)})\n   * </ul>\n   */\n  public void testRandomScoring() throws Exception {\n    Random random = random();\n    final int iterations = atLeast(10);\n    for (int i = 0; i < iterations; i++) {\n      // pull a new similarity to switch up parameters\n      Similarity similarity = getSimilarity(random);\n      for (int j = 0; j < 10; j++) {\n        // for each norm value...\n        for (int k = 1; k < 256; k++) {\n          CollectionStatistics corpus = newCorpus(random, k);\n          for (int l = 0; l < 10; l++) {\n            TermStatistics term = newTerm(random, corpus);\n            final float freq;\n            if (term.totalTermFreq() == term.docFreq()) {\n              // omit TF\n              freq = 1;\n            } else if (term.docFreq() == 1) {\n              // only one document, all the instances must be here.\n              freq = Math.toIntExact(term.totalTermFreq());\n            } else {\n              // there is at least one other document, and those must have at least 1 instance each.\n              int upperBound = Math.toIntExact(Math.min(term.totalTermFreq() - term.docFreq() + 1, Integer.MAX_VALUE));\n              if (random.nextBoolean()) {\n                // integer freq\n                switch (random.nextInt(3)) {\n                  case 0:\n                    // smallest freq\n                    freq = 1;\n                    break;\n                  case 1:\n                    // largest freq\n                    freq = upperBound;\n                    break;\n                  default:\n                    // random freq\n                    freq = TestUtil.nextInt(random, 1, upperBound);\n                    break;\n                }\n              } else {\n                // float freq\n                float freqCandidate;\n                switch (random.nextInt(2)) {\n                  case 0:\n                    // smallest freq\n                    freqCandidate = Float.MIN_VALUE;\n                    break;\n                  default:\n                    // random freq\n                    freqCandidate = upperBound * random.nextFloat();\n                    break;\n                }\n                // we need to be 2nd float value at a minimum, the pairwise test will check MIN_VALUE in this case.\n                // this avoids testing frequencies of 0 which seem wrong to allow (we should enforce computeSlopFactor etc)\n                if (freqCandidate <= Float.MIN_VALUE) {\n                  freqCandidate = Math.nextUp(Float.MIN_VALUE);\n                }\n                freq = freqCandidate;\n              }\n            }\n            // we just limit the test to \"reasonable\" boost values but don't enforce this anywhere.\n            // too big, and you are asking for overflow. that's hard for a sim to enforce (but definitely possible)\n            // for now, we just want to detect overflow where its a real bug/hazard in the computation with reasonable inputs.\n            final float boost;\n            switch (random.nextInt(5)) {\n              case 0:\n                // minimum value (not enforced)\n                boost = 0F;\n                break;\n              case 1:\n                // tiny value\n                boost = Float.MIN_VALUE;\n                break;\n              case 2:\n                // no-op value (sometimes treated special in explanations)\n                boost = 1F;\n                break;\n              case 3:\n                // maximum value (not enforceD)\n                boost = Integer.MAX_VALUE;\n                break;\n              default:\n                // random value\n                boost = random.nextFloat() * Integer.MAX_VALUE;\n                break;\n            }\n            doTestScoring(similarity, corpus, term, boost, freq, k);\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Tests scoring across a bunch of random terms/corpora/frequencies for each possible document length.\n   * It does the following checks:\n   * <ul>\n   *   <li>scores are non-negative and finite.\n   *   <li>score matches the explanation exactly.\n   *   <li>internal explanations calculations are sane (e.g. sum of: and so on actually compute sums)\n   *   <li>scores don't decrease as term frequencies increase: e.g. score(freq=N + 1) &gt;= score(freq=N)\n   *   <li>scores don't decrease as documents get shorter, e.g. score(len=M) &gt;= score(len=M+1)\n   *   <li>scores don't decrease as terms get rarer, e.g. score(term=N) &gt;= score(term=N+1)\n   *   <li>scoring works for floating point frequencies (e.g. sloppy phrase and span queries will work)\n   *   <li>scoring works for reasonably large 64-bit statistic values (e.g. distributed search will work)\n   *   <li>scoring works for reasonably large boost values (0 .. Integer.MAX_VALUE, e.g. query boosts will work)\n   *   <li>scoring works for parameters randomized within valid ranges (see {@link #getSimilarity(Random)})\n   * </ul>\n   */\n  public void testRandomScoring() throws Exception {\n    Random random = random();\n    final int iterations = atLeast(10);\n    for (int i = 0; i < iterations; i++) {\n      // pull a new similarity to switch up parameters\n      Similarity similarity = getSimilarity(random);\n      for (int j = 0; j < 10; j++) {\n        // for each norm value...\n        for (int k = 0; k < NORM_VALUES.size(); k++) {\n          CollectionStatistics corpus = newCorpus(random, k);\n          for (int l = 0; l < 10; l++) {\n            TermStatistics term = newTerm(random, corpus);\n            final float freq;\n            if (term.totalTermFreq() == term.docFreq()) {\n              // omit TF\n              freq = 1;\n            } else if (term.docFreq() == 1) {\n              // only one document, all the instances must be here.\n              freq = Math.toIntExact(term.totalTermFreq());\n            } else {\n              // there is at least one other document, and those must have at least 1 instance each.\n              int upperBound = Math.toIntExact(Math.min(term.totalTermFreq() - term.docFreq() + 1, Integer.MAX_VALUE));\n              if (random.nextBoolean()) {\n                // integer freq\n                switch (random.nextInt(3)) {\n                  case 0:\n                    // smallest freq\n                    freq = 1;\n                    break;\n                  case 1:\n                    // largest freq\n                    freq = upperBound;\n                    break;\n                  default:\n                    // random freq\n                    freq = TestUtil.nextInt(random, 1, upperBound);\n                    break;\n                }\n              } else {\n                // float freq\n                float freqCandidate;\n                switch (random.nextInt(2)) {\n                  case 0:\n                    // smallest freq\n                    freqCandidate = Float.MIN_VALUE;\n                    break;\n                  default:\n                    // random freq\n                    freqCandidate = upperBound * random.nextFloat();\n                    break;\n                }\n                // we need to be 2nd float value at a minimum, the pairwise test will check MIN_VALUE in this case.\n                // this avoids testing frequencies of 0 which seem wrong to allow (we should enforce computeSlopFactor etc)\n                if (freqCandidate <= Float.MIN_VALUE) {\n                  freqCandidate = Math.nextUp(Float.MIN_VALUE);\n                }\n                freq = freqCandidate;\n              }\n            }\n            // we just limit the test to \"reasonable\" boost values but don't enforce this anywhere.\n            // too big, and you are asking for overflow. that's hard for a sim to enforce (but definitely possible)\n            // for now, we just want to detect overflow where its a real bug/hazard in the computation with reasonable inputs.\n            final float boost;\n            switch (random.nextInt(5)) {\n              case 0:\n                // minimum value (not enforced)\n                boost = 0F;\n                break;\n              case 1:\n                // tiny value\n                boost = Float.MIN_VALUE;\n                break;\n              case 2:\n                // no-op value (sometimes treated special in explanations)\n                boost = 1F;\n                break;\n              case 3:\n                // maximum value (not enforceD)\n                boost = Integer.MAX_VALUE;\n                break;\n              default:\n                // random value\n                boost = random.nextFloat() * Integer.MAX_VALUE;\n                break;\n            }\n            doTestScoring(similarity, corpus, term, boost, freq, k);\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f","date":1579652839,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/search/similarities/BaseSimilarityTestCase#testRandomScoring().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/search/similarities/BaseSimilarityTestCase#testRandomScoring().mjava","sourceNew":"  /**\n   * Tests scoring across a bunch of random terms/corpora/frequencies for each possible document length.\n   * It does the following checks:\n   * <ul>\n   *   <li>scores are non-negative and finite.\n   *   <li>score matches the explanation exactly.\n   *   <li>internal explanations calculations are sane (e.g. sum of: and so on actually compute sums)\n   *   <li>scores don't decrease as term frequencies increase: e.g. score(freq=N + 1) &gt;= score(freq=N)\n   *   <li>scores don't decrease as documents get shorter, e.g. score(len=M) &gt;= score(len=M+1)\n   *   <li>scores don't decrease as terms get rarer, e.g. score(term=N) &gt;= score(term=N+1)\n   *   <li>scoring works for floating point frequencies (e.g. sloppy phrase and span queries will work)\n   *   <li>scoring works for reasonably large 64-bit statistic values (e.g. distributed search will work)\n   *   <li>scoring works for reasonably large boost values (0 .. Integer.MAX_VALUE, e.g. query boosts will work)\n   *   <li>scoring works for parameters randomized within valid ranges (see {@link #getSimilarity(Random)})\n   * </ul>\n   */\n  public void testRandomScoring() throws Exception {\n    Random random = random();\n    final int iterations = atLeast(3);\n    for (int i = 0; i < iterations; i++) {\n      // pull a new similarity to switch up parameters\n      Similarity similarity = getSimilarity(random);\n      for (int j = 0; j < 3; j++) {\n        // for each norm value...\n        for (int k = 1; k < 256; k++) {\n          CollectionStatistics corpus = newCorpus(random, k);\n          for (int l = 0; l < 10; l++) {\n            TermStatistics term = newTerm(random, corpus);\n            final float freq;\n            if (term.totalTermFreq() == term.docFreq()) {\n              // omit TF\n              freq = 1;\n            } else if (term.docFreq() == 1) {\n              // only one document, all the instances must be here.\n              freq = Math.toIntExact(term.totalTermFreq());\n            } else {\n              // there is at least one other document, and those must have at least 1 instance each.\n              int upperBound = Math.toIntExact(Math.min(term.totalTermFreq() - term.docFreq() + 1, Integer.MAX_VALUE));\n              if (random.nextBoolean()) {\n                // integer freq\n                switch (random.nextInt(3)) {\n                  case 0:\n                    // smallest freq\n                    freq = 1;\n                    break;\n                  case 1:\n                    // largest freq\n                    freq = upperBound;\n                    break;\n                  default:\n                    // random freq\n                    freq = TestUtil.nextInt(random, 1, upperBound);\n                    break;\n                }\n              } else {\n                // float freq\n                float freqCandidate;\n                switch (random.nextInt(2)) {\n                  case 0:\n                    // smallest freq\n                    freqCandidate = Float.MIN_VALUE;\n                    break;\n                  default:\n                    // random freq\n                    freqCandidate = upperBound * random.nextFloat();\n                    break;\n                }\n                // we need to be 2nd float value at a minimum, the pairwise test will check MIN_VALUE in this case.\n                // this avoids testing frequencies of 0 which seem wrong to allow (we should enforce computeSlopFactor etc)\n                if (freqCandidate <= Float.MIN_VALUE) {\n                  freqCandidate = Math.nextUp(Float.MIN_VALUE);\n                }\n                freq = freqCandidate;\n              }\n            }\n            // we just limit the test to \"reasonable\" boost values but don't enforce this anywhere.\n            // too big, and you are asking for overflow. that's hard for a sim to enforce (but definitely possible)\n            // for now, we just want to detect overflow where its a real bug/hazard in the computation with reasonable inputs.\n            final float boost;\n            switch (random.nextInt(5)) {\n              case 0:\n                // minimum value (not enforced)\n                boost = 0F;\n                break;\n              case 1:\n                // tiny value\n                boost = Float.MIN_VALUE;\n                break;\n              case 2:\n                // no-op value (sometimes treated special in explanations)\n                boost = 1F;\n                break;\n              case 3:\n                // maximum value (not enforceD)\n                boost = Integer.MAX_VALUE;\n                break;\n              default:\n                // random value\n                boost = random.nextFloat() * Integer.MAX_VALUE;\n                break;\n            }\n            doTestScoring(similarity, corpus, term, boost, freq, k);\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Tests scoring across a bunch of random terms/corpora/frequencies for each possible document length.\n   * It does the following checks:\n   * <ul>\n   *   <li>scores are non-negative and finite.\n   *   <li>score matches the explanation exactly.\n   *   <li>internal explanations calculations are sane (e.g. sum of: and so on actually compute sums)\n   *   <li>scores don't decrease as term frequencies increase: e.g. score(freq=N + 1) &gt;= score(freq=N)\n   *   <li>scores don't decrease as documents get shorter, e.g. score(len=M) &gt;= score(len=M+1)\n   *   <li>scores don't decrease as terms get rarer, e.g. score(term=N) &gt;= score(term=N+1)\n   *   <li>scoring works for floating point frequencies (e.g. sloppy phrase and span queries will work)\n   *   <li>scoring works for reasonably large 64-bit statistic values (e.g. distributed search will work)\n   *   <li>scoring works for reasonably large boost values (0 .. Integer.MAX_VALUE, e.g. query boosts will work)\n   *   <li>scoring works for parameters randomized within valid ranges (see {@link #getSimilarity(Random)})\n   * </ul>\n   */\n  public void testRandomScoring() throws Exception {\n    Random random = random();\n    final int iterations = atLeast(10);\n    for (int i = 0; i < iterations; i++) {\n      // pull a new similarity to switch up parameters\n      Similarity similarity = getSimilarity(random);\n      for (int j = 0; j < 10; j++) {\n        // for each norm value...\n        for (int k = 1; k < 256; k++) {\n          CollectionStatistics corpus = newCorpus(random, k);\n          for (int l = 0; l < 10; l++) {\n            TermStatistics term = newTerm(random, corpus);\n            final float freq;\n            if (term.totalTermFreq() == term.docFreq()) {\n              // omit TF\n              freq = 1;\n            } else if (term.docFreq() == 1) {\n              // only one document, all the instances must be here.\n              freq = Math.toIntExact(term.totalTermFreq());\n            } else {\n              // there is at least one other document, and those must have at least 1 instance each.\n              int upperBound = Math.toIntExact(Math.min(term.totalTermFreq() - term.docFreq() + 1, Integer.MAX_VALUE));\n              if (random.nextBoolean()) {\n                // integer freq\n                switch (random.nextInt(3)) {\n                  case 0:\n                    // smallest freq\n                    freq = 1;\n                    break;\n                  case 1:\n                    // largest freq\n                    freq = upperBound;\n                    break;\n                  default:\n                    // random freq\n                    freq = TestUtil.nextInt(random, 1, upperBound);\n                    break;\n                }\n              } else {\n                // float freq\n                float freqCandidate;\n                switch (random.nextInt(2)) {\n                  case 0:\n                    // smallest freq\n                    freqCandidate = Float.MIN_VALUE;\n                    break;\n                  default:\n                    // random freq\n                    freqCandidate = upperBound * random.nextFloat();\n                    break;\n                }\n                // we need to be 2nd float value at a minimum, the pairwise test will check MIN_VALUE in this case.\n                // this avoids testing frequencies of 0 which seem wrong to allow (we should enforce computeSlopFactor etc)\n                if (freqCandidate <= Float.MIN_VALUE) {\n                  freqCandidate = Math.nextUp(Float.MIN_VALUE);\n                }\n                freq = freqCandidate;\n              }\n            }\n            // we just limit the test to \"reasonable\" boost values but don't enforce this anywhere.\n            // too big, and you are asking for overflow. that's hard for a sim to enforce (but definitely possible)\n            // for now, we just want to detect overflow where its a real bug/hazard in the computation with reasonable inputs.\n            final float boost;\n            switch (random.nextInt(5)) {\n              case 0:\n                // minimum value (not enforced)\n                boost = 0F;\n                break;\n              case 1:\n                // tiny value\n                boost = Float.MIN_VALUE;\n                break;\n              case 2:\n                // no-op value (sometimes treated special in explanations)\n                boost = 1F;\n                break;\n              case 3:\n                // maximum value (not enforceD)\n                boost = Integer.MAX_VALUE;\n                break;\n              default:\n                // random value\n                boost = random.nextFloat() * Integer.MAX_VALUE;\n                break;\n            }\n            doTestScoring(similarity, corpus, term, boost, freq, k);\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"57c6c784f777a2cc8fa014507ea129526822714d","date":1579733373,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/search/similarities/BaseSimilarityTestCase#testRandomScoring().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/search/similarities/BaseSimilarityTestCase#testRandomScoring().mjava","sourceNew":"  /**\n   * Tests scoring across a bunch of random terms/corpora/frequencies for each possible document length.\n   * It does the following checks:\n   * <ul>\n   *   <li>scores are non-negative and finite.\n   *   <li>score matches the explanation exactly.\n   *   <li>internal explanations calculations are sane (e.g. sum of: and so on actually compute sums)\n   *   <li>scores don't decrease as term frequencies increase: e.g. score(freq=N + 1) &gt;= score(freq=N)\n   *   <li>scores don't decrease as documents get shorter, e.g. score(len=M) &gt;= score(len=M+1)\n   *   <li>scores don't decrease as terms get rarer, e.g. score(term=N) &gt;= score(term=N+1)\n   *   <li>scoring works for floating point frequencies (e.g. sloppy phrase and span queries will work)\n   *   <li>scoring works for reasonably large 64-bit statistic values (e.g. distributed search will work)\n   *   <li>scoring works for reasonably large boost values (0 .. Integer.MAX_VALUE, e.g. query boosts will work)\n   *   <li>scoring works for parameters randomized within valid ranges (see {@link #getSimilarity(Random)})\n   * </ul>\n   */\n  public void testRandomScoring() throws Exception {\n    Random random = random();\n    final int iterations = atLeast(1);\n    for (int i = 0; i < iterations; i++) {\n      // pull a new similarity to switch up parameters\n      Similarity similarity = getSimilarity(random);\n      for (int j = 0; j < 3; j++) {\n        // for each norm value...\n        for (int k = 1; k < 256; k++) {\n          CollectionStatistics corpus = newCorpus(random, k);\n          for (int l = 0; l < 10; l++) {\n            TermStatistics term = newTerm(random, corpus);\n            final float freq;\n            if (term.totalTermFreq() == term.docFreq()) {\n              // omit TF\n              freq = 1;\n            } else if (term.docFreq() == 1) {\n              // only one document, all the instances must be here.\n              freq = Math.toIntExact(term.totalTermFreq());\n            } else {\n              // there is at least one other document, and those must have at least 1 instance each.\n              int upperBound = Math.toIntExact(Math.min(term.totalTermFreq() - term.docFreq() + 1, Integer.MAX_VALUE));\n              if (random.nextBoolean()) {\n                // integer freq\n                switch (random.nextInt(3)) {\n                  case 0:\n                    // smallest freq\n                    freq = 1;\n                    break;\n                  case 1:\n                    // largest freq\n                    freq = upperBound;\n                    break;\n                  default:\n                    // random freq\n                    freq = TestUtil.nextInt(random, 1, upperBound);\n                    break;\n                }\n              } else {\n                // float freq\n                float freqCandidate;\n                switch (random.nextInt(2)) {\n                  case 0:\n                    // smallest freq\n                    freqCandidate = Float.MIN_VALUE;\n                    break;\n                  default:\n                    // random freq\n                    freqCandidate = upperBound * random.nextFloat();\n                    break;\n                }\n                // we need to be 2nd float value at a minimum, the pairwise test will check MIN_VALUE in this case.\n                // this avoids testing frequencies of 0 which seem wrong to allow (we should enforce computeSlopFactor etc)\n                if (freqCandidate <= Float.MIN_VALUE) {\n                  freqCandidate = Math.nextUp(Float.MIN_VALUE);\n                }\n                freq = freqCandidate;\n              }\n            }\n            // we just limit the test to \"reasonable\" boost values but don't enforce this anywhere.\n            // too big, and you are asking for overflow. that's hard for a sim to enforce (but definitely possible)\n            // for now, we just want to detect overflow where its a real bug/hazard in the computation with reasonable inputs.\n            final float boost;\n            switch (random.nextInt(5)) {\n              case 0:\n                // minimum value (not enforced)\n                boost = 0F;\n                break;\n              case 1:\n                // tiny value\n                boost = Float.MIN_VALUE;\n                break;\n              case 2:\n                // no-op value (sometimes treated special in explanations)\n                boost = 1F;\n                break;\n              case 3:\n                // maximum value (not enforceD)\n                boost = Integer.MAX_VALUE;\n                break;\n              default:\n                // random value\n                boost = random.nextFloat() * Integer.MAX_VALUE;\n                break;\n            }\n            doTestScoring(similarity, corpus, term, boost, freq, k);\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Tests scoring across a bunch of random terms/corpora/frequencies for each possible document length.\n   * It does the following checks:\n   * <ul>\n   *   <li>scores are non-negative and finite.\n   *   <li>score matches the explanation exactly.\n   *   <li>internal explanations calculations are sane (e.g. sum of: and so on actually compute sums)\n   *   <li>scores don't decrease as term frequencies increase: e.g. score(freq=N + 1) &gt;= score(freq=N)\n   *   <li>scores don't decrease as documents get shorter, e.g. score(len=M) &gt;= score(len=M+1)\n   *   <li>scores don't decrease as terms get rarer, e.g. score(term=N) &gt;= score(term=N+1)\n   *   <li>scoring works for floating point frequencies (e.g. sloppy phrase and span queries will work)\n   *   <li>scoring works for reasonably large 64-bit statistic values (e.g. distributed search will work)\n   *   <li>scoring works for reasonably large boost values (0 .. Integer.MAX_VALUE, e.g. query boosts will work)\n   *   <li>scoring works for parameters randomized within valid ranges (see {@link #getSimilarity(Random)})\n   * </ul>\n   */\n  public void testRandomScoring() throws Exception {\n    Random random = random();\n    final int iterations = atLeast(3);\n    for (int i = 0; i < iterations; i++) {\n      // pull a new similarity to switch up parameters\n      Similarity similarity = getSimilarity(random);\n      for (int j = 0; j < 3; j++) {\n        // for each norm value...\n        for (int k = 1; k < 256; k++) {\n          CollectionStatistics corpus = newCorpus(random, k);\n          for (int l = 0; l < 10; l++) {\n            TermStatistics term = newTerm(random, corpus);\n            final float freq;\n            if (term.totalTermFreq() == term.docFreq()) {\n              // omit TF\n              freq = 1;\n            } else if (term.docFreq() == 1) {\n              // only one document, all the instances must be here.\n              freq = Math.toIntExact(term.totalTermFreq());\n            } else {\n              // there is at least one other document, and those must have at least 1 instance each.\n              int upperBound = Math.toIntExact(Math.min(term.totalTermFreq() - term.docFreq() + 1, Integer.MAX_VALUE));\n              if (random.nextBoolean()) {\n                // integer freq\n                switch (random.nextInt(3)) {\n                  case 0:\n                    // smallest freq\n                    freq = 1;\n                    break;\n                  case 1:\n                    // largest freq\n                    freq = upperBound;\n                    break;\n                  default:\n                    // random freq\n                    freq = TestUtil.nextInt(random, 1, upperBound);\n                    break;\n                }\n              } else {\n                // float freq\n                float freqCandidate;\n                switch (random.nextInt(2)) {\n                  case 0:\n                    // smallest freq\n                    freqCandidate = Float.MIN_VALUE;\n                    break;\n                  default:\n                    // random freq\n                    freqCandidate = upperBound * random.nextFloat();\n                    break;\n                }\n                // we need to be 2nd float value at a minimum, the pairwise test will check MIN_VALUE in this case.\n                // this avoids testing frequencies of 0 which seem wrong to allow (we should enforce computeSlopFactor etc)\n                if (freqCandidate <= Float.MIN_VALUE) {\n                  freqCandidate = Math.nextUp(Float.MIN_VALUE);\n                }\n                freq = freqCandidate;\n              }\n            }\n            // we just limit the test to \"reasonable\" boost values but don't enforce this anywhere.\n            // too big, and you are asking for overflow. that's hard for a sim to enforce (but definitely possible)\n            // for now, we just want to detect overflow where its a real bug/hazard in the computation with reasonable inputs.\n            final float boost;\n            switch (random.nextInt(5)) {\n              case 0:\n                // minimum value (not enforced)\n                boost = 0F;\n                break;\n              case 1:\n                // tiny value\n                boost = Float.MIN_VALUE;\n                break;\n              case 2:\n                // no-op value (sometimes treated special in explanations)\n                boost = 1F;\n                break;\n              case 3:\n                // maximum value (not enforceD)\n                boost = Integer.MAX_VALUE;\n                break;\n              default:\n                // random value\n                boost = random.nextFloat() * Integer.MAX_VALUE;\n                break;\n            }\n            doTestScoring(similarity, corpus, term, boost, freq, k);\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"57c6c784f777a2cc8fa014507ea129526822714d":["fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f"],"fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f":["b94236357aaa22b76c10629851fe4e376e0cea82"],"b94236357aaa22b76c10629851fe4e376e0cea82":["83d379038462cf6dcf64cc9e9a49053c4bb78011","68d6cb7f0f019661a784bd0e5a21e85b5f812af6"],"68d6cb7f0f019661a784bd0e5a21e85b5f812af6":["83d379038462cf6dcf64cc9e9a49053c4bb78011"],"83d379038462cf6dcf64cc9e9a49053c4bb78011":["d523b8189b211dd1630166aa77b8c88bb48b3fcc"],"ad1dc49b5314cfdb82a7ea40d2f92f07fe8cee46":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"086ffe31d8fba0110227db122974163709ecc1b4":["ad1dc49b5314cfdb82a7ea40d2f92f07fe8cee46"],"d523b8189b211dd1630166aa77b8c88bb48b3fcc":["ad1dc49b5314cfdb82a7ea40d2f92f07fe8cee46","086ffe31d8fba0110227db122974163709ecc1b4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["57c6c784f777a2cc8fa014507ea129526822714d"]},"commit2Childs":{"57c6c784f777a2cc8fa014507ea129526822714d":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f":["57c6c784f777a2cc8fa014507ea129526822714d"],"b94236357aaa22b76c10629851fe4e376e0cea82":["fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f"],"68d6cb7f0f019661a784bd0e5a21e85b5f812af6":["b94236357aaa22b76c10629851fe4e376e0cea82"],"83d379038462cf6dcf64cc9e9a49053c4bb78011":["b94236357aaa22b76c10629851fe4e376e0cea82","68d6cb7f0f019661a784bd0e5a21e85b5f812af6"],"ad1dc49b5314cfdb82a7ea40d2f92f07fe8cee46":["086ffe31d8fba0110227db122974163709ecc1b4","d523b8189b211dd1630166aa77b8c88bb48b3fcc"],"086ffe31d8fba0110227db122974163709ecc1b4":["d523b8189b211dd1630166aa77b8c88bb48b3fcc"],"d523b8189b211dd1630166aa77b8c88bb48b3fcc":["83d379038462cf6dcf64cc9e9a49053c4bb78011"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["ad1dc49b5314cfdb82a7ea40d2f92f07fe8cee46"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}