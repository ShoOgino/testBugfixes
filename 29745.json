{"path":"solr/core/src/test/org/apache/solr/cloud/CdcrReplicationHandlerTest#testReplicationWithBufferedUpdates().mjava","commits":[{"id":"6776c9bdacef00ce712b87d1c8e999ae61c1c6a1","date":1448389841,"type":0,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/CdcrReplicationHandlerTest#testReplicationWithBufferedUpdates().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Test the scenario where the slave is killed while the leader is still receiving updates.\n   * The slave should buffer updates while in recovery, then replay them at the end of the recovery.\n   * If updates were properly buffered and replayed, then the slave should have the same number of documents\n   * than the leader. This checks if cdcr tlog replication interferes with buffered updates - SOLR-8263.\n   */\n  @Test\n  @ShardsFixed(num = 2)\n  public void testReplicationWithBufferedUpdates() throws Exception {\n    List<CloudJettyRunner> slaves = this.getShardToSlaveJetty(SOURCE_COLLECTION, SHARD1);\n\n    AtomicInteger numDocs = new AtomicInteger(0);\n    ScheduledExecutorService executor = Executors.newSingleThreadScheduledExecutor(new DefaultSolrThreadFactory(\"cdcr-test-update-scheduler\"));\n    executor.scheduleWithFixedDelay(new UpdateThread(numDocs), 10, 10, TimeUnit.MILLISECONDS);\n\n    // Restart the slave node to trigger Replication strategy\n    this.restartServer(slaves.get(0));\n\n    // shutdown the update thread and wait for its completion\n    executor.shutdown();\n    executor.awaitTermination(500, TimeUnit.MILLISECONDS);\n\n    // check that we have the expected number of documents in the cluster\n    assertNumDocs(numDocs.get(), SOURCE_COLLECTION);\n\n    // check that we have the expected number of documents on the slave\n    assertNumDocs(numDocs.get(), slaves.get(0));\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cfa6bab72dc1ef7209657e6685f9204e2e49bac8","date":1448391014,"type":4,"author":"Erick Erickson","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/test/org/apache/solr/cloud/CdcrReplicationHandlerTest#testReplicationWithBufferedUpdates().mjava","sourceNew":null,"sourceOld":"  /**\n   * Test the scenario where the slave is killed while the leader is still receiving updates.\n   * The slave should buffer updates while in recovery, then replay them at the end of the recovery.\n   * If updates were properly buffered and replayed, then the slave should have the same number of documents\n   * than the leader. This checks if cdcr tlog replication interferes with buffered updates - SOLR-8263.\n   */\n  @Test\n  @ShardsFixed(num = 2)\n  public void testReplicationWithBufferedUpdates() throws Exception {\n    List<CloudJettyRunner> slaves = this.getShardToSlaveJetty(SOURCE_COLLECTION, SHARD1);\n\n    AtomicInteger numDocs = new AtomicInteger(0);\n    ScheduledExecutorService executor = Executors.newSingleThreadScheduledExecutor(new DefaultSolrThreadFactory(\"cdcr-test-update-scheduler\"));\n    executor.scheduleWithFixedDelay(new UpdateThread(numDocs), 10, 10, TimeUnit.MILLISECONDS);\n\n    // Restart the slave node to trigger Replication strategy\n    this.restartServer(slaves.get(0));\n\n    // shutdown the update thread and wait for its completion\n    executor.shutdown();\n    executor.awaitTermination(500, TimeUnit.MILLISECONDS);\n\n    // check that we have the expected number of documents in the cluster\n    assertNumDocs(numDocs.get(), SOURCE_COLLECTION);\n\n    // check that we have the expected number of documents on the slave\n    assertNumDocs(numDocs.get(), slaves.get(0));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"77cae179618908dcb534af567cdf3019505ada6c","date":1449365361,"type":0,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/CdcrReplicationHandlerTest#testReplicationWithBufferedUpdates().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Test the scenario where the slave is killed while the leader is still receiving updates.\n   * The slave should buffer updates while in recovery, then replay them at the end of the recovery.\n   * If updates were properly buffered and replayed, then the slave should have the same number of documents\n   * than the leader. This checks if cdcr tlog replication interferes with buffered updates - SOLR-8263.\n   */\n  @Test\n  @ShardsFixed(num = 2)\n  public void testReplicationWithBufferedUpdates() throws Exception {\n    List<CloudJettyRunner> slaves = this.getShardToSlaveJetty(SOURCE_COLLECTION, SHARD1);\n\n    AtomicInteger numDocs = new AtomicInteger(0);\n    ScheduledExecutorService executor = Executors.newSingleThreadScheduledExecutor(new DefaultSolrThreadFactory(\"cdcr-test-update-scheduler\"));\n    executor.scheduleWithFixedDelay(new UpdateThread(numDocs), 10, 10, TimeUnit.MILLISECONDS);\n\n    // Restart the slave node to trigger Replication strategy\n    this.restartServer(slaves.get(0));\n\n    // shutdown the update thread and wait for its completion\n    executor.shutdown();\n    executor.awaitTermination(500, TimeUnit.MILLISECONDS);\n\n    // check that we have the expected number of documents in the cluster\n    assertNumDocs(numDocs.get(), SOURCE_COLLECTION);\n\n    // check that we have the expected number of documents on the slave\n    assertNumDocs(numDocs.get(), slaves.get(0));\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7da9d8536c11576df10e348efa79d2739170936","date":1509475399,"type":5,"author":"Varun Thacker","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/cdcr/CdcrReplicationHandlerTest#testReplicationWithBufferedUpdates().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/CdcrReplicationHandlerTest#testReplicationWithBufferedUpdates().mjava","sourceNew":"  /**\n   * Test the scenario where the slave is killed while the leader is still receiving updates.\n   * The slave should buffer updates while in recovery, then replay them at the end of the recovery.\n   * If updates were properly buffered and replayed, then the slave should have the same number of documents\n   * than the leader. This checks if cdcr tlog replication interferes with buffered updates - SOLR-8263.\n   */\n  @Test\n  @ShardsFixed(num = 2)\n  public void testReplicationWithBufferedUpdates() throws Exception {\n    List<CloudJettyRunner> slaves = this.getShardToSlaveJetty(SOURCE_COLLECTION, SHARD1);\n\n    AtomicInteger numDocs = new AtomicInteger(0);\n    ScheduledExecutorService executor = Executors.newSingleThreadScheduledExecutor(new DefaultSolrThreadFactory(\"cdcr-test-update-scheduler\"));\n    executor.scheduleWithFixedDelay(new UpdateThread(numDocs), 10, 10, TimeUnit.MILLISECONDS);\n\n    // Restart the slave node to trigger Replication strategy\n    this.restartServer(slaves.get(0));\n\n    // shutdown the update thread and wait for its completion\n    executor.shutdown();\n    executor.awaitTermination(500, TimeUnit.MILLISECONDS);\n\n    // check that we have the expected number of documents in the cluster\n    assertNumDocs(numDocs.get(), SOURCE_COLLECTION);\n\n    // check that we have the expected number of documents on the slave\n    assertNumDocs(numDocs.get(), slaves.get(0));\n  }\n\n","sourceOld":"  /**\n   * Test the scenario where the slave is killed while the leader is still receiving updates.\n   * The slave should buffer updates while in recovery, then replay them at the end of the recovery.\n   * If updates were properly buffered and replayed, then the slave should have the same number of documents\n   * than the leader. This checks if cdcr tlog replication interferes with buffered updates - SOLR-8263.\n   */\n  @Test\n  @ShardsFixed(num = 2)\n  public void testReplicationWithBufferedUpdates() throws Exception {\n    List<CloudJettyRunner> slaves = this.getShardToSlaveJetty(SOURCE_COLLECTION, SHARD1);\n\n    AtomicInteger numDocs = new AtomicInteger(0);\n    ScheduledExecutorService executor = Executors.newSingleThreadScheduledExecutor(new DefaultSolrThreadFactory(\"cdcr-test-update-scheduler\"));\n    executor.scheduleWithFixedDelay(new UpdateThread(numDocs), 10, 10, TimeUnit.MILLISECONDS);\n\n    // Restart the slave node to trigger Replication strategy\n    this.restartServer(slaves.get(0));\n\n    // shutdown the update thread and wait for its completion\n    executor.shutdown();\n    executor.awaitTermination(500, TimeUnit.MILLISECONDS);\n\n    // check that we have the expected number of documents in the cluster\n    assertNumDocs(numDocs.get(), SOURCE_COLLECTION);\n\n    // check that we have the expected number of documents on the slave\n    assertNumDocs(numDocs.get(), slaves.get(0));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1a885a9bb91342e3cf8bfcf53610ab8d6307ef3d","date":1509475828,"type":1,"author":"Varun Thacker","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/CdcrReplicationHandlerTest#testReplicationWithBufferedUpdates().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/cdcr/CdcrReplicationHandlerTest#testReplicationWithBufferedUpdates().mjava","sourceNew":"  /**\n   * Test the scenario where the slave is killed while the leader is still receiving updates.\n   * The slave should buffer updates while in recovery, then replay them at the end of the recovery.\n   * If updates were properly buffered and replayed, then the slave should have the same number of documents\n   * than the leader. This checks if cdcr tlog replication interferes with buffered updates - SOLR-8263.\n   */\n  @Test\n  @ShardsFixed(num = 2)\n  public void testReplicationWithBufferedUpdates() throws Exception {\n    List<CloudJettyRunner> slaves = this.getShardToSlaveJetty(SOURCE_COLLECTION, SHARD1);\n\n    AtomicInteger numDocs = new AtomicInteger(0);\n    ScheduledExecutorService executor = Executors.newSingleThreadScheduledExecutor(new DefaultSolrThreadFactory(\"cdcr-test-update-scheduler\"));\n    executor.scheduleWithFixedDelay(new UpdateThread(numDocs), 10, 10, TimeUnit.MILLISECONDS);\n\n    // Restart the slave node to trigger Replication strategy\n    this.restartServer(slaves.get(0));\n\n    // shutdown the update thread and wait for its completion\n    executor.shutdown();\n    executor.awaitTermination(500, TimeUnit.MILLISECONDS);\n\n    // check that we have the expected number of documents in the cluster\n    assertNumDocs(numDocs.get(), SOURCE_COLLECTION);\n\n    // check that we have the expected number of documents on the slave\n    assertNumDocs(numDocs.get(), slaves.get(0));\n  }\n\n","sourceOld":"  /**\n   * Test the scenario where the slave is killed while the leader is still receiving updates.\n   * The slave should buffer updates while in recovery, then replay them at the end of the recovery.\n   * If updates were properly buffered and replayed, then the slave should have the same number of documents\n   * than the leader. This checks if cdcr tlog replication interferes with buffered updates - SOLR-8263.\n   */\n  @Test\n  @ShardsFixed(num = 2)\n  public void testReplicationWithBufferedUpdates() throws Exception {\n    List<CloudJettyRunner> slaves = this.getShardToSlaveJetty(SOURCE_COLLECTION, SHARD1);\n\n    AtomicInteger numDocs = new AtomicInteger(0);\n    ScheduledExecutorService executor = Executors.newSingleThreadScheduledExecutor(new DefaultSolrThreadFactory(\"cdcr-test-update-scheduler\"));\n    executor.scheduleWithFixedDelay(new UpdateThread(numDocs), 10, 10, TimeUnit.MILLISECONDS);\n\n    // Restart the slave node to trigger Replication strategy\n    this.restartServer(slaves.get(0));\n\n    // shutdown the update thread and wait for its completion\n    executor.shutdown();\n    executor.awaitTermination(500, TimeUnit.MILLISECONDS);\n\n    // check that we have the expected number of documents in the cluster\n    assertNumDocs(numDocs.get(), SOURCE_COLLECTION);\n\n    // check that we have the expected number of documents on the slave\n    assertNumDocs(numDocs.get(), slaves.get(0));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fba58147c9753bb58bbfbd2ebbbf02aecc1eafde","date":1509748659,"type":5,"author":"Varun Thacker","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/cdcr/CdcrReplicationHandlerTest#testReplicationWithBufferedUpdates().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/CdcrReplicationHandlerTest#testReplicationWithBufferedUpdates().mjava","sourceNew":"  /**\n   * Test the scenario where the slave is killed while the leader is still receiving updates.\n   * The slave should buffer updates while in recovery, then replay them at the end of the recovery.\n   * If updates were properly buffered and replayed, then the slave should have the same number of documents\n   * than the leader. This checks if cdcr tlog replication interferes with buffered updates - SOLR-8263.\n   */\n  @Test\n  @ShardsFixed(num = 2)\n  public void testReplicationWithBufferedUpdates() throws Exception {\n    List<CloudJettyRunner> slaves = this.getShardToSlaveJetty(SOURCE_COLLECTION, SHARD1);\n\n    AtomicInteger numDocs = new AtomicInteger(0);\n    ScheduledExecutorService executor = Executors.newSingleThreadScheduledExecutor(new DefaultSolrThreadFactory(\"cdcr-test-update-scheduler\"));\n    executor.scheduleWithFixedDelay(new UpdateThread(numDocs), 10, 10, TimeUnit.MILLISECONDS);\n\n    // Restart the slave node to trigger Replication strategy\n    this.restartServer(slaves.get(0));\n\n    // shutdown the update thread and wait for its completion\n    executor.shutdown();\n    executor.awaitTermination(500, TimeUnit.MILLISECONDS);\n\n    // check that we have the expected number of documents in the cluster\n    assertNumDocs(numDocs.get(), SOURCE_COLLECTION);\n\n    // check that we have the expected number of documents on the slave\n    assertNumDocs(numDocs.get(), slaves.get(0));\n  }\n\n","sourceOld":"  /**\n   * Test the scenario where the slave is killed while the leader is still receiving updates.\n   * The slave should buffer updates while in recovery, then replay them at the end of the recovery.\n   * If updates were properly buffered and replayed, then the slave should have the same number of documents\n   * than the leader. This checks if cdcr tlog replication interferes with buffered updates - SOLR-8263.\n   */\n  @Test\n  @ShardsFixed(num = 2)\n  public void testReplicationWithBufferedUpdates() throws Exception {\n    List<CloudJettyRunner> slaves = this.getShardToSlaveJetty(SOURCE_COLLECTION, SHARD1);\n\n    AtomicInteger numDocs = new AtomicInteger(0);\n    ScheduledExecutorService executor = Executors.newSingleThreadScheduledExecutor(new DefaultSolrThreadFactory(\"cdcr-test-update-scheduler\"));\n    executor.scheduleWithFixedDelay(new UpdateThread(numDocs), 10, 10, TimeUnit.MILLISECONDS);\n\n    // Restart the slave node to trigger Replication strategy\n    this.restartServer(slaves.get(0));\n\n    // shutdown the update thread and wait for its completion\n    executor.shutdown();\n    executor.awaitTermination(500, TimeUnit.MILLISECONDS);\n\n    // check that we have the expected number of documents in the cluster\n    assertNumDocs(numDocs.get(), SOURCE_COLLECTION);\n\n    // check that we have the expected number of documents on the slave\n    assertNumDocs(numDocs.get(), slaves.get(0));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d523b8189b211dd1630166aa77b8c88bb48b3fcc","date":1510144168,"type":5,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/cdcr/CdcrReplicationHandlerTest#testReplicationWithBufferedUpdates().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/CdcrReplicationHandlerTest#testReplicationWithBufferedUpdates().mjava","sourceNew":"  /**\n   * Test the scenario where the slave is killed while the leader is still receiving updates.\n   * The slave should buffer updates while in recovery, then replay them at the end of the recovery.\n   * If updates were properly buffered and replayed, then the slave should have the same number of documents\n   * than the leader. This checks if cdcr tlog replication interferes with buffered updates - SOLR-8263.\n   */\n  @Test\n  @ShardsFixed(num = 2)\n  public void testReplicationWithBufferedUpdates() throws Exception {\n    List<CloudJettyRunner> slaves = this.getShardToSlaveJetty(SOURCE_COLLECTION, SHARD1);\n\n    AtomicInteger numDocs = new AtomicInteger(0);\n    ScheduledExecutorService executor = Executors.newSingleThreadScheduledExecutor(new DefaultSolrThreadFactory(\"cdcr-test-update-scheduler\"));\n    executor.scheduleWithFixedDelay(new UpdateThread(numDocs), 10, 10, TimeUnit.MILLISECONDS);\n\n    // Restart the slave node to trigger Replication strategy\n    this.restartServer(slaves.get(0));\n\n    // shutdown the update thread and wait for its completion\n    executor.shutdown();\n    executor.awaitTermination(500, TimeUnit.MILLISECONDS);\n\n    // check that we have the expected number of documents in the cluster\n    assertNumDocs(numDocs.get(), SOURCE_COLLECTION);\n\n    // check that we have the expected number of documents on the slave\n    assertNumDocs(numDocs.get(), slaves.get(0));\n  }\n\n","sourceOld":"  /**\n   * Test the scenario where the slave is killed while the leader is still receiving updates.\n   * The slave should buffer updates while in recovery, then replay them at the end of the recovery.\n   * If updates were properly buffered and replayed, then the slave should have the same number of documents\n   * than the leader. This checks if cdcr tlog replication interferes with buffered updates - SOLR-8263.\n   */\n  @Test\n  @ShardsFixed(num = 2)\n  public void testReplicationWithBufferedUpdates() throws Exception {\n    List<CloudJettyRunner> slaves = this.getShardToSlaveJetty(SOURCE_COLLECTION, SHARD1);\n\n    AtomicInteger numDocs = new AtomicInteger(0);\n    ScheduledExecutorService executor = Executors.newSingleThreadScheduledExecutor(new DefaultSolrThreadFactory(\"cdcr-test-update-scheduler\"));\n    executor.scheduleWithFixedDelay(new UpdateThread(numDocs), 10, 10, TimeUnit.MILLISECONDS);\n\n    // Restart the slave node to trigger Replication strategy\n    this.restartServer(slaves.get(0));\n\n    // shutdown the update thread and wait for its completion\n    executor.shutdown();\n    executor.awaitTermination(500, TimeUnit.MILLISECONDS);\n\n    // check that we have the expected number of documents in the cluster\n    assertNumDocs(numDocs.get(), SOURCE_COLLECTION);\n\n    // check that we have the expected number of documents on the slave\n    assertNumDocs(numDocs.get(), slaves.get(0));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"77cae179618908dcb534af567cdf3019505ada6c":["cfa6bab72dc1ef7209657e6685f9204e2e49bac8"],"fba58147c9753bb58bbfbd2ebbbf02aecc1eafde":["1a885a9bb91342e3cf8bfcf53610ab8d6307ef3d"],"e7da9d8536c11576df10e348efa79d2739170936":["77cae179618908dcb534af567cdf3019505ada6c"],"d523b8189b211dd1630166aa77b8c88bb48b3fcc":["1a885a9bb91342e3cf8bfcf53610ab8d6307ef3d","fba58147c9753bb58bbfbd2ebbbf02aecc1eafde"],"cfa6bab72dc1ef7209657e6685f9204e2e49bac8":["6776c9bdacef00ce712b87d1c8e999ae61c1c6a1"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"6776c9bdacef00ce712b87d1c8e999ae61c1c6a1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"1a885a9bb91342e3cf8bfcf53610ab8d6307ef3d":["e7da9d8536c11576df10e348efa79d2739170936"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d523b8189b211dd1630166aa77b8c88bb48b3fcc"]},"commit2Childs":{"77cae179618908dcb534af567cdf3019505ada6c":["e7da9d8536c11576df10e348efa79d2739170936"],"fba58147c9753bb58bbfbd2ebbbf02aecc1eafde":["d523b8189b211dd1630166aa77b8c88bb48b3fcc"],"e7da9d8536c11576df10e348efa79d2739170936":["1a885a9bb91342e3cf8bfcf53610ab8d6307ef3d"],"d523b8189b211dd1630166aa77b8c88bb48b3fcc":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cfa6bab72dc1ef7209657e6685f9204e2e49bac8":["77cae179618908dcb534af567cdf3019505ada6c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["6776c9bdacef00ce712b87d1c8e999ae61c1c6a1"],"6776c9bdacef00ce712b87d1c8e999ae61c1c6a1":["cfa6bab72dc1ef7209657e6685f9204e2e49bac8"],"1a885a9bb91342e3cf8bfcf53610ab8d6307ef3d":["fba58147c9753bb58bbfbd2ebbbf02aecc1eafde","d523b8189b211dd1630166aa77b8c88bb48b3fcc"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}