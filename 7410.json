{"path":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","commits":[{"id":"038e2a9b07e2f8ae58336613cea227bf8b973484","date":1346850972,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","sourceNew":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + blockTermCount);\n        if (blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes, 0, len);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes, 0, len);\n        metaDataUpto = 0;\n\n        state.termBlockOrd = 0;\n\n        postingsReader.readTermsBlock(in, fieldInfo, state);\n\n        blocksSinceSeek++;\n        indexIsCurrent = indexIsCurrent && (blocksSinceSeek < indexReader.getDivisor());\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","sourceOld":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + blockTermCount);\n        if (blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes, 0, len);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes, 0, len);\n        metaDataUpto = 0;\n\n        state.termBlockOrd = 0;\n\n        postingsReader.readTermsBlock(in, fieldInfo, state);\n\n        blocksSinceSeek++;\n        indexIsCurrent = indexIsCurrent && (blocksSinceSeek < indexReader.getDivisor());\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a45bec74b98f6fc05f52770cfb425739e6563960","date":1375119292,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","sourceNew":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + blockTermCount);\n        if (blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes, 0, len);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes, 0, len);\n        metaDataUpto = 0;\n\n        state.termBlockOrd = 0;\n\n        postingsReader.readTermsBlock(in, fieldInfo, state);\n\n        indexIsCurrent = false;\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","sourceOld":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + blockTermCount);\n        if (blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes, 0, len);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes, 0, len);\n        metaDataUpto = 0;\n\n        state.termBlockOrd = 0;\n\n        postingsReader.readTermsBlock(in, fieldInfo, state);\n\n        blocksSinceSeek++;\n        indexIsCurrent = indexIsCurrent && (blocksSinceSeek < indexReader.getDivisor());\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","date":1376366778,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","sourceNew":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + blockTermCount);\n        if (blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes, 0, len);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes, 0, len);\n        metaDataUpto = 0;\n\n        state.termBlockOrd = 0;\n\n        postingsReader.readTermsBlock(in, fieldInfo, state);\n\n        indexIsCurrent = false;\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","sourceOld":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + blockTermCount);\n        if (blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes, 0, len);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes, 0, len);\n        metaDataUpto = 0;\n\n        state.termBlockOrd = 0;\n\n        postingsReader.readTermsBlock(in, fieldInfo, state);\n\n        blocksSinceSeek++;\n        indexIsCurrent = indexIsCurrent && (blocksSinceSeek < indexReader.getDivisor());\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1a88d7b0899b7d22dcbd4cf8ca35d9ec9850ab98","date":1377268487,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","sourceNew":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + blockTermCount);\n        if (blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes, 0, len);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes, 0, len);\n\n        // metadata\n        len = in.readVInt();\n        if (bytes == null) {\n          bytes = new byte[ArrayUtil.oversize(len, 1)];\n          bytesReader = new ByteArrayDataInput();\n        } else if (bytes.length < len) {\n          bytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        in.readBytes(bytes, 0, len);\n        bytesReader.reset(bytes, 0, len);\n\n        metaDataUpto = 0;\n        state.termBlockOrd = 0;\n\n        indexIsCurrent = false;\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","sourceOld":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + blockTermCount);\n        if (blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes, 0, len);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes, 0, len);\n        metaDataUpto = 0;\n\n        state.termBlockOrd = 0;\n\n        postingsReader.readTermsBlock(in, fieldInfo, state);\n\n        indexIsCurrent = false;\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2f948dd442d23baa6cbb28daf77c8db78b351329","date":1378742876,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","sourceNew":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + blockTermCount);\n        if (blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes, 0, len);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes, 0, len);\n\n        // metadata\n        len = in.readVInt();\n        if (bytes == null) {\n          bytes = new byte[ArrayUtil.oversize(len, 1)];\n          bytesReader = new ByteArrayDataInput();\n        } else if (bytes.length < len) {\n          bytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        in.readBytes(bytes, 0, len);\n        bytesReader.reset(bytes, 0, len);\n\n        metaDataUpto = 0;\n        state.termBlockOrd = 0;\n\n        indexIsCurrent = false;\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","sourceOld":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + blockTermCount);\n        if (blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes, 0, len);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes, 0, len);\n        metaDataUpto = 0;\n\n        state.termBlockOrd = 0;\n\n        postingsReader.readTermsBlock(in, fieldInfo, state);\n\n        indexIsCurrent = false;\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"1a88d7b0899b7d22dcbd4cf8ca35d9ec9850ab98":["8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee"],"a45bec74b98f6fc05f52770cfb425739e6563960":["038e2a9b07e2f8ae58336613cea227bf8b973484"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["038e2a9b07e2f8ae58336613cea227bf8b973484"],"2f948dd442d23baa6cbb28daf77c8db78b351329":["a45bec74b98f6fc05f52770cfb425739e6563960","1a88d7b0899b7d22dcbd4cf8ca35d9ec9850ab98"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["2f948dd442d23baa6cbb28daf77c8db78b351329"],"038e2a9b07e2f8ae58336613cea227bf8b973484":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"1a88d7b0899b7d22dcbd4cf8ca35d9ec9850ab98":["2f948dd442d23baa6cbb28daf77c8db78b351329"],"a45bec74b98f6fc05f52770cfb425739e6563960":["2f948dd442d23baa6cbb28daf77c8db78b351329"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["038e2a9b07e2f8ae58336613cea227bf8b973484"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["1a88d7b0899b7d22dcbd4cf8ca35d9ec9850ab98"],"2f948dd442d23baa6cbb28daf77c8db78b351329":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"038e2a9b07e2f8ae58336613cea227bf8b973484":["a45bec74b98f6fc05f52770cfb425739e6563960","8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}