{"path":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","commits":[{"id":"73bb5a57dc75b54a39494f99986599cae7dff417","date":1361040620,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"/dev/null","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    FieldCache.Longs longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = FieldCache.DEFAULT.getLongs(ctx.reader(), fieldName, true);\n            break;\n          case INT:\n            final FieldCache.Ints ints = FieldCache.DEFAULT.getInts(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return ints.get(docID);\n              }\n            };\n            break;\n          case FLOAT:\n            final FieldCache.Floats floats = FieldCache.DEFAULT.getFloats(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return Float.floatToIntBits(floats.get(docID));\n              }\n            };\n            break;\n          case DOUBLE:\n            final FieldCache.Doubles doubles = FieldCache.DEFAULT.getDoubles(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return Double.doubleToLongBits(doubles.get(docID));\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = FieldCache.DEFAULT.getDocsWithField(ctx.reader(), fieldName);\n      }\n      if (docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, longs.get(doc - ctx.docBase), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<Integer>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<Entry>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on a field which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<String>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term = termsEnum.next();\n          final CharsRef spare = new CharsRef();\n          for (int skipped = hashTable.size; skipped < offset && term != null; ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<String, Integer>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        final CharsRef spare = new CharsRef();\n        BytesRef term = termsEnum.next();\n        for (int i = 0; i < offset && term != null; ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"95303ff3749680c743b9425f9cf99e6e4065e8a8","date":1361061922,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"/dev/null","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    FieldCache.Longs longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = FieldCache.DEFAULT.getLongs(ctx.reader(), fieldName, true);\n            break;\n          case INT:\n            final FieldCache.Ints ints = FieldCache.DEFAULT.getInts(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return ints.get(docID);\n              }\n            };\n            break;\n          case FLOAT:\n            final FieldCache.Floats floats = FieldCache.DEFAULT.getFloats(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return Float.floatToIntBits(floats.get(docID));\n              }\n            };\n            break;\n          case DOUBLE:\n            final FieldCache.Doubles doubles = FieldCache.DEFAULT.getDoubles(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return Double.doubleToLongBits(doubles.get(docID));\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = FieldCache.DEFAULT.getDocsWithField(ctx.reader(), fieldName);\n      }\n      if (docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, longs.get(doc - ctx.docBase), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<Integer>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<Entry>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on a field which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<String>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term = termsEnum.next();\n          final CharsRef spare = new CharsRef();\n          for (int skipped = hashTable.size; skipped < offset && term != null; ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<String, Integer>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        final CharsRef spare = new CharsRef();\n        BytesRef term = termsEnum.next();\n        for (int i = 0; i < offset && term != null; ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fa122aa6bc90e14eb49c0efee7cda631a87d8574","date":1361636893,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    FieldCache.Longs longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = FieldCache.DEFAULT.getLongs(ctx.reader(), fieldName, true);\n            break;\n          case INT:\n            final FieldCache.Ints ints = FieldCache.DEFAULT.getInts(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return ints.get(docID);\n              }\n            };\n            break;\n          case FLOAT:\n            final FieldCache.Floats floats = FieldCache.DEFAULT.getFloats(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return Float.floatToIntBits(floats.get(docID));\n              }\n            };\n            break;\n          case DOUBLE:\n            final FieldCache.Doubles doubles = FieldCache.DEFAULT.getDoubles(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return Double.doubleToLongBits(doubles.get(docID));\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = FieldCache.DEFAULT.getDocsWithField(ctx.reader(), fieldName);\n      }\n      if (docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, longs.get(doc - ctx.docBase), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<Integer>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<Entry>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on a field which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<String>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRef spare = new CharsRef();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<String, Integer>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator(null);\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRef spare = new CharsRef();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    FieldCache.Longs longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = FieldCache.DEFAULT.getLongs(ctx.reader(), fieldName, true);\n            break;\n          case INT:\n            final FieldCache.Ints ints = FieldCache.DEFAULT.getInts(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return ints.get(docID);\n              }\n            };\n            break;\n          case FLOAT:\n            final FieldCache.Floats floats = FieldCache.DEFAULT.getFloats(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return Float.floatToIntBits(floats.get(docID));\n              }\n            };\n            break;\n          case DOUBLE:\n            final FieldCache.Doubles doubles = FieldCache.DEFAULT.getDoubles(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return Double.doubleToLongBits(doubles.get(docID));\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = FieldCache.DEFAULT.getDocsWithField(ctx.reader(), fieldName);\n      }\n      if (docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, longs.get(doc - ctx.docBase), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<Integer>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<Entry>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on a field which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<String>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term = termsEnum.next();\n          final CharsRef spare = new CharsRef();\n          for (int skipped = hashTable.size; skipped < offset && term != null; ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<String, Integer>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        final CharsRef spare = new CharsRef();\n        BytesRef term = termsEnum.next();\n        for (int i = 0; i < offset && term != null; ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ac2858a7f8ab1782bc8e74c04fca8305efb4b4df","date":1365520767,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    FieldCache.Longs longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = FieldCache.DEFAULT.getLongs(ctx.reader(), fieldName, true);\n            break;\n          case INT:\n            final FieldCache.Ints ints = FieldCache.DEFAULT.getInts(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return ints.get(docID);\n              }\n            };\n            break;\n          case FLOAT:\n            final FieldCache.Floats floats = FieldCache.DEFAULT.getFloats(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.floatToSortableInt(floats.get(docID));\n              }\n            };\n            break;\n          case DOUBLE:\n            final FieldCache.Doubles doubles = FieldCache.DEFAULT.getDoubles(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.doubleToSortableLong(doubles.get(docID));\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = FieldCache.DEFAULT.getDocsWithField(ctx.reader(), fieldName);\n      }\n      if (docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, longs.get(doc - ctx.docBase), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<Integer>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<Entry>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on a field which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<String>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRef spare = new CharsRef();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<String, Integer>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator(null);\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRef spare = new CharsRef();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    FieldCache.Longs longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = FieldCache.DEFAULT.getLongs(ctx.reader(), fieldName, true);\n            break;\n          case INT:\n            final FieldCache.Ints ints = FieldCache.DEFAULT.getInts(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return ints.get(docID);\n              }\n            };\n            break;\n          case FLOAT:\n            final FieldCache.Floats floats = FieldCache.DEFAULT.getFloats(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return Float.floatToIntBits(floats.get(docID));\n              }\n            };\n            break;\n          case DOUBLE:\n            final FieldCache.Doubles doubles = FieldCache.DEFAULT.getDoubles(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return Double.doubleToLongBits(doubles.get(docID));\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = FieldCache.DEFAULT.getDocsWithField(ctx.reader(), fieldName);\n      }\n      if (docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, longs.get(doc - ctx.docBase), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<Integer>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<Entry>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on a field which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<String>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRef spare = new CharsRef();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<String, Integer>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator(null);\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRef spare = new CharsRef();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"11a746437bc5c0a0b3df0337ed249c387c812871","date":1376687959,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    FieldCache.Longs longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = FieldCache.DEFAULT.getLongs(ctx.reader(), fieldName, true);\n            break;\n          case INT:\n            final FieldCache.Ints ints = FieldCache.DEFAULT.getInts(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return ints.get(docID);\n              }\n            };\n            break;\n          case FLOAT:\n            final FieldCache.Floats floats = FieldCache.DEFAULT.getFloats(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.floatToSortableInt(floats.get(docID));\n              }\n            };\n            break;\n          case DOUBLE:\n            final FieldCache.Doubles doubles = FieldCache.DEFAULT.getDoubles(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.doubleToSortableLong(doubles.get(docID));\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = FieldCache.DEFAULT.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<Integer>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<Entry>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on a field which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<String>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRef spare = new CharsRef();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<String, Integer>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator(null);\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRef spare = new CharsRef();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    FieldCache.Longs longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = FieldCache.DEFAULT.getLongs(ctx.reader(), fieldName, true);\n            break;\n          case INT:\n            final FieldCache.Ints ints = FieldCache.DEFAULT.getInts(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return ints.get(docID);\n              }\n            };\n            break;\n          case FLOAT:\n            final FieldCache.Floats floats = FieldCache.DEFAULT.getFloats(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.floatToSortableInt(floats.get(docID));\n              }\n            };\n            break;\n          case DOUBLE:\n            final FieldCache.Doubles doubles = FieldCache.DEFAULT.getDoubles(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.doubleToSortableLong(doubles.get(docID));\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = FieldCache.DEFAULT.getDocsWithField(ctx.reader(), fieldName);\n      }\n      if (docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, longs.get(doc - ctx.docBase), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<Integer>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<Entry>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on a field which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<String>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRef spare = new CharsRef();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<String, Integer>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator(null);\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRef spare = new CharsRef();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"10307c7cc22e7e4087990972985e1d1043f01442","date":1376933032,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    FieldCache.Longs longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = FieldCache.DEFAULT.getLongs(ctx.reader(), fieldName, true);\n            break;\n          case INT:\n            final FieldCache.Ints ints = FieldCache.DEFAULT.getInts(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return ints.get(docID);\n              }\n            };\n            break;\n          case FLOAT:\n            final FieldCache.Floats floats = FieldCache.DEFAULT.getFloats(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.floatToSortableInt(floats.get(docID));\n              }\n            };\n            break;\n          case DOUBLE:\n            final FieldCache.Doubles doubles = FieldCache.DEFAULT.getDoubles(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.doubleToSortableLong(doubles.get(docID));\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = FieldCache.DEFAULT.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<Integer>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<Entry>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<String>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRef spare = new CharsRef();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<String, Integer>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator(null);\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRef spare = new CharsRef();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    FieldCache.Longs longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = FieldCache.DEFAULT.getLongs(ctx.reader(), fieldName, true);\n            break;\n          case INT:\n            final FieldCache.Ints ints = FieldCache.DEFAULT.getInts(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return ints.get(docID);\n              }\n            };\n            break;\n          case FLOAT:\n            final FieldCache.Floats floats = FieldCache.DEFAULT.getFloats(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.floatToSortableInt(floats.get(docID));\n              }\n            };\n            break;\n          case DOUBLE:\n            final FieldCache.Doubles doubles = FieldCache.DEFAULT.getDoubles(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.doubleToSortableLong(doubles.get(docID));\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = FieldCache.DEFAULT.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<Integer>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<Entry>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on a field which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<String>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRef spare = new CharsRef();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<String, Integer>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator(null);\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRef spare = new CharsRef();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff","date":1377034255,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    FieldCache.Longs longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = FieldCache.DEFAULT.getLongs(ctx.reader(), fieldName, true);\n            break;\n          case INT:\n            final FieldCache.Ints ints = FieldCache.DEFAULT.getInts(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return ints.get(docID);\n              }\n            };\n            break;\n          case FLOAT:\n            final FieldCache.Floats floats = FieldCache.DEFAULT.getFloats(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.floatToSortableInt(floats.get(docID));\n              }\n            };\n            break;\n          case DOUBLE:\n            final FieldCache.Doubles doubles = FieldCache.DEFAULT.getDoubles(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.doubleToSortableLong(doubles.get(docID));\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = FieldCache.DEFAULT.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<Integer>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<Entry>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<String>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRef spare = new CharsRef();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<String, Integer>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator(null);\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRef spare = new CharsRef();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    FieldCache.Longs longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = FieldCache.DEFAULT.getLongs(ctx.reader(), fieldName, true);\n            break;\n          case INT:\n            final FieldCache.Ints ints = FieldCache.DEFAULT.getInts(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return ints.get(docID);\n              }\n            };\n            break;\n          case FLOAT:\n            final FieldCache.Floats floats = FieldCache.DEFAULT.getFloats(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.floatToSortableInt(floats.get(docID));\n              }\n            };\n            break;\n          case DOUBLE:\n            final FieldCache.Doubles doubles = FieldCache.DEFAULT.getDoubles(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.doubleToSortableLong(doubles.get(docID));\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = FieldCache.DEFAULT.getDocsWithField(ctx.reader(), fieldName);\n      }\n      if (docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, longs.get(doc - ctx.docBase), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<Integer>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<Entry>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on a field which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<String>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRef spare = new CharsRef();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<String, Integer>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator(null);\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRef spare = new CharsRef();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    FieldCache.Longs longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = FieldCache.DEFAULT.getLongs(ctx.reader(), fieldName, true);\n            break;\n          case INT:\n            final FieldCache.Ints ints = FieldCache.DEFAULT.getInts(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return ints.get(docID);\n              }\n            };\n            break;\n          case FLOAT:\n            final FieldCache.Floats floats = FieldCache.DEFAULT.getFloats(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.floatToSortableInt(floats.get(docID));\n              }\n            };\n            break;\n          case DOUBLE:\n            final FieldCache.Doubles doubles = FieldCache.DEFAULT.getDoubles(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.doubleToSortableLong(doubles.get(docID));\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = FieldCache.DEFAULT.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<Integer>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<Entry>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<String>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRef spare = new CharsRef();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<String, Integer>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator(null);\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRef spare = new CharsRef();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    FieldCache.Longs longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = FieldCache.DEFAULT.getLongs(ctx.reader(), fieldName, true);\n            break;\n          case INT:\n            final FieldCache.Ints ints = FieldCache.DEFAULT.getInts(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return ints.get(docID);\n              }\n            };\n            break;\n          case FLOAT:\n            final FieldCache.Floats floats = FieldCache.DEFAULT.getFloats(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.floatToSortableInt(floats.get(docID));\n              }\n            };\n            break;\n          case DOUBLE:\n            final FieldCache.Doubles doubles = FieldCache.DEFAULT.getDoubles(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.doubleToSortableLong(doubles.get(docID));\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = FieldCache.DEFAULT.getDocsWithField(ctx.reader(), fieldName);\n      }\n      if (docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, longs.get(doc - ctx.docBase), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<Integer>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<Entry>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on a field which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<String>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRef spare = new CharsRef();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<String, Integer>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator(null);\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRef spare = new CharsRef();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    FieldCache.Longs longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = FieldCache.DEFAULT.getLongs(ctx.reader(), fieldName, true);\n            break;\n          case INT:\n            final FieldCache.Ints ints = FieldCache.DEFAULT.getInts(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return ints.get(docID);\n              }\n            };\n            break;\n          case FLOAT:\n            final FieldCache.Floats floats = FieldCache.DEFAULT.getFloats(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.floatToSortableInt(floats.get(docID));\n              }\n            };\n            break;\n          case DOUBLE:\n            final FieldCache.Doubles doubles = FieldCache.DEFAULT.getDoubles(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.doubleToSortableLong(doubles.get(docID));\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = FieldCache.DEFAULT.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRef spare = new CharsRef();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator(null);\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRef spare = new CharsRef();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    FieldCache.Longs longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = FieldCache.DEFAULT.getLongs(ctx.reader(), fieldName, true);\n            break;\n          case INT:\n            final FieldCache.Ints ints = FieldCache.DEFAULT.getInts(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return ints.get(docID);\n              }\n            };\n            break;\n          case FLOAT:\n            final FieldCache.Floats floats = FieldCache.DEFAULT.getFloats(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.floatToSortableInt(floats.get(docID));\n              }\n            };\n            break;\n          case DOUBLE:\n            final FieldCache.Doubles doubles = FieldCache.DEFAULT.getDoubles(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.doubleToSortableLong(doubles.get(docID));\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = FieldCache.DEFAULT.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<Integer>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<Entry>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<String>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRef spare = new CharsRef();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<String, Integer>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator(null);\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRef spare = new CharsRef();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c","date":1399816179,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    NumericDocValues longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = FieldCache.DEFAULT.getNumerics(ctx.reader(), fieldName, FieldCache.NUMERIC_UTILS_LONG_PARSER, true);\n            break;\n          case INT:\n            longs = FieldCache.DEFAULT.getNumerics(ctx.reader(), fieldName, FieldCache.NUMERIC_UTILS_INT_PARSER, true);\n            break;\n          case FLOAT:\n            final NumericDocValues floats = FieldCache.DEFAULT.getNumerics(ctx.reader(), fieldName, FieldCache.NUMERIC_UTILS_FLOAT_PARSER, true);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = floats.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            final NumericDocValues doubles = FieldCache.DEFAULT.getNumerics(ctx.reader(), fieldName, FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, true);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = doubles.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = FieldCache.DEFAULT.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRef spare = new CharsRef();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator(null);\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRef spare = new CharsRef();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    FieldCache.Longs longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = FieldCache.DEFAULT.getLongs(ctx.reader(), fieldName, true);\n            break;\n          case INT:\n            final FieldCache.Ints ints = FieldCache.DEFAULT.getInts(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return ints.get(docID);\n              }\n            };\n            break;\n          case FLOAT:\n            final FieldCache.Floats floats = FieldCache.DEFAULT.getFloats(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.floatToSortableInt(floats.get(docID));\n              }\n            };\n            break;\n          case DOUBLE:\n            final FieldCache.Doubles doubles = FieldCache.DEFAULT.getDoubles(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.doubleToSortableLong(doubles.get(docID));\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = FieldCache.DEFAULT.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRef spare = new CharsRef();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator(null);\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRef spare = new CharsRef();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e28b14e7783d24ca69089f13ddadadbd2afdcb29","date":1399840701,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    NumericDocValues longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            final NumericDocValues floats = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = floats.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            final NumericDocValues doubles = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = doubles.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRef spare = new CharsRef();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator(null);\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRef spare = new CharsRef();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    NumericDocValues longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = FieldCache.DEFAULT.getNumerics(ctx.reader(), fieldName, FieldCache.NUMERIC_UTILS_LONG_PARSER, true);\n            break;\n          case INT:\n            longs = FieldCache.DEFAULT.getNumerics(ctx.reader(), fieldName, FieldCache.NUMERIC_UTILS_INT_PARSER, true);\n            break;\n          case FLOAT:\n            final NumericDocValues floats = FieldCache.DEFAULT.getNumerics(ctx.reader(), fieldName, FieldCache.NUMERIC_UTILS_FLOAT_PARSER, true);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = floats.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            final NumericDocValues doubles = FieldCache.DEFAULT.getNumerics(ctx.reader(), fieldName, FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, true);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = doubles.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = FieldCache.DEFAULT.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRef spare = new CharsRef();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator(null);\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRef spare = new CharsRef();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"93dd449115a9247533e44bab47e8429e5dccbc6d","date":1400258396,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    NumericDocValues longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            final NumericDocValues floats = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = floats.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            final NumericDocValues doubles = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = doubles.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRef spare = new CharsRef();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator(null);\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRef spare = new CharsRef();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    FieldCache.Longs longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = FieldCache.DEFAULT.getLongs(ctx.reader(), fieldName, true);\n            break;\n          case INT:\n            final FieldCache.Ints ints = FieldCache.DEFAULT.getInts(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return ints.get(docID);\n              }\n            };\n            break;\n          case FLOAT:\n            final FieldCache.Floats floats = FieldCache.DEFAULT.getFloats(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.floatToSortableInt(floats.get(docID));\n              }\n            };\n            break;\n          case DOUBLE:\n            final FieldCache.Doubles doubles = FieldCache.DEFAULT.getDoubles(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.doubleToSortableLong(doubles.get(docID));\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = FieldCache.DEFAULT.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRef spare = new CharsRef();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator(null);\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRef spare = new CharsRef();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"56572ec06f1407c066d6b7399413178b33176cd8","date":1400495675,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    NumericDocValues longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            final NumericDocValues floats = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = floats.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            final NumericDocValues doubles = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = doubles.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRef spare = new CharsRef();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator(null);\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRef spare = new CharsRef();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    FieldCache.Longs longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = FieldCache.DEFAULT.getLongs(ctx.reader(), fieldName, true);\n            break;\n          case INT:\n            final FieldCache.Ints ints = FieldCache.DEFAULT.getInts(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return ints.get(docID);\n              }\n            };\n            break;\n          case FLOAT:\n            final FieldCache.Floats floats = FieldCache.DEFAULT.getFloats(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.floatToSortableInt(floats.get(docID));\n              }\n            };\n            break;\n          case DOUBLE:\n            final FieldCache.Doubles doubles = FieldCache.DEFAULT.getDoubles(ctx.reader(), fieldName, true);\n            longs = new FieldCache.Longs() {\n              @Override\n              public long get(int docID) {\n                return NumericUtils.doubleToSortableLong(doubles.get(docID));\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = FieldCache.DEFAULT.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRef spare = new CharsRef();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator(null);\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRef spare = new CharsRef();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50","date":1407854805,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    NumericDocValues longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            final NumericDocValues floats = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = floats.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            final NumericDocValues doubles = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = doubles.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator(null);\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    NumericDocValues longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            final NumericDocValues floats = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = floats.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            final NumericDocValues doubles = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = doubles.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRef spare = new CharsRef();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator(null);\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRef spare = new CharsRef();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":["73bb5a57dc75b54a39494f99986599cae7dff417"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            final NumericDocValues floats = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = floats.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            final NumericDocValues doubles = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = doubles.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getLeafReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getLeafReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator(null);\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();\n    AtomicReaderContext ctx = null;\n    NumericDocValues longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            final NumericDocValues floats = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = floats.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            final NumericDocValues doubles = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = doubles.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator(null);\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","date":1428522487,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            final NumericDocValues floats = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = floats.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            final NumericDocValues doubles = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = doubles.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getLeafReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getLeafReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            final NumericDocValues floats = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = floats.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            final NumericDocValues doubles = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = doubles.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getLeafReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator(null);\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getLeafReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator(null);\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"770342641f7b505eaa8dccdc666158bff2419109","date":1449868421,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final org.apache.lucene.document.FieldType.LegacyNumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            final NumericDocValues floats = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = floats.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            final NumericDocValues doubles = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = doubles.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getLeafReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getLeafReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            final NumericDocValues floats = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = floats.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            final NumericDocValues doubles = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = doubles.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getLeafReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getLeafReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5af5ba0166322092193d4c29880b0f7670fc7ca0","date":1471440525,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final LegacyNumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            final NumericDocValues floats = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = floats.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            final NumericDocValues doubles = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = doubles.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getLeafReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getLeafReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final org.apache.lucene.document.FieldType.LegacyNumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            final NumericDocValues floats = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = floats.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            final NumericDocValues doubles = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = doubles.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getLeafReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getLeafReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","date":1471496851,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final LegacyNumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            final NumericDocValues floats = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = floats.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            final NumericDocValues doubles = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = doubles.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getLeafReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getLeafReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final org.apache.lucene.document.FieldType.LegacyNumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            final NumericDocValues floats = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = floats.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            final NumericDocValues doubles = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = doubles.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getLeafReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getLeafReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"403d05f7f8d69b65659157eff1bc1d2717f04c66","date":1471692961,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final LegacyNumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            final NumericDocValues floats = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = floats.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            final NumericDocValues doubles = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = doubles.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getLeafReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getLeafReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final org.apache.lucene.document.FieldType.LegacyNumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            final NumericDocValues floats = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = floats.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            final NumericDocValues doubles = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = doubles.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getLeafReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getLeafReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6652c74b2358a0b13223817a6a793bf1c9d0749d","date":1474465301,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final LegacyNumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getLeafReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getLeafReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final LegacyNumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            final NumericDocValues floats = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = floats.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            final NumericDocValues doubles = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = doubles.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getLeafReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getLeafReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"16bac49f6115a1b41829c30623e001005ebb7ad7","date":1475239647,"type":3,"author":"Christine Poerschke","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final LegacyNumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getLeafReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getLeafReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final LegacyNumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getLeafReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getLeafReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final LegacyNumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getLeafReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getLeafReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final LegacyNumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            final NumericDocValues floats = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = floats.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            final NumericDocValues doubles = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = doubles.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getLeafReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getLeafReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5d217880fb207e1a26143863d06cf461c7cabeec","date":1475691535,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final LegacyNumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getLeafReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getLeafReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final LegacyNumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getLeafReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getLeafReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e07c409cff8701e4dc3d45934b021a949a5a8822","date":1475694629,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final LegacyNumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final LegacyNumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getLeafReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getLeafReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final LegacyNumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final org.apache.lucene.document.FieldType.LegacyNumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    Bits docsWithField = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            final NumericDocValues floats = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = floats.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            final NumericDocValues doubles = DocValues.getNumeric(ctx.reader(), fieldName);\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new NumericDocValues() {\n              @Override\n              public long get(int docID) {\n                long bits = doubles.get(docID);\n                if (bits<0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), fieldName);\n      }\n      long v = longs.get(doc - ctx.docBase);\n      if (v != 0 || docsWithField.get(doc - ctx.docBase)) {\n        hashTable.add(doc, v, 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getLeafReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getLeafReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c72f6f0907682b825869d7878ba72d8259dabc91","date":1480192617,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final LegacyNumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final LegacyNumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"727bb765ff2542275f6d31f67be18d7104bae148","date":1480353976,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final LegacyNumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final LegacyNumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"931cb705e783c7f07cdb110a0cb03bad79fe1b2b","date":1480386986,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final LegacyNumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final LegacyNumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is not indexed\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3d81baa64023bbb9b43f6d929ee168b105940d30","date":1486492702,"type":3,"author":"Tomas Fernandez Lobbe","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumberType numericType = ft.getNumberType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n          case DATE:\n          case INTEGER:\n            // Long, Date and Integer\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError(\"Unexpected type: \" + numericType);\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final LegacyNumericType numericType = ft.getNumericType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case INT:\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"744b111b17d15d490a648eb021bfa240e7f11556","date":1487008069,"type":3,"author":"Tomas Fernandez Lobbe","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    if (sf.multiValued()) {\n      // TODO: evaluate using getCountsMultiValued for singleValued numerics with SingletonSortedNumericDocValues\n      return getCountsMultiValued(searcher, docs, fieldName, offset, limit, mincount, missing, sort);\n    }\n    return getCountsSingleValue(searcher, docs, fieldName, offset, limit, mincount, missing, sort);\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumberType numericType = ft.getNumberType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n          case DATE:\n          case INTEGER:\n            // Long, Date and Integer\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError(\"Unexpected type: \" + numericType);\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"022a4de90e0479b604264ca9c2e134c996454ab3","date":1487118265,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    if (sf.multiValued()) {\n      // TODO: evaluate using getCountsMultiValued for singleValued numerics with SingletonSortedNumericDocValues\n      return getCountsMultiValued(searcher, docs, fieldName, offset, limit, mincount, missing, sort);\n    }\n    return getCountsSingleValue(searcher, docs, fieldName, offset, limit, mincount, missing, sort);\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumberType numericType = ft.getNumberType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n          case DATE:\n          case INTEGER:\n            // Long, Date and Integer\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError(\"Unexpected type: \" + numericType);\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b49c0eb9ef7bc25609a89e7986ce7e6eeb9c9d96","date":1487122334,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumberType numericType = ft.getNumberType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n          case DATE:\n          case INTEGER:\n            // Long, Date and Integer\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError(\"Unexpected type: \" + numericType);\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    if (sf.multiValued()) {\n      // TODO: evaluate using getCountsMultiValued for singleValued numerics with SingletonSortedNumericDocValues\n      return getCountsMultiValued(searcher, docs, fieldName, offset, limit, mincount, missing, sort);\n    }\n    return getCountsSingleValue(searcher, docs, fieldName, offset, limit, mincount, missing, sort);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"897b06b1364bd1f658a8be7591e43f0851458e7f","date":1487123008,"type":3,"author":"Ishan Chattopadhyaya","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/NumericFacets#getCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String).mjava","sourceNew":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    if (sf.multiValued()) {\n      // TODO: evaluate using getCountsMultiValued for singleValued numerics with SingletonSortedNumericDocValues\n      return getCountsMultiValued(searcher, docs, fieldName, offset, limit, mincount, missing, sort);\n    }\n    return getCountsSingleValue(searcher, docs, fieldName, offset, limit, mincount, missing, sort);\n  }\n\n","sourceOld":"  public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    final boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumberType numericType = ft.getNumberType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    // 1. accumulate\n    final HashTable hashTable = new HashTable();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n          case DATE:\n          case INTEGER:\n            // Long, Date and Integer\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError(\"Unexpected type: \" + numericType);\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    // 2. select top-k facet values\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    // 4. build the NamedList\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    // This stuff is complicated because if facet.mincount=0, the counts needs\n    // to be merged with terms from the terms dict\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      // Only keep items we're interested in\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      // Entries from the PQ first, then using the terms dictionary\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { // need to merge with the term dict\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        // Add zeros until there are limit results\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      // sort=index, mincount=0 and we have less than limit items\n      // => Merge the PQ and the terms dictionary on the fly\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"10307c7cc22e7e4087990972985e1d1043f01442":["11a746437bc5c0a0b3df0337ed249c387c812871"],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["770342641f7b505eaa8dccdc666158bff2419109","5af5ba0166322092193d4c29880b0f7670fc7ca0"],"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"56572ec06f1407c066d6b7399413178b33176cd8":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","93dd449115a9247533e44bab47e8429e5dccbc6d"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"ac2858a7f8ab1782bc8e74c04fca8305efb4b4df":["fa122aa6bc90e14eb49c0efee7cda631a87d8574"],"11a746437bc5c0a0b3df0337ed249c387c812871":["ac2858a7f8ab1782bc8e74c04fca8305efb4b4df"],"022a4de90e0479b604264ca9c2e134c996454ab3":["3d81baa64023bbb9b43f6d929ee168b105940d30","744b111b17d15d490a648eb021bfa240e7f11556"],"95303ff3749680c743b9425f9cf99e6e4065e8a8":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","73bb5a57dc75b54a39494f99986599cae7dff417"],"c72f6f0907682b825869d7878ba72d8259dabc91":["e07c409cff8701e4dc3d45934b021a949a5a8822"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["403d05f7f8d69b65659157eff1bc1d2717f04c66","16bac49f6115a1b41829c30623e001005ebb7ad7"],"931cb705e783c7f07cdb110a0cb03bad79fe1b2b":["e07c409cff8701e4dc3d45934b021a949a5a8822","c72f6f0907682b825869d7878ba72d8259dabc91"],"e28b14e7783d24ca69089f13ddadadbd2afdcb29":["b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c"],"73bb5a57dc75b54a39494f99986599cae7dff417":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff"],"16bac49f6115a1b41829c30623e001005ebb7ad7":["6652c74b2358a0b13223817a6a793bf1c9d0749d"],"403d05f7f8d69b65659157eff1bc1d2717f04c66":["770342641f7b505eaa8dccdc666158bff2419109","2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"897b06b1364bd1f658a8be7591e43f0851458e7f":["b49c0eb9ef7bc25609a89e7986ce7e6eeb9c9d96"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["ac2858a7f8ab1782bc8e74c04fca8305efb4b4df","e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff"],"fa122aa6bc90e14eb49c0efee7cda631a87d8574":["73bb5a57dc75b54a39494f99986599cae7dff417"],"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"5af5ba0166322092193d4c29880b0f7670fc7ca0":["770342641f7b505eaa8dccdc666158bff2419109"],"e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff":["ac2858a7f8ab1782bc8e74c04fca8305efb4b4df","10307c7cc22e7e4087990972985e1d1043f01442"],"e07c409cff8701e4dc3d45934b021a949a5a8822":["5d217880fb207e1a26143863d06cf461c7cabeec"],"770342641f7b505eaa8dccdc666158bff2419109":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["770342641f7b505eaa8dccdc666158bff2419109","e07c409cff8701e4dc3d45934b021a949a5a8822"],"b49c0eb9ef7bc25609a89e7986ce7e6eeb9c9d96":["022a4de90e0479b604264ca9c2e134c996454ab3"],"727bb765ff2542275f6d31f67be18d7104bae148":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","c72f6f0907682b825869d7878ba72d8259dabc91"],"3d81baa64023bbb9b43f6d929ee168b105940d30":["931cb705e783c7f07cdb110a0cb03bad79fe1b2b"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","e28b14e7783d24ca69089f13ddadadbd2afdcb29"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["403d05f7f8d69b65659157eff1bc1d2717f04c66"],"5d217880fb207e1a26143863d06cf461c7cabeec":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"744b111b17d15d490a648eb021bfa240e7f11556":["3d81baa64023bbb9b43f6d929ee168b105940d30"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["897b06b1364bd1f658a8be7591e43f0851458e7f"]},"commit2Childs":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["770342641f7b505eaa8dccdc666158bff2419109"],"10307c7cc22e7e4087990972985e1d1043f01442":["e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff"],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["403d05f7f8d69b65659157eff1bc1d2717f04c66"],"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"56572ec06f1407c066d6b7399413178b33176cd8":[],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"ac2858a7f8ab1782bc8e74c04fca8305efb4b4df":["11a746437bc5c0a0b3df0337ed249c387c812871","3dffec77fb8f7d0e9ca4869dddd6af94528b4576","e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff"],"11a746437bc5c0a0b3df0337ed249c387c812871":["10307c7cc22e7e4087990972985e1d1043f01442"],"022a4de90e0479b604264ca9c2e134c996454ab3":["b49c0eb9ef7bc25609a89e7986ce7e6eeb9c9d96"],"95303ff3749680c743b9425f9cf99e6e4065e8a8":[],"c72f6f0907682b825869d7878ba72d8259dabc91":["931cb705e783c7f07cdb110a0cb03bad79fe1b2b","727bb765ff2542275f6d31f67be18d7104bae148"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["95303ff3749680c743b9425f9cf99e6e4065e8a8","73bb5a57dc75b54a39494f99986599cae7dff417"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["5d217880fb207e1a26143863d06cf461c7cabeec"],"931cb705e783c7f07cdb110a0cb03bad79fe1b2b":["3d81baa64023bbb9b43f6d929ee168b105940d30"],"e28b14e7783d24ca69089f13ddadadbd2afdcb29":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"73bb5a57dc75b54a39494f99986599cae7dff417":["95303ff3749680c743b9425f9cf99e6e4065e8a8","fa122aa6bc90e14eb49c0efee7cda631a87d8574"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["56572ec06f1407c066d6b7399413178b33176cd8","b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c","93dd449115a9247533e44bab47e8429e5dccbc6d"],"16bac49f6115a1b41829c30623e001005ebb7ad7":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"403d05f7f8d69b65659157eff1bc1d2717f04c66":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"897b06b1364bd1f658a8be7591e43f0851458e7f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"fa122aa6bc90e14eb49c0efee7cda631a87d8574":["ac2858a7f8ab1782bc8e74c04fca8305efb4b4df"],"5af5ba0166322092193d4c29880b0f7670fc7ca0":["2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c":["e28b14e7783d24ca69089f13ddadadbd2afdcb29"],"e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","3dffec77fb8f7d0e9ca4869dddd6af94528b4576"],"e07c409cff8701e4dc3d45934b021a949a5a8822":["c72f6f0907682b825869d7878ba72d8259dabc91","931cb705e783c7f07cdb110a0cb03bad79fe1b2b","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"770342641f7b505eaa8dccdc666158bff2419109":["2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","403d05f7f8d69b65659157eff1bc1d2717f04c66","5af5ba0166322092193d4c29880b0f7670fc7ca0","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["727bb765ff2542275f6d31f67be18d7104bae148"],"b49c0eb9ef7bc25609a89e7986ce7e6eeb9c9d96":["897b06b1364bd1f658a8be7591e43f0851458e7f"],"727bb765ff2542275f6d31f67be18d7104bae148":[],"3d81baa64023bbb9b43f6d929ee168b105940d30":["022a4de90e0479b604264ca9c2e134c996454ab3","744b111b17d15d490a648eb021bfa240e7f11556"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50","56572ec06f1407c066d6b7399413178b33176cd8"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["16bac49f6115a1b41829c30623e001005ebb7ad7"],"5d217880fb207e1a26143863d06cf461c7cabeec":["e07c409cff8701e4dc3d45934b021a949a5a8822"],"744b111b17d15d490a648eb021bfa240e7f11556":["022a4de90e0479b604264ca9c2e134c996454ab3"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["56572ec06f1407c066d6b7399413178b33176cd8","95303ff3749680c743b9425f9cf99e6e4065e8a8","3dffec77fb8f7d0e9ca4869dddd6af94528b4576","727bb765ff2542275f6d31f67be18d7104bae148","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}