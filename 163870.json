{"path":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","commits":[{"id":"893269407f5c988a4b2ee13c7ffc01ec43136c1d","date":1268598046,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n    tokenList = indexPart.get(\"org.apache.solr.analysis.EnglishPorterFilter\");\n    assertNotNull(\"Expcting EnglishPorterFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.solr.analysis.EnglishPorterFilter\");\n    assertNotNull(\"Expcting EnglishPorterFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, null, false));\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8","date":1268599006,"type":4,"author":"Mark Robert Miller","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":null,"sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n    tokenList = indexPart.get(\"org.apache.solr.analysis.EnglishPorterFilter\");\n    assertNotNull(\"Expcting EnglishPorterFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.solr.analysis.EnglishPorterFilter\");\n    assertNotNull(\"Expcting EnglishPorterFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n    tokenList = indexPart.get(\"org.apache.solr.analysis.EnglishPorterFilter\");\n    assertNotNull(\"Expcting EnglishPorterFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.solr.analysis.EnglishPorterFilter\");\n    assertNotNull(\"Expcting EnglishPorterFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n    tokenList = indexPart.get(\"org.apache.solr.analysis.EnglishPorterFilter\");\n    assertNotNull(\"Expcting EnglishPorterFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.solr.analysis.EnglishPorterFilter\");\n    assertNotNull(\"Expcting EnglishPorterFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1da8d55113b689b06716246649de6f62430f15c0","date":1453508340,"type":0,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n    tokenList = indexPart.get(\"org.apache.solr.analysis.EnglishPorterFilter\");\n    assertNotNull(\"Expcting EnglishPorterFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.solr.analysis.EnglishPorterFilter\");\n    assertNotNull(\"Expcting EnglishPorterFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, null, false));\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9c8b12bda3f5864b27e3e04df1be4f6736ec067a","date":1270088127,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.snowball.SnowballFilter\");\n    assertNotNull(\"Expcting SnowballFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.snowball.SnowballFilter\");\n    assertNotNull(\"Expcting SnowballFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n    tokenList = indexPart.get(\"org.apache.solr.analysis.EnglishPorterFilter\");\n    assertNotNull(\"Expcting EnglishPorterFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.solr.analysis.EnglishPorterFilter\");\n    assertNotNull(\"Expcting EnglishPorterFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3b8d301760a29119d797c1dd47f32bd442c1b562","date":1270142652,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.snowball.SnowballFilter\");\n    assertNotNull(\"Expcting SnowballFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.snowball.SnowballFilter\");\n    assertNotNull(\"Expcting SnowballFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"689f35bd9818b47b8d9fe96cf06518228e949ab6","date":1272894884,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"790c3f61c9b891d66d919c5d10db9fa5216eb0f1","date":1274818604,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bd0a7f04b5a49a00149b867e7d51f632fb8a4664","date":1279497978,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0487f900016b7da69f089f740e28192189ef3972","date":1307810819,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, \"1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, \"2\", null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, \"3\", null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, \"4\", null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, \"5\", null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, \"6\", null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, \"7\", null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, \"8\", null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, \"9\", null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, \"10\", null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, \"1/1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, \"2/2\", null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, \"3/3\", null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, \"4/4\", null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, \"5/5\", null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, \"6/6\", null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, \"7/7\", null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, \"8/8\", null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, \"9/9\", null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, \"10/10\", null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, \"1/1/1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, \"2/2/2\", null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, \"3/3/3\", null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, \"4/4/4\", null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, \"5/5/5\", null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, \"6/6/6\", null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, \"7/7/7\", null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, \"8/8/8\", null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, \"9/9/9\", null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, \"10/10/10\", null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, \"2/2/2/1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, \"3/3/3/2\", null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, \"4/4/4/3\", null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, \"5/5/5/4\", null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, \"6/6/6/5\", null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, \"8/8/8/6\", null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, \"9/9/9/7\", null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, \"10/10/10/8\", null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, \"2/2/2/1/1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, \"3/3/3/2/2\", null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, \"4/4/4/3/3\", null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, \"5/5/5/4/4\", null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, \"6/6/6/5/5\", null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, \"8/8/8/6/6\", null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, \"9/9/9/7/7\", null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, \"10/10/10/8/8\", null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, \"1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, \"2\", null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, \"1/1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, \"2/2\", null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, \"1/1/1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, \"2/2/2\", null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, \"1/1/1/1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, \"2/2/2/2\", null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, \"1/1/1/1/1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, \"2/2/2/2/2\", null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, \"1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, \"2\", null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, \"3\", null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, \"4\", null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, \"5\", null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, \"6\", null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, \"7\", null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, \"8\", null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, \"9\", null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, \"10\", null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, \"1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, \"2\", null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, \"1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, \"2\", null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, \"3\", null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, \"4\", null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, \"5\", null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, \"6\", null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, \"7\", null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, \"8\", null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, \"9\", null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, \"10\", null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, \"1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, \"2\", null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, \"1\", null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, \"1\", null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"02dba75457528db0b73837ff68f971ecb715ab78","date":1307981000,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, \"1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, \"2\", null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, \"3\", null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, \"4\", null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, \"5\", null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, \"6\", null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, \"7\", null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, \"8\", null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, \"9\", null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, \"10\", null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, \"1/1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, \"2/2\", null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, \"3/3\", null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, \"4/4\", null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, \"5/5\", null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, \"6/6\", null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, \"7/7\", null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, \"8/8\", null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, \"9/9\", null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, \"10/10\", null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, \"1/1/1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, \"2/2/2\", null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, \"3/3/3\", null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, \"4/4/4\", null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, \"5/5/5\", null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, \"6/6/6\", null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, \"7/7/7\", null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, \"8/8/8\", null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, \"9/9/9\", null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, \"10/10/10\", null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, \"2/2/2/1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, \"3/3/3/2\", null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, \"4/4/4/3\", null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, \"5/5/5/4\", null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, \"6/6/6/5\", null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, \"8/8/8/6\", null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, \"9/9/9/7\", null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, \"10/10/10/8\", null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, \"2/2/2/1/1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, \"3/3/3/2/2\", null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, \"4/4/4/3/3\", null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, \"5/5/5/4/4\", null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, \"6/6/6/5/5\", null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, \"8/8/8/6/6\", null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, \"9/9/9/7/7\", null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, \"10/10/10/8/8\", null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, \"1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, \"2\", null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, \"1/1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, \"2/2\", null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, \"1/1/1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, \"2/2/2\", null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, \"1/1/1/1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, \"2/2/2/2\", null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, \"1/1/1/1/1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, \"2/2/2/2/2\", null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, \"1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, \"2\", null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, \"3\", null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, \"4\", null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, \"5\", null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, \"6\", null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, \"7\", null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, \"8\", null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, \"9\", null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, \"10\", null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, \"1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, \"2\", null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, \"1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, \"2\", null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, \"3\", null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, \"4\", null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, \"5\", null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, \"6\", null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, \"7\", null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, \"8\", null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, \"9\", null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, \"10\", null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, \"1\", null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, \"2\", null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, \"1\", null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, \"1\", null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9ed208afa1e7aa98899ddb1dedfddedddf898253","date":1308079587,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":5,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"893269407f5c988a4b2ee13c7ffc01ec43136c1d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8":["893269407f5c988a4b2ee13c7ffc01ec43136c1d"],"c26f00b574427b55127e869b935845554afde1fa":["02dba75457528db0b73837ff68f971ecb715ab78","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"0487f900016b7da69f089f740e28192189ef3972":["bd0a7f04b5a49a00149b867e7d51f632fb8a4664"],"5f4e87790277826a2aea119328600dfb07761f32":["790c3f61c9b891d66d919c5d10db9fa5216eb0f1","bd0a7f04b5a49a00149b867e7d51f632fb8a4664"],"9ed208afa1e7aa98899ddb1dedfddedddf898253":["bd0a7f04b5a49a00149b867e7d51f632fb8a4664","02dba75457528db0b73837ff68f971ecb715ab78"],"9c8b12bda3f5864b27e3e04df1be4f6736ec067a":["1da8d55113b689b06716246649de6f62430f15c0"],"1da8d55113b689b06716246649de6f62430f15c0":["a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8","ad94625fb8d088209f46650c8097196fec67f00c"],"689f35bd9818b47b8d9fe96cf06518228e949ab6":["3b8d301760a29119d797c1dd47f32bd442c1b562"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["9ed208afa1e7aa98899ddb1dedfddedddf898253"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"02dba75457528db0b73837ff68f971ecb715ab78":["0487f900016b7da69f089f740e28192189ef3972"],"a258fbb26824fd104ed795e5d9033d2d040049ee":["02dba75457528db0b73837ff68f971ecb715ab78"],"ad94625fb8d088209f46650c8097196fec67f00c":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c26f00b574427b55127e869b935845554afde1fa"],"bd0a7f04b5a49a00149b867e7d51f632fb8a4664":["790c3f61c9b891d66d919c5d10db9fa5216eb0f1"],"790c3f61c9b891d66d919c5d10db9fa5216eb0f1":["689f35bd9818b47b8d9fe96cf06518228e949ab6"],"3b8d301760a29119d797c1dd47f32bd442c1b562":["9c8b12bda3f5864b27e3e04df1be4f6736ec067a"]},"commit2Childs":{"893269407f5c988a4b2ee13c7ffc01ec43136c1d":["a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8"],"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8":["1da8d55113b689b06716246649de6f62430f15c0"],"c26f00b574427b55127e869b935845554afde1fa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["ad94625fb8d088209f46650c8097196fec67f00c"],"0487f900016b7da69f089f740e28192189ef3972":["02dba75457528db0b73837ff68f971ecb715ab78"],"5f4e87790277826a2aea119328600dfb07761f32":[],"9ed208afa1e7aa98899ddb1dedfddedddf898253":["c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"9c8b12bda3f5864b27e3e04df1be4f6736ec067a":["3b8d301760a29119d797c1dd47f32bd442c1b562"],"1da8d55113b689b06716246649de6f62430f15c0":["9c8b12bda3f5864b27e3e04df1be4f6736ec067a"],"689f35bd9818b47b8d9fe96cf06518228e949ab6":["790c3f61c9b891d66d919c5d10db9fa5216eb0f1"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["893269407f5c988a4b2ee13c7ffc01ec43136c1d"],"02dba75457528db0b73837ff68f971ecb715ab78":["c26f00b574427b55127e869b935845554afde1fa","9ed208afa1e7aa98899ddb1dedfddedddf898253","a258fbb26824fd104ed795e5d9033d2d040049ee"],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"ad94625fb8d088209f46650c8097196fec67f00c":["1da8d55113b689b06716246649de6f62430f15c0"],"bd0a7f04b5a49a00149b867e7d51f632fb8a4664":["0487f900016b7da69f089f740e28192189ef3972","5f4e87790277826a2aea119328600dfb07761f32","9ed208afa1e7aa98899ddb1dedfddedddf898253"],"790c3f61c9b891d66d919c5d10db9fa5216eb0f1":["5f4e87790277826a2aea119328600dfb07761f32","bd0a7f04b5a49a00149b867e7d51f632fb8a4664"],"3b8d301760a29119d797c1dd47f32bd442c1b562":["689f35bd9818b47b8d9fe96cf06518228e949ab6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5f4e87790277826a2aea119328600dfb07761f32","a258fbb26824fd104ed795e5d9033d2d040049ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}