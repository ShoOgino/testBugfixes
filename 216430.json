{"path":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader#BlockTermsReader(TermsIndexReaderBase,Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","commits":[{"id":"eee5f2a24465d2c9a5f86ab84b7c35041a30fda8","date":1373996650,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader#BlockTermsReader(TermsIndexReaderBase,Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader#BlockTermsReader(TermsIndexReaderBase,Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,int,String).mjava","sourceNew":"  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,\n                          String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    // this.segment = segment;\n    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),\n                       context);\n\n    boolean success = false;\n    try {\n      version = readHeader(in);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid number of fields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final long termsStartPointer = in.readVLong();\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate fields: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        in.close();\n      }\n    }\n\n    this.indexReader = indexReader;\n  }\n\n","sourceOld":"  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,\n                          int termsCacheSize, String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n    termsCache = new DoubleBarrelLRUCache<FieldAndTerm,BlockTermState>(termsCacheSize);\n\n    // this.segment = segment;\n    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),\n                       context);\n\n    boolean success = false;\n    try {\n      version = readHeader(in);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid number of fields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final long termsStartPointer = in.readVLong();\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate fields: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        in.close();\n      }\n    }\n\n    this.indexReader = indexReader;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","date":1376366778,"type":1,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader#BlockTermsReader(TermsIndexReaderBase,Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader#BlockTermsReader(TermsIndexReaderBase,Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,int,String).mjava","sourceNew":"  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,\n                          String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    // this.segment = segment;\n    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),\n                       context);\n\n    boolean success = false;\n    try {\n      version = readHeader(in);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid number of fields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final long termsStartPointer = in.readVLong();\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate fields: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        in.close();\n      }\n    }\n\n    this.indexReader = indexReader;\n  }\n\n","sourceOld":"  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,\n                          int termsCacheSize, String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n    termsCache = new DoubleBarrelLRUCache<FieldAndTerm,BlockTermState>(termsCacheSize);\n\n    // this.segment = segment;\n    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),\n                       context);\n\n    boolean success = false;\n    try {\n      version = readHeader(in);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid number of fields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final long termsStartPointer = in.readVLong();\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate fields: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        in.close();\n      }\n    }\n\n    this.indexReader = indexReader;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1a88d7b0899b7d22dcbd4cf8ca35d9ec9850ab98","date":1377268487,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader#BlockTermsReader(TermsIndexReaderBase,Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader#BlockTermsReader(TermsIndexReaderBase,Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","sourceNew":"  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,\n                          String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    // this.segment = segment;\n    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),\n                       context);\n\n    boolean success = false;\n    try {\n      version = readHeader(in);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid number of fields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final long termsStartPointer = in.readVLong();\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount, longsSize));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate fields: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        in.close();\n      }\n    }\n\n    this.indexReader = indexReader;\n  }\n\n","sourceOld":"  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,\n                          String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    // this.segment = segment;\n    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),\n                       context);\n\n    boolean success = false;\n    try {\n      version = readHeader(in);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid number of fields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final long termsStartPointer = in.readVLong();\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate fields: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        in.close();\n      }\n    }\n\n    this.indexReader = indexReader;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e3263230f04a1aa8d431d722fdfce583a9542c18","date":1377603209,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader#BlockTermsReader(TermsIndexReaderBase,Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader#BlockTermsReader(TermsIndexReaderBase,Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","sourceNew":"  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,\n                          String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    // this.segment = segment;\n    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),\n                       context);\n\n    boolean success = false;\n    try {\n      version = readHeader(in);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid number of fields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final long termsStartPointer = in.readVLong();\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount, longsSize));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate fields: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        in.close();\n      }\n    }\n\n    this.indexReader = indexReader;\n  }\n\n","sourceOld":"  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,\n                          String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    // this.segment = segment;\n    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),\n                       context);\n\n    boolean success = false;\n    try {\n      version = readHeader(in);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid number of fields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final long termsStartPointer = in.readVLong();\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount, longsSize));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate fields: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        in.close();\n      }\n    }\n\n    this.indexReader = indexReader;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2f948dd442d23baa6cbb28daf77c8db78b351329","date":1378742876,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader#BlockTermsReader(TermsIndexReaderBase,Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader#BlockTermsReader(TermsIndexReaderBase,Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","sourceNew":"  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,\n                          String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    // this.segment = segment;\n    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),\n                       context);\n\n    boolean success = false;\n    try {\n      version = readHeader(in);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid number of fields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final long termsStartPointer = in.readVLong();\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount, longsSize));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate fields: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        in.close();\n      }\n    }\n\n    this.indexReader = indexReader;\n  }\n\n","sourceOld":"  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,\n                          String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    // this.segment = segment;\n    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),\n                       context);\n\n    boolean success = false;\n    try {\n      version = readHeader(in);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid number of fields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final long termsStartPointer = in.readVLong();\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate fields: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        in.close();\n      }\n    }\n\n    this.indexReader = indexReader;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ac34f0c5bb9274821fb0cb18075234e02002e9bf","date":1402508126,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader#BlockTermsReader(TermsIndexReaderBase,Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader#BlockTermsReader(TermsIndexReaderBase,Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","sourceNew":"  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,\n                          String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = segment;\n    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),\n                       context);\n\n    boolean success = false;\n    try {\n      version = readHeader(in);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid number of fields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final long termsStartPointer = in.readVLong();\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount, longsSize));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate fields: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        in.close();\n      }\n    }\n\n    this.indexReader = indexReader;\n  }\n\n","sourceOld":"  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,\n                          String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    // this.segment = segment;\n    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),\n                       context);\n\n    boolean success = false;\n    try {\n      version = readHeader(in);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid number of fields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final long termsStartPointer = in.readVLong();\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount, longsSize));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate fields: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        in.close();\n      }\n    }\n\n    this.indexReader = indexReader;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0c8cfaac638acc80d26d08288440ede37e6539e9","date":1402678000,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader#BlockTermsReader(TermsIndexReaderBase,Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader#BlockTermsReader(TermsIndexReaderBase,Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","sourceNew":"  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,\n                          String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    // this.segment = segment;\n    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),\n                       context);\n\n    boolean success = false;\n    try {\n      version = readHeader(in);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid number of fields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final long termsStartPointer = in.readVLong();\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount, longsSize));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate fields: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        in.close();\n      }\n    }\n\n    this.indexReader = indexReader;\n  }\n\n","sourceOld":"  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,\n                          String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = segment;\n    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),\n                       context);\n\n    boolean success = false;\n    try {\n      version = readHeader(in);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid number of fields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final long termsStartPointer = in.readVLong();\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount, longsSize));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate fields: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        in.close();\n      }\n    }\n\n    this.indexReader = indexReader;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4b612f3f700a1ca999f12198b7a33c65b4a96fd0","date":1406127397,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader#BlockTermsReader(TermsIndexReaderBase,Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader#BlockTermsReader(TermsIndexReaderBase,Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","sourceNew":"  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,\n                          String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    // this.segment = segment;\n    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),\n                       context);\n\n    boolean success = false;\n    try {\n      version = readHeader(in);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n      \n      if (version >= BlockTermsWriter.VERSION_CHECKSUM) {      \n        // NOTE: data file is too costly to verify checksum against all the bytes on open,\n        // but for now we at least verify proper structure of the checksum footer: which looks\n        // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n        // such as file truncation.\n        CodecUtil.retrieveChecksum(in);\n      }\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid number of fields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final long termsStartPointer = in.readVLong();\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount, longsSize));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate fields: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        in.close();\n      }\n    }\n\n    this.indexReader = indexReader;\n  }\n\n","sourceOld":"  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,\n                          String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    // this.segment = segment;\n    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),\n                       context);\n\n    boolean success = false;\n    try {\n      version = readHeader(in);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid number of fields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final long termsStartPointer = in.readVLong();\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount, longsSize));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate fields: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        in.close();\n      }\n    }\n\n    this.indexReader = indexReader;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9a70ce9bddc6f985feb8e5e182aebe20872328d4","date":1411172748,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader#BlockTermsReader(TermsIndexReaderBase,Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader#BlockTermsReader(TermsIndexReaderBase,Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","sourceNew":"  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,\n                          String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    // this.segment = segment;\n    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),\n                       context);\n\n    boolean success = false;\n    try {\n      version = readHeader(in);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n      \n      if (version >= BlockTermsWriter.VERSION_CHECKSUM) {      \n        // NOTE: data file is too costly to verify checksum against all the bytes on open,\n        // but for now we at least verify proper structure of the checksum footer: which looks\n        // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n        // such as file truncation.\n        CodecUtil.retrieveChecksum(in);\n      }\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid number of fields: \" + numFields, in);\n      }\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final long termsStartPointer = in.readVLong();\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount, longsSize));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate fields: \" + fieldInfo.name, in);\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        in.close();\n      }\n    }\n\n    this.indexReader = indexReader;\n  }\n\n","sourceOld":"  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,\n                          String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    // this.segment = segment;\n    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),\n                       context);\n\n    boolean success = false;\n    try {\n      version = readHeader(in);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n      \n      if (version >= BlockTermsWriter.VERSION_CHECKSUM) {      \n        // NOTE: data file is too costly to verify checksum against all the bytes on open,\n        // but for now we at least verify proper structure of the checksum footer: which looks\n        // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n        // such as file truncation.\n        CodecUtil.retrieveChecksum(in);\n      }\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid number of fields: \" + numFields + \" (resource=\" + in + \")\");\n      }\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final long termsStartPointer = in.readVLong();\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount() + \" (resource=\" + in + \")\");\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount + \" (resource=\" + in + \")\");\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq + \" (resource=\" + in + \")\");\n        }\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount, longsSize));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate fields: \" + fieldInfo.name + \" (resource=\" + in + \")\");\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        in.close();\n      }\n    }\n\n    this.indexReader = indexReader;\n  }\n\n","bugFix":["8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8d35e791a979f62bb7484ccd9f83fa833c1606e8","date":1412164445,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader#BlockTermsReader(TermsIndexReaderBase,Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader#BlockTermsReader(TermsIndexReaderBase,Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","sourceNew":"  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,\n                          String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    // this.segment = segment;\n    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),\n                       context);\n\n    boolean success = false;\n    try {\n      CodecUtil.checkSegmentHeader(in, BlockTermsWriter.CODEC_NAME, \n                                       BlockTermsWriter.VERSION_START,\n                                       BlockTermsWriter.VERSION_CURRENT,\n                                       info.getId(), segmentSuffix);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid number of fields: \" + numFields, in);\n      }\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final long termsStartPointer = in.readVLong();\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount, longsSize));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate fields: \" + fieldInfo.name, in);\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        in.close();\n      }\n    }\n\n    this.indexReader = indexReader;\n  }\n\n","sourceOld":"  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,\n                          String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    // this.segment = segment;\n    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),\n                       context);\n\n    boolean success = false;\n    try {\n      version = readHeader(in);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n      \n      if (version >= BlockTermsWriter.VERSION_CHECKSUM) {      \n        // NOTE: data file is too costly to verify checksum against all the bytes on open,\n        // but for now we at least verify proper structure of the checksum footer: which looks\n        // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n        // such as file truncation.\n        CodecUtil.retrieveChecksum(in);\n      }\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid number of fields: \" + numFields, in);\n      }\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final long termsStartPointer = in.readVLong();\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount, longsSize));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate fields: \" + fieldInfo.name, in);\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        in.close();\n      }\n    }\n\n    this.indexReader = indexReader;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cdfd4a8fd5df3958475e4dde3633adb237373a27","date":1412166431,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader#BlockTermsReader(TermsIndexReaderBase,PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader#BlockTermsReader(TermsIndexReaderBase,Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","sourceNew":"  public BlockTermsReader(TermsIndexReaderBase indexReader, PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n    \n    String filename = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, BlockTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(filename, state.context);\n\n    boolean success = false;\n    try {\n      CodecUtil.checkSegmentHeader(in, BlockTermsWriter.CODEC_NAME, \n                                       BlockTermsWriter.VERSION_START,\n                                       BlockTermsWriter.VERSION_CURRENT,\n                                       state.segmentInfo.getId(), state.segmentSuffix);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid number of fields: \" + numFields, in);\n      }\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final long termsStartPointer = in.readVLong();\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount, longsSize));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate fields: \" + fieldInfo.name, in);\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        in.close();\n      }\n    }\n\n    this.indexReader = indexReader;\n  }\n\n","sourceOld":"  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,\n                          String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    // this.segment = segment;\n    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),\n                       context);\n\n    boolean success = false;\n    try {\n      CodecUtil.checkSegmentHeader(in, BlockTermsWriter.CODEC_NAME, \n                                       BlockTermsWriter.VERSION_START,\n                                       BlockTermsWriter.VERSION_CURRENT,\n                                       info.getId(), segmentSuffix);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid number of fields: \" + numFields, in);\n      }\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final long termsStartPointer = in.readVLong();\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount, longsSize));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate fields: \" + fieldInfo.name, in);\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        in.close();\n      }\n    }\n\n    this.indexReader = indexReader;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9bb9a29a5e71a90295f175df8919802993142c9a","date":1412517673,"type":5,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader#BlockTermsReader(TermsIndexReaderBase,PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader#BlockTermsReader(TermsIndexReaderBase,Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","sourceNew":"  public BlockTermsReader(TermsIndexReaderBase indexReader, PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n    \n    String filename = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, BlockTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(filename, state.context);\n\n    boolean success = false;\n    try {\n      CodecUtil.checkSegmentHeader(in, BlockTermsWriter.CODEC_NAME, \n                                       BlockTermsWriter.VERSION_START,\n                                       BlockTermsWriter.VERSION_CURRENT,\n                                       state.segmentInfo.getId(), state.segmentSuffix);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid number of fields: \" + numFields, in);\n      }\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final long termsStartPointer = in.readVLong();\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount, longsSize));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate fields: \" + fieldInfo.name, in);\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        in.close();\n      }\n    }\n\n    this.indexReader = indexReader;\n  }\n\n","sourceOld":"  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,\n                          String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    // this.segment = segment;\n    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),\n                       context);\n\n    boolean success = false;\n    try {\n      version = readHeader(in);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n      \n      if (version >= BlockTermsWriter.VERSION_CHECKSUM) {      \n        // NOTE: data file is too costly to verify checksum against all the bytes on open,\n        // but for now we at least verify proper structure of the checksum footer: which looks\n        // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n        // such as file truncation.\n        CodecUtil.retrieveChecksum(in);\n      }\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid number of fields: \" + numFields, in);\n      }\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        final long termsStartPointer = in.readVLong();\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount, longsSize));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate fields: \" + fieldInfo.name, in);\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        in.close();\n      }\n    }\n\n    this.indexReader = indexReader;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"cdfd4a8fd5df3958475e4dde3633adb237373a27":["8d35e791a979f62bb7484ccd9f83fa833c1606e8"],"4b612f3f700a1ca999f12198b7a33c65b4a96fd0":["2f948dd442d23baa6cbb28daf77c8db78b351329"],"0c8cfaac638acc80d26d08288440ede37e6539e9":["ac34f0c5bb9274821fb0cb18075234e02002e9bf"],"e3263230f04a1aa8d431d722fdfce583a9542c18":["1a88d7b0899b7d22dcbd4cf8ca35d9ec9850ab98"],"1a88d7b0899b7d22dcbd4cf8ca35d9ec9850ab98":["8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee"],"9a70ce9bddc6f985feb8e5e182aebe20872328d4":["4b612f3f700a1ca999f12198b7a33c65b4a96fd0"],"8d35e791a979f62bb7484ccd9f83fa833c1606e8":["9a70ce9bddc6f985feb8e5e182aebe20872328d4"],"9bb9a29a5e71a90295f175df8919802993142c9a":["9a70ce9bddc6f985feb8e5e182aebe20872328d4","cdfd4a8fd5df3958475e4dde3633adb237373a27"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"ac34f0c5bb9274821fb0cb18075234e02002e9bf":["2f948dd442d23baa6cbb28daf77c8db78b351329"],"eee5f2a24465d2c9a5f86ab84b7c35041a30fda8":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"2f948dd442d23baa6cbb28daf77c8db78b351329":["eee5f2a24465d2c9a5f86ab84b7c35041a30fda8","e3263230f04a1aa8d431d722fdfce583a9542c18"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9bb9a29a5e71a90295f175df8919802993142c9a"]},"commit2Childs":{"cdfd4a8fd5df3958475e4dde3633adb237373a27":["9bb9a29a5e71a90295f175df8919802993142c9a"],"4b612f3f700a1ca999f12198b7a33c65b4a96fd0":["9a70ce9bddc6f985feb8e5e182aebe20872328d4"],"0c8cfaac638acc80d26d08288440ede37e6539e9":[],"e3263230f04a1aa8d431d722fdfce583a9542c18":["2f948dd442d23baa6cbb28daf77c8db78b351329"],"1a88d7b0899b7d22dcbd4cf8ca35d9ec9850ab98":["e3263230f04a1aa8d431d722fdfce583a9542c18"],"8d35e791a979f62bb7484ccd9f83fa833c1606e8":["cdfd4a8fd5df3958475e4dde3633adb237373a27"],"9a70ce9bddc6f985feb8e5e182aebe20872328d4":["8d35e791a979f62bb7484ccd9f83fa833c1606e8","9bb9a29a5e71a90295f175df8919802993142c9a"],"9bb9a29a5e71a90295f175df8919802993142c9a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","eee5f2a24465d2c9a5f86ab84b7c35041a30fda8"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["1a88d7b0899b7d22dcbd4cf8ca35d9ec9850ab98"],"ac34f0c5bb9274821fb0cb18075234e02002e9bf":["0c8cfaac638acc80d26d08288440ede37e6539e9"],"2f948dd442d23baa6cbb28daf77c8db78b351329":["4b612f3f700a1ca999f12198b7a33c65b4a96fd0","ac34f0c5bb9274821fb0cb18075234e02002e9bf"],"eee5f2a24465d2c9a5f86ab84b7c35041a30fda8":["2f948dd442d23baa6cbb28daf77c8db78b351329"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["0c8cfaac638acc80d26d08288440ede37e6539e9","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}