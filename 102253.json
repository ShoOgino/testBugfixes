{"path":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","commits":[{"id":"955c32f886db6f6356c9fcdea6b1f1cb4effda24","date":1270581567,"type":0,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","pathOld":"/dev/null","sourceNew":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n\n    if (mergeState.fieldInfo.omitTermFreqAndPositions) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(mergeState.multiDeletedDocs, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final int numDocs = postingsConsumer.merge(mergeState, docsEnum);\n          if (numDocs > 0) {\n            finishTerm(term, numDocs);\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(mergeState.multiDeletedDocs, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final int numDocs = postingsConsumer.merge(mergeState, postingsEnum);\n          if (numDocs > 0) {\n            finishTerm(term, numDocs);\n          }\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"433777d1eaf9998136cd16515dc0e1eb26f5d535","date":1273839120,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","sourceNew":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n\n    if (mergeState.fieldInfo.omitTermFreqAndPositions) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(mergeState.multiDeletedDocs, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final int numDocs = postingsConsumer.merge(mergeState, docsEnum);\n          if (numDocs > 0) {\n            finishTerm(term, numDocs);\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(mergeState.multiDeletedDocs, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final int numDocs = postingsConsumer.merge(mergeState, postingsEnum);\n          if (numDocs > 0) {\n            finishTerm(term, numDocs);\n          }\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n\n    if (mergeState.fieldInfo.omitTermFreqAndPositions) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(mergeState.multiDeletedDocs, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final int numDocs = postingsConsumer.merge(mergeState, docsEnum);\n          if (numDocs > 0) {\n            finishTerm(term, numDocs);\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(mergeState.multiDeletedDocs, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final int numDocs = postingsConsumer.merge(mergeState, postingsEnum);\n          if (numDocs > 0) {\n            finishTerm(term, numDocs);\n          }\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6ecd298fdc085e7eba27afa7fae58df1ba1a2808","date":1295102557,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","sourceNew":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n\n    if (mergeState.fieldInfo.omitTermFreqAndPositions) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(mergeState.multiDeletedDocs, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(mergeState.multiDeletedDocs, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq);\n  }\n\n","sourceOld":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n\n    if (mergeState.fieldInfo.omitTermFreqAndPositions) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(mergeState.multiDeletedDocs, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final int numDocs = postingsConsumer.merge(mergeState, docsEnum);\n          if (numDocs > 0) {\n            finishTerm(term, numDocs);\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(mergeState.multiDeletedDocs, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final int numDocs = postingsConsumer.merge(mergeState, postingsEnum);\n          if (numDocs > 0) {\n            finishTerm(term, numDocs);\n          }\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"16843358872ed92ba92888ab99df297550b9a36a","date":1295144724,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","sourceNew":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n\n    if (mergeState.fieldInfo.omitTermFreqAndPositions) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(mergeState.multiDeletedDocs, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(mergeState.multiDeletedDocs, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq);\n  }\n\n","sourceOld":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n\n    if (mergeState.fieldInfo.omitTermFreqAndPositions) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(mergeState.multiDeletedDocs, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final int numDocs = postingsConsumer.merge(mergeState, docsEnum);\n          if (numDocs > 0) {\n            finishTerm(term, numDocs);\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(mergeState.multiDeletedDocs, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final int numDocs = postingsConsumer.merge(mergeState, postingsEnum);\n          if (numDocs > 0) {\n            finishTerm(term, numDocs);\n          }\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e623f9a0e45508ab149c2fb3e0fd0c2503f98186","date":1295889977,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","sourceNew":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDF = 0;\n\n    if (mergeState.fieldInfo.omitTermFreqAndPositions) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(mergeState.multiDeletedDocs, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumDF += stats.docFreq;\n            if (sumDF > 60000) {\n              mergeState.checkAbort.work(sumDF/5.0);\n              sumDF = 0;\n            }\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(mergeState.multiDeletedDocs, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDF += stats.docFreq;\n            if (sumDF > 60000) {\n              mergeState.checkAbort.work(sumDF/5.0);\n              sumDF = 0;\n            }\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq);\n  }\n\n","sourceOld":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n\n    if (mergeState.fieldInfo.omitTermFreqAndPositions) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(mergeState.multiDeletedDocs, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(mergeState.multiDeletedDocs, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bb9b72f7c3d7827c64dd4ec580ded81778da361d","date":1295897920,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","sourceNew":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDF = 0;\n\n    if (mergeState.fieldInfo.omitTermFreqAndPositions) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(mergeState.multiDeletedDocs, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumDF += stats.docFreq;\n            if (sumDF > 60000) {\n              mergeState.checkAbort.work(sumDF/5.0);\n              sumDF = 0;\n            }\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(mergeState.multiDeletedDocs, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDF += stats.docFreq;\n            if (sumDF > 60000) {\n              mergeState.checkAbort.work(sumDF/5.0);\n              sumDF = 0;\n            }\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq);\n  }\n\n","sourceOld":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n\n    if (mergeState.fieldInfo.omitTermFreqAndPositions) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(mergeState.multiDeletedDocs, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(mergeState.multiDeletedDocs, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","sourceNew":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDF = 0;\n\n    if (mergeState.fieldInfo.omitTermFreqAndPositions) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(mergeState.multiDeletedDocs, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumDF += stats.docFreq;\n            if (sumDF > 60000) {\n              mergeState.checkAbort.work(sumDF/5.0);\n              sumDF = 0;\n            }\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(mergeState.multiDeletedDocs, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDF += stats.docFreq;\n            if (sumDF > 60000) {\n              mergeState.checkAbort.work(sumDF/5.0);\n              sumDF = 0;\n            }\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq);\n  }\n\n","sourceOld":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n\n    if (mergeState.fieldInfo.omitTermFreqAndPositions) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(mergeState.multiDeletedDocs, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final int numDocs = postingsConsumer.merge(mergeState, docsEnum);\n          if (numDocs > 0) {\n            finishTerm(term, numDocs);\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(mergeState.multiDeletedDocs, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final int numDocs = postingsConsumer.merge(mergeState, postingsEnum);\n          if (numDocs > 0) {\n            finishTerm(term, numDocs);\n          }\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","date":1309960478,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","sourceNew":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDF = 0;\n\n    if (mergeState.fieldInfo.omitTermFreqAndPositions) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(mergeState.multiLiveDocs, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumDF += stats.docFreq;\n            if (sumDF > 60000) {\n              mergeState.checkAbort.work(sumDF/5.0);\n              sumDF = 0;\n            }\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(mergeState.multiLiveDocs, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDF += stats.docFreq;\n            if (sumDF > 60000) {\n              mergeState.checkAbort.work(sumDF/5.0);\n              sumDF = 0;\n            }\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq);\n  }\n\n","sourceOld":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDF = 0;\n\n    if (mergeState.fieldInfo.omitTermFreqAndPositions) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(mergeState.multiDeletedDocs, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumDF += stats.docFreq;\n            if (sumDF > 60000) {\n              mergeState.checkAbort.work(sumDF/5.0);\n              sumDF = 0;\n            }\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(mergeState.multiDeletedDocs, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDF += stats.docFreq;\n            if (sumDF > 60000) {\n              mergeState.checkAbort.work(sumDF/5.0);\n              sumDF = 0;\n            }\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"817d8435e9135b756f08ce6710ab0baac51bdf88","date":1309986993,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","sourceNew":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDF = 0;\n\n    if (mergeState.fieldInfo.omitTermFreqAndPositions) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(mergeState.multiLiveDocs, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumDF += stats.docFreq;\n            if (sumDF > 60000) {\n              mergeState.checkAbort.work(sumDF/5.0);\n              sumDF = 0;\n            }\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(mergeState.multiLiveDocs, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDF += stats.docFreq;\n            if (sumDF > 60000) {\n              mergeState.checkAbort.work(sumDF/5.0);\n              sumDF = 0;\n            }\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq);\n  }\n\n","sourceOld":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDF = 0;\n\n    if (mergeState.fieldInfo.omitTermFreqAndPositions) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(mergeState.multiDeletedDocs, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumDF += stats.docFreq;\n            if (sumDF > 60000) {\n              mergeState.checkAbort.work(sumDF/5.0);\n              sumDF = 0;\n            }\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(mergeState.multiDeletedDocs, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDF += stats.docFreq;\n            if (sumDF > 60000) {\n              mergeState.checkAbort.work(sumDF/5.0);\n              sumDF = 0;\n            }\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","sourceNew":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDF = 0;\n\n    if (mergeState.fieldInfo.omitTermFreqAndPositions) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(mergeState.multiLiveDocs, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumDF += stats.docFreq;\n            if (sumDF > 60000) {\n              mergeState.checkAbort.work(sumDF/5.0);\n              sumDF = 0;\n            }\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(mergeState.multiLiveDocs, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDF += stats.docFreq;\n            if (sumDF > 60000) {\n              mergeState.checkAbort.work(sumDF/5.0);\n              sumDF = 0;\n            }\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq);\n  }\n\n","sourceOld":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDF = 0;\n\n    if (mergeState.fieldInfo.omitTermFreqAndPositions) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(mergeState.multiDeletedDocs, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumDF += stats.docFreq;\n            if (sumDF > 60000) {\n              mergeState.checkAbort.work(sumDF/5.0);\n              sumDF = 0;\n            }\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(mergeState.multiDeletedDocs, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDF += stats.docFreq;\n            if (sumDF > 60000) {\n              mergeState.checkAbort.work(sumDF/5.0);\n              sumDF = 0;\n            }\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f4f2f9fd0a641ccc0cc6a4fb4e53d7ec1ab14a94","date":1310159023,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","sourceNew":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n    long sumDFsinceLastAbortCheck = 0;\n\n    if (mergeState.fieldInfo.omitTermFreqAndPositions) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(mergeState.multiLiveDocs, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(mergeState.multiLiveDocs, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq, sumDocFreq);\n  }\n\n","sourceOld":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDF = 0;\n\n    if (mergeState.fieldInfo.omitTermFreqAndPositions) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(mergeState.multiLiveDocs, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumDF += stats.docFreq;\n            if (sumDF > 60000) {\n              mergeState.checkAbort.work(sumDF/5.0);\n              sumDF = 0;\n            }\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(mergeState.multiLiveDocs, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDF += stats.docFreq;\n            if (sumDF > 60000) {\n              mergeState.checkAbort.work(sumDF/5.0);\n              sumDF = 0;\n            }\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"63639dd66fd5bd9b90bc24dd596ae01575f27cc4","date":1310237454,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","sourceNew":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n    long sumDFsinceLastAbortCheck = 0;\n\n    if (mergeState.fieldInfo.omitTermFreqAndPositions) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(mergeState.multiLiveDocs, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(mergeState.multiLiveDocs, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq, sumDocFreq);\n  }\n\n","sourceOld":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDF = 0;\n\n    if (mergeState.fieldInfo.omitTermFreqAndPositions) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(mergeState.multiLiveDocs, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumDF += stats.docFreq;\n            if (sumDF > 60000) {\n              mergeState.checkAbort.work(sumDF/5.0);\n              sumDF = 0;\n            }\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(mergeState.multiLiveDocs, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDF += stats.docFreq;\n            if (sumDF > 60000) {\n              mergeState.checkAbort.work(sumDF/5.0);\n              sumDF = 0;\n            }\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2afd23a6f1242190c3409d8d81d5c5912d607fc9","date":1310477482,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","sourceNew":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n    long sumDFsinceLastAbortCheck = 0;\n\n    if (mergeState.fieldInfo.indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(mergeState.multiLiveDocs, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(mergeState.multiLiveDocs, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq, sumDocFreq);\n  }\n\n","sourceOld":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n    long sumDFsinceLastAbortCheck = 0;\n\n    if (mergeState.fieldInfo.omitTermFreqAndPositions) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(mergeState.multiLiveDocs, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(mergeState.multiLiveDocs, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq, sumDocFreq);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d53e0e0de54796610619b32f911e89d9fb752c4c","date":1310918942,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","sourceNew":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n    long sumDFsinceLastAbortCheck = 0;\n\n    if (mergeState.fieldInfo.indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(null, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq, sumDocFreq);\n  }\n\n","sourceOld":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n    long sumDFsinceLastAbortCheck = 0;\n\n    if (mergeState.fieldInfo.indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(mergeState.multiLiveDocs, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(mergeState.multiLiveDocs, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq, sumDocFreq);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"25433c5cacacb7a2055d62d4d36b0daf210e0a10","date":1315747522,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","sourceNew":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n    long sumDFsinceLastAbortCheck = 0;\n    FixedBitSet visitedDocs = new FixedBitSet(mergeState.mergedDocCount);\n\n    if (mergeState.fieldInfo.indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum, visitedDocs);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(null, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum, visitedDocs);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n    long sumDFsinceLastAbortCheck = 0;\n\n    if (mergeState.fieldInfo.indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(null, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq, sumDocFreq);\n  }\n\n","bugFix":null,"bugIntro":["29e23e367cc757f42cdfce2bcbf21e68cd209cda"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"06584e6e98d592b34e1329b384182f368d2025e8","date":1320850353,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","sourceNew":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n    long sumDFsinceLastAbortCheck = 0;\n    FixedBitSet visitedDocs = new FixedBitSet(mergeState.mergedDocCount);\n\n    if (mergeState.fieldInfo.indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum, visitedDocs);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(null, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.payloadProcessorProvider != null) {\n            for (int i = 0; i < mergeState.readers.size(); i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum, visitedDocs);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n    long sumDFsinceLastAbortCheck = 0;\n    FixedBitSet visitedDocs = new FixedBitSet(mergeState.mergedDocCount);\n\n    if (mergeState.fieldInfo.indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum, visitedDocs);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(null, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.hasPayloadProcessorProvider) {\n            for (int i = 0; i < mergeState.readerCount; i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum, visitedDocs);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"872cff1d3a554e0cd64014cd97f88d3002b0f491","date":1323024658,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","sourceNew":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n    long sumDFsinceLastAbortCheck = 0;\n    FixedBitSet visitedDocs = new FixedBitSet(mergeState.mergedDocCount);\n\n    if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsEnumIn, false);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum, visitedDocs);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.docFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    } else if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS) {\n      if (docsAndFreqsEnum == null) {\n        docsAndFreqsEnum = new MappingMultiDocsEnum();\n      }\n      docsAndFreqsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsAndFreqsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsAndFreqsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsAndFreqsEnumIn, true);\n        assert docsAndFreqsEnumIn != null;\n        docsAndFreqsEnum.reset(docsAndFreqsEnumIn);\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, docsAndFreqsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    } else {\n      assert mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(null, postingsEnumIn);\n        assert postingsEnumIn != null;\n        postingsEnum.reset(postingsEnumIn);\n        // set PayloadProcessor\n        if (mergeState.payloadProcessorProvider != null) {\n          for (int i = 0; i < mergeState.readers.size(); i++) {\n            if (mergeState.dirPayloadProcessor[i] != null) {\n              mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n            }\n          }\n        }\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n    long sumDFsinceLastAbortCheck = 0;\n    FixedBitSet visitedDocs = new FixedBitSet(mergeState.mergedDocCount);\n\n    if (mergeState.fieldInfo.indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum, visitedDocs);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(null, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.payloadProcessorProvider != null) {\n            for (int i = 0; i < mergeState.readers.size(); i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum, visitedDocs);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":["02331260bb246364779cb6f04919ca47900d01bb"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b65b350ca9588f9fc76ce7d6804160d06c45ff42","date":1323026297,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","sourceNew":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n    long sumDFsinceLastAbortCheck = 0;\n    FixedBitSet visitedDocs = new FixedBitSet(mergeState.mergedDocCount);\n\n    if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsEnumIn, false);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum, visitedDocs);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.docFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    } else if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS) {\n      if (docsAndFreqsEnum == null) {\n        docsAndFreqsEnum = new MappingMultiDocsEnum();\n      }\n      docsAndFreqsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsAndFreqsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsAndFreqsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsAndFreqsEnumIn, true);\n        assert docsAndFreqsEnumIn != null;\n        docsAndFreqsEnum.reset(docsAndFreqsEnumIn);\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, docsAndFreqsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    } else {\n      assert mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(null, postingsEnumIn);\n        assert postingsEnumIn != null;\n        postingsEnum.reset(postingsEnumIn);\n        // set PayloadProcessor\n        if (mergeState.payloadProcessorProvider != null) {\n          for (int i = 0; i < mergeState.readers.size(); i++) {\n            if (mergeState.dirPayloadProcessor[i] != null) {\n              mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n            }\n          }\n        }\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n    long sumDFsinceLastAbortCheck = 0;\n    FixedBitSet visitedDocs = new FixedBitSet(mergeState.mergedDocCount);\n\n    if (mergeState.fieldInfo.indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsEnumIn);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum, visitedDocs);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    } else {\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(null, postingsEnumIn);\n        if (postingsEnumIn != null) {\n          postingsEnum.reset(postingsEnumIn);\n          // set PayloadProcessor\n          if (mergeState.payloadProcessorProvider != null) {\n            for (int i = 0; i < mergeState.readers.size(); i++) {\n              if (mergeState.dirPayloadProcessor[i] != null) {\n                mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n              }\n            }\n          }\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum, visitedDocs);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.totalTermFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0ae5e3ed1232483b7b8a014f175a5fe43595982","date":1324062192,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer#merge(MergeState,TermsEnum).mjava","sourceNew":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n    long sumDFsinceLastAbortCheck = 0;\n    FixedBitSet visitedDocs = new FixedBitSet(mergeState.mergedDocCount);\n\n    if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsEnumIn, false);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum, visitedDocs);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.docFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    } else if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS) {\n      if (docsAndFreqsEnum == null) {\n        docsAndFreqsEnum = new MappingMultiDocsEnum();\n      }\n      docsAndFreqsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsAndFreqsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsAndFreqsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsAndFreqsEnumIn, true);\n        assert docsAndFreqsEnumIn != null;\n        docsAndFreqsEnum.reset(docsAndFreqsEnumIn);\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, docsAndFreqsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    } else {\n      assert mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(null, postingsEnumIn);\n        assert postingsEnumIn != null;\n        postingsEnum.reset(postingsEnumIn);\n        // set PayloadProcessor\n        if (mergeState.payloadProcessorProvider != null) {\n          for (int i = 0; i < mergeState.readers.size(); i++) {\n            if (mergeState.dirPayloadProcessor[i] != null) {\n              mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n            }\n          }\n        }\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {\n\n    BytesRef term;\n    assert termsEnum != null;\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n    long sumDFsinceLastAbortCheck = 0;\n    FixedBitSet visitedDocs = new FixedBitSet(mergeState.mergedDocCount);\n\n    if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {\n      if (docsEnum == null) {\n        docsEnum = new MappingMultiDocsEnum();\n      }\n      docsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsEnumIn, false);\n        if (docsEnumIn != null) {\n          docsEnum.reset(docsEnumIn);\n          final PostingsConsumer postingsConsumer = startTerm(term);\n          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum, visitedDocs);\n          if (stats.docFreq > 0) {\n            finishTerm(term, stats);\n            sumTotalTermFreq += stats.docFreq;\n            sumDFsinceLastAbortCheck += stats.docFreq;\n            sumDocFreq += stats.docFreq;\n            if (sumDFsinceLastAbortCheck > 60000) {\n              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n              sumDFsinceLastAbortCheck = 0;\n            }\n          }\n        }\n      }\n    } else if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS) {\n      if (docsAndFreqsEnum == null) {\n        docsAndFreqsEnum = new MappingMultiDocsEnum();\n      }\n      docsAndFreqsEnum.setMergeState(mergeState);\n\n      MultiDocsEnum docsAndFreqsEnumIn = null;\n\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        docsAndFreqsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsAndFreqsEnumIn, true);\n        assert docsAndFreqsEnumIn != null;\n        docsAndFreqsEnum.reset(docsAndFreqsEnumIn);\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, docsAndFreqsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    } else {\n      assert mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;\n      if (postingsEnum == null) {\n        postingsEnum = new MappingMultiDocsAndPositionsEnum();\n      }\n      postingsEnum.setMergeState(mergeState);\n      MultiDocsAndPositionsEnum postingsEnumIn = null;\n      while((term = termsEnum.next()) != null) {\n        // We can pass null for liveDocs, because the\n        // mapping enum will skip the non-live docs:\n        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(null, postingsEnumIn);\n        assert postingsEnumIn != null;\n        postingsEnum.reset(postingsEnumIn);\n        // set PayloadProcessor\n        if (mergeState.payloadProcessorProvider != null) {\n          for (int i = 0; i < mergeState.readers.size(); i++) {\n            if (mergeState.dirPayloadProcessor[i] != null) {\n              mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);\n            }\n          }\n        }\n        final PostingsConsumer postingsConsumer = startTerm(term);\n        final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum, visitedDocs);\n        if (stats.docFreq > 0) {\n          finishTerm(term, stats);\n          sumTotalTermFreq += stats.totalTermFreq;\n          sumDFsinceLastAbortCheck += stats.docFreq;\n          sumDocFreq += stats.docFreq;\n          if (sumDFsinceLastAbortCheck > 60000) {\n            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);\n            sumDFsinceLastAbortCheck = 0;\n          }\n        }\n      }\n    }\n\n    finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"2afd23a6f1242190c3409d8d81d5c5912d607fc9":["f4f2f9fd0a641ccc0cc6a4fb4e53d7ec1ab14a94"],"25433c5cacacb7a2055d62d4d36b0daf210e0a10":["d53e0e0de54796610619b32f911e89d9fb752c4c"],"d53e0e0de54796610619b32f911e89d9fb752c4c":["2afd23a6f1242190c3409d8d81d5c5912d607fc9"],"06584e6e98d592b34e1329b384182f368d2025e8":["25433c5cacacb7a2055d62d4d36b0daf210e0a10"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["e623f9a0e45508ab149c2fb3e0fd0c2503f98186"],"e623f9a0e45508ab149c2fb3e0fd0c2503f98186":["6ecd298fdc085e7eba27afa7fae58df1ba1a2808"],"16843358872ed92ba92888ab99df297550b9a36a":["433777d1eaf9998136cd16515dc0e1eb26f5d535","6ecd298fdc085e7eba27afa7fae58df1ba1a2808"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"872cff1d3a554e0cd64014cd97f88d3002b0f491":["06584e6e98d592b34e1329b384182f368d2025e8"],"f4f2f9fd0a641ccc0cc6a4fb4e53d7ec1ab14a94":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"63639dd66fd5bd9b90bc24dd596ae01575f27cc4":["817d8435e9135b756f08ce6710ab0baac51bdf88","f4f2f9fd0a641ccc0cc6a4fb4e53d7ec1ab14a94"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["e623f9a0e45508ab149c2fb3e0fd0c2503f98186","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["e623f9a0e45508ab149c2fb3e0fd0c2503f98186","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"bb9b72f7c3d7827c64dd4ec580ded81778da361d":["16843358872ed92ba92888ab99df297550b9a36a","e623f9a0e45508ab149c2fb3e0fd0c2503f98186"],"b65b350ca9588f9fc76ce7d6804160d06c45ff42":["06584e6e98d592b34e1329b384182f368d2025e8","872cff1d3a554e0cd64014cd97f88d3002b0f491"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["433777d1eaf9998136cd16515dc0e1eb26f5d535","e623f9a0e45508ab149c2fb3e0fd0c2503f98186"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"433777d1eaf9998136cd16515dc0e1eb26f5d535":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"a0ae5e3ed1232483b7b8a014f175a5fe43595982":["872cff1d3a554e0cd64014cd97f88d3002b0f491"],"6ecd298fdc085e7eba27afa7fae58df1ba1a2808":["433777d1eaf9998136cd16515dc0e1eb26f5d535"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0ae5e3ed1232483b7b8a014f175a5fe43595982"]},"commit2Childs":{"2afd23a6f1242190c3409d8d81d5c5912d607fc9":["d53e0e0de54796610619b32f911e89d9fb752c4c"],"25433c5cacacb7a2055d62d4d36b0daf210e0a10":["06584e6e98d592b34e1329b384182f368d2025e8"],"d53e0e0de54796610619b32f911e89d9fb752c4c":["25433c5cacacb7a2055d62d4d36b0daf210e0a10"],"06584e6e98d592b34e1329b384182f368d2025e8":["872cff1d3a554e0cd64014cd97f88d3002b0f491","b65b350ca9588f9fc76ce7d6804160d06c45ff42"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["f4f2f9fd0a641ccc0cc6a4fb4e53d7ec1ab14a94","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88"],"e623f9a0e45508ab149c2fb3e0fd0c2503f98186":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","bb9b72f7c3d7827c64dd4ec580ded81778da361d","29ef99d61cda9641b6250bf9567329a6e65f901d"],"16843358872ed92ba92888ab99df297550b9a36a":["bb9b72f7c3d7827c64dd4ec580ded81778da361d"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["433777d1eaf9998136cd16515dc0e1eb26f5d535"],"f4f2f9fd0a641ccc0cc6a4fb4e53d7ec1ab14a94":["2afd23a6f1242190c3409d8d81d5c5912d607fc9","63639dd66fd5bd9b90bc24dd596ae01575f27cc4"],"872cff1d3a554e0cd64014cd97f88d3002b0f491":["b65b350ca9588f9fc76ce7d6804160d06c45ff42","a0ae5e3ed1232483b7b8a014f175a5fe43595982"],"63639dd66fd5bd9b90bc24dd596ae01575f27cc4":[],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"817d8435e9135b756f08ce6710ab0baac51bdf88":["63639dd66fd5bd9b90bc24dd596ae01575f27cc4"],"bb9b72f7c3d7827c64dd4ec580ded81778da361d":[],"b65b350ca9588f9fc76ce7d6804160d06c45ff42":[],"29ef99d61cda9641b6250bf9567329a6e65f901d":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"433777d1eaf9998136cd16515dc0e1eb26f5d535":["16843358872ed92ba92888ab99df297550b9a36a","29ef99d61cda9641b6250bf9567329a6e65f901d","6ecd298fdc085e7eba27afa7fae58df1ba1a2808"],"a0ae5e3ed1232483b7b8a014f175a5fe43595982":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"6ecd298fdc085e7eba27afa7fae58df1ba1a2808":["e623f9a0e45508ab149c2fb3e0fd0c2503f98186","16843358872ed92ba92888ab99df297550b9a36a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["63639dd66fd5bd9b90bc24dd596ae01575f27cc4","d083e83f225b11e5fdd900e83d26ddb385b6955c","bb9b72f7c3d7827c64dd4ec580ded81778da361d","b65b350ca9588f9fc76ce7d6804160d06c45ff42","29ef99d61cda9641b6250bf9567329a6e65f901d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}