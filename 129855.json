{"path":"lucene/sandbox/src/test/org/apache/lucene/search/intervals/TestPayloadFilteredInterval#testPayloadFilteredInterval().mjava","commits":[{"id":"6a89baad4cff5e4827dfb236be940e03a3f748d7","date":1552902060,"type":0,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/sandbox/src/test/org/apache/lucene/search/intervals/TestPayloadFilteredInterval#testPayloadFilteredInterval().mjava","pathOld":"/dev/null","sourceNew":"  public void testPayloadFilteredInterval() throws Exception {\n\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tok = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        return new TokenStreamComponents(tok, new SimplePayloadFilter(tok));\n      }\n    };\n\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000)).setMergePolicy(newLogMergePolicy()));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"a sentence with words repeated words words quite often words\", Field.Store.NO));\n    writer.addDocument(doc);\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    // SimplePayloadFilter stores a payload for each term at position n containing\n    // the bytes 'pos:n'\n\n    IntervalsSource source = Intervals.term(\"words\", b -> b.utf8ToString().endsWith(\"5\") == false);\n    assertEquals(\"PAYLOAD_FILTERED(words)\", source.toString());\n\n    IntervalIterator it = source.intervals(\"field\", reader.leaves().get(0));\n\n    assertEquals(0, it.nextDoc());\n    assertEquals(3, it.nextInterval());\n    assertEquals(6, it.nextInterval());\n    assertEquals(9, it.nextInterval());\n    assertEquals(IntervalIterator.NO_MORE_INTERVALS, it.nextInterval());\n\n    MatchesIterator mi = source.matches(\"field\", reader.leaves().get(0), 0);\n    assertNotNull(mi);\n    assertTrue(mi.next());\n    assertEquals(3, mi.startPosition());\n    assertTrue(mi.next());\n    assertEquals(6, mi.startPosition());\n    assertTrue(mi.next());\n    assertEquals(9, mi.startPosition());\n    assertFalse(mi.next());\n\n    reader.close();\n    directory.close();\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"97ee2282ff806e9bc9d705f389cf40451ab81c3e","date":1561721333,"type":5,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/intervals/TestPayloadFilteredInterval#testPayloadFilteredInterval().mjava","pathOld":"lucene/sandbox/src/test/org/apache/lucene/search/intervals/TestPayloadFilteredInterval#testPayloadFilteredInterval().mjava","sourceNew":"  public void testPayloadFilteredInterval() throws Exception {\n\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tok = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        return new TokenStreamComponents(tok, new SimplePayloadFilter(tok));\n      }\n    };\n\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000)).setMergePolicy(newLogMergePolicy()));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"a sentence with words repeated words words quite often words\", Field.Store.NO));\n    writer.addDocument(doc);\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    // SimplePayloadFilter stores a payload for each term at position n containing\n    // the bytes 'pos:n'\n\n    IntervalsSource source = Intervals.term(\"words\", b -> b.utf8ToString().endsWith(\"5\") == false);\n    assertEquals(\"PAYLOAD_FILTERED(words)\", source.toString());\n\n    IntervalIterator it = source.intervals(\"field\", reader.leaves().get(0));\n\n    assertEquals(0, it.nextDoc());\n    assertEquals(3, it.nextInterval());\n    assertEquals(6, it.nextInterval());\n    assertEquals(9, it.nextInterval());\n    assertEquals(IntervalIterator.NO_MORE_INTERVALS, it.nextInterval());\n\n    MatchesIterator mi = source.matches(\"field\", reader.leaves().get(0), 0);\n    assertNotNull(mi);\n    assertTrue(mi.next());\n    assertEquals(3, mi.startPosition());\n    assertTrue(mi.next());\n    assertEquals(6, mi.startPosition());\n    assertTrue(mi.next());\n    assertEquals(9, mi.startPosition());\n    assertFalse(mi.next());\n\n    reader.close();\n    directory.close();\n\n  }\n\n","sourceOld":"  public void testPayloadFilteredInterval() throws Exception {\n\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tok = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        return new TokenStreamComponents(tok, new SimplePayloadFilter(tok));\n      }\n    };\n\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000)).setMergePolicy(newLogMergePolicy()));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"a sentence with words repeated words words quite often words\", Field.Store.NO));\n    writer.addDocument(doc);\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    // SimplePayloadFilter stores a payload for each term at position n containing\n    // the bytes 'pos:n'\n\n    IntervalsSource source = Intervals.term(\"words\", b -> b.utf8ToString().endsWith(\"5\") == false);\n    assertEquals(\"PAYLOAD_FILTERED(words)\", source.toString());\n\n    IntervalIterator it = source.intervals(\"field\", reader.leaves().get(0));\n\n    assertEquals(0, it.nextDoc());\n    assertEquals(3, it.nextInterval());\n    assertEquals(6, it.nextInterval());\n    assertEquals(9, it.nextInterval());\n    assertEquals(IntervalIterator.NO_MORE_INTERVALS, it.nextInterval());\n\n    MatchesIterator mi = source.matches(\"field\", reader.leaves().get(0), 0);\n    assertNotNull(mi);\n    assertTrue(mi.next());\n    assertEquals(3, mi.startPosition());\n    assertTrue(mi.next());\n    assertEquals(6, mi.startPosition());\n    assertTrue(mi.next());\n    assertEquals(9, mi.startPosition());\n    assertFalse(mi.next());\n\n    reader.close();\n    directory.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"97ee2282ff806e9bc9d705f389cf40451ab81c3e":["6a89baad4cff5e4827dfb236be940e03a3f748d7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"6a89baad4cff5e4827dfb236be940e03a3f748d7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["97ee2282ff806e9bc9d705f389cf40451ab81c3e"]},"commit2Childs":{"97ee2282ff806e9bc9d705f389cf40451ab81c3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["6a89baad4cff5e4827dfb236be940e03a3f748d7"],"6a89baad4cff5e4827dfb236be940e03a3f748d7":["97ee2282ff806e9bc9d705f389cf40451ab81c3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}