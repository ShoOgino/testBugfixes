{"path":"lucene/contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","pathOld":"contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","sourceNew":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Map<String,Integer> tokenMap = new HashMap<String,Integer>();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = createAnalyzer();\n    for (Fieldable field : doc.getFields()) {\n      String fieldName = field.name();\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          TermAttribute termAtt = stream.addAttribute(TermAttribute.class);\n          PositionIncrementAttribute posIncrAtt = stream.addAttribute(PositionIncrementAttribute.class);\n          \n          try {\n            while (stream.incrementToken()) {\n              position += (posIncrAtt.getPositionIncrement() - 1);\n              position++;\n              String name = termAtt.term();\n              Integer Count = tokenMap.get(name);\n              if (Count == null) { // not in there yet\n                tokenMap.put(name, Integer.valueOf(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenMap.put(name, Integer.valueOf(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Map.Entry<String,Integer>[] sortedHash = getSortedMapEntries(tokenMap);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Map.Entry<String,Integer> currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","sourceOld":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Map<String,Integer> tokenMap = new HashMap<String,Integer>();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = createAnalyzer();\n    for (Fieldable field : doc.getFields()) {\n      String fieldName = field.name();\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          TermAttribute termAtt = stream.addAttribute(TermAttribute.class);\n          PositionIncrementAttribute posIncrAtt = stream.addAttribute(PositionIncrementAttribute.class);\n          \n          try {\n            while (stream.incrementToken()) {\n              position += (posIncrAtt.getPositionIncrement() - 1);\n              position++;\n              String name = termAtt.term();\n              Integer Count = tokenMap.get(name);\n              if (Count == null) { // not in there yet\n                tokenMap.put(name, Integer.valueOf(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenMap.put(name, Integer.valueOf(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Map.Entry<String,Integer>[] sortedHash = getSortedMapEntries(tokenMap);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Map.Entry<String,Integer> currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a7347509fad0711ac30cb15a746e9a3830a38ebd","date":1275388513,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","pathOld":"lucene/contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","sourceNew":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Map<String,Integer> tokenMap = new HashMap<String,Integer>();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = createAnalyzer();\n    for (Fieldable field : doc.getFields()) {\n      String fieldName = field.name();\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);\n          PositionIncrementAttribute posIncrAtt = stream.addAttribute(PositionIncrementAttribute.class);\n          \n          try {\n            while (stream.incrementToken()) {\n              position += (posIncrAtt.getPositionIncrement() - 1);\n              position++;\n              String name = termAtt.toString();\n              Integer Count = tokenMap.get(name);\n              if (Count == null) { // not in there yet\n                tokenMap.put(name, Integer.valueOf(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenMap.put(name, Integer.valueOf(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Map.Entry<String,Integer>[] sortedHash = getSortedMapEntries(tokenMap);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Map.Entry<String,Integer> currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","sourceOld":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Map<String,Integer> tokenMap = new HashMap<String,Integer>();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = createAnalyzer();\n    for (Fieldable field : doc.getFields()) {\n      String fieldName = field.name();\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          TermAttribute termAtt = stream.addAttribute(TermAttribute.class);\n          PositionIncrementAttribute posIncrAtt = stream.addAttribute(PositionIncrementAttribute.class);\n          \n          try {\n            while (stream.incrementToken()) {\n              position += (posIncrAtt.getPositionIncrement() - 1);\n              position++;\n              String name = termAtt.term();\n              Integer Count = tokenMap.get(name);\n              if (Count == null) { // not in there yet\n                tokenMap.put(name, Integer.valueOf(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenMap.put(name, Integer.valueOf(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Map.Entry<String,Integer>[] sortedHash = getSortedMapEntries(tokenMap);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Map.Entry<String,Integer> currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1a60ec39bd57efcc33334c405a8a63bdaefa512c","date":1305309033,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","pathOld":"lucene/contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","sourceNew":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Map<String,Integer> tokenMap = new HashMap<String,Integer>();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = createAnalyzer();\n    for (Fieldable field : doc.getFields()) {\n      String fieldName = field.name();\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.reusableTokenStream(fieldName, reader);\n          CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);\n          PositionIncrementAttribute posIncrAtt = stream.addAttribute(PositionIncrementAttribute.class);\n          \n          try {\n            stream.reset();\n            while (stream.incrementToken()) {\n              position += (posIncrAtt.getPositionIncrement() - 1);\n              position++;\n              String name = termAtt.toString();\n              Integer Count = tokenMap.get(name);\n              if (Count == null) { // not in there yet\n                tokenMap.put(name, Integer.valueOf(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenMap.put(name, Integer.valueOf(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n            stream.end();\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Map.Entry<String,Integer>[] sortedHash = getSortedMapEntries(tokenMap);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Map.Entry<String,Integer> currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","sourceOld":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Map<String,Integer> tokenMap = new HashMap<String,Integer>();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = createAnalyzer();\n    for (Fieldable field : doc.getFields()) {\n      String fieldName = field.name();\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);\n          PositionIncrementAttribute posIncrAtt = stream.addAttribute(PositionIncrementAttribute.class);\n          \n          try {\n            while (stream.incrementToken()) {\n              position += (posIncrAtt.getPositionIncrement() - 1);\n              position++;\n              String name = termAtt.toString();\n              Integer Count = tokenMap.get(name);\n              if (Count == null) { // not in there yet\n                tokenMap.put(name, Integer.valueOf(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenMap.put(name, Integer.valueOf(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Map.Entry<String,Integer>[] sortedHash = getSortedMapEntries(tokenMap);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Map.Entry<String,Integer> currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c3a8a449466c1ff7ce2274fe73dab487256964b4","date":1305735867,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","pathOld":"lucene/contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","sourceNew":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Map<String,Integer> tokenMap = new HashMap<String,Integer>();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = createAnalyzer();\n    for (Fieldable field : doc.getFields()) {\n      String fieldName = field.name();\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.reusableTokenStream(fieldName, reader);\n          CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);\n          PositionIncrementAttribute posIncrAtt = stream.addAttribute(PositionIncrementAttribute.class);\n          \n          try {\n            stream.reset();\n            while (stream.incrementToken()) {\n              position += (posIncrAtt.getPositionIncrement() - 1);\n              position++;\n              String name = termAtt.toString();\n              Integer Count = tokenMap.get(name);\n              if (Count == null) { // not in there yet\n                tokenMap.put(name, Integer.valueOf(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenMap.put(name, Integer.valueOf(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n            stream.end();\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Map.Entry<String,Integer>[] sortedHash = getSortedMapEntries(tokenMap);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Map.Entry<String,Integer> currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","sourceOld":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Map<String,Integer> tokenMap = new HashMap<String,Integer>();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = createAnalyzer();\n    for (Fieldable field : doc.getFields()) {\n      String fieldName = field.name();\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);\n          PositionIncrementAttribute posIncrAtt = stream.addAttribute(PositionIncrementAttribute.class);\n          \n          try {\n            while (stream.incrementToken()) {\n              position += (posIncrAtt.getPositionIncrement() - 1);\n              position++;\n              String name = termAtt.toString();\n              Integer Count = tokenMap.get(name);\n              if (Count == null) { // not in there yet\n                tokenMap.put(name, Integer.valueOf(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenMap.put(name, Integer.valueOf(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Map.Entry<String,Integer>[] sortedHash = getSortedMapEntries(tokenMap);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Map.Entry<String,Integer> currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","pathOld":"lucene/contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","sourceNew":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Map<String,Integer> tokenMap = new HashMap<String,Integer>();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = createAnalyzer();\n    for (Fieldable field : doc.getFields()) {\n      String fieldName = field.name();\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.reusableTokenStream(fieldName, reader);\n          CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);\n          PositionIncrementAttribute posIncrAtt = stream.addAttribute(PositionIncrementAttribute.class);\n          \n          try {\n            stream.reset();\n            while (stream.incrementToken()) {\n              position += (posIncrAtt.getPositionIncrement() - 1);\n              position++;\n              String name = termAtt.toString();\n              Integer Count = tokenMap.get(name);\n              if (Count == null) { // not in there yet\n                tokenMap.put(name, Integer.valueOf(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenMap.put(name, Integer.valueOf(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n            stream.end();\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Map.Entry<String,Integer>[] sortedHash = getSortedMapEntries(tokenMap);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Map.Entry<String,Integer> currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","sourceOld":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Map<String,Integer> tokenMap = new HashMap<String,Integer>();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = createAnalyzer();\n    for (Fieldable field : doc.getFields()) {\n      String fieldName = field.name();\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);\n          PositionIncrementAttribute posIncrAtt = stream.addAttribute(PositionIncrementAttribute.class);\n          \n          try {\n            while (stream.incrementToken()) {\n              position += (posIncrAtt.getPositionIncrement() - 1);\n              position++;\n              String name = termAtt.toString();\n              Integer Count = tokenMap.get(name);\n              if (Count == null) { // not in there yet\n                tokenMap.put(name, Integer.valueOf(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenMap.put(name, Integer.valueOf(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Map.Entry<String,Integer>[] sortedHash = getSortedMapEntries(tokenMap);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Map.Entry<String,Integer> currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a55c46ea262f9033bd9ab60542dea4b38abef33","date":1306109444,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","sourceNew":null,"sourceOld":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Map<String,Integer> tokenMap = new HashMap<String,Integer>();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = createAnalyzer();\n    for (Fieldable field : doc.getFields()) {\n      String fieldName = field.name();\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.reusableTokenStream(fieldName, reader);\n          CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);\n          PositionIncrementAttribute posIncrAtt = stream.addAttribute(PositionIncrementAttribute.class);\n          \n          try {\n            stream.reset();\n            while (stream.incrementToken()) {\n              position += (posIncrAtt.getPositionIncrement() - 1);\n              position++;\n              String name = termAtt.toString();\n              Integer Count = tokenMap.get(name);\n              if (Count == null) { // not in there yet\n                tokenMap.put(name, Integer.valueOf(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenMap.put(name, Integer.valueOf(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n            stream.end();\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Map.Entry<String,Integer>[] sortedHash = getSortedMapEntries(tokenMap);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Map.Entry<String,Integer> currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae132b768aece5bf21cda14e2f17fba66eb6f7d6","date":1306128032,"type":4,"author":"Steven Rowe","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","sourceNew":null,"sourceOld":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Map<String,Integer> tokenMap = new HashMap<String,Integer>();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = createAnalyzer();\n    for (Fieldable field : doc.getFields()) {\n      String fieldName = field.name();\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.reusableTokenStream(fieldName, reader);\n          CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);\n          PositionIncrementAttribute posIncrAtt = stream.addAttribute(PositionIncrementAttribute.class);\n          \n          try {\n            stream.reset();\n            while (stream.incrementToken()) {\n              position += (posIncrAtt.getPositionIncrement() - 1);\n              position++;\n              String name = termAtt.toString();\n              Integer Count = tokenMap.get(name);\n              if (Count == null) { // not in there yet\n                tokenMap.put(name, Integer.valueOf(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenMap.put(name, Integer.valueOf(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n            stream.end();\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Map.Entry<String,Integer>[] sortedHash = getSortedMapEntries(tokenMap);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Map.Entry<String,Integer> currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a","date":1306150983,"type":4,"author":"Simon Willnauer","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","sourceNew":null,"sourceOld":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Map<String,Integer> tokenMap = new HashMap<String,Integer>();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = createAnalyzer();\n    for (Fieldable field : doc.getFields()) {\n      String fieldName = field.name();\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.reusableTokenStream(fieldName, reader);\n          CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);\n          PositionIncrementAttribute posIncrAtt = stream.addAttribute(PositionIncrementAttribute.class);\n          \n          try {\n            stream.reset();\n            while (stream.incrementToken()) {\n              position += (posIncrAtt.getPositionIncrement() - 1);\n              position++;\n              String name = termAtt.toString();\n              Integer Count = tokenMap.get(name);\n              if (Count == null) { // not in there yet\n                tokenMap.put(name, Integer.valueOf(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenMap.put(name, Integer.valueOf(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n            stream.end();\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Map.Entry<String,Integer>[] sortedHash = getSortedMapEntries(tokenMap);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Map.Entry<String,Integer> currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"1a60ec39bd57efcc33334c405a8a63bdaefa512c":["a7347509fad0711ac30cb15a746e9a3830a38ebd"],"ae132b768aece5bf21cda14e2f17fba66eb6f7d6":["a3776dccca01c11e7046323cfad46a3b4a471233","3a55c46ea262f9033bd9ab60542dea4b38abef33"],"5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a":["c3a8a449466c1ff7ce2274fe73dab487256964b4","3a55c46ea262f9033bd9ab60542dea4b38abef33"],"c3a8a449466c1ff7ce2274fe73dab487256964b4":["a7347509fad0711ac30cb15a746e9a3830a38ebd","1a60ec39bd57efcc33334c405a8a63bdaefa512c"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a7347509fad0711ac30cb15a746e9a3830a38ebd","1a60ec39bd57efcc33334c405a8a63bdaefa512c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3a55c46ea262f9033bd9ab60542dea4b38abef33":["1a60ec39bd57efcc33334c405a8a63bdaefa512c"],"a7347509fad0711ac30cb15a746e9a3830a38ebd":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a55c46ea262f9033bd9ab60542dea4b38abef33"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"1a60ec39bd57efcc33334c405a8a63bdaefa512c":["c3a8a449466c1ff7ce2274fe73dab487256964b4","a3776dccca01c11e7046323cfad46a3b4a471233","3a55c46ea262f9033bd9ab60542dea4b38abef33"],"ae132b768aece5bf21cda14e2f17fba66eb6f7d6":[],"5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a":[],"c3a8a449466c1ff7ce2274fe73dab487256964b4":["5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a"],"a3776dccca01c11e7046323cfad46a3b4a471233":["ae132b768aece5bf21cda14e2f17fba66eb6f7d6"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"3a55c46ea262f9033bd9ab60542dea4b38abef33":["ae132b768aece5bf21cda14e2f17fba66eb6f7d6","5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a7347509fad0711ac30cb15a746e9a3830a38ebd":["1a60ec39bd57efcc33334c405a8a63bdaefa512c","c3a8a449466c1ff7ce2274fe73dab487256964b4","a3776dccca01c11e7046323cfad46a3b4a471233"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a7347509fad0711ac30cb15a746e9a3830a38ebd"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ae132b768aece5bf21cda14e2f17fba66eb6f7d6","5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}