{"path":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyQueryDeletes(BufferedUpdatesStream.SegmentState[]).mjava","commits":[{"id":"f4363cd33f6eff7fb4753574a441e2d18c1022a4","date":1498067235,"type":0,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyQueryDeletes(BufferedUpdatesStream.SegmentState[]).mjava","pathOld":"/dev/null","sourceNew":"  // Delete by query\n  private long applyQueryDeletes(BufferedUpdatesStream.SegmentState[] segStates) throws IOException {\n\n    if (deleteQueries.length == 0) {\n      return 0;\n    }\n\n    long startNS = System.nanoTime();\n\n    long delCount = 0;\n    for (BufferedUpdatesStream.SegmentState segState : segStates) {\n\n      if (delGen < segState.delGen) {\n        // segment is newer than this deletes packet\n        continue;\n      }\n      \n      if (segState.rld.refCount() == 1) {\n        // This means we are the only remaining reference to this segment, meaning\n        // it was merged away while we were running, so we can safely skip running\n        // because we will run on the newly merged segment next:\n        continue;\n      }\n\n      final LeafReaderContext readerContext = segState.reader.getContext();\n      for (int i = 0; i < deleteQueries.length; i++) {\n        Query query = deleteQueries[i];\n        int limit;\n        if (delGen == segState.delGen) {\n          assert privateSegment != null;\n          limit = deleteQueryLimits[i];\n        } else {\n          limit = Integer.MAX_VALUE;\n        }\n        final IndexSearcher searcher = new IndexSearcher(readerContext.reader());\n        searcher.setQueryCache(null);\n        final Weight weight = searcher.createNormalizedWeight(query, false);\n        final Scorer scorer = weight.scorer(readerContext);\n        if (scorer != null) {\n          final DocIdSetIterator it = scorer.iterator();\n\n          int docID;\n          while ((docID = it.nextDoc()) < limit)  {\n            if (segState.rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyQueryDeletes took %.2f msec for %d segments and %d queries; %d new deletions\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       segStates.length,\n                                       deleteQueries.length,\n                                       delCount));\n    }\n    \n    return delCount;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":0,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyQueryDeletes(BufferedUpdatesStream.SegmentState[]).mjava","pathOld":"/dev/null","sourceNew":"  // Delete by query\n  private long applyQueryDeletes(BufferedUpdatesStream.SegmentState[] segStates) throws IOException {\n\n    if (deleteQueries.length == 0) {\n      return 0;\n    }\n\n    long startNS = System.nanoTime();\n\n    long delCount = 0;\n    for (BufferedUpdatesStream.SegmentState segState : segStates) {\n\n      if (delGen < segState.delGen) {\n        // segment is newer than this deletes packet\n        continue;\n      }\n      \n      if (segState.rld.refCount() == 1) {\n        // This means we are the only remaining reference to this segment, meaning\n        // it was merged away while we were running, so we can safely skip running\n        // because we will run on the newly merged segment next:\n        continue;\n      }\n\n      final LeafReaderContext readerContext = segState.reader.getContext();\n      for (int i = 0; i < deleteQueries.length; i++) {\n        Query query = deleteQueries[i];\n        int limit;\n        if (delGen == segState.delGen) {\n          assert privateSegment != null;\n          limit = deleteQueryLimits[i];\n        } else {\n          limit = Integer.MAX_VALUE;\n        }\n        final IndexSearcher searcher = new IndexSearcher(readerContext.reader());\n        searcher.setQueryCache(null);\n        final Weight weight = searcher.createNormalizedWeight(query, false);\n        final Scorer scorer = weight.scorer(readerContext);\n        if (scorer != null) {\n          final DocIdSetIterator it = scorer.iterator();\n\n          int docID;\n          while ((docID = it.nextDoc()) < limit)  {\n            if (segState.rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyQueryDeletes took %.2f msec for %d segments and %d queries; %d new deletions\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       segStates.length,\n                                       deleteQueries.length,\n                                       delCount));\n    }\n    \n    return delCount;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyQueryDeletes(BufferedUpdatesStream.SegmentState[]).mjava","pathOld":"/dev/null","sourceNew":"  // Delete by query\n  private long applyQueryDeletes(BufferedUpdatesStream.SegmentState[] segStates) throws IOException {\n\n    if (deleteQueries.length == 0) {\n      return 0;\n    }\n\n    long startNS = System.nanoTime();\n\n    long delCount = 0;\n    for (BufferedUpdatesStream.SegmentState segState : segStates) {\n\n      if (delGen < segState.delGen) {\n        // segment is newer than this deletes packet\n        continue;\n      }\n      \n      if (segState.rld.refCount() == 1) {\n        // This means we are the only remaining reference to this segment, meaning\n        // it was merged away while we were running, so we can safely skip running\n        // because we will run on the newly merged segment next:\n        continue;\n      }\n\n      final LeafReaderContext readerContext = segState.reader.getContext();\n      for (int i = 0; i < deleteQueries.length; i++) {\n        Query query = deleteQueries[i];\n        int limit;\n        if (delGen == segState.delGen) {\n          assert privateSegment != null;\n          limit = deleteQueryLimits[i];\n        } else {\n          limit = Integer.MAX_VALUE;\n        }\n        final IndexSearcher searcher = new IndexSearcher(readerContext.reader());\n        searcher.setQueryCache(null);\n        final Weight weight = searcher.createNormalizedWeight(query, false);\n        final Scorer scorer = weight.scorer(readerContext);\n        if (scorer != null) {\n          final DocIdSetIterator it = scorer.iterator();\n\n          int docID;\n          while ((docID = it.nextDoc()) < limit)  {\n            if (segState.rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyQueryDeletes took %.2f msec for %d segments and %d queries; %d new deletions\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       segStates.length,\n                                       deleteQueries.length,\n                                       delCount));\n    }\n    \n    return delCount;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9fc47cb7b4346802411bb432f501ed0673d7119e","date":1512640179,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyQueryDeletes(BufferedUpdatesStream.SegmentState[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyQueryDeletes(BufferedUpdatesStream.SegmentState[]).mjava","sourceNew":"  // Delete by query\n  private long applyQueryDeletes(BufferedUpdatesStream.SegmentState[] segStates) throws IOException {\n\n    if (deleteQueries.length == 0) {\n      return 0;\n    }\n\n    long startNS = System.nanoTime();\n\n    long delCount = 0;\n    for (BufferedUpdatesStream.SegmentState segState : segStates) {\n\n      if (delGen < segState.delGen) {\n        // segment is newer than this deletes packet\n        continue;\n      }\n      \n      if (segState.rld.refCount() == 1) {\n        // This means we are the only remaining reference to this segment, meaning\n        // it was merged away while we were running, so we can safely skip running\n        // because we will run on the newly merged segment next:\n        continue;\n      }\n\n      final LeafReaderContext readerContext = segState.reader.getContext();\n      for (int i = 0; i < deleteQueries.length; i++) {\n        Query query = deleteQueries[i];\n        int limit;\n        if (delGen == segState.delGen) {\n          assert privateSegment != null;\n          limit = deleteQueryLimits[i];\n        } else {\n          limit = Integer.MAX_VALUE;\n        }\n        final IndexSearcher searcher = new IndexSearcher(readerContext.reader());\n        searcher.setQueryCache(null);\n        final Weight weight = searcher.createNormalizedWeight(query, ScoreMode.COMPLETE_NO_SCORES);\n        final Scorer scorer = weight.scorer(readerContext);\n        if (scorer != null) {\n          final DocIdSetIterator it = scorer.iterator();\n\n          int docID;\n          while ((docID = it.nextDoc()) < limit)  {\n            if (segState.rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyQueryDeletes took %.2f msec for %d segments and %d queries; %d new deletions\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       segStates.length,\n                                       deleteQueries.length,\n                                       delCount));\n    }\n    \n    return delCount;\n  }\n\n","sourceOld":"  // Delete by query\n  private long applyQueryDeletes(BufferedUpdatesStream.SegmentState[] segStates) throws IOException {\n\n    if (deleteQueries.length == 0) {\n      return 0;\n    }\n\n    long startNS = System.nanoTime();\n\n    long delCount = 0;\n    for (BufferedUpdatesStream.SegmentState segState : segStates) {\n\n      if (delGen < segState.delGen) {\n        // segment is newer than this deletes packet\n        continue;\n      }\n      \n      if (segState.rld.refCount() == 1) {\n        // This means we are the only remaining reference to this segment, meaning\n        // it was merged away while we were running, so we can safely skip running\n        // because we will run on the newly merged segment next:\n        continue;\n      }\n\n      final LeafReaderContext readerContext = segState.reader.getContext();\n      for (int i = 0; i < deleteQueries.length; i++) {\n        Query query = deleteQueries[i];\n        int limit;\n        if (delGen == segState.delGen) {\n          assert privateSegment != null;\n          limit = deleteQueryLimits[i];\n        } else {\n          limit = Integer.MAX_VALUE;\n        }\n        final IndexSearcher searcher = new IndexSearcher(readerContext.reader());\n        searcher.setQueryCache(null);\n        final Weight weight = searcher.createNormalizedWeight(query, false);\n        final Scorer scorer = weight.scorer(readerContext);\n        if (scorer != null) {\n          final DocIdSetIterator it = scorer.iterator();\n\n          int docID;\n          while ((docID = it.nextDoc()) < limit)  {\n            if (segState.rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyQueryDeletes took %.2f msec for %d segments and %d queries; %d new deletions\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       segStates.length,\n                                       deleteQueries.length,\n                                       delCount));\n    }\n    \n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"417142ff08fda9cf0b72d5133e63097a166c6458","date":1512729693,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyQueryDeletes(BufferedUpdatesStream.SegmentState[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyQueryDeletes(BufferedUpdatesStream.SegmentState[]).mjava","sourceNew":"  // Delete by query\n  private long applyQueryDeletes(BufferedUpdatesStream.SegmentState[] segStates) throws IOException {\n\n    if (deleteQueries.length == 0) {\n      return 0;\n    }\n\n    long startNS = System.nanoTime();\n\n    long delCount = 0;\n    for (BufferedUpdatesStream.SegmentState segState : segStates) {\n\n      if (delGen < segState.delGen) {\n        // segment is newer than this deletes packet\n        continue;\n      }\n      \n      if (segState.rld.refCount() == 1) {\n        // This means we are the only remaining reference to this segment, meaning\n        // it was merged away while we were running, so we can safely skip running\n        // because we will run on the newly merged segment next:\n        continue;\n      }\n\n      final LeafReaderContext readerContext = segState.reader.getContext();\n      for (int i = 0; i < deleteQueries.length; i++) {\n        Query query = deleteQueries[i];\n        int limit;\n        if (delGen == segState.delGen) {\n          assert privateSegment != null;\n          limit = deleteQueryLimits[i];\n        } else {\n          limit = Integer.MAX_VALUE;\n        }\n        final IndexSearcher searcher = new IndexSearcher(readerContext.reader());\n        searcher.setQueryCache(null);\n        final Weight weight = searcher.createNormalizedWeight(query, ScoreMode.COMPLETE_NO_SCORES);\n        final Scorer scorer = weight.scorer(readerContext);\n        if (scorer != null) {\n          final DocIdSetIterator it = scorer.iterator();\n\n          int docID;\n          while ((docID = it.nextDoc()) < limit)  {\n            if (segState.rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyQueryDeletes took %.2f msec for %d segments and %d queries; %d new deletions\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       segStates.length,\n                                       deleteQueries.length,\n                                       delCount));\n    }\n    \n    return delCount;\n  }\n\n","sourceOld":"  // Delete by query\n  private long applyQueryDeletes(BufferedUpdatesStream.SegmentState[] segStates) throws IOException {\n\n    if (deleteQueries.length == 0) {\n      return 0;\n    }\n\n    long startNS = System.nanoTime();\n\n    long delCount = 0;\n    for (BufferedUpdatesStream.SegmentState segState : segStates) {\n\n      if (delGen < segState.delGen) {\n        // segment is newer than this deletes packet\n        continue;\n      }\n      \n      if (segState.rld.refCount() == 1) {\n        // This means we are the only remaining reference to this segment, meaning\n        // it was merged away while we were running, so we can safely skip running\n        // because we will run on the newly merged segment next:\n        continue;\n      }\n\n      final LeafReaderContext readerContext = segState.reader.getContext();\n      for (int i = 0; i < deleteQueries.length; i++) {\n        Query query = deleteQueries[i];\n        int limit;\n        if (delGen == segState.delGen) {\n          assert privateSegment != null;\n          limit = deleteQueryLimits[i];\n        } else {\n          limit = Integer.MAX_VALUE;\n        }\n        final IndexSearcher searcher = new IndexSearcher(readerContext.reader());\n        searcher.setQueryCache(null);\n        final Weight weight = searcher.createNormalizedWeight(query, false);\n        final Scorer scorer = weight.scorer(readerContext);\n        if (scorer != null) {\n          final DocIdSetIterator it = scorer.iterator();\n\n          int docID;\n          while ((docID = it.nextDoc()) < limit)  {\n            if (segState.rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyQueryDeletes took %.2f msec for %d segments and %d queries; %d new deletions\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       segStates.length,\n                                       deleteQueries.length,\n                                       delCount));\n    }\n    \n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"475584d5e08a22ad3fc7babefe006d77bc744567","date":1523282824,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyQueryDeletes(BufferedUpdatesStream.SegmentState[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyQueryDeletes(BufferedUpdatesStream.SegmentState[]).mjava","sourceNew":"  // Delete by query\n  private long applyQueryDeletes(BufferedUpdatesStream.SegmentState[] segStates) throws IOException {\n\n    if (deleteQueries.length == 0) {\n      return 0;\n    }\n\n    long startNS = System.nanoTime();\n\n    long delCount = 0;\n    for (BufferedUpdatesStream.SegmentState segState : segStates) {\n\n      if (delGen < segState.delGen) {\n        // segment is newer than this deletes packet\n        continue;\n      }\n      \n      if (segState.rld.refCount() == 1) {\n        // This means we are the only remaining reference to this segment, meaning\n        // it was merged away while we were running, so we can safely skip running\n        // because we will run on the newly merged segment next:\n        continue;\n      }\n\n      final LeafReaderContext readerContext = segState.reader.getContext();\n      for (int i = 0; i < deleteQueries.length; i++) {\n        Query query = deleteQueries[i];\n        int limit;\n        if (delGen == segState.delGen) {\n          assert privateSegment != null;\n          limit = deleteQueryLimits[i];\n        } else {\n          limit = Integer.MAX_VALUE;\n        }\n        final IndexSearcher searcher = new IndexSearcher(readerContext.reader());\n        searcher.setQueryCache(null);\n        query = searcher.rewrite(query);\n        final Weight weight = searcher.createWeight(query, ScoreMode.COMPLETE_NO_SCORES, 1);\n        final Scorer scorer = weight.scorer(readerContext);\n        if (scorer != null) {\n          final DocIdSetIterator it = scorer.iterator();\n\n          int docID;\n          while ((docID = it.nextDoc()) < limit)  {\n            if (segState.rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyQueryDeletes took %.2f msec for %d segments and %d queries; %d new deletions\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       segStates.length,\n                                       deleteQueries.length,\n                                       delCount));\n    }\n    \n    return delCount;\n  }\n\n","sourceOld":"  // Delete by query\n  private long applyQueryDeletes(BufferedUpdatesStream.SegmentState[] segStates) throws IOException {\n\n    if (deleteQueries.length == 0) {\n      return 0;\n    }\n\n    long startNS = System.nanoTime();\n\n    long delCount = 0;\n    for (BufferedUpdatesStream.SegmentState segState : segStates) {\n\n      if (delGen < segState.delGen) {\n        // segment is newer than this deletes packet\n        continue;\n      }\n      \n      if (segState.rld.refCount() == 1) {\n        // This means we are the only remaining reference to this segment, meaning\n        // it was merged away while we were running, so we can safely skip running\n        // because we will run on the newly merged segment next:\n        continue;\n      }\n\n      final LeafReaderContext readerContext = segState.reader.getContext();\n      for (int i = 0; i < deleteQueries.length; i++) {\n        Query query = deleteQueries[i];\n        int limit;\n        if (delGen == segState.delGen) {\n          assert privateSegment != null;\n          limit = deleteQueryLimits[i];\n        } else {\n          limit = Integer.MAX_VALUE;\n        }\n        final IndexSearcher searcher = new IndexSearcher(readerContext.reader());\n        searcher.setQueryCache(null);\n        final Weight weight = searcher.createNormalizedWeight(query, ScoreMode.COMPLETE_NO_SCORES);\n        final Scorer scorer = weight.scorer(readerContext);\n        if (scorer != null) {\n          final DocIdSetIterator it = scorer.iterator();\n\n          int docID;\n          while ((docID = it.nextDoc()) < limit)  {\n            if (segState.rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyQueryDeletes took %.2f msec for %d segments and %d queries; %d new deletions\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       segStates.length,\n                                       deleteQueries.length,\n                                       delCount));\n    }\n    \n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d58e44159788900f4a2113b84463dc3fbbf80f20","date":1523319203,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyQueryDeletes(BufferedUpdatesStream.SegmentState[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyQueryDeletes(BufferedUpdatesStream.SegmentState[]).mjava","sourceNew":"  // Delete by query\n  private long applyQueryDeletes(BufferedUpdatesStream.SegmentState[] segStates) throws IOException {\n\n    if (deleteQueries.length == 0) {\n      return 0;\n    }\n\n    long startNS = System.nanoTime();\n\n    long delCount = 0;\n    for (BufferedUpdatesStream.SegmentState segState : segStates) {\n\n      if (delGen < segState.delGen) {\n        // segment is newer than this deletes packet\n        continue;\n      }\n      \n      if (segState.rld.refCount() == 1) {\n        // This means we are the only remaining reference to this segment, meaning\n        // it was merged away while we were running, so we can safely skip running\n        // because we will run on the newly merged segment next:\n        continue;\n      }\n\n      final LeafReaderContext readerContext = segState.reader.getContext();\n      for (int i = 0; i < deleteQueries.length; i++) {\n        Query query = deleteQueries[i];\n        int limit;\n        if (delGen == segState.delGen) {\n          assert privateSegment != null;\n          limit = deleteQueryLimits[i];\n        } else {\n          limit = Integer.MAX_VALUE;\n        }\n        final IndexSearcher searcher = new IndexSearcher(readerContext.reader());\n        searcher.setQueryCache(null);\n        query = searcher.rewrite(query);\n        final Weight weight = searcher.createWeight(query, ScoreMode.COMPLETE_NO_SCORES, 1);\n        final Scorer scorer = weight.scorer(readerContext);\n        if (scorer != null) {\n          final DocIdSetIterator it = scorer.iterator();\n\n          int docID;\n          while ((docID = it.nextDoc()) < limit)  {\n            if (segState.rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyQueryDeletes took %.2f msec for %d segments and %d queries; %d new deletions\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       segStates.length,\n                                       deleteQueries.length,\n                                       delCount));\n    }\n    \n    return delCount;\n  }\n\n","sourceOld":"  // Delete by query\n  private long applyQueryDeletes(BufferedUpdatesStream.SegmentState[] segStates) throws IOException {\n\n    if (deleteQueries.length == 0) {\n      return 0;\n    }\n\n    long startNS = System.nanoTime();\n\n    long delCount = 0;\n    for (BufferedUpdatesStream.SegmentState segState : segStates) {\n\n      if (delGen < segState.delGen) {\n        // segment is newer than this deletes packet\n        continue;\n      }\n      \n      if (segState.rld.refCount() == 1) {\n        // This means we are the only remaining reference to this segment, meaning\n        // it was merged away while we were running, so we can safely skip running\n        // because we will run on the newly merged segment next:\n        continue;\n      }\n\n      final LeafReaderContext readerContext = segState.reader.getContext();\n      for (int i = 0; i < deleteQueries.length; i++) {\n        Query query = deleteQueries[i];\n        int limit;\n        if (delGen == segState.delGen) {\n          assert privateSegment != null;\n          limit = deleteQueryLimits[i];\n        } else {\n          limit = Integer.MAX_VALUE;\n        }\n        final IndexSearcher searcher = new IndexSearcher(readerContext.reader());\n        searcher.setQueryCache(null);\n        final Weight weight = searcher.createNormalizedWeight(query, ScoreMode.COMPLETE_NO_SCORES);\n        final Scorer scorer = weight.scorer(readerContext);\n        if (scorer != null) {\n          final DocIdSetIterator it = scorer.iterator();\n\n          int docID;\n          while ((docID = it.nextDoc()) < limit)  {\n            if (segState.rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyQueryDeletes took %.2f msec for %d segments and %d queries; %d new deletions\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       segStates.length,\n                                       deleteQueries.length,\n                                       delCount));\n    }\n    \n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"129a1284cd4fd41a8a14e474d98379adadb87d25","date":1535390327,"type":3,"author":"Jim Ferenczi","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyQueryDeletes(BufferedUpdatesStream.SegmentState[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyQueryDeletes(BufferedUpdatesStream.SegmentState[]).mjava","sourceNew":"  // Delete by query\n  private long applyQueryDeletes(BufferedUpdatesStream.SegmentState[] segStates) throws IOException {\n\n    if (deleteQueries.length == 0) {\n      return 0;\n    }\n\n    long startNS = System.nanoTime();\n\n    long delCount = 0;\n    for (BufferedUpdatesStream.SegmentState segState : segStates) {\n\n      if (delGen < segState.delGen) {\n        // segment is newer than this deletes packet\n        continue;\n      }\n      \n      if (segState.rld.refCount() == 1) {\n        // This means we are the only remaining reference to this segment, meaning\n        // it was merged away while we were running, so we can safely skip running\n        // because we will run on the newly merged segment next:\n        continue;\n      }\n\n      final LeafReaderContext readerContext = segState.reader.getContext();\n      for (int i = 0; i < deleteQueries.length; i++) {\n        Query query = deleteQueries[i];\n        int limit;\n        if (delGen == segState.delGen) {\n          assert privateSegment != null;\n          limit = deleteQueryLimits[i];\n        } else {\n          limit = Integer.MAX_VALUE;\n        }\n        final IndexSearcher searcher = new IndexSearcher(readerContext.reader());\n        searcher.setQueryCache(null);\n        query = searcher.rewrite(query);\n        final Weight weight = searcher.createWeight(query, ScoreMode.COMPLETE_NO_SCORES, 1);\n        final Scorer scorer = weight.scorer(readerContext);\n        if (scorer != null) {\n          final DocIdSetIterator it = scorer.iterator();\n          if (segState.rld.sortMap != null && limit != Integer.MAX_VALUE) {\n            assert privateSegment != null;\n            // This segment was sorted on flush; we must apply seg-private deletes carefully in this case:\n            int docID;\n            while ((docID = it.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              // The limit is in the pre-sorted doc space:\n              if (segState.rld.sortMap.newToOld(docID) < limit) {\n                if (segState.rld.delete(docID)) {\n                  delCount++;\n                }\n              }\n            }\n          } else {\n            int docID;\n            while ((docID = it.nextDoc()) < limit) {\n              if (segState.rld.delete(docID)) {\n                delCount++;\n              }\n            }\n          }\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyQueryDeletes took %.2f msec for %d segments and %d queries; %d new deletions\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       segStates.length,\n                                       deleteQueries.length,\n                                       delCount));\n    }\n    \n    return delCount;\n  }\n\n","sourceOld":"  // Delete by query\n  private long applyQueryDeletes(BufferedUpdatesStream.SegmentState[] segStates) throws IOException {\n\n    if (deleteQueries.length == 0) {\n      return 0;\n    }\n\n    long startNS = System.nanoTime();\n\n    long delCount = 0;\n    for (BufferedUpdatesStream.SegmentState segState : segStates) {\n\n      if (delGen < segState.delGen) {\n        // segment is newer than this deletes packet\n        continue;\n      }\n      \n      if (segState.rld.refCount() == 1) {\n        // This means we are the only remaining reference to this segment, meaning\n        // it was merged away while we were running, so we can safely skip running\n        // because we will run on the newly merged segment next:\n        continue;\n      }\n\n      final LeafReaderContext readerContext = segState.reader.getContext();\n      for (int i = 0; i < deleteQueries.length; i++) {\n        Query query = deleteQueries[i];\n        int limit;\n        if (delGen == segState.delGen) {\n          assert privateSegment != null;\n          limit = deleteQueryLimits[i];\n        } else {\n          limit = Integer.MAX_VALUE;\n        }\n        final IndexSearcher searcher = new IndexSearcher(readerContext.reader());\n        searcher.setQueryCache(null);\n        query = searcher.rewrite(query);\n        final Weight weight = searcher.createWeight(query, ScoreMode.COMPLETE_NO_SCORES, 1);\n        final Scorer scorer = weight.scorer(readerContext);\n        if (scorer != null) {\n          final DocIdSetIterator it = scorer.iterator();\n\n          int docID;\n          while ((docID = it.nextDoc()) < limit)  {\n            if (segState.rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyQueryDeletes took %.2f msec for %d segments and %d queries; %d new deletions\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       segStates.length,\n                                       deleteQueries.length,\n                                       delCount));\n    }\n    \n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"129a1284cd4fd41a8a14e474d98379adadb87d25":["d58e44159788900f4a2113b84463dc3fbbf80f20"],"d58e44159788900f4a2113b84463dc3fbbf80f20":["417142ff08fda9cf0b72d5133e63097a166c6458","475584d5e08a22ad3fc7babefe006d77bc744567"],"475584d5e08a22ad3fc7babefe006d77bc744567":["417142ff08fda9cf0b72d5133e63097a166c6458"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9fc47cb7b4346802411bb432f501ed0673d7119e":["28288370235ed02234a64753cdbf0c6ec096304a"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"28288370235ed02234a64753cdbf0c6ec096304a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["129a1284cd4fd41a8a14e474d98379adadb87d25"],"417142ff08fda9cf0b72d5133e63097a166c6458":["28288370235ed02234a64753cdbf0c6ec096304a","9fc47cb7b4346802411bb432f501ed0673d7119e"]},"commit2Childs":{"129a1284cd4fd41a8a14e474d98379adadb87d25":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d58e44159788900f4a2113b84463dc3fbbf80f20":["129a1284cd4fd41a8a14e474d98379adadb87d25"],"475584d5e08a22ad3fc7babefe006d77bc744567":["d58e44159788900f4a2113b84463dc3fbbf80f20"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["f4363cd33f6eff7fb4753574a441e2d18c1022a4","b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"9fc47cb7b4346802411bb432f501ed0673d7119e":["417142ff08fda9cf0b72d5133e63097a166c6458"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":[],"28288370235ed02234a64753cdbf0c6ec096304a":["9fc47cb7b4346802411bb432f501ed0673d7119e","417142ff08fda9cf0b72d5133e63097a166c6458"],"417142ff08fda9cf0b72d5133e63097a166c6458":["d58e44159788900f4a2113b84463dc3fbbf80f20","475584d5e08a22ad3fc7babefe006d77bc744567"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}