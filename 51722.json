{"path":"src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,boolean,boolean).mjava","commits":[{"id":"c9727734a64d33a1345c9251f53eb375f04c583e","date":1158874656,"type":0,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,boolean,boolean).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int limit, boolean zeros, boolean missing) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    FieldCache.StringIndex si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), fieldName);\n    int[] count = new int[si.lookup.length];\n    DocIterator iter = docs.iterator();\n    while (iter.hasNext()) {\n      count[si.order[iter.nextDoc()]]++;\n    }\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n    // of the top 'N'\n\n    BoundedTreeSet<CountPair<String,Integer>> queue=null;\n\n    if (limit>=0) {\n      // TODO: compare performance of BoundedTreeSet compare against priority queue?\n      queue = new BoundedTreeSet<CountPair<String,Integer>>(limit);\n    }\n\n    int min=-1;  // the smallest value in the top 'N' values\n    for (int i=1; i<count.length; i++) {\n      int c = count[i];\n      if (c==0 && !zeros) continue;\n      if (limit<0) {\n        res.add(ft.indexedToReadable(si.lookup[i]), c);\n      } else if (c>min) {\n        // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n        // index order, so we already know that the keys are ordered.  This can be very\n        // important if a lot of the counts are repeated (like zero counts would be).\n        queue.add(new CountPair<String,Integer>(ft.indexedToReadable(si.lookup[i]), c));\n        if (queue.size()>=limit) min=queue.last().val;\n      }\n    }\n\n    if (limit>=0) {\n      for (CountPair<String,Integer> p : queue) {\n        res.add(p.key, p.val);\n      }\n    }\n\n\n    if (missing) res.add(null, count[0]);\n    return res;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9691bd012f05632bf944115412ed90daae112f68","date":1168971974,"type":5,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,boolean).mjava","pathOld":"src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,boolean,boolean).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, boolean sort) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    FieldCache.StringIndex si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), fieldName);\n    final int[] count = new int[si.lookup.length];\n    DocIterator iter = docs.iterator();\n    while (iter.hasNext()) {\n      count[si.order[iter.nextDoc()]]++;\n    }\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n    // of the top 'N'\n\n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    if (sort) {\n      // TODO: compare performance of BoundedTreeSet compare against priority queue?\n      int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n      final BoundedTreeSet<CountPair<String,Integer>> queue = new BoundedTreeSet<CountPair<String,Integer>>(maxsize);\n      int min=mincount-1;  // the smallest value in the top 'N' values\n      for (int i=1; i<count.length; i++) {\n        int c = count[i];\n        if (c>min) {\n          // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n          // index order, so we already know that the keys are ordered.  This can be very\n          // important if a lot of the counts are repeated (like zero counts would be).\n          queue.add(new CountPair<String,Integer>(ft.indexedToReadable(si.lookup[i]), c));\n          if (queue.size()>=maxsize) min=queue.last().val;\n        }\n      }\n      for (CountPair<String,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        res.add(p.key, p.val);\n      }\n    } else if (mincount<=0) {\n      // This is an optimization... if mincount<=0 and we aren't sorting then\n      // we know exactly where to start and end in the fieldcache.\n      for (int i=offset+1; i<offset+1+limit; i++) {\n        res.add(ft.indexedToReadable(si.lookup[i]),count[i]);\n      }\n    } else {\n      for (int i=1; i<count.length; i++) {\n        int c = count[i];\n        if (c<mincount || --off>=0) continue;\n        if (--lim<0) break;\n        res.add(ft.indexedToReadable(si.lookup[i]), c);      \n      }\n    }\n\n    if (missing) res.add(null, count[0]);\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int limit, boolean zeros, boolean missing) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    FieldCache.StringIndex si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), fieldName);\n    int[] count = new int[si.lookup.length];\n    DocIterator iter = docs.iterator();\n    while (iter.hasNext()) {\n      count[si.order[iter.nextDoc()]]++;\n    }\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n    // of the top 'N'\n\n    BoundedTreeSet<CountPair<String,Integer>> queue=null;\n\n    if (limit>=0) {\n      // TODO: compare performance of BoundedTreeSet compare against priority queue?\n      queue = new BoundedTreeSet<CountPair<String,Integer>>(limit);\n    }\n\n    int min=-1;  // the smallest value in the top 'N' values\n    for (int i=1; i<count.length; i++) {\n      int c = count[i];\n      if (c==0 && !zeros) continue;\n      if (limit<0) {\n        res.add(ft.indexedToReadable(si.lookup[i]), c);\n      } else if (c>min) {\n        // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n        // index order, so we already know that the keys are ordered.  This can be very\n        // important if a lot of the counts are repeated (like zero counts would be).\n        queue.add(new CountPair<String,Integer>(ft.indexedToReadable(si.lookup[i]), c));\n        if (queue.size()>=limit) min=queue.last().val;\n      }\n    }\n\n    if (limit>=0) {\n      for (CountPair<String,Integer> p : queue) {\n        res.add(p.key, p.val);\n      }\n    }\n\n\n    if (missing) res.add(null, count[0]);\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9691bd012f05632bf944115412ed90daae112f68":["c9727734a64d33a1345c9251f53eb375f04c583e"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"c9727734a64d33a1345c9251f53eb375f04c583e":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"9691bd012f05632bf944115412ed90daae112f68":[],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["c9727734a64d33a1345c9251f53eb375f04c583e"],"c9727734a64d33a1345c9251f53eb375f04c583e":["9691bd012f05632bf944115412ed90daae112f68"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["9691bd012f05632bf944115412ed90daae112f68","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"pathCommit":null}