{"path":"backwards/src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","commits":[{"id":"480d01e5b0ef8efb136d51670fec297ae5ae2c9c","date":1268821447,"type":0,"author":"Uwe Schindler","isMerge":false,"pathNew":"backwards/src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  @Override\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    final Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    final Set<Term> terms = new HashSet<Term>();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    final Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    final HashMap<Term,Integer> dfMap = new HashMap<Term,Integer>();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], Integer.valueOf(aggregatedDfs[i]));\n    }\n\n    // step4\n    final int numDocs = maxDoc();\n    final CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/backwards/src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","pathOld":"backwards/src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","sourceNew":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  @Override\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    final Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    final Set<Term> terms = new HashSet<Term>();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    final Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    final HashMap<Term,Integer> dfMap = new HashMap<Term,Integer>();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], Integer.valueOf(aggregatedDfs[i]));\n    }\n\n    // step4\n    final int numDocs = maxDoc();\n    final CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","sourceOld":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  @Override\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    final Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    final Set<Term> terms = new HashSet<Term>();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    final Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    final HashMap<Term,Integer> dfMap = new HashMap<Term,Integer>();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], Integer.valueOf(aggregatedDfs[i]));\n    }\n\n    // step4\n    final int numDocs = maxDoc();\n    final CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"480d01e5b0ef8efb136d51670fec297ae5ae2c9c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["480d01e5b0ef8efb136d51670fec297ae5ae2c9c"]},"commit2Childs":{"480d01e5b0ef8efb136d51670fec297ae5ae2c9c":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["480d01e5b0ef8efb136d51670fec297ae5ae2c9c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}