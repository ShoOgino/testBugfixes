{"path":"lucene/facet/src/examples/org/apache/lucene/facet/example/adaptive/AdaptiveSearcher#searchWithFacets(Directory,Directory).mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/facet/src/examples/org/apache/lucene/facet/example/adaptive/AdaptiveSearcher#searchWithFacets(Directory,Directory).mjava","pathOld":"modules/facet/src/examples/org/apache/lucene/facet/example/adaptive/AdaptiveSearcher#searchWithFacets(Directory,Directory).mjava","sourceNew":"  /**\n   * Search with facets through the {@link AdaptiveFacetsAccumulator} \n   * @param indexDir Directory of the search index.\n   * @param taxoDir Directory of the taxonomy index.\n   * @throws Exception on error (no detailed exception handling here for sample simplicity\n   * @return facet results\n   */\n  public static List<FacetResult> searchWithFacets (Directory indexDir, Directory taxoDir) throws Exception {\n    // prepare index reader and taxonomy.\n    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    IndexReader indexReader = IndexReader.open(indexDir);\n    \n    // prepare searcher to search against\n    IndexSearcher searcher = new IndexSearcher(indexReader);\n    \n    // faceted search is working in 2 steps: \n    // 1. collect matching documents\n    // 2. aggregate facets for collected documents and\n    //    generate the requested faceted results from the aggregated facets\n    \n    // step 1: collect matching documents into a collector\n    Query q = new TermQuery(new Term(SimpleUtils.TEXT,\"white\"));\n    ExampleUtils.log(\"Query: \"+q);\n    \n    // regular collector for scoring matched documents\n    TopScoreDocCollector topDocsCollector = TopScoreDocCollector.create(10, true); \n    \n    // docids collector for guiding facets accumulation (scoring disabled)\n    ScoredDocIdCollector docIdsCollecor = ScoredDocIdCollector.create(indexReader.maxDoc(), false);\n    \n    // Faceted search parameters indicate which facets are we interested in \n    FacetSearchParams facetSearchParams = new FacetSearchParams();\n    facetSearchParams.addFacetRequest(new CountFacetRequest(new CategoryPath(\"root\",\"a\"), 10));\n    \n    // search, into both collectors. note: in case only facets accumulation \n    // is required, the topDocCollector part can be totally discarded\n    searcher.search(q, MultiCollector.wrap(topDocsCollector, docIdsCollecor));\n        \n    // Obtain facets results and print them\n    AdaptiveFacetsAccumulator accumulator = new AdaptiveFacetsAccumulator(facetSearchParams, indexReader, taxo);\n    List<FacetResult> res = accumulator.accumulate(docIdsCollecor.getScoredDocIDs());\n    \n    int i = 0;\n    for (FacetResult facetResult : res) {\n      ExampleUtils.log(\"Res \"+(i++)+\": \"+facetResult);\n    }\n    \n    // we're done, close the index reader and the taxonomy.\n    indexReader.close();\n    taxo.close();\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Search with facets through the {@link AdaptiveFacetsAccumulator} \n   * @param indexDir Directory of the search index.\n   * @param taxoDir Directory of the taxonomy index.\n   * @throws Exception on error (no detailed exception handling here for sample simplicity\n   * @return facet results\n   */\n  public static List<FacetResult> searchWithFacets (Directory indexDir, Directory taxoDir) throws Exception {\n    // prepare index reader and taxonomy.\n    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    IndexReader indexReader = IndexReader.open(indexDir);\n    \n    // prepare searcher to search against\n    IndexSearcher searcher = new IndexSearcher(indexReader);\n    \n    // faceted search is working in 2 steps: \n    // 1. collect matching documents\n    // 2. aggregate facets for collected documents and\n    //    generate the requested faceted results from the aggregated facets\n    \n    // step 1: collect matching documents into a collector\n    Query q = new TermQuery(new Term(SimpleUtils.TEXT,\"white\"));\n    ExampleUtils.log(\"Query: \"+q);\n    \n    // regular collector for scoring matched documents\n    TopScoreDocCollector topDocsCollector = TopScoreDocCollector.create(10, true); \n    \n    // docids collector for guiding facets accumulation (scoring disabled)\n    ScoredDocIdCollector docIdsCollecor = ScoredDocIdCollector.create(indexReader.maxDoc(), false);\n    \n    // Faceted search parameters indicate which facets are we interested in \n    FacetSearchParams facetSearchParams = new FacetSearchParams();\n    facetSearchParams.addFacetRequest(new CountFacetRequest(new CategoryPath(\"root\",\"a\"), 10));\n    \n    // search, into both collectors. note: in case only facets accumulation \n    // is required, the topDocCollector part can be totally discarded\n    searcher.search(q, MultiCollector.wrap(topDocsCollector, docIdsCollecor));\n        \n    // Obtain facets results and print them\n    AdaptiveFacetsAccumulator accumulator = new AdaptiveFacetsAccumulator(facetSearchParams, indexReader, taxo);\n    List<FacetResult> res = accumulator.accumulate(docIdsCollecor.getScoredDocIDs());\n    \n    int i = 0;\n    for (FacetResult facetResult : res) {\n      ExampleUtils.log(\"Res \"+(i++)+\": \"+facetResult);\n    }\n    \n    // we're done, close the index reader and the taxonomy.\n    indexReader.close();\n    taxo.close();\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","date":1338430031,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/facet/src/examples/org/apache/lucene/facet/example/adaptive/AdaptiveSearcher#searchWithFacets(Directory,Directory).mjava","pathOld":"lucene/facet/src/examples/org/apache/lucene/facet/example/adaptive/AdaptiveSearcher#searchWithFacets(Directory,Directory).mjava","sourceNew":"  /**\n   * Search with facets through the {@link AdaptiveFacetsAccumulator} \n   * @param indexDir Directory of the search index.\n   * @param taxoDir Directory of the taxonomy index.\n   * @throws Exception on error (no detailed exception handling here for sample simplicity\n   * @return facet results\n   */\n  public static List<FacetResult> searchWithFacets (Directory indexDir, Directory taxoDir) throws Exception {\n    // prepare index reader and taxonomy.\n    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    IndexReader indexReader = DirectoryReader.open(indexDir);\n    \n    // prepare searcher to search against\n    IndexSearcher searcher = new IndexSearcher(indexReader);\n    \n    // faceted search is working in 2 steps: \n    // 1. collect matching documents\n    // 2. aggregate facets for collected documents and\n    //    generate the requested faceted results from the aggregated facets\n    \n    // step 1: collect matching documents into a collector\n    Query q = new TermQuery(new Term(SimpleUtils.TEXT,\"white\"));\n    ExampleUtils.log(\"Query: \"+q);\n    \n    // regular collector for scoring matched documents\n    TopScoreDocCollector topDocsCollector = TopScoreDocCollector.create(10, true); \n    \n    // docids collector for guiding facets accumulation (scoring disabled)\n    ScoredDocIdCollector docIdsCollecor = ScoredDocIdCollector.create(indexReader.maxDoc(), false);\n    \n    // Faceted search parameters indicate which facets are we interested in \n    FacetSearchParams facetSearchParams = new FacetSearchParams();\n    facetSearchParams.addFacetRequest(new CountFacetRequest(new CategoryPath(\"root\",\"a\"), 10));\n    \n    // search, into both collectors. note: in case only facets accumulation \n    // is required, the topDocCollector part can be totally discarded\n    searcher.search(q, MultiCollector.wrap(topDocsCollector, docIdsCollecor));\n        \n    // Obtain facets results and print them\n    AdaptiveFacetsAccumulator accumulator = new AdaptiveFacetsAccumulator(facetSearchParams, indexReader, taxo);\n    List<FacetResult> res = accumulator.accumulate(docIdsCollecor.getScoredDocIDs());\n    \n    int i = 0;\n    for (FacetResult facetResult : res) {\n      ExampleUtils.log(\"Res \"+(i++)+\": \"+facetResult);\n    }\n    \n    // we're done, close the index reader and the taxonomy.\n    indexReader.close();\n    taxo.close();\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Search with facets through the {@link AdaptiveFacetsAccumulator} \n   * @param indexDir Directory of the search index.\n   * @param taxoDir Directory of the taxonomy index.\n   * @throws Exception on error (no detailed exception handling here for sample simplicity\n   * @return facet results\n   */\n  public static List<FacetResult> searchWithFacets (Directory indexDir, Directory taxoDir) throws Exception {\n    // prepare index reader and taxonomy.\n    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    IndexReader indexReader = IndexReader.open(indexDir);\n    \n    // prepare searcher to search against\n    IndexSearcher searcher = new IndexSearcher(indexReader);\n    \n    // faceted search is working in 2 steps: \n    // 1. collect matching documents\n    // 2. aggregate facets for collected documents and\n    //    generate the requested faceted results from the aggregated facets\n    \n    // step 1: collect matching documents into a collector\n    Query q = new TermQuery(new Term(SimpleUtils.TEXT,\"white\"));\n    ExampleUtils.log(\"Query: \"+q);\n    \n    // regular collector for scoring matched documents\n    TopScoreDocCollector topDocsCollector = TopScoreDocCollector.create(10, true); \n    \n    // docids collector for guiding facets accumulation (scoring disabled)\n    ScoredDocIdCollector docIdsCollecor = ScoredDocIdCollector.create(indexReader.maxDoc(), false);\n    \n    // Faceted search parameters indicate which facets are we interested in \n    FacetSearchParams facetSearchParams = new FacetSearchParams();\n    facetSearchParams.addFacetRequest(new CountFacetRequest(new CategoryPath(\"root\",\"a\"), 10));\n    \n    // search, into both collectors. note: in case only facets accumulation \n    // is required, the topDocCollector part can be totally discarded\n    searcher.search(q, MultiCollector.wrap(topDocsCollector, docIdsCollecor));\n        \n    // Obtain facets results and print them\n    AdaptiveFacetsAccumulator accumulator = new AdaptiveFacetsAccumulator(facetSearchParams, indexReader, taxo);\n    List<FacetResult> res = accumulator.accumulate(docIdsCollecor.getScoredDocIDs());\n    \n    int i = 0;\n    for (FacetResult facetResult : res) {\n      ExampleUtils.log(\"Res \"+(i++)+\": \"+facetResult);\n    }\n    \n    // we're done, close the index reader and the taxonomy.\n    indexReader.close();\n    taxo.close();\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d4c6c7f3cda7a0595cabd16e5e9107ca29852708","date":1355402234,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/examples/org/apache/lucene/facet/example/adaptive/AdaptiveSearcher#searchWithFacets(Directory,Directory).mjava","pathOld":"lucene/facet/src/examples/org/apache/lucene/facet/example/adaptive/AdaptiveSearcher#searchWithFacets(Directory,Directory).mjava","sourceNew":"  /**\n   * Search with facets through the {@link AdaptiveFacetsAccumulator} \n   * @param indexDir Directory of the search index.\n   * @param taxoDir Directory of the taxonomy index.\n   * @throws Exception on error (no detailed exception handling here for sample simplicity\n   * @return facet results\n   */\n  public static List<FacetResult> searchWithFacets (Directory indexDir, Directory taxoDir) throws Exception {\n    // prepare index reader and taxonomy.\n    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    IndexReader indexReader = DirectoryReader.open(indexDir);\n    \n    // prepare searcher to search against\n    IndexSearcher searcher = new IndexSearcher(indexReader);\n    \n    // faceted search is working in 2 steps: \n    // 1. collect matching documents\n    // 2. aggregate facets for collected documents and\n    //    generate the requested faceted results from the aggregated facets\n    \n    // step 1: collect matching documents into a collector\n    Query q = new TermQuery(new Term(SimpleUtils.TEXT,\"white\"));\n    ExampleUtils.log(\"Query: \"+q);\n    \n    // regular collector for scoring matched documents\n    TopScoreDocCollector topDocsCollector = TopScoreDocCollector.create(10, true); \n    \n    // docids collector for guiding facets accumulation (scoring disabled)\n    ScoredDocIdCollector docIdsCollecor = ScoredDocIdCollector.create(indexReader.maxDoc(), false);\n    \n    // Faceted search parameters indicate which facets are we interested in \n    FacetSearchParams facetSearchParams = new FacetSearchParams(\n        new CountFacetRequest(new CategoryPath(\"root\", \"a\"), 10));\n    \n    // search, into both collectors. note: in case only facets accumulation \n    // is required, the topDocCollector part can be totally discarded\n    searcher.search(q, MultiCollector.wrap(topDocsCollector, docIdsCollecor));\n        \n    // Obtain facets results and print them\n    AdaptiveFacetsAccumulator accumulator = new AdaptiveFacetsAccumulator(facetSearchParams, indexReader, taxo);\n    List<FacetResult> res = accumulator.accumulate(docIdsCollecor.getScoredDocIDs());\n    \n    int i = 0;\n    for (FacetResult facetResult : res) {\n      ExampleUtils.log(\"Res \"+(i++)+\": \"+facetResult);\n    }\n    \n    // we're done, close the index reader and the taxonomy.\n    indexReader.close();\n    taxo.close();\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Search with facets through the {@link AdaptiveFacetsAccumulator} \n   * @param indexDir Directory of the search index.\n   * @param taxoDir Directory of the taxonomy index.\n   * @throws Exception on error (no detailed exception handling here for sample simplicity\n   * @return facet results\n   */\n  public static List<FacetResult> searchWithFacets (Directory indexDir, Directory taxoDir) throws Exception {\n    // prepare index reader and taxonomy.\n    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    IndexReader indexReader = DirectoryReader.open(indexDir);\n    \n    // prepare searcher to search against\n    IndexSearcher searcher = new IndexSearcher(indexReader);\n    \n    // faceted search is working in 2 steps: \n    // 1. collect matching documents\n    // 2. aggregate facets for collected documents and\n    //    generate the requested faceted results from the aggregated facets\n    \n    // step 1: collect matching documents into a collector\n    Query q = new TermQuery(new Term(SimpleUtils.TEXT,\"white\"));\n    ExampleUtils.log(\"Query: \"+q);\n    \n    // regular collector for scoring matched documents\n    TopScoreDocCollector topDocsCollector = TopScoreDocCollector.create(10, true); \n    \n    // docids collector for guiding facets accumulation (scoring disabled)\n    ScoredDocIdCollector docIdsCollecor = ScoredDocIdCollector.create(indexReader.maxDoc(), false);\n    \n    // Faceted search parameters indicate which facets are we interested in \n    FacetSearchParams facetSearchParams = new FacetSearchParams();\n    facetSearchParams.addFacetRequest(new CountFacetRequest(new CategoryPath(\"root\",\"a\"), 10));\n    \n    // search, into both collectors. note: in case only facets accumulation \n    // is required, the topDocCollector part can be totally discarded\n    searcher.search(q, MultiCollector.wrap(topDocsCollector, docIdsCollecor));\n        \n    // Obtain facets results and print them\n    AdaptiveFacetsAccumulator accumulator = new AdaptiveFacetsAccumulator(facetSearchParams, indexReader, taxo);\n    List<FacetResult> res = accumulator.accumulate(docIdsCollecor.getScoredDocIDs());\n    \n    int i = 0;\n    for (FacetResult facetResult : res) {\n      ExampleUtils.log(\"Res \"+(i++)+\": \"+facetResult);\n    }\n    \n    // we're done, close the index reader and the taxonomy.\n    indexReader.close();\n    taxo.close();\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/facet/src/examples/org/apache/lucene/facet/example/adaptive/AdaptiveSearcher#searchWithFacets(Directory,Directory).mjava","pathOld":"lucene/facet/src/examples/org/apache/lucene/facet/example/adaptive/AdaptiveSearcher#searchWithFacets(Directory,Directory).mjava","sourceNew":"  /**\n   * Search with facets through the {@link AdaptiveFacetsAccumulator} \n   * @param indexDir Directory of the search index.\n   * @param taxoDir Directory of the taxonomy index.\n   * @throws Exception on error (no detailed exception handling here for sample simplicity\n   * @return facet results\n   */\n  public static List<FacetResult> searchWithFacets (Directory indexDir, Directory taxoDir) throws Exception {\n    // prepare index reader and taxonomy.\n    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    IndexReader indexReader = DirectoryReader.open(indexDir);\n    \n    // prepare searcher to search against\n    IndexSearcher searcher = new IndexSearcher(indexReader);\n    \n    // faceted search is working in 2 steps: \n    // 1. collect matching documents\n    // 2. aggregate facets for collected documents and\n    //    generate the requested faceted results from the aggregated facets\n    \n    // step 1: collect matching documents into a collector\n    Query q = new TermQuery(new Term(SimpleUtils.TEXT,\"white\"));\n    ExampleUtils.log(\"Query: \"+q);\n    \n    // regular collector for scoring matched documents\n    TopScoreDocCollector topDocsCollector = TopScoreDocCollector.create(10, true); \n    \n    // docids collector for guiding facets accumulation (scoring disabled)\n    ScoredDocIdCollector docIdsCollecor = ScoredDocIdCollector.create(indexReader.maxDoc(), false);\n    \n    // Faceted search parameters indicate which facets are we interested in \n    FacetSearchParams facetSearchParams = new FacetSearchParams(\n        new CountFacetRequest(new CategoryPath(\"root\", \"a\"), 10));\n    \n    // search, into both collectors. note: in case only facets accumulation \n    // is required, the topDocCollector part can be totally discarded\n    searcher.search(q, MultiCollector.wrap(topDocsCollector, docIdsCollecor));\n        \n    // Obtain facets results and print them\n    AdaptiveFacetsAccumulator accumulator = new AdaptiveFacetsAccumulator(facetSearchParams, indexReader, taxo);\n    List<FacetResult> res = accumulator.accumulate(docIdsCollecor.getScoredDocIDs());\n    \n    int i = 0;\n    for (FacetResult facetResult : res) {\n      ExampleUtils.log(\"Res \"+(i++)+\": \"+facetResult);\n    }\n    \n    // we're done, close the index reader and the taxonomy.\n    indexReader.close();\n    taxo.close();\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Search with facets through the {@link AdaptiveFacetsAccumulator} \n   * @param indexDir Directory of the search index.\n   * @param taxoDir Directory of the taxonomy index.\n   * @throws Exception on error (no detailed exception handling here for sample simplicity\n   * @return facet results\n   */\n  public static List<FacetResult> searchWithFacets (Directory indexDir, Directory taxoDir) throws Exception {\n    // prepare index reader and taxonomy.\n    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    IndexReader indexReader = DirectoryReader.open(indexDir);\n    \n    // prepare searcher to search against\n    IndexSearcher searcher = new IndexSearcher(indexReader);\n    \n    // faceted search is working in 2 steps: \n    // 1. collect matching documents\n    // 2. aggregate facets for collected documents and\n    //    generate the requested faceted results from the aggregated facets\n    \n    // step 1: collect matching documents into a collector\n    Query q = new TermQuery(new Term(SimpleUtils.TEXT,\"white\"));\n    ExampleUtils.log(\"Query: \"+q);\n    \n    // regular collector for scoring matched documents\n    TopScoreDocCollector topDocsCollector = TopScoreDocCollector.create(10, true); \n    \n    // docids collector for guiding facets accumulation (scoring disabled)\n    ScoredDocIdCollector docIdsCollecor = ScoredDocIdCollector.create(indexReader.maxDoc(), false);\n    \n    // Faceted search parameters indicate which facets are we interested in \n    FacetSearchParams facetSearchParams = new FacetSearchParams();\n    facetSearchParams.addFacetRequest(new CountFacetRequest(new CategoryPath(\"root\",\"a\"), 10));\n    \n    // search, into both collectors. note: in case only facets accumulation \n    // is required, the topDocCollector part can be totally discarded\n    searcher.search(q, MultiCollector.wrap(topDocsCollector, docIdsCollecor));\n        \n    // Obtain facets results and print them\n    AdaptiveFacetsAccumulator accumulator = new AdaptiveFacetsAccumulator(facetSearchParams, indexReader, taxo);\n    List<FacetResult> res = accumulator.accumulate(docIdsCollecor.getScoredDocIDs());\n    \n    int i = 0;\n    for (FacetResult facetResult : res) {\n      ExampleUtils.log(\"Res \"+(i++)+\": \"+facetResult);\n    }\n    \n    // we're done, close the index reader and the taxonomy.\n    indexReader.close();\n    taxo.close();\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1289047c4a6e31121c9d3a8f4c7a3fb30179f0fc","date":1359570667,"type":5,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/demo/src/java/org/apache/lucene/demo/facet/adaptive/AdaptiveSearcher#searchWithFacets(Directory,Directory).mjava","pathOld":"lucene/facet/src/examples/org/apache/lucene/facet/example/adaptive/AdaptiveSearcher#searchWithFacets(Directory,Directory).mjava","sourceNew":"  /**\n   * Search with facets through the {@link AdaptiveFacetsAccumulator} \n   * @param indexDir Directory of the search index.\n   * @param taxoDir Directory of the taxonomy index.\n   * @throws Exception on error (no detailed exception handling here for sample simplicity\n   * @return facet results\n   */\n  public static List<FacetResult> searchWithFacets (Directory indexDir, Directory taxoDir) throws Exception {\n    // prepare index reader and taxonomy.\n    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    IndexReader indexReader = DirectoryReader.open(indexDir);\n    \n    // prepare searcher to search against\n    IndexSearcher searcher = new IndexSearcher(indexReader);\n    \n    // faceted search is working in 2 steps: \n    // 1. collect matching documents\n    // 2. aggregate facets for collected documents and\n    //    generate the requested faceted results from the aggregated facets\n    \n    // step 1: collect matching documents into a collector\n    Query q = new TermQuery(new Term(SimpleUtils.TEXT,\"white\"));\n    ExampleUtils.log(\"Query: \"+q);\n    \n    // regular collector for scoring matched documents\n    TopScoreDocCollector topDocsCollector = TopScoreDocCollector.create(10, true); \n    \n    // docids collector for guiding facets accumulation (scoring disabled)\n    ScoredDocIdCollector docIdsCollecor = ScoredDocIdCollector.create(indexReader.maxDoc(), false);\n    \n    // Faceted search parameters indicate which facets are we interested in \n    FacetSearchParams facetSearchParams = new FacetSearchParams(\n        new CountFacetRequest(new CategoryPath(\"root\", \"a\"), 10));\n    \n    // search, into both collectors. note: in case only facets accumulation \n    // is required, the topDocCollector part can be totally discarded\n    searcher.search(q, MultiCollector.wrap(topDocsCollector, docIdsCollecor));\n        \n    // Obtain facets results and print them\n    AdaptiveFacetsAccumulator accumulator = new AdaptiveFacetsAccumulator(facetSearchParams, indexReader, taxo);\n    List<FacetResult> res = accumulator.accumulate(docIdsCollecor.getScoredDocIDs());\n    \n    int i = 0;\n    for (FacetResult facetResult : res) {\n      ExampleUtils.log(\"Res \"+(i++)+\": \"+facetResult);\n    }\n    \n    // we're done, close the index reader and the taxonomy.\n    indexReader.close();\n    taxo.close();\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Search with facets through the {@link AdaptiveFacetsAccumulator} \n   * @param indexDir Directory of the search index.\n   * @param taxoDir Directory of the taxonomy index.\n   * @throws Exception on error (no detailed exception handling here for sample simplicity\n   * @return facet results\n   */\n  public static List<FacetResult> searchWithFacets (Directory indexDir, Directory taxoDir) throws Exception {\n    // prepare index reader and taxonomy.\n    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    IndexReader indexReader = DirectoryReader.open(indexDir);\n    \n    // prepare searcher to search against\n    IndexSearcher searcher = new IndexSearcher(indexReader);\n    \n    // faceted search is working in 2 steps: \n    // 1. collect matching documents\n    // 2. aggregate facets for collected documents and\n    //    generate the requested faceted results from the aggregated facets\n    \n    // step 1: collect matching documents into a collector\n    Query q = new TermQuery(new Term(SimpleUtils.TEXT,\"white\"));\n    ExampleUtils.log(\"Query: \"+q);\n    \n    // regular collector for scoring matched documents\n    TopScoreDocCollector topDocsCollector = TopScoreDocCollector.create(10, true); \n    \n    // docids collector for guiding facets accumulation (scoring disabled)\n    ScoredDocIdCollector docIdsCollecor = ScoredDocIdCollector.create(indexReader.maxDoc(), false);\n    \n    // Faceted search parameters indicate which facets are we interested in \n    FacetSearchParams facetSearchParams = new FacetSearchParams(\n        new CountFacetRequest(new CategoryPath(\"root\", \"a\"), 10));\n    \n    // search, into both collectors. note: in case only facets accumulation \n    // is required, the topDocCollector part can be totally discarded\n    searcher.search(q, MultiCollector.wrap(topDocsCollector, docIdsCollecor));\n        \n    // Obtain facets results and print them\n    AdaptiveFacetsAccumulator accumulator = new AdaptiveFacetsAccumulator(facetSearchParams, indexReader, taxo);\n    List<FacetResult> res = accumulator.accumulate(docIdsCollecor.getScoredDocIDs());\n    \n    int i = 0;\n    for (FacetResult facetResult : res) {\n      ExampleUtils.log(\"Res \"+(i++)+\": \"+facetResult);\n    }\n    \n    // we're done, close the index reader and the taxonomy.\n    indexReader.close();\n    taxo.close();\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"61d5f95d14e5b9b046998c51e16709a398c15226","date":1359603451,"type":5,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/demo/src/java/org/apache/lucene/demo/facet/adaptive/AdaptiveSearcher#searchWithFacets(Directory,Directory).mjava","pathOld":"lucene/facet/src/examples/org/apache/lucene/facet/example/adaptive/AdaptiveSearcher#searchWithFacets(Directory,Directory).mjava","sourceNew":"  /**\n   * Search with facets through the {@link AdaptiveFacetsAccumulator} \n   * @param indexDir Directory of the search index.\n   * @param taxoDir Directory of the taxonomy index.\n   * @throws Exception on error (no detailed exception handling here for sample simplicity\n   * @return facet results\n   */\n  public static List<FacetResult> searchWithFacets (Directory indexDir, Directory taxoDir) throws Exception {\n    // prepare index reader and taxonomy.\n    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    IndexReader indexReader = DirectoryReader.open(indexDir);\n    \n    // prepare searcher to search against\n    IndexSearcher searcher = new IndexSearcher(indexReader);\n    \n    // faceted search is working in 2 steps: \n    // 1. collect matching documents\n    // 2. aggregate facets for collected documents and\n    //    generate the requested faceted results from the aggregated facets\n    \n    // step 1: collect matching documents into a collector\n    Query q = new TermQuery(new Term(SimpleUtils.TEXT,\"white\"));\n    ExampleUtils.log(\"Query: \"+q);\n    \n    // regular collector for scoring matched documents\n    TopScoreDocCollector topDocsCollector = TopScoreDocCollector.create(10, true); \n    \n    // docids collector for guiding facets accumulation (scoring disabled)\n    ScoredDocIdCollector docIdsCollecor = ScoredDocIdCollector.create(indexReader.maxDoc(), false);\n    \n    // Faceted search parameters indicate which facets are we interested in \n    FacetSearchParams facetSearchParams = new FacetSearchParams(\n        new CountFacetRequest(new CategoryPath(\"root\", \"a\"), 10));\n    \n    // search, into both collectors. note: in case only facets accumulation \n    // is required, the topDocCollector part can be totally discarded\n    searcher.search(q, MultiCollector.wrap(topDocsCollector, docIdsCollecor));\n        \n    // Obtain facets results and print them\n    AdaptiveFacetsAccumulator accumulator = new AdaptiveFacetsAccumulator(facetSearchParams, indexReader, taxo);\n    List<FacetResult> res = accumulator.accumulate(docIdsCollecor.getScoredDocIDs());\n    \n    int i = 0;\n    for (FacetResult facetResult : res) {\n      ExampleUtils.log(\"Res \"+(i++)+\": \"+facetResult);\n    }\n    \n    // we're done, close the index reader and the taxonomy.\n    indexReader.close();\n    taxo.close();\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Search with facets through the {@link AdaptiveFacetsAccumulator} \n   * @param indexDir Directory of the search index.\n   * @param taxoDir Directory of the taxonomy index.\n   * @throws Exception on error (no detailed exception handling here for sample simplicity\n   * @return facet results\n   */\n  public static List<FacetResult> searchWithFacets (Directory indexDir, Directory taxoDir) throws Exception {\n    // prepare index reader and taxonomy.\n    TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    IndexReader indexReader = DirectoryReader.open(indexDir);\n    \n    // prepare searcher to search against\n    IndexSearcher searcher = new IndexSearcher(indexReader);\n    \n    // faceted search is working in 2 steps: \n    // 1. collect matching documents\n    // 2. aggregate facets for collected documents and\n    //    generate the requested faceted results from the aggregated facets\n    \n    // step 1: collect matching documents into a collector\n    Query q = new TermQuery(new Term(SimpleUtils.TEXT,\"white\"));\n    ExampleUtils.log(\"Query: \"+q);\n    \n    // regular collector for scoring matched documents\n    TopScoreDocCollector topDocsCollector = TopScoreDocCollector.create(10, true); \n    \n    // docids collector for guiding facets accumulation (scoring disabled)\n    ScoredDocIdCollector docIdsCollecor = ScoredDocIdCollector.create(indexReader.maxDoc(), false);\n    \n    // Faceted search parameters indicate which facets are we interested in \n    FacetSearchParams facetSearchParams = new FacetSearchParams(\n        new CountFacetRequest(new CategoryPath(\"root\", \"a\"), 10));\n    \n    // search, into both collectors. note: in case only facets accumulation \n    // is required, the topDocCollector part can be totally discarded\n    searcher.search(q, MultiCollector.wrap(topDocsCollector, docIdsCollecor));\n        \n    // Obtain facets results and print them\n    AdaptiveFacetsAccumulator accumulator = new AdaptiveFacetsAccumulator(facetSearchParams, indexReader, taxo);\n    List<FacetResult> res = accumulator.accumulate(docIdsCollecor.getScoredDocIDs());\n    \n    int i = 0;\n    for (FacetResult facetResult : res) {\n      ExampleUtils.log(\"Res \"+(i++)+\": \"+facetResult);\n    }\n    \n    // we're done, close the index reader and the taxonomy.\n    indexReader.close();\n    taxo.close();\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","d4c6c7f3cda7a0595cabd16e5e9107ca29852708"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"61d5f95d14e5b9b046998c51e16709a398c15226":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","1289047c4a6e31121c9d3a8f4c7a3fb30179f0fc"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"d4c6c7f3cda7a0595cabd16e5e9107ca29852708":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1289047c4a6e31121c9d3a8f4c7a3fb30179f0fc":["d4c6c7f3cda7a0595cabd16e5e9107ca29852708"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["1289047c4a6e31121c9d3a8f4c7a3fb30179f0fc"]},"commit2Childs":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["61d5f95d14e5b9b046998c51e16709a398c15226"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"61d5f95d14e5b9b046998c51e16709a398c15226":[],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","d4c6c7f3cda7a0595cabd16e5e9107ca29852708"],"d4c6c7f3cda7a0595cabd16e5e9107ca29852708":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","1289047c4a6e31121c9d3a8f4c7a3fb30179f0fc"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"1289047c4a6e31121c9d3a8f4c7a3fb30179f0fc":["61d5f95d14e5b9b046998c51e16709a398c15226","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["61d5f95d14e5b9b046998c51e16709a398c15226","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}