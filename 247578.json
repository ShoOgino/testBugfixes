{"path":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","commits":[{"id":"31f025ae60076ae95274433f3fe8e6ace2857a87","date":1326669465,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandom() throws Exception {\n    // Currently only SimpleText can index offsets into postings:\n    Assume.assumeTrue(Codec.getDefault().getName().equals(\"SimpleText\"));\n\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random.nextBoolean()) {\n          text = \"a\";\n        } else if (random.nextBoolean()) {\n          text = \"b\";\n        } else if (random.nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random.nextBoolean() ? 1 : random.nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random.nextBoolean() ? 0 : random.nextInt(5);\n        final int tokenOffset = random.nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader sub : r.getSequentialSubReaders()) {\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random.nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2","a78a90fc9701e511308346ea29f4f5e548bb39fe","d4d69c535930b5cce125cff868d40f6373dc27d4","02331260bb246364779cb6f04919ca47900d01bb"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a44b232879361a7ace3520b5b313094a9a35e044","date":1327356188,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random.nextBoolean()) {\n          text = \"a\";\n        } else if (random.nextBoolean()) {\n          text = \"b\";\n        } else if (random.nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random.nextBoolean() ? 1 : random.nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random.nextBoolean() ? 0 : random.nextInt(5);\n        final int tokenOffset = random.nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader sub : r.getSequentialSubReaders()) {\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random.nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // Currently only SimpleText can index offsets into postings:\n    Assume.assumeTrue(Codec.getDefault().getName().equals(\"SimpleText\"));\n\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random.nextBoolean()) {\n          text = \"a\";\n        } else if (random.nextBoolean()) {\n          text = \"b\";\n        } else if (random.nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random.nextBoolean() ? 1 : random.nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random.nextBoolean() ? 0 : random.nextInt(5);\n        final int tokenOffset = random.nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader sub : r.getSequentialSubReaders()) {\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random.nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5a89676536a5d3e2e875a9eed6b3f22a63cca643","date":1327356915,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random.nextBoolean()) {\n          text = \"a\";\n        } else if (random.nextBoolean()) {\n          text = \"b\";\n        } else if (random.nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random.nextBoolean() ? 1 : random.nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random.nextBoolean() ? 0 : random.nextInt(5);\n        final int tokenOffset = random.nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader sub : r.getSequentialSubReaders()) {\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random.nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // Currently only SimpleText can index offsets into postings:\n    Assume.assumeTrue(Codec.getDefault().getName().equals(\"SimpleText\"));\n\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random.nextBoolean()) {\n          text = \"a\";\n        } else if (random.nextBoolean()) {\n          text = \"b\";\n        } else if (random.nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random.nextBoolean() ? 1 : random.nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random.nextBoolean() ? 0 : random.nextInt(5);\n        final int tokenOffset = random.nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader sub : r.getSequentialSubReaders()) {\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random.nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","date":1327523564,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random.nextBoolean()) {\n          text = \"a\";\n        } else if (random.nextBoolean()) {\n          text = \"b\";\n        } else if (random.nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random.nextBoolean() ? 1 : random.nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random.nextBoolean() ? 0 : random.nextInt(5);\n        final int tokenOffset = random.nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader sub : r.getSequentialSubReaders()) {\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random.nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // Currently only SimpleText can index offsets into postings:\n    Assume.assumeTrue(Codec.getDefault().getName().equals(\"SimpleText\"));\n\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random.nextBoolean()) {\n          text = \"a\";\n        } else if (random.nextBoolean()) {\n          text = \"b\";\n        } else if (random.nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random.nextBoolean() ? 1 : random.nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random.nextBoolean() ? 0 : random.nextInt(5);\n        final int tokenOffset = random.nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader sub : r.getSequentialSubReaders()) {\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random.nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"913fa4c710b6d1168655966e59f0f4de566907a8","date":1327858476,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random.nextBoolean()) {\n          text = \"a\";\n        } else if (random.nextBoolean()) {\n          text = \"b\";\n        } else if (random.nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random.nextBoolean() ? 1 : random.nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random.nextBoolean() ? 0 : random.nextInt(5);\n        final int tokenOffset = random.nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicIndexReader sub = (AtomicIndexReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random.nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random.nextBoolean()) {\n          text = \"a\";\n        } else if (random.nextBoolean()) {\n          text = \"b\";\n        } else if (random.nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random.nextBoolean() ? 1 : random.nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random.nextBoolean() ? 0 : random.nextInt(5);\n        final int tokenOffset = random.nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader sub : r.getSequentialSubReaders()) {\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random.nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["ced66195b26fdb1f77ee00e2a77ec6918dedd766"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"da6d5ac19a80d65b1e864251f155d30960353b7e","date":1327881054,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random.nextBoolean()) {\n          text = \"a\";\n        } else if (random.nextBoolean()) {\n          text = \"b\";\n        } else if (random.nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random.nextBoolean() ? 1 : random.nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random.nextBoolean() ? 0 : random.nextInt(5);\n        final int tokenOffset = random.nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random.nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random.nextBoolean()) {\n          text = \"a\";\n        } else if (random.nextBoolean()) {\n          text = \"b\";\n        } else if (random.nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random.nextBoolean() ? 1 : random.nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random.nextBoolean() ? 0 : random.nextInt(5);\n        final int tokenOffset = random.nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicIndexReader sub = (AtomicIndexReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random.nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["ced66195b26fdb1f77ee00e2a77ec6918dedd766"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5cab9a86bd67202d20b6adc463008c8e982b070a","date":1327966443,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random.nextBoolean()) {\n          text = \"a\";\n        } else if (random.nextBoolean()) {\n          text = \"b\";\n        } else if (random.nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random.nextBoolean() ? 1 : random.nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random.nextBoolean() ? 0 : random.nextInt(5);\n        final int tokenOffset = random.nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random.nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random.nextBoolean()) {\n          text = \"a\";\n        } else if (random.nextBoolean()) {\n          text = \"b\";\n        } else if (random.nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random.nextBoolean() ? 1 : random.nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random.nextBoolean() ? 0 : random.nextInt(5);\n        final int tokenOffset = random.nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader sub : r.getSequentialSubReaders()) {\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random.nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random.nextBoolean()) {\n          text = \"a\";\n        } else if (random.nextBoolean()) {\n          text = \"b\";\n        } else if (random.nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random.nextBoolean() ? 1 : random.nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random.nextBoolean() ? 0 : random.nextInt(5);\n        final int tokenOffset = random.nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random.nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random.nextBoolean()) {\n          text = \"a\";\n        } else if (random.nextBoolean()) {\n          text = \"b\";\n        } else if (random.nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random.nextBoolean() ? 1 : random.nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random.nextBoolean() ? 0 : random.nextInt(5);\n        final int tokenOffset = random.nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random.nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"5a89676536a5d3e2e875a9eed6b3f22a63cca643":["31f025ae60076ae95274433f3fe8e6ace2857a87","a44b232879361a7ace3520b5b313094a9a35e044"],"da6d5ac19a80d65b1e864251f155d30960353b7e":["913fa4c710b6d1168655966e59f0f4de566907a8"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"913fa4c710b6d1168655966e59f0f4de566907a8":["5a89676536a5d3e2e875a9eed6b3f22a63cca643"],"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d":["31f025ae60076ae95274433f3fe8e6ace2857a87","a44b232879361a7ace3520b5b313094a9a35e044"],"a44b232879361a7ace3520b5b313094a9a35e044":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["a44b232879361a7ace3520b5b313094a9a35e044","da6d5ac19a80d65b1e864251f155d30960353b7e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"5a89676536a5d3e2e875a9eed6b3f22a63cca643":["913fa4c710b6d1168655966e59f0f4de566907a8"],"da6d5ac19a80d65b1e864251f155d30960353b7e":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"913fa4c710b6d1168655966e59f0f4de566907a8":["da6d5ac19a80d65b1e864251f155d30960353b7e"],"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d":[],"a44b232879361a7ace3520b5b313094a9a35e044":["5a89676536a5d3e2e875a9eed6b3f22a63cca643","a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","5cab9a86bd67202d20b6adc463008c8e982b070a"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["5a89676536a5d3e2e875a9eed6b3f22a63cca643","a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","a44b232879361a7ace3520b5b313094a9a35e044"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}