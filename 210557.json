{"path":"solr/core/src/test/org/apache/solr/cloud/api/collections/ConcurrentCreateCollectionTest#testConcurrentCreatePlacement().mjava","commits":[{"id":"2c7f2abe6ac4f6d10bf9e8378b81b87cf40e9b64","date":1576276664,"type":0,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/ConcurrentCreateCollectionTest#testConcurrentCreatePlacement().mjava","pathOld":"/dev/null","sourceNew":"  public void testConcurrentCreatePlacement() throws Exception {\n    final int nThreads = 20;\n    final int createsPerThread = 1;\n    final int repFactor = 1;\n\n    final CloudSolrClient client = cluster.getSolrClient();\n\n    final AtomicInteger collectionNum = new AtomicInteger();\n    Thread[] indexThreads = new Thread[nThreads];\n\n\n    for (int i=0; i<nThreads; i++) {\n      indexThreads[i] = new Thread(() -> {\n        try {\n          for (int j=0; j<createsPerThread; j++) {\n            int num = collectionNum.incrementAndGet();\n            String collectionName = \"collection\" + num;\n            CollectionAdminRequest\n                .createCollection(\"collection\" + num, \"conf\", 1, repFactor)\n                .setMaxShardsPerNode(1)\n                .process(client);\n            cluster.waitForActiveCollection(collectionName, 1, repFactor);\n            // Thread.sleep(5000);\n          }\n        } catch (Exception e) {\n          fail(e.getMessage());\n        }\n      });\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.start();\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    Map<String,List<Replica>> map = new HashMap<>();\n    ClusterState cstate = client.getZkStateReader().getClusterState();\n    for (DocCollection collection : cstate.getCollectionsMap().values()) {\n      for (Replica replica : collection.getReplicas()) {\n        String url = replica.getBaseUrl();\n        List<Replica> replicas = map.get(url);\n        if (replicas == null) {\n          replicas = new ArrayList<>();\n          map.put(url, replicas);\n        }\n        replicas.add(replica);\n      }\n    }\n\n    // check if nodes are balanced\n    List<Replica> prev = null;\n    for (List<Replica> replicas : map.values()) {\n      if (prev != null && prev.size() != replicas.size()) {\n        log.error(\"UNBALANCED CLUSTER: prev node replica count=\" + prev.size() + \" current=\" + replicas.size() + \"\\n\" + cstate.getCollectionsMap());\n        log.error(\"Replica lists per node: \" + map);\n        assertEquals(prev.size(), replicas.size());\n      }\n      prev = replicas;\n    }\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"baf0c78c7a5f19bf2f5648cdadaab16181019d72","date":1576276664,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/ConcurrentCreateCollectionTest#testConcurrentCreatePlacement().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/ConcurrentCreateCollectionTest#testConcurrentCreatePlacement().mjava","sourceNew":"  public void testConcurrentCreatePlacement() throws Exception {\n    final int nThreads = 20;\n    final int createsPerThread = 1;\n    final int repFactor = 1;\n    final boolean useClusterPolicy = true;\n    final boolean useCollectionPolicy = false;\n\n    final CloudSolrClient client = cluster.getSolrClient();\n\n\n    if (useClusterPolicy) {\n      String setClusterPolicyCommand = \"{\" +\n          \" 'set-cluster-policy': [\" +\n          // \"      {'cores':'<100', 'node':'#ANY'},\" +\n          \"      {'replica':'<2', 'shard': '#EACH', 'node': '#ANY'},\" +\n          \"    ]\" +\n          \"}\";\n\n      SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, setClusterPolicyCommand);\n      client.request(req);\n    }\n\n    if (useCollectionPolicy) {\n      // NOTE: the meer act of setting this named policy prevents LegacyAssignStrategy from being used, even if the policy is\n      // not used during collection creation.\n      String commands =  \"{set-policy :{policy1 : [{replica:'<2' , node:'#ANY'}]}}\";\n      client.request(CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, commands));\n    }\n\n\n    final AtomicInteger collectionNum = new AtomicInteger();\n    Thread[] indexThreads = new Thread[nThreads];\n\n    for (int i=0; i<nThreads; i++) {\n      indexThreads[i] = new Thread(() -> {\n        try {\n          for (int j=0; j<createsPerThread; j++) {\n            int num = collectionNum.incrementAndGet();\n            String collectionName = \"collection\" + num;\n            CollectionAdminRequest.Create createReq = CollectionAdminRequest\n                .createCollection(collectionName, \"conf\", 1, repFactor)\n                .setMaxShardsPerNode(1);\n            createReq.setWaitForFinalState(false);\n            if (useCollectionPolicy) {\n              createReq.setPolicy(\"policy1\");\n            }\n\n            createReq.process(client);\n            // cluster.waitForActiveCollection(collectionName, 1, repFactor);\n            // Thread.sleep(10000);\n          }\n        } catch (Exception e) {\n          fail(e.getMessage());\n        }\n      });\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.start();\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    int expectedTotalReplicas = nThreads * createsPerThread * repFactor;\n    int expectedPerNode = expectedTotalReplicas / NODES;\n\n    Map<String,List<Replica>> replicaMap = new HashMap<>();\n    ClusterState cstate = client.getZkStateReader().getClusterState();\n    for (DocCollection collection : cstate.getCollectionsMap().values()) {\n      for (Replica replica : collection.getReplicas()) {\n        String url = replica.getBaseUrl();\n        List<Replica> replicas = replicaMap.get(url);\n        if (replicas == null) {\n          replicas = new ArrayList<>();\n          replicaMap.put(url, replicas);\n        }\n        replicas.add(replica);\n      }\n    }\n\n    // check if nodes are balanced\n    boolean failed = false;\n    for (List<Replica> replicas : replicaMap.values()) {\n      if (replicas.size() != expectedPerNode ) {\n        failed = true;\n        log.error(\"UNBALANCED CLUSTER: expected replicas per node \" + expectedPerNode +  \" but got \" + replicas.size());\n      }\n    }\n\n    if (failed) {\n      log.error(\"Cluster state \" + cstate.getCollectionsMap());\n    }\n\n    assertEquals(replicaMap.size(),  NODES);  // make sure something was created\n\n    assertTrue(!failed);\n  }\n\n","sourceOld":"  public void testConcurrentCreatePlacement() throws Exception {\n    final int nThreads = 20;\n    final int createsPerThread = 1;\n    final int repFactor = 1;\n\n    final CloudSolrClient client = cluster.getSolrClient();\n\n    final AtomicInteger collectionNum = new AtomicInteger();\n    Thread[] indexThreads = new Thread[nThreads];\n\n\n    for (int i=0; i<nThreads; i++) {\n      indexThreads[i] = new Thread(() -> {\n        try {\n          for (int j=0; j<createsPerThread; j++) {\n            int num = collectionNum.incrementAndGet();\n            String collectionName = \"collection\" + num;\n            CollectionAdminRequest\n                .createCollection(\"collection\" + num, \"conf\", 1, repFactor)\n                .setMaxShardsPerNode(1)\n                .process(client);\n            cluster.waitForActiveCollection(collectionName, 1, repFactor);\n            // Thread.sleep(5000);\n          }\n        } catch (Exception e) {\n          fail(e.getMessage());\n        }\n      });\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.start();\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    Map<String,List<Replica>> map = new HashMap<>();\n    ClusterState cstate = client.getZkStateReader().getClusterState();\n    for (DocCollection collection : cstate.getCollectionsMap().values()) {\n      for (Replica replica : collection.getReplicas()) {\n        String url = replica.getBaseUrl();\n        List<Replica> replicas = map.get(url);\n        if (replicas == null) {\n          replicas = new ArrayList<>();\n          map.put(url, replicas);\n        }\n        replicas.add(replica);\n      }\n    }\n\n    // check if nodes are balanced\n    List<Replica> prev = null;\n    for (List<Replica> replicas : map.values()) {\n      if (prev != null && prev.size() != replicas.size()) {\n        log.error(\"UNBALANCED CLUSTER: prev node replica count=\" + prev.size() + \" current=\" + replicas.size() + \"\\n\" + cstate.getCollectionsMap());\n        log.error(\"Replica lists per node: \" + map);\n        assertEquals(prev.size(), replicas.size());\n      }\n      prev = replicas;\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1983c3206274e794a3d58501a0c4c3d4fd56555f","date":1576276664,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/ConcurrentCreateCollectionTest#testConcurrentCreatePlacement().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/ConcurrentCreateCollectionTest#testConcurrentCreatePlacement().mjava","sourceNew":"  public void testConcurrentCreatePlacement() throws Exception {\n    final int nThreads = 2;\n    final int createsPerThread = 1;\n    final int nShards = 1;\n    final int repFactor = 2;\n    final boolean useClusterPolicy = false;\n    final boolean useCollectionPolicy = true;\n    final boolean startUnbalanced = true; // can help make a smaller test that can still reproduce an issue.\n    final int unbalancedSize = 1; // the number of replicas to create first\n    final boolean stopNode = false;  // only applicable when startUnbalanced==true... stops a node during first collection creation, then restarts\n\n    final CloudSolrClient client = cluster.getSolrClient();\n\n\n    if (startUnbalanced) {\n      /*** This produces a failure (multiple replicas of single shard on same node) when run with NODES=4 and\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 2;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = true;\n       // NOTE: useClusterPolicy=true seems to fix it! So does putting both creates in a single thread!\n       // NOTE: even creating a single replica to start with causes failure later on.\n\n       Also reproduced with smaller cluster: NODES=2 and\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 1;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = true;\n\n       Also, with NODES=3:\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 1;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = false;\n\n       // Also succeeded in replicating a bug where all 5 replicas were on a single node: CORES=5, nThreads=5, repFactor=5,\n       //    unbalancedSize = 16 (4 replicas on each of the up nodes), stopNode=true\n       ***/\n\n\n      JettySolrRunner downJetty = cluster.getJettySolrRunners().get(0);\n      if (stopNode) {\n        cluster.stopJettySolrRunner(downJetty);\n      }\n\n      String cname = \"STARTCOLLECTION\";\n      CollectionAdminRequest.Create creq = CollectionAdminRequest\n          // nocommit .createCollection(cname, \"conf\", NODES - 1, NODES - 1)\n          .createCollection(cname, \"conf\", unbalancedSize, 1)\n          .setMaxShardsPerNode(100);\n      creq.setWaitForFinalState(true);\n      // creq.setAutoAddReplicas(true);\n      if (useCollectionPolicy) { creq.setPolicy(\"policy1\"); }\n      creq.process(client);\n\n      if (stopNode) {\n        // this will start it with a new port.... does it matter?\n        cluster.startJettySolrRunner(downJetty);\n      }\n    }\n\n\n\n    if (useClusterPolicy) {\n      String setClusterPolicyCommand = \"{\" +\n          \" 'set-cluster-policy': [\" +\n          // \"      {'cores':'<100', 'node':'#ANY'},\" +\n          \"      {'replica':'<2', 'shard': '#EACH', 'node': '#ANY'},\" +\n          // \"      {'replica':'<2', 'node': '#ANY'},\" +\n          \"    ]\" +\n          \"}\";\n\n      SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, setClusterPolicyCommand);\n      client.request(req);\n    }\n\n    if (useCollectionPolicy) {\n      // NOTE: the mere act of setting this named policy prevents LegacyAssignStrategy from being used, even if the policy is\n      // not used during collection creation.\n      String commands =  \"{set-policy : {\" +\n          \" policy1 : [{replica:'<2' , node:'#ANY'}]\" +\n          \",policy2 : [{replica:'<2' , shard:'#EACH', node:'#ANY'}]\" +\n          \"}}\";\n      client.request(CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, commands));\n\n      /*** take defaults for cluster preferences\n      String cmd = \"{\" +\n          \" 'set-cluster-preferences': [\" +\n          // \"      {'cores':'<100', 'node':'#ANY'},\" +\n          \"      {minimize:cores}\" +\n          \"    ]\" +\n          \"}\";\n\n      SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, cmd);\n      client.request(req);\n       ***/\n    }\n\n    /***\n    SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.GET, null);\n    SolrResponse response = req.process(client);\n    log.info(\"######### AUTOSCALE \" + response);\n     ***/\n\n\n    byte[] data = client.getZkStateReader().getZkClient().getData(\"/autoscaling.json\", null, null, true);\n    log.info(\"AUTOSCALE DATA: \" + new String(data, \"UTF-8\"));\n\n    final AtomicInteger collectionNum = new AtomicInteger();\n    Thread[] indexThreads = new Thread[nThreads];\n\n    for (int i=0; i<nThreads; i++) {\n      indexThreads[i] = new Thread(() -> {\n        try {\n          for (int j=0; j<createsPerThread; j++) {\n            int num = collectionNum.incrementAndGet();\n            // Thread.sleep(num*1000); // nocommit\n            String collectionName = \"collection\" + num;\n            CollectionAdminRequest.Create createReq = CollectionAdminRequest\n                .createCollection(collectionName, \"conf\", nShards, repFactor)\n                // .setMaxShardsPerNode(1) // should be default\n                ;\n            createReq.setWaitForFinalState(false);\n            if (useCollectionPolicy) {\n              createReq.setPolicy(\"policy1\");\n            }\n            createReq.setAutoAddReplicas(true);\n\n            createReq.process(client);\n            // cluster.waitForActiveCollection(collectionName, 1, repFactor);\n            // Thread.sleep(10000);\n          }\n        } catch (Exception e) {\n          fail(e.getMessage());\n        }\n      });\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.start();\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    int expectedTotalReplicas = unbalancedSize + nThreads * createsPerThread * nShards * repFactor;\n    int expectedPerNode = expectedTotalReplicas / NODES;\n    boolean expectBalanced = (expectedPerNode * NODES == expectedTotalReplicas);\n\n    Map<String,List<Replica>> replicaMap = new HashMap<>();\n    ClusterState cstate = client.getZkStateReader().getClusterState();\n    for (DocCollection collection : cstate.getCollectionsMap().values()) {\n      for (Replica replica : collection.getReplicas()) {\n        String url = replica.getBaseUrl();\n        List<Replica> replicas = replicaMap.get(url);\n        if (replicas == null) {\n          replicas = new ArrayList<>();\n          replicaMap.put(url, replicas);\n        }\n        replicas.add(replica);\n      }\n    }\n\n    // check if nodes are balanced\n    boolean failed = false;\n    for (List<Replica> replicas : replicaMap.values()) {\n      if (replicas.size() != expectedPerNode ) {\n        if (expectBalanced) {\n          failed = true;\n        }\n        log.error(\"UNBALANCED CLUSTER: expected replicas per node \" + expectedPerNode +  \" but got \" + replicas.size());\n      }\n    }\n\n    // check if there were multiple replicas of the same shard placed on the same node\n    for (DocCollection collection : cstate.getCollectionsMap().values()) {\n      for (Slice slice : collection.getSlices()) {\n        Map<String, Replica> nodeToReplica = new HashMap<>();\n        for (Replica replica : slice.getReplicas()) {\n          Replica prev = nodeToReplica.put(replica.getBaseUrl(), replica);\n          if (prev != null) {\n            failed = true;\n            // NOTE: with a replication factor > 2, this will print multiple times per bad slice.\n            log.error(\"MULTIPLE REPLICAS OF SINGLE SHARD ON SAME NODE: r1=\" + prev + \" r2=\" + replica);\n          }\n        }\n      }\n    }\n\n    if (failed) {\n      log.error(\"Cluster state \" + cstate.getCollectionsMap());\n    }\n\n    assertEquals(replicaMap.size(),  NODES);  // make sure something was created\n\n    assertTrue(!failed);\n  }\n\n","sourceOld":"  public void testConcurrentCreatePlacement() throws Exception {\n    final int nThreads = 20;\n    final int createsPerThread = 1;\n    final int repFactor = 1;\n    final boolean useClusterPolicy = true;\n    final boolean useCollectionPolicy = false;\n\n    final CloudSolrClient client = cluster.getSolrClient();\n\n\n    if (useClusterPolicy) {\n      String setClusterPolicyCommand = \"{\" +\n          \" 'set-cluster-policy': [\" +\n          // \"      {'cores':'<100', 'node':'#ANY'},\" +\n          \"      {'replica':'<2', 'shard': '#EACH', 'node': '#ANY'},\" +\n          \"    ]\" +\n          \"}\";\n\n      SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, setClusterPolicyCommand);\n      client.request(req);\n    }\n\n    if (useCollectionPolicy) {\n      // NOTE: the meer act of setting this named policy prevents LegacyAssignStrategy from being used, even if the policy is\n      // not used during collection creation.\n      String commands =  \"{set-policy :{policy1 : [{replica:'<2' , node:'#ANY'}]}}\";\n      client.request(CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, commands));\n    }\n\n\n    final AtomicInteger collectionNum = new AtomicInteger();\n    Thread[] indexThreads = new Thread[nThreads];\n\n    for (int i=0; i<nThreads; i++) {\n      indexThreads[i] = new Thread(() -> {\n        try {\n          for (int j=0; j<createsPerThread; j++) {\n            int num = collectionNum.incrementAndGet();\n            String collectionName = \"collection\" + num;\n            CollectionAdminRequest.Create createReq = CollectionAdminRequest\n                .createCollection(collectionName, \"conf\", 1, repFactor)\n                .setMaxShardsPerNode(1);\n            createReq.setWaitForFinalState(false);\n            if (useCollectionPolicy) {\n              createReq.setPolicy(\"policy1\");\n            }\n\n            createReq.process(client);\n            // cluster.waitForActiveCollection(collectionName, 1, repFactor);\n            // Thread.sleep(10000);\n          }\n        } catch (Exception e) {\n          fail(e.getMessage());\n        }\n      });\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.start();\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    int expectedTotalReplicas = nThreads * createsPerThread * repFactor;\n    int expectedPerNode = expectedTotalReplicas / NODES;\n\n    Map<String,List<Replica>> replicaMap = new HashMap<>();\n    ClusterState cstate = client.getZkStateReader().getClusterState();\n    for (DocCollection collection : cstate.getCollectionsMap().values()) {\n      for (Replica replica : collection.getReplicas()) {\n        String url = replica.getBaseUrl();\n        List<Replica> replicas = replicaMap.get(url);\n        if (replicas == null) {\n          replicas = new ArrayList<>();\n          replicaMap.put(url, replicas);\n        }\n        replicas.add(replica);\n      }\n    }\n\n    // check if nodes are balanced\n    boolean failed = false;\n    for (List<Replica> replicas : replicaMap.values()) {\n      if (replicas.size() != expectedPerNode ) {\n        failed = true;\n        log.error(\"UNBALANCED CLUSTER: expected replicas per node \" + expectedPerNode +  \" but got \" + replicas.size());\n      }\n    }\n\n    if (failed) {\n      log.error(\"Cluster state \" + cstate.getCollectionsMap());\n    }\n\n    assertEquals(replicaMap.size(),  NODES);  // make sure something was created\n\n    assertTrue(!failed);\n  }\n\n","bugFix":null,"bugIntro":["67a8d31cfc3fe149dba4b571b310a8d7b9c2a227"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"67a8d31cfc3fe149dba4b571b310a8d7b9c2a227","date":1576333873,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/ConcurrentCreateCollectionTest#testConcurrentCreatePlacement().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/ConcurrentCreateCollectionTest#testConcurrentCreatePlacement().mjava","sourceNew":"  public void testConcurrentCreatePlacement() throws Exception {\n    final int nThreads = 2;\n    final int createsPerThread = 1;\n    final int nShards = 1;\n    final int repFactor = 2;\n    final boolean useClusterPolicy = false;\n    final boolean useCollectionPolicy = true;\n    final boolean startUnbalanced = true; // can help make a smaller test that can still reproduce an issue.\n    final int unbalancedSize = 1; // the number of replicas to create first\n    final boolean stopNode = false;  // only applicable when startUnbalanced==true... stops a node during first collection creation, then restarts\n\n    final CloudSolrClient client = cluster.getSolrClient();\n\n\n    if (startUnbalanced) {\n      /*** This produces a failure (multiple replicas of single shard on same node) when run with NODES=4 and\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 2;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = true;\n       // NOTE: useClusterPolicy=true seems to fix it! So does putting both creates in a single thread!\n       // NOTE: even creating a single replica to start with causes failure later on.\n\n       Also reproduced with smaller cluster: NODES=2 and\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 1;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = true;\n\n       Also, with NODES=3:\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 1;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = false;\n\n       // Also succeeded in replicating a bug where all 5 replicas were on a single node: CORES=5, nThreads=5, repFactor=5,\n       //    unbalancedSize = 16 (4 replicas on each of the up nodes), stopNode=true\n       ***/\n\n\n      JettySolrRunner downJetty = cluster.getJettySolrRunners().get(0);\n      if (stopNode) {\n        cluster.stopJettySolrRunner(downJetty);\n      }\n\n      String cname = \"STARTCOLLECTION\";\n      CollectionAdminRequest.Create creq = CollectionAdminRequest\n          //  .createCollection(cname, \"conf\", NODES - 1, NODES - 1)\n          .createCollection(cname, \"conf\", unbalancedSize, 1)\n          .setMaxShardsPerNode(100);\n      creq.setWaitForFinalState(true);\n      // creq.setAutoAddReplicas(true);\n      if (useCollectionPolicy) { creq.setPolicy(\"policy1\"); }\n      creq.process(client);\n\n      if (stopNode) {\n        // this will start it with a new port.... does it matter?\n        cluster.startJettySolrRunner(downJetty);\n      }\n    }\n\n\n\n    if (useClusterPolicy) {\n      String setClusterPolicyCommand = \"{\" +\n          \" 'set-cluster-policy': [\" +\n          // \"      {'cores':'<100', 'node':'#ANY'},\" +\n          \"      {'replica':'<2', 'shard': '#EACH', 'node': '#ANY'},\" +\n          // \"      {'replica':'<2', 'node': '#ANY'},\" +\n          \"    ]\" +\n          \"}\";\n\n      SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, setClusterPolicyCommand);\n      client.request(req);\n    }\n\n    if (useCollectionPolicy) {\n      // NOTE: the mere act of setting this named policy prevents LegacyAssignStrategy from being used, even if the policy is\n      // not used during collection creation.\n      String commands =  \"{set-policy : {\" +\n          \" policy1 : [{replica:'<2' , node:'#ANY'}]\" +\n          \",policy2 : [{replica:'<2' , shard:'#EACH', node:'#ANY'}]\" +\n          \"}}\";\n      client.request(CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, commands));\n\n      /*** take defaults for cluster preferences\n      String cmd = \"{\" +\n          \" 'set-cluster-preferences': [\" +\n          // \"      {'cores':'<100', 'node':'#ANY'},\" +\n          \"      {minimize:cores}\" +\n          \"    ]\" +\n          \"}\";\n\n      SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, cmd);\n      client.request(req);\n       ***/\n    }\n\n    /***\n    SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.GET, null);\n    SolrResponse response = req.process(client);\n    log.info(\"######### AUTOSCALE \" + response);\n     ***/\n\n\n    byte[] data = client.getZkStateReader().getZkClient().getData(\"/autoscaling.json\", null, null, true);\n    log.info(\"AUTOSCALE DATA: \" + new String(data, \"UTF-8\"));\n\n    final AtomicInteger collectionNum = new AtomicInteger();\n    Thread[] indexThreads = new Thread[nThreads];\n\n    for (int i=0; i<nThreads; i++) {\n      indexThreads[i] = new Thread(() -> {\n        try {\n          for (int j=0; j<createsPerThread; j++) {\n            int num = collectionNum.incrementAndGet();\n            // Thread.sleep(num*1000);\n            String collectionName = \"collection\" + num;\n            CollectionAdminRequest.Create createReq = CollectionAdminRequest\n                .createCollection(collectionName, \"conf\", nShards, repFactor)\n                // .setMaxShardsPerNode(1) // should be default\n                ;\n            createReq.setWaitForFinalState(false);\n            if (useCollectionPolicy) {\n              createReq.setPolicy(\"policy1\");\n            }\n            createReq.setAutoAddReplicas(true);\n\n            createReq.process(client);\n            // cluster.waitForActiveCollection(collectionName, 1, repFactor);\n            // Thread.sleep(10000);\n          }\n        } catch (Exception e) {\n          fail(e.getMessage());\n        }\n      });\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.start();\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    int expectedTotalReplicas = unbalancedSize + nThreads * createsPerThread * nShards * repFactor;\n    int expectedPerNode = expectedTotalReplicas / NODES;\n    boolean expectBalanced = (expectedPerNode * NODES == expectedTotalReplicas);\n\n    Map<String,List<Replica>> replicaMap = new HashMap<>();\n    ClusterState cstate = client.getZkStateReader().getClusterState();\n    for (DocCollection collection : cstate.getCollectionsMap().values()) {\n      for (Replica replica : collection.getReplicas()) {\n        String url = replica.getBaseUrl();\n        List<Replica> replicas = replicaMap.get(url);\n        if (replicas == null) {\n          replicas = new ArrayList<>();\n          replicaMap.put(url, replicas);\n        }\n        replicas.add(replica);\n      }\n    }\n\n    // check if nodes are balanced\n    boolean failed = false;\n    for (List<Replica> replicas : replicaMap.values()) {\n      if (replicas.size() != expectedPerNode ) {\n        if (expectBalanced) {\n          failed = true;\n        }\n        log.error(\"UNBALANCED CLUSTER: expected replicas per node \" + expectedPerNode +  \" but got \" + replicas.size());\n      }\n    }\n\n    // check if there were multiple replicas of the same shard placed on the same node\n    for (DocCollection collection : cstate.getCollectionsMap().values()) {\n      for (Slice slice : collection.getSlices()) {\n        Map<String, Replica> nodeToReplica = new HashMap<>();\n        for (Replica replica : slice.getReplicas()) {\n          Replica prev = nodeToReplica.put(replica.getBaseUrl(), replica);\n          if (prev != null) {\n            failed = true;\n            // NOTE: with a replication factor > 2, this will print multiple times per bad slice.\n            log.error(\"MULTIPLE REPLICAS OF SINGLE SHARD ON SAME NODE: r1=\" + prev + \" r2=\" + replica);\n          }\n        }\n      }\n    }\n\n    if (failed) {\n      log.error(\"Cluster state \" + cstate.getCollectionsMap());\n    }\n\n    assertEquals(replicaMap.size(),  NODES);  // make sure something was created\n\n    assertTrue(!failed);\n  }\n\n","sourceOld":"  public void testConcurrentCreatePlacement() throws Exception {\n    final int nThreads = 2;\n    final int createsPerThread = 1;\n    final int nShards = 1;\n    final int repFactor = 2;\n    final boolean useClusterPolicy = false;\n    final boolean useCollectionPolicy = true;\n    final boolean startUnbalanced = true; // can help make a smaller test that can still reproduce an issue.\n    final int unbalancedSize = 1; // the number of replicas to create first\n    final boolean stopNode = false;  // only applicable when startUnbalanced==true... stops a node during first collection creation, then restarts\n\n    final CloudSolrClient client = cluster.getSolrClient();\n\n\n    if (startUnbalanced) {\n      /*** This produces a failure (multiple replicas of single shard on same node) when run with NODES=4 and\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 2;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = true;\n       // NOTE: useClusterPolicy=true seems to fix it! So does putting both creates in a single thread!\n       // NOTE: even creating a single replica to start with causes failure later on.\n\n       Also reproduced with smaller cluster: NODES=2 and\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 1;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = true;\n\n       Also, with NODES=3:\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 1;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = false;\n\n       // Also succeeded in replicating a bug where all 5 replicas were on a single node: CORES=5, nThreads=5, repFactor=5,\n       //    unbalancedSize = 16 (4 replicas on each of the up nodes), stopNode=true\n       ***/\n\n\n      JettySolrRunner downJetty = cluster.getJettySolrRunners().get(0);\n      if (stopNode) {\n        cluster.stopJettySolrRunner(downJetty);\n      }\n\n      String cname = \"STARTCOLLECTION\";\n      CollectionAdminRequest.Create creq = CollectionAdminRequest\n          // nocommit .createCollection(cname, \"conf\", NODES - 1, NODES - 1)\n          .createCollection(cname, \"conf\", unbalancedSize, 1)\n          .setMaxShardsPerNode(100);\n      creq.setWaitForFinalState(true);\n      // creq.setAutoAddReplicas(true);\n      if (useCollectionPolicy) { creq.setPolicy(\"policy1\"); }\n      creq.process(client);\n\n      if (stopNode) {\n        // this will start it with a new port.... does it matter?\n        cluster.startJettySolrRunner(downJetty);\n      }\n    }\n\n\n\n    if (useClusterPolicy) {\n      String setClusterPolicyCommand = \"{\" +\n          \" 'set-cluster-policy': [\" +\n          // \"      {'cores':'<100', 'node':'#ANY'},\" +\n          \"      {'replica':'<2', 'shard': '#EACH', 'node': '#ANY'},\" +\n          // \"      {'replica':'<2', 'node': '#ANY'},\" +\n          \"    ]\" +\n          \"}\";\n\n      SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, setClusterPolicyCommand);\n      client.request(req);\n    }\n\n    if (useCollectionPolicy) {\n      // NOTE: the mere act of setting this named policy prevents LegacyAssignStrategy from being used, even if the policy is\n      // not used during collection creation.\n      String commands =  \"{set-policy : {\" +\n          \" policy1 : [{replica:'<2' , node:'#ANY'}]\" +\n          \",policy2 : [{replica:'<2' , shard:'#EACH', node:'#ANY'}]\" +\n          \"}}\";\n      client.request(CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, commands));\n\n      /*** take defaults for cluster preferences\n      String cmd = \"{\" +\n          \" 'set-cluster-preferences': [\" +\n          // \"      {'cores':'<100', 'node':'#ANY'},\" +\n          \"      {minimize:cores}\" +\n          \"    ]\" +\n          \"}\";\n\n      SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, cmd);\n      client.request(req);\n       ***/\n    }\n\n    /***\n    SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.GET, null);\n    SolrResponse response = req.process(client);\n    log.info(\"######### AUTOSCALE \" + response);\n     ***/\n\n\n    byte[] data = client.getZkStateReader().getZkClient().getData(\"/autoscaling.json\", null, null, true);\n    log.info(\"AUTOSCALE DATA: \" + new String(data, \"UTF-8\"));\n\n    final AtomicInteger collectionNum = new AtomicInteger();\n    Thread[] indexThreads = new Thread[nThreads];\n\n    for (int i=0; i<nThreads; i++) {\n      indexThreads[i] = new Thread(() -> {\n        try {\n          for (int j=0; j<createsPerThread; j++) {\n            int num = collectionNum.incrementAndGet();\n            // Thread.sleep(num*1000); // nocommit\n            String collectionName = \"collection\" + num;\n            CollectionAdminRequest.Create createReq = CollectionAdminRequest\n                .createCollection(collectionName, \"conf\", nShards, repFactor)\n                // .setMaxShardsPerNode(1) // should be default\n                ;\n            createReq.setWaitForFinalState(false);\n            if (useCollectionPolicy) {\n              createReq.setPolicy(\"policy1\");\n            }\n            createReq.setAutoAddReplicas(true);\n\n            createReq.process(client);\n            // cluster.waitForActiveCollection(collectionName, 1, repFactor);\n            // Thread.sleep(10000);\n          }\n        } catch (Exception e) {\n          fail(e.getMessage());\n        }\n      });\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.start();\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    int expectedTotalReplicas = unbalancedSize + nThreads * createsPerThread * nShards * repFactor;\n    int expectedPerNode = expectedTotalReplicas / NODES;\n    boolean expectBalanced = (expectedPerNode * NODES == expectedTotalReplicas);\n\n    Map<String,List<Replica>> replicaMap = new HashMap<>();\n    ClusterState cstate = client.getZkStateReader().getClusterState();\n    for (DocCollection collection : cstate.getCollectionsMap().values()) {\n      for (Replica replica : collection.getReplicas()) {\n        String url = replica.getBaseUrl();\n        List<Replica> replicas = replicaMap.get(url);\n        if (replicas == null) {\n          replicas = new ArrayList<>();\n          replicaMap.put(url, replicas);\n        }\n        replicas.add(replica);\n      }\n    }\n\n    // check if nodes are balanced\n    boolean failed = false;\n    for (List<Replica> replicas : replicaMap.values()) {\n      if (replicas.size() != expectedPerNode ) {\n        if (expectBalanced) {\n          failed = true;\n        }\n        log.error(\"UNBALANCED CLUSTER: expected replicas per node \" + expectedPerNode +  \" but got \" + replicas.size());\n      }\n    }\n\n    // check if there were multiple replicas of the same shard placed on the same node\n    for (DocCollection collection : cstate.getCollectionsMap().values()) {\n      for (Slice slice : collection.getSlices()) {\n        Map<String, Replica> nodeToReplica = new HashMap<>();\n        for (Replica replica : slice.getReplicas()) {\n          Replica prev = nodeToReplica.put(replica.getBaseUrl(), replica);\n          if (prev != null) {\n            failed = true;\n            // NOTE: with a replication factor > 2, this will print multiple times per bad slice.\n            log.error(\"MULTIPLE REPLICAS OF SINGLE SHARD ON SAME NODE: r1=\" + prev + \" r2=\" + replica);\n          }\n        }\n      }\n    }\n\n    if (failed) {\n      log.error(\"Cluster state \" + cstate.getCollectionsMap());\n    }\n\n    assertEquals(replicaMap.size(),  NODES);  // make sure something was created\n\n    assertTrue(!failed);\n  }\n\n","bugFix":["1983c3206274e794a3d58501a0c4c3d4fd56555f"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"06595b0c22c7d3075c4104d3820cccf95d9d8a43","date":1576491645,"type":0,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/ConcurrentCreateCollectionTest#testConcurrentCreatePlacement().mjava","pathOld":"/dev/null","sourceNew":"  public void testConcurrentCreatePlacement() throws Exception {\n    final int nThreads = 2;\n    final int createsPerThread = 1;\n    final int nShards = 1;\n    final int repFactor = 2;\n    final boolean useClusterPolicy = false;\n    final boolean useCollectionPolicy = true;\n    final boolean startUnbalanced = true; // can help make a smaller test that can still reproduce an issue.\n    final int unbalancedSize = 1; // the number of replicas to create first\n    final boolean stopNode = false;  // only applicable when startUnbalanced==true... stops a node during first collection creation, then restarts\n\n    final CloudSolrClient client = cluster.getSolrClient();\n\n\n    if (startUnbalanced) {\n      /*** This produces a failure (multiple replicas of single shard on same node) when run with NODES=4 and\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 2;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = true;\n       // NOTE: useClusterPolicy=true seems to fix it! So does putting both creates in a single thread!\n       // NOTE: even creating a single replica to start with causes failure later on.\n\n       Also reproduced with smaller cluster: NODES=2 and\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 1;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = true;\n\n       Also, with NODES=3:\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 1;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = false;\n\n       // Also succeeded in replicating a bug where all 5 replicas were on a single node: CORES=5, nThreads=5, repFactor=5,\n       //    unbalancedSize = 16 (4 replicas on each of the up nodes), stopNode=true\n       ***/\n\n\n      JettySolrRunner downJetty = cluster.getJettySolrRunners().get(0);\n      if (stopNode) {\n        cluster.stopJettySolrRunner(downJetty);\n      }\n\n      String cname = \"STARTCOLLECTION\";\n      CollectionAdminRequest.Create creq = CollectionAdminRequest\n          //  .createCollection(cname, \"conf\", NODES - 1, NODES - 1)\n          .createCollection(cname, \"conf\", unbalancedSize, 1)\n          .setMaxShardsPerNode(100);\n      creq.setWaitForFinalState(true);\n      // creq.setAutoAddReplicas(true);\n      if (useCollectionPolicy) { creq.setPolicy(\"policy1\"); }\n      creq.process(client);\n\n      if (stopNode) {\n        // this will start it with a new port.... does it matter?\n        cluster.startJettySolrRunner(downJetty);\n      }\n    }\n\n\n\n    if (useClusterPolicy) {\n      String setClusterPolicyCommand = \"{\" +\n          \" 'set-cluster-policy': [\" +\n          // \"      {'cores':'<100', 'node':'#ANY'},\" +\n          \"      {'replica':'<2', 'shard': '#EACH', 'node': '#ANY'},\" +\n          // \"      {'replica':'<2', 'node': '#ANY'},\" +\n          \"    ]\" +\n          \"}\";\n\n      SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, setClusterPolicyCommand);\n      client.request(req);\n    }\n\n    if (useCollectionPolicy) {\n      // NOTE: the mere act of setting this named policy prevents LegacyAssignStrategy from being used, even if the policy is\n      // not used during collection creation.\n      String commands =  \"{set-policy : {\" +\n          \" policy1 : [{replica:'<2' , node:'#ANY'}]\" +\n          \",policy2 : [{replica:'<2' , shard:'#EACH', node:'#ANY'}]\" +\n          \"}}\";\n      client.request(CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, commands));\n\n      /*** take defaults for cluster preferences\n      String cmd = \"{\" +\n          \" 'set-cluster-preferences': [\" +\n          // \"      {'cores':'<100', 'node':'#ANY'},\" +\n          \"      {minimize:cores}\" +\n          \"    ]\" +\n          \"}\";\n\n      SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, cmd);\n      client.request(req);\n       ***/\n    }\n\n    /***\n    SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.GET, null);\n    SolrResponse response = req.process(client);\n    log.info(\"######### AUTOSCALE \" + response);\n     ***/\n\n\n    byte[] data = client.getZkStateReader().getZkClient().getData(\"/autoscaling.json\", null, null, true);\n    log.info(\"AUTOSCALE DATA: \" + new String(data, \"UTF-8\"));\n\n    final AtomicInteger collectionNum = new AtomicInteger();\n    Thread[] indexThreads = new Thread[nThreads];\n\n    for (int i=0; i<nThreads; i++) {\n      indexThreads[i] = new Thread(() -> {\n        try {\n          for (int j=0; j<createsPerThread; j++) {\n            int num = collectionNum.incrementAndGet();\n            // Thread.sleep(num*1000);\n            String collectionName = \"collection\" + num;\n            CollectionAdminRequest.Create createReq = CollectionAdminRequest\n                .createCollection(collectionName, \"conf\", nShards, repFactor)\n                // .setMaxShardsPerNode(1) // should be default\n                ;\n            createReq.setWaitForFinalState(false);\n            if (useCollectionPolicy) {\n              createReq.setPolicy(\"policy1\");\n            }\n            createReq.setAutoAddReplicas(true);\n\n            createReq.process(client);\n            // cluster.waitForActiveCollection(collectionName, 1, repFactor);\n            // Thread.sleep(10000);\n          }\n        } catch (Exception e) {\n          fail(e.getMessage());\n        }\n      });\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.start();\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    int expectedTotalReplicas = unbalancedSize + nThreads * createsPerThread * nShards * repFactor;\n    int expectedPerNode = expectedTotalReplicas / NODES;\n    boolean expectBalanced = (expectedPerNode * NODES == expectedTotalReplicas);\n\n    Map<String,List<Replica>> replicaMap = new HashMap<>();\n    ClusterState cstate = client.getZkStateReader().getClusterState();\n    for (DocCollection collection : cstate.getCollectionsMap().values()) {\n      for (Replica replica : collection.getReplicas()) {\n        String url = replica.getBaseUrl();\n        List<Replica> replicas = replicaMap.get(url);\n        if (replicas == null) {\n          replicas = new ArrayList<>();\n          replicaMap.put(url, replicas);\n        }\n        replicas.add(replica);\n      }\n    }\n\n    // check if nodes are balanced\n    boolean failed = false;\n    for (List<Replica> replicas : replicaMap.values()) {\n      if (replicas.size() != expectedPerNode ) {\n        if (expectBalanced) {\n          failed = true;\n        }\n        log.error(\"UNBALANCED CLUSTER: expected replicas per node \" + expectedPerNode +  \" but got \" + replicas.size());\n      }\n    }\n\n    // check if there were multiple replicas of the same shard placed on the same node\n    for (DocCollection collection : cstate.getCollectionsMap().values()) {\n      for (Slice slice : collection.getSlices()) {\n        Map<String, Replica> nodeToReplica = new HashMap<>();\n        for (Replica replica : slice.getReplicas()) {\n          Replica prev = nodeToReplica.put(replica.getBaseUrl(), replica);\n          if (prev != null) {\n            failed = true;\n            // NOTE: with a replication factor > 2, this will print multiple times per bad slice.\n            log.error(\"MULTIPLE REPLICAS OF SINGLE SHARD ON SAME NODE: r1=\" + prev + \" r2=\" + replica);\n          }\n        }\n      }\n    }\n\n    if (failed) {\n      log.error(\"Cluster state \" + cstate.getCollectionsMap());\n    }\n\n    assertEquals(replicaMap.size(),  NODES);  // make sure something was created\n\n    assertTrue(!failed);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4","date":1588172214,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/ConcurrentCreateCollectionTest#testConcurrentCreatePlacement().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/ConcurrentCreateCollectionTest#testConcurrentCreatePlacement().mjava","sourceNew":"  public void testConcurrentCreatePlacement() throws Exception {\n    final int nThreads = 2;\n    final int createsPerThread = 1;\n    final int nShards = 1;\n    final int repFactor = 2;\n    final boolean useClusterPolicy = false;\n    final boolean useCollectionPolicy = true;\n    final boolean startUnbalanced = true; // can help make a smaller test that can still reproduce an issue.\n    final int unbalancedSize = 1; // the number of replicas to create first\n    final boolean stopNode = false;  // only applicable when startUnbalanced==true... stops a node during first collection creation, then restarts\n\n    final CloudSolrClient client = cluster.getSolrClient();\n\n\n    if (startUnbalanced) {\n      /*** This produces a failure (multiple replicas of single shard on same node) when run with NODES=4 and\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 2;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = true;\n       // NOTE: useClusterPolicy=true seems to fix it! So does putting both creates in a single thread!\n       // NOTE: even creating a single replica to start with causes failure later on.\n\n       Also reproduced with smaller cluster: NODES=2 and\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 1;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = true;\n\n       Also, with NODES=3:\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 1;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = false;\n\n       // Also succeeded in replicating a bug where all 5 replicas were on a single node: CORES=5, nThreads=5, repFactor=5,\n       //    unbalancedSize = 16 (4 replicas on each of the up nodes), stopNode=true\n       ***/\n\n\n      JettySolrRunner downJetty = cluster.getJettySolrRunners().get(0);\n      if (stopNode) {\n        cluster.stopJettySolrRunner(downJetty);\n      }\n\n      String cname = \"STARTCOLLECTION\";\n      CollectionAdminRequest.Create creq = CollectionAdminRequest\n          //  .createCollection(cname, \"conf\", NODES - 1, NODES - 1)\n          .createCollection(cname, \"conf\", unbalancedSize, 1)\n          .setMaxShardsPerNode(100);\n      creq.setWaitForFinalState(true);\n      // creq.setAutoAddReplicas(true);\n      if (useCollectionPolicy) { creq.setPolicy(\"policy1\"); }\n      creq.process(client);\n\n      if (stopNode) {\n        // this will start it with a new port.... does it matter?\n        cluster.startJettySolrRunner(downJetty);\n      }\n    }\n\n\n\n    if (useClusterPolicy) {\n      String setClusterPolicyCommand = \"{\" +\n          \" 'set-cluster-policy': [\" +\n          // \"      {'cores':'<100', 'node':'#ANY'},\" +\n          \"      {'replica':'<2', 'shard': '#EACH', 'node': '#ANY'},\" +\n          // \"      {'replica':'<2', 'node': '#ANY'},\" +\n          \"    ]\" +\n          \"}\";\n\n      SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, setClusterPolicyCommand);\n      client.request(req);\n    }\n\n    if (useCollectionPolicy) {\n      // NOTE: the mere act of setting this named policy prevents LegacyAssignStrategy from being used, even if the policy is\n      // not used during collection creation.\n      String commands =  \"{set-policy : {\" +\n          \" policy1 : [{replica:'<2' , node:'#ANY'}]\" +\n          \",policy2 : [{replica:'<2' , shard:'#EACH', node:'#ANY'}]\" +\n          \"}}\";\n      client.request(CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, commands));\n\n      /*** take defaults for cluster preferences\n      String cmd = \"{\" +\n          \" 'set-cluster-preferences': [\" +\n          // \"      {'cores':'<100', 'node':'#ANY'},\" +\n          \"      {minimize:cores}\" +\n          \"    ]\" +\n          \"}\";\n\n      SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, cmd);\n      client.request(req);\n       ***/\n    }\n\n    /***\n    SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.GET, null);\n    SolrResponse response = req.process(client);\n    log.info(\"######### AUTOSCALE {}\", response);\n     ***/\n\n\n    byte[] data = client.getZkStateReader().getZkClient().getData(\"/autoscaling.json\", null, null, true);\n    if (log.isInfoEnabled()) {\n      log.info(\"AUTOSCALE DATA: {}\", new String(data, \"UTF-8\"));\n    }\n\n    final AtomicInteger collectionNum = new AtomicInteger();\n    Thread[] indexThreads = new Thread[nThreads];\n\n    for (int i=0; i<nThreads; i++) {\n      indexThreads[i] = new Thread(() -> {\n        try {\n          for (int j=0; j<createsPerThread; j++) {\n            int num = collectionNum.incrementAndGet();\n            // Thread.sleep(num*1000);\n            String collectionName = \"collection\" + num;\n            CollectionAdminRequest.Create createReq = CollectionAdminRequest\n                .createCollection(collectionName, \"conf\", nShards, repFactor)\n                // .setMaxShardsPerNode(1) // should be default\n                ;\n            createReq.setWaitForFinalState(false);\n            if (useCollectionPolicy) {\n              createReq.setPolicy(\"policy1\");\n            }\n            createReq.setAutoAddReplicas(true);\n\n            createReq.process(client);\n            // cluster.waitForActiveCollection(collectionName, 1, repFactor);\n            // Thread.sleep(10000);\n          }\n        } catch (Exception e) {\n          fail(e.getMessage());\n        }\n      });\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.start();\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    int expectedTotalReplicas = unbalancedSize + nThreads * createsPerThread * nShards * repFactor;\n    int expectedPerNode = expectedTotalReplicas / NODES;\n    boolean expectBalanced = (expectedPerNode * NODES == expectedTotalReplicas);\n\n    Map<String,List<Replica>> replicaMap = new HashMap<>();\n    ClusterState cstate = client.getZkStateReader().getClusterState();\n    for (DocCollection collection : cstate.getCollectionsMap().values()) {\n      for (Replica replica : collection.getReplicas()) {\n        String url = replica.getBaseUrl();\n        List<Replica> replicas = replicaMap.get(url);\n        if (replicas == null) {\n          replicas = new ArrayList<>();\n          replicaMap.put(url, replicas);\n        }\n        replicas.add(replica);\n      }\n    }\n\n    // check if nodes are balanced\n    boolean failed = false;\n    for (List<Replica> replicas : replicaMap.values()) {\n      if (replicas.size() != expectedPerNode ) {\n        if (expectBalanced) {\n          failed = true;\n        }\n        log.error(\"UNBALANCED CLUSTER: expected replicas per node {} but got {}\", expectedPerNode, replicas.size());\n      }\n    }\n\n    // check if there were multiple replicas of the same shard placed on the same node\n    for (DocCollection collection : cstate.getCollectionsMap().values()) {\n      for (Slice slice : collection.getSlices()) {\n        Map<String, Replica> nodeToReplica = new HashMap<>();\n        for (Replica replica : slice.getReplicas()) {\n          Replica prev = nodeToReplica.put(replica.getBaseUrl(), replica);\n          if (prev != null) {\n            failed = true;\n            // NOTE: with a replication factor > 2, this will print multiple times per bad slice.\n            log.error(\"MULTIPLE REPLICAS OF SINGLE SHARD ON SAME NODE: r1={} r2={}\", prev, replica);\n          }\n        }\n      }\n    }\n\n    if (failed) {\n      log.error(\"Cluster state {}\", cstate.getCollectionsMap());\n    }\n\n    assertEquals(replicaMap.size(),  NODES);  // make sure something was created\n\n    assertTrue(!failed);\n  }\n\n","sourceOld":"  public void testConcurrentCreatePlacement() throws Exception {\n    final int nThreads = 2;\n    final int createsPerThread = 1;\n    final int nShards = 1;\n    final int repFactor = 2;\n    final boolean useClusterPolicy = false;\n    final boolean useCollectionPolicy = true;\n    final boolean startUnbalanced = true; // can help make a smaller test that can still reproduce an issue.\n    final int unbalancedSize = 1; // the number of replicas to create first\n    final boolean stopNode = false;  // only applicable when startUnbalanced==true... stops a node during first collection creation, then restarts\n\n    final CloudSolrClient client = cluster.getSolrClient();\n\n\n    if (startUnbalanced) {\n      /*** This produces a failure (multiple replicas of single shard on same node) when run with NODES=4 and\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 2;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = true;\n       // NOTE: useClusterPolicy=true seems to fix it! So does putting both creates in a single thread!\n       // NOTE: even creating a single replica to start with causes failure later on.\n\n       Also reproduced with smaller cluster: NODES=2 and\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 1;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = true;\n\n       Also, with NODES=3:\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 1;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = false;\n\n       // Also succeeded in replicating a bug where all 5 replicas were on a single node: CORES=5, nThreads=5, repFactor=5,\n       //    unbalancedSize = 16 (4 replicas on each of the up nodes), stopNode=true\n       ***/\n\n\n      JettySolrRunner downJetty = cluster.getJettySolrRunners().get(0);\n      if (stopNode) {\n        cluster.stopJettySolrRunner(downJetty);\n      }\n\n      String cname = \"STARTCOLLECTION\";\n      CollectionAdminRequest.Create creq = CollectionAdminRequest\n          //  .createCollection(cname, \"conf\", NODES - 1, NODES - 1)\n          .createCollection(cname, \"conf\", unbalancedSize, 1)\n          .setMaxShardsPerNode(100);\n      creq.setWaitForFinalState(true);\n      // creq.setAutoAddReplicas(true);\n      if (useCollectionPolicy) { creq.setPolicy(\"policy1\"); }\n      creq.process(client);\n\n      if (stopNode) {\n        // this will start it with a new port.... does it matter?\n        cluster.startJettySolrRunner(downJetty);\n      }\n    }\n\n\n\n    if (useClusterPolicy) {\n      String setClusterPolicyCommand = \"{\" +\n          \" 'set-cluster-policy': [\" +\n          // \"      {'cores':'<100', 'node':'#ANY'},\" +\n          \"      {'replica':'<2', 'shard': '#EACH', 'node': '#ANY'},\" +\n          // \"      {'replica':'<2', 'node': '#ANY'},\" +\n          \"    ]\" +\n          \"}\";\n\n      SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, setClusterPolicyCommand);\n      client.request(req);\n    }\n\n    if (useCollectionPolicy) {\n      // NOTE: the mere act of setting this named policy prevents LegacyAssignStrategy from being used, even if the policy is\n      // not used during collection creation.\n      String commands =  \"{set-policy : {\" +\n          \" policy1 : [{replica:'<2' , node:'#ANY'}]\" +\n          \",policy2 : [{replica:'<2' , shard:'#EACH', node:'#ANY'}]\" +\n          \"}}\";\n      client.request(CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, commands));\n\n      /*** take defaults for cluster preferences\n      String cmd = \"{\" +\n          \" 'set-cluster-preferences': [\" +\n          // \"      {'cores':'<100', 'node':'#ANY'},\" +\n          \"      {minimize:cores}\" +\n          \"    ]\" +\n          \"}\";\n\n      SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, cmd);\n      client.request(req);\n       ***/\n    }\n\n    /***\n    SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.GET, null);\n    SolrResponse response = req.process(client);\n    log.info(\"######### AUTOSCALE \" + response);\n     ***/\n\n\n    byte[] data = client.getZkStateReader().getZkClient().getData(\"/autoscaling.json\", null, null, true);\n    log.info(\"AUTOSCALE DATA: \" + new String(data, \"UTF-8\"));\n\n    final AtomicInteger collectionNum = new AtomicInteger();\n    Thread[] indexThreads = new Thread[nThreads];\n\n    for (int i=0; i<nThreads; i++) {\n      indexThreads[i] = new Thread(() -> {\n        try {\n          for (int j=0; j<createsPerThread; j++) {\n            int num = collectionNum.incrementAndGet();\n            // Thread.sleep(num*1000);\n            String collectionName = \"collection\" + num;\n            CollectionAdminRequest.Create createReq = CollectionAdminRequest\n                .createCollection(collectionName, \"conf\", nShards, repFactor)\n                // .setMaxShardsPerNode(1) // should be default\n                ;\n            createReq.setWaitForFinalState(false);\n            if (useCollectionPolicy) {\n              createReq.setPolicy(\"policy1\");\n            }\n            createReq.setAutoAddReplicas(true);\n\n            createReq.process(client);\n            // cluster.waitForActiveCollection(collectionName, 1, repFactor);\n            // Thread.sleep(10000);\n          }\n        } catch (Exception e) {\n          fail(e.getMessage());\n        }\n      });\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.start();\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    int expectedTotalReplicas = unbalancedSize + nThreads * createsPerThread * nShards * repFactor;\n    int expectedPerNode = expectedTotalReplicas / NODES;\n    boolean expectBalanced = (expectedPerNode * NODES == expectedTotalReplicas);\n\n    Map<String,List<Replica>> replicaMap = new HashMap<>();\n    ClusterState cstate = client.getZkStateReader().getClusterState();\n    for (DocCollection collection : cstate.getCollectionsMap().values()) {\n      for (Replica replica : collection.getReplicas()) {\n        String url = replica.getBaseUrl();\n        List<Replica> replicas = replicaMap.get(url);\n        if (replicas == null) {\n          replicas = new ArrayList<>();\n          replicaMap.put(url, replicas);\n        }\n        replicas.add(replica);\n      }\n    }\n\n    // check if nodes are balanced\n    boolean failed = false;\n    for (List<Replica> replicas : replicaMap.values()) {\n      if (replicas.size() != expectedPerNode ) {\n        if (expectBalanced) {\n          failed = true;\n        }\n        log.error(\"UNBALANCED CLUSTER: expected replicas per node \" + expectedPerNode +  \" but got \" + replicas.size());\n      }\n    }\n\n    // check if there were multiple replicas of the same shard placed on the same node\n    for (DocCollection collection : cstate.getCollectionsMap().values()) {\n      for (Slice slice : collection.getSlices()) {\n        Map<String, Replica> nodeToReplica = new HashMap<>();\n        for (Replica replica : slice.getReplicas()) {\n          Replica prev = nodeToReplica.put(replica.getBaseUrl(), replica);\n          if (prev != null) {\n            failed = true;\n            // NOTE: with a replication factor > 2, this will print multiple times per bad slice.\n            log.error(\"MULTIPLE REPLICAS OF SINGLE SHARD ON SAME NODE: r1=\" + prev + \" r2=\" + replica);\n          }\n        }\n      }\n    }\n\n    if (failed) {\n      log.error(\"Cluster state \" + cstate.getCollectionsMap());\n    }\n\n    assertEquals(replicaMap.size(),  NODES);  // make sure something was created\n\n    assertTrue(!failed);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aa2585c33d5d66a1c837c312221eb55ddb3c4300","date":1592493170,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/ConcurrentCreateCollectionTest#testConcurrentCreatePlacement().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/ConcurrentCreateCollectionTest#testConcurrentCreatePlacement().mjava","sourceNew":"  public void testConcurrentCreatePlacement() throws Exception {\n    final int nThreads = 2;\n    final int createsPerThread = 1;\n    final int nShards = 1;\n    final int repFactor = 2;\n    final boolean useClusterPolicy = false;\n    final boolean useCollectionPolicy = true;\n    final boolean startUnbalanced = true; // can help make a smaller test that can still reproduce an issue.\n    final int unbalancedSize = 1; // the number of replicas to create first\n    final boolean stopNode = false;  // only applicable when startUnbalanced==true... stops a node during first collection creation, then restarts\n\n    final CloudSolrClient client = cluster.getSolrClient();\n\n\n    if (startUnbalanced) {\n      /*** This produces a failure (multiple replicas of single shard on same node) when run with NODES=4 and\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 2;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = true;\n       // NOTE: useClusterPolicy=true seems to fix it! So does putting both creates in a single thread!\n       // NOTE: even creating a single replica to start with causes failure later on.\n\n       Also reproduced with smaller cluster: NODES=2 and\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 1;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = true;\n\n       Also, with NODES=3:\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 1;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = false;\n\n       // Also succeeded in replicating a bug where all 5 replicas were on a single node: CORES=5, nThreads=5, repFactor=5,\n       //    unbalancedSize = 16 (4 replicas on each of the up nodes), stopNode=true\n       ***/\n\n\n      JettySolrRunner downJetty = cluster.getJettySolrRunners().get(0);\n      if (stopNode) {\n        cluster.stopJettySolrRunner(downJetty);\n      }\n\n      String cname = \"STARTCOLLECTION\";\n      CollectionAdminRequest.Create creq = CollectionAdminRequest\n          //  .createCollection(cname, \"conf\", NODES - 1, NODES - 1)\n          .createCollection(cname, \"conf\", unbalancedSize, 1)\n          .setMaxShardsPerNode(100);\n      creq.setWaitForFinalState(true);\n      // creq.setAutoAddReplicas(true);\n      if (useCollectionPolicy) { creq.setPolicy(\"policy1\"); }\n      creq.process(client);\n\n      if (stopNode) {\n        // this will start it with a new port.... does it matter?\n        cluster.startJettySolrRunner(downJetty);\n      }\n    }\n\n\n\n    if (useClusterPolicy) {\n      String setClusterPolicyCommand = \"{\" +\n          \" 'set-cluster-policy': [\" +\n          // \"      {'cores':'<100', 'node':'#ANY'},\" +\n          \"      {'replica':'<2', 'shard': '#EACH', 'node': '#ANY'},\" +\n          // \"      {'replica':'<2', 'node': '#ANY'},\" +\n          \"    ]\" +\n          \"}\";\n\n      @SuppressWarnings({\"rawtypes\"})\n      SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, setClusterPolicyCommand);\n      client.request(req);\n    }\n\n    if (useCollectionPolicy) {\n      // NOTE: the mere act of setting this named policy prevents LegacyAssignStrategy from being used, even if the policy is\n      // not used during collection creation.\n      String commands =  \"{set-policy : {\" +\n          \" policy1 : [{replica:'<2' , node:'#ANY'}]\" +\n          \",policy2 : [{replica:'<2' , shard:'#EACH', node:'#ANY'}]\" +\n          \"}}\";\n      client.request(CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, commands));\n\n      /*** take defaults for cluster preferences\n      String cmd = \"{\" +\n          \" 'set-cluster-preferences': [\" +\n          // \"      {'cores':'<100', 'node':'#ANY'},\" +\n          \"      {minimize:cores}\" +\n          \"    ]\" +\n          \"}\";\n\n      SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, cmd);\n      client.request(req);\n       ***/\n    }\n\n    /***\n    SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.GET, null);\n    SolrResponse response = req.process(client);\n    log.info(\"######### AUTOSCALE {}\", response);\n     ***/\n\n\n    byte[] data = client.getZkStateReader().getZkClient().getData(\"/autoscaling.json\", null, null, true);\n    if (log.isInfoEnabled()) {\n      log.info(\"AUTOSCALE DATA: {}\", new String(data, \"UTF-8\"));\n    }\n\n    final AtomicInteger collectionNum = new AtomicInteger();\n    Thread[] indexThreads = new Thread[nThreads];\n\n    for (int i=0; i<nThreads; i++) {\n      indexThreads[i] = new Thread(() -> {\n        try {\n          for (int j=0; j<createsPerThread; j++) {\n            int num = collectionNum.incrementAndGet();\n            // Thread.sleep(num*1000);\n            String collectionName = \"collection\" + num;\n            CollectionAdminRequest.Create createReq = CollectionAdminRequest\n                .createCollection(collectionName, \"conf\", nShards, repFactor)\n                // .setMaxShardsPerNode(1) // should be default\n                ;\n            createReq.setWaitForFinalState(false);\n            if (useCollectionPolicy) {\n              createReq.setPolicy(\"policy1\");\n            }\n            createReq.setAutoAddReplicas(true);\n\n            createReq.process(client);\n            // cluster.waitForActiveCollection(collectionName, 1, repFactor);\n            // Thread.sleep(10000);\n          }\n        } catch (Exception e) {\n          fail(e.getMessage());\n        }\n      });\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.start();\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    int expectedTotalReplicas = unbalancedSize + nThreads * createsPerThread * nShards * repFactor;\n    int expectedPerNode = expectedTotalReplicas / NODES;\n    boolean expectBalanced = (expectedPerNode * NODES == expectedTotalReplicas);\n\n    Map<String,List<Replica>> replicaMap = new HashMap<>();\n    ClusterState cstate = client.getZkStateReader().getClusterState();\n    for (DocCollection collection : cstate.getCollectionsMap().values()) {\n      for (Replica replica : collection.getReplicas()) {\n        String url = replica.getBaseUrl();\n        List<Replica> replicas = replicaMap.get(url);\n        if (replicas == null) {\n          replicas = new ArrayList<>();\n          replicaMap.put(url, replicas);\n        }\n        replicas.add(replica);\n      }\n    }\n\n    // check if nodes are balanced\n    boolean failed = false;\n    for (List<Replica> replicas : replicaMap.values()) {\n      if (replicas.size() != expectedPerNode ) {\n        if (expectBalanced) {\n          failed = true;\n        }\n        log.error(\"UNBALANCED CLUSTER: expected replicas per node {} but got {}\", expectedPerNode, replicas.size());\n      }\n    }\n\n    // check if there were multiple replicas of the same shard placed on the same node\n    for (DocCollection collection : cstate.getCollectionsMap().values()) {\n      for (Slice slice : collection.getSlices()) {\n        Map<String, Replica> nodeToReplica = new HashMap<>();\n        for (Replica replica : slice.getReplicas()) {\n          Replica prev = nodeToReplica.put(replica.getBaseUrl(), replica);\n          if (prev != null) {\n            failed = true;\n            // NOTE: with a replication factor > 2, this will print multiple times per bad slice.\n            log.error(\"MULTIPLE REPLICAS OF SINGLE SHARD ON SAME NODE: r1={} r2={}\", prev, replica);\n          }\n        }\n      }\n    }\n\n    if (failed) {\n      log.error(\"Cluster state {}\", cstate.getCollectionsMap());\n    }\n\n    assertEquals(replicaMap.size(),  NODES);  // make sure something was created\n\n    assertTrue(!failed);\n  }\n\n","sourceOld":"  public void testConcurrentCreatePlacement() throws Exception {\n    final int nThreads = 2;\n    final int createsPerThread = 1;\n    final int nShards = 1;\n    final int repFactor = 2;\n    final boolean useClusterPolicy = false;\n    final boolean useCollectionPolicy = true;\n    final boolean startUnbalanced = true; // can help make a smaller test that can still reproduce an issue.\n    final int unbalancedSize = 1; // the number of replicas to create first\n    final boolean stopNode = false;  // only applicable when startUnbalanced==true... stops a node during first collection creation, then restarts\n\n    final CloudSolrClient client = cluster.getSolrClient();\n\n\n    if (startUnbalanced) {\n      /*** This produces a failure (multiple replicas of single shard on same node) when run with NODES=4 and\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 2;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = true;\n       // NOTE: useClusterPolicy=true seems to fix it! So does putting both creates in a single thread!\n       // NOTE: even creating a single replica to start with causes failure later on.\n\n       Also reproduced with smaller cluster: NODES=2 and\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 1;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = true;\n\n       Also, with NODES=3:\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 1;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = false;\n\n       // Also succeeded in replicating a bug where all 5 replicas were on a single node: CORES=5, nThreads=5, repFactor=5,\n       //    unbalancedSize = 16 (4 replicas on each of the up nodes), stopNode=true\n       ***/\n\n\n      JettySolrRunner downJetty = cluster.getJettySolrRunners().get(0);\n      if (stopNode) {\n        cluster.stopJettySolrRunner(downJetty);\n      }\n\n      String cname = \"STARTCOLLECTION\";\n      CollectionAdminRequest.Create creq = CollectionAdminRequest\n          //  .createCollection(cname, \"conf\", NODES - 1, NODES - 1)\n          .createCollection(cname, \"conf\", unbalancedSize, 1)\n          .setMaxShardsPerNode(100);\n      creq.setWaitForFinalState(true);\n      // creq.setAutoAddReplicas(true);\n      if (useCollectionPolicy) { creq.setPolicy(\"policy1\"); }\n      creq.process(client);\n\n      if (stopNode) {\n        // this will start it with a new port.... does it matter?\n        cluster.startJettySolrRunner(downJetty);\n      }\n    }\n\n\n\n    if (useClusterPolicy) {\n      String setClusterPolicyCommand = \"{\" +\n          \" 'set-cluster-policy': [\" +\n          // \"      {'cores':'<100', 'node':'#ANY'},\" +\n          \"      {'replica':'<2', 'shard': '#EACH', 'node': '#ANY'},\" +\n          // \"      {'replica':'<2', 'node': '#ANY'},\" +\n          \"    ]\" +\n          \"}\";\n\n      SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, setClusterPolicyCommand);\n      client.request(req);\n    }\n\n    if (useCollectionPolicy) {\n      // NOTE: the mere act of setting this named policy prevents LegacyAssignStrategy from being used, even if the policy is\n      // not used during collection creation.\n      String commands =  \"{set-policy : {\" +\n          \" policy1 : [{replica:'<2' , node:'#ANY'}]\" +\n          \",policy2 : [{replica:'<2' , shard:'#EACH', node:'#ANY'}]\" +\n          \"}}\";\n      client.request(CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, commands));\n\n      /*** take defaults for cluster preferences\n      String cmd = \"{\" +\n          \" 'set-cluster-preferences': [\" +\n          // \"      {'cores':'<100', 'node':'#ANY'},\" +\n          \"      {minimize:cores}\" +\n          \"    ]\" +\n          \"}\";\n\n      SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, cmd);\n      client.request(req);\n       ***/\n    }\n\n    /***\n    SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.GET, null);\n    SolrResponse response = req.process(client);\n    log.info(\"######### AUTOSCALE {}\", response);\n     ***/\n\n\n    byte[] data = client.getZkStateReader().getZkClient().getData(\"/autoscaling.json\", null, null, true);\n    if (log.isInfoEnabled()) {\n      log.info(\"AUTOSCALE DATA: {}\", new String(data, \"UTF-8\"));\n    }\n\n    final AtomicInteger collectionNum = new AtomicInteger();\n    Thread[] indexThreads = new Thread[nThreads];\n\n    for (int i=0; i<nThreads; i++) {\n      indexThreads[i] = new Thread(() -> {\n        try {\n          for (int j=0; j<createsPerThread; j++) {\n            int num = collectionNum.incrementAndGet();\n            // Thread.sleep(num*1000);\n            String collectionName = \"collection\" + num;\n            CollectionAdminRequest.Create createReq = CollectionAdminRequest\n                .createCollection(collectionName, \"conf\", nShards, repFactor)\n                // .setMaxShardsPerNode(1) // should be default\n                ;\n            createReq.setWaitForFinalState(false);\n            if (useCollectionPolicy) {\n              createReq.setPolicy(\"policy1\");\n            }\n            createReq.setAutoAddReplicas(true);\n\n            createReq.process(client);\n            // cluster.waitForActiveCollection(collectionName, 1, repFactor);\n            // Thread.sleep(10000);\n          }\n        } catch (Exception e) {\n          fail(e.getMessage());\n        }\n      });\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.start();\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    int expectedTotalReplicas = unbalancedSize + nThreads * createsPerThread * nShards * repFactor;\n    int expectedPerNode = expectedTotalReplicas / NODES;\n    boolean expectBalanced = (expectedPerNode * NODES == expectedTotalReplicas);\n\n    Map<String,List<Replica>> replicaMap = new HashMap<>();\n    ClusterState cstate = client.getZkStateReader().getClusterState();\n    for (DocCollection collection : cstate.getCollectionsMap().values()) {\n      for (Replica replica : collection.getReplicas()) {\n        String url = replica.getBaseUrl();\n        List<Replica> replicas = replicaMap.get(url);\n        if (replicas == null) {\n          replicas = new ArrayList<>();\n          replicaMap.put(url, replicas);\n        }\n        replicas.add(replica);\n      }\n    }\n\n    // check if nodes are balanced\n    boolean failed = false;\n    for (List<Replica> replicas : replicaMap.values()) {\n      if (replicas.size() != expectedPerNode ) {\n        if (expectBalanced) {\n          failed = true;\n        }\n        log.error(\"UNBALANCED CLUSTER: expected replicas per node {} but got {}\", expectedPerNode, replicas.size());\n      }\n    }\n\n    // check if there were multiple replicas of the same shard placed on the same node\n    for (DocCollection collection : cstate.getCollectionsMap().values()) {\n      for (Slice slice : collection.getSlices()) {\n        Map<String, Replica> nodeToReplica = new HashMap<>();\n        for (Replica replica : slice.getReplicas()) {\n          Replica prev = nodeToReplica.put(replica.getBaseUrl(), replica);\n          if (prev != null) {\n            failed = true;\n            // NOTE: with a replication factor > 2, this will print multiple times per bad slice.\n            log.error(\"MULTIPLE REPLICAS OF SINGLE SHARD ON SAME NODE: r1={} r2={}\", prev, replica);\n          }\n        }\n      }\n    }\n\n    if (failed) {\n      log.error(\"Cluster state {}\", cstate.getCollectionsMap());\n    }\n\n    assertEquals(replicaMap.size(),  NODES);  // make sure something was created\n\n    assertTrue(!failed);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e","date":1594223844,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/ConcurrentCreateCollectionTest#testConcurrentCreatePlacement().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/ConcurrentCreateCollectionTest#testConcurrentCreatePlacement().mjava","sourceNew":"  public void testConcurrentCreatePlacement() throws Exception {\n    final int nThreads = 2;\n    final int createsPerThread = 1;\n    final int nShards = 1;\n    final int repFactor = 2;\n    final boolean useClusterPolicy = false;\n    final boolean useCollectionPolicy = true;\n    final boolean startUnbalanced = true; // can help make a smaller test that can still reproduce an issue.\n    final int unbalancedSize = 1; // the number of replicas to create first\n    final boolean stopNode = false;  // only applicable when startUnbalanced==true... stops a node during first collection creation, then restarts\n\n    final CloudSolrClient client = cluster.getSolrClient();\n\n\n    if (startUnbalanced) {\n      /*** This produces a failure (multiple replicas of single shard on same node) when run with NODES=4 and\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 2;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = true;\n       // NOTE: useClusterPolicy=true seems to fix it! So does putting both creates in a single thread!\n       // NOTE: even creating a single replica to start with causes failure later on.\n\n       Also reproduced with smaller cluster: NODES=2 and\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 1;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = true;\n\n       Also, with NODES=3:\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 1;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = false;\n\n       // Also succeeded in replicating a bug where all 5 replicas were on a single node: CORES=5, nThreads=5, repFactor=5,\n       //    unbalancedSize = 16 (4 replicas on each of the up nodes), stopNode=true\n       ***/\n\n\n      JettySolrRunner downJetty = cluster.getJettySolrRunners().get(0);\n      if (stopNode) {\n        cluster.stopJettySolrRunner(downJetty);\n      }\n\n      String cname = \"STARTCOLLECTION\";\n      CollectionAdminRequest.Create creq = CollectionAdminRequest\n          //  .createCollection(cname, \"conf\", NODES - 1, NODES - 1)\n          .createCollection(cname, \"conf\", unbalancedSize, 1);\n      creq.setWaitForFinalState(true);\n      // creq.setAutoAddReplicas(true);\n      if (useCollectionPolicy) { creq.setPolicy(\"policy1\"); }\n      creq.process(client);\n\n      if (stopNode) {\n        // this will start it with a new port.... does it matter?\n        cluster.startJettySolrRunner(downJetty);\n      }\n    }\n\n\n\n    if (useClusterPolicy) {\n      String setClusterPolicyCommand = \"{\" +\n          \" 'set-cluster-policy': [\" +\n          // \"      {'cores':'<100', 'node':'#ANY'},\" +\n          \"      {'replica':'<2', 'shard': '#EACH', 'node': '#ANY'},\" +\n          // \"      {'replica':'<2', 'node': '#ANY'},\" +\n          \"    ]\" +\n          \"}\";\n\n      @SuppressWarnings({\"rawtypes\"})\n      SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, setClusterPolicyCommand);\n      client.request(req);\n    }\n\n    if (useCollectionPolicy) {\n      // NOTE: the mere act of setting this named policy prevents LegacyAssignStrategy from being used, even if the policy is\n      // not used during collection creation.\n      String commands =  \"{set-policy : {\" +\n          \" policy1 : [{replica:'<2' , node:'#ANY'}]\" +\n          \",policy2 : [{replica:'<2' , shard:'#EACH', node:'#ANY'}]\" +\n          \"}}\";\n      client.request(CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, commands));\n\n      /*** take defaults for cluster preferences\n      String cmd = \"{\" +\n          \" 'set-cluster-preferences': [\" +\n          // \"      {'cores':'<100', 'node':'#ANY'},\" +\n          \"      {minimize:cores}\" +\n          \"    ]\" +\n          \"}\";\n\n      SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, cmd);\n      client.request(req);\n       ***/\n    }\n\n    /***\n    SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.GET, null);\n    SolrResponse response = req.process(client);\n    log.info(\"######### AUTOSCALE {}\", response);\n     ***/\n\n\n    byte[] data = client.getZkStateReader().getZkClient().getData(\"/autoscaling.json\", null, null, true);\n    if (log.isInfoEnabled()) {\n      log.info(\"AUTOSCALE DATA: {}\", new String(data, \"UTF-8\"));\n    }\n\n    final AtomicInteger collectionNum = new AtomicInteger();\n    Thread[] indexThreads = new Thread[nThreads];\n\n    for (int i=0; i<nThreads; i++) {\n      indexThreads[i] = new Thread(() -> {\n        try {\n          for (int j=0; j<createsPerThread; j++) {\n            int num = collectionNum.incrementAndGet();\n            // Thread.sleep(num*1000);\n            String collectionName = \"collection\" + num;\n            CollectionAdminRequest.Create createReq = CollectionAdminRequest\n                .createCollection(collectionName, \"conf\", nShards, repFactor)\n                // .setMaxShardsPerNode(1) // should be default\n                ;\n            createReq.setWaitForFinalState(false);\n            if (useCollectionPolicy) {\n              createReq.setPolicy(\"policy1\");\n            }\n            createReq.setAutoAddReplicas(true);\n\n            createReq.process(client);\n            // cluster.waitForActiveCollection(collectionName, 1, repFactor);\n            // Thread.sleep(10000);\n          }\n        } catch (Exception e) {\n          fail(e.getMessage());\n        }\n      });\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.start();\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    int expectedTotalReplicas = unbalancedSize + nThreads * createsPerThread * nShards * repFactor;\n    int expectedPerNode = expectedTotalReplicas / NODES;\n    boolean expectBalanced = (expectedPerNode * NODES == expectedTotalReplicas);\n\n    Map<String,List<Replica>> replicaMap = new HashMap<>();\n    ClusterState cstate = client.getZkStateReader().getClusterState();\n    for (DocCollection collection : cstate.getCollectionsMap().values()) {\n      for (Replica replica : collection.getReplicas()) {\n        String url = replica.getBaseUrl();\n        List<Replica> replicas = replicaMap.get(url);\n        if (replicas == null) {\n          replicas = new ArrayList<>();\n          replicaMap.put(url, replicas);\n        }\n        replicas.add(replica);\n      }\n    }\n\n    // check if nodes are balanced\n    boolean failed = false;\n    for (List<Replica> replicas : replicaMap.values()) {\n      if (replicas.size() != expectedPerNode ) {\n        if (expectBalanced) {\n          failed = true;\n        }\n        log.error(\"UNBALANCED CLUSTER: expected replicas per node {} but got {}\", expectedPerNode, replicas.size());\n      }\n    }\n\n    // check if there were multiple replicas of the same shard placed on the same node\n    for (DocCollection collection : cstate.getCollectionsMap().values()) {\n      for (Slice slice : collection.getSlices()) {\n        Map<String, Replica> nodeToReplica = new HashMap<>();\n        for (Replica replica : slice.getReplicas()) {\n          Replica prev = nodeToReplica.put(replica.getBaseUrl(), replica);\n          if (prev != null) {\n            failed = true;\n            // NOTE: with a replication factor > 2, this will print multiple times per bad slice.\n            log.error(\"MULTIPLE REPLICAS OF SINGLE SHARD ON SAME NODE: r1={} r2={}\", prev, replica);\n          }\n        }\n      }\n    }\n\n    if (failed) {\n      log.error(\"Cluster state {}\", cstate.getCollectionsMap());\n    }\n\n    assertEquals(replicaMap.size(),  NODES);  // make sure something was created\n\n    assertTrue(!failed);\n  }\n\n","sourceOld":"  public void testConcurrentCreatePlacement() throws Exception {\n    final int nThreads = 2;\n    final int createsPerThread = 1;\n    final int nShards = 1;\n    final int repFactor = 2;\n    final boolean useClusterPolicy = false;\n    final boolean useCollectionPolicy = true;\n    final boolean startUnbalanced = true; // can help make a smaller test that can still reproduce an issue.\n    final int unbalancedSize = 1; // the number of replicas to create first\n    final boolean stopNode = false;  // only applicable when startUnbalanced==true... stops a node during first collection creation, then restarts\n\n    final CloudSolrClient client = cluster.getSolrClient();\n\n\n    if (startUnbalanced) {\n      /*** This produces a failure (multiple replicas of single shard on same node) when run with NODES=4 and\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 2;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = true;\n       // NOTE: useClusterPolicy=true seems to fix it! So does putting both creates in a single thread!\n       // NOTE: even creating a single replica to start with causes failure later on.\n\n       Also reproduced with smaller cluster: NODES=2 and\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 1;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = true;\n\n       Also, with NODES=3:\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 1;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = false;\n\n       // Also succeeded in replicating a bug where all 5 replicas were on a single node: CORES=5, nThreads=5, repFactor=5,\n       //    unbalancedSize = 16 (4 replicas on each of the up nodes), stopNode=true\n       ***/\n\n\n      JettySolrRunner downJetty = cluster.getJettySolrRunners().get(0);\n      if (stopNode) {\n        cluster.stopJettySolrRunner(downJetty);\n      }\n\n      String cname = \"STARTCOLLECTION\";\n      CollectionAdminRequest.Create creq = CollectionAdminRequest\n          //  .createCollection(cname, \"conf\", NODES - 1, NODES - 1)\n          .createCollection(cname, \"conf\", unbalancedSize, 1)\n          .setMaxShardsPerNode(100);\n      creq.setWaitForFinalState(true);\n      // creq.setAutoAddReplicas(true);\n      if (useCollectionPolicy) { creq.setPolicy(\"policy1\"); }\n      creq.process(client);\n\n      if (stopNode) {\n        // this will start it with a new port.... does it matter?\n        cluster.startJettySolrRunner(downJetty);\n      }\n    }\n\n\n\n    if (useClusterPolicy) {\n      String setClusterPolicyCommand = \"{\" +\n          \" 'set-cluster-policy': [\" +\n          // \"      {'cores':'<100', 'node':'#ANY'},\" +\n          \"      {'replica':'<2', 'shard': '#EACH', 'node': '#ANY'},\" +\n          // \"      {'replica':'<2', 'node': '#ANY'},\" +\n          \"    ]\" +\n          \"}\";\n\n      @SuppressWarnings({\"rawtypes\"})\n      SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, setClusterPolicyCommand);\n      client.request(req);\n    }\n\n    if (useCollectionPolicy) {\n      // NOTE: the mere act of setting this named policy prevents LegacyAssignStrategy from being used, even if the policy is\n      // not used during collection creation.\n      String commands =  \"{set-policy : {\" +\n          \" policy1 : [{replica:'<2' , node:'#ANY'}]\" +\n          \",policy2 : [{replica:'<2' , shard:'#EACH', node:'#ANY'}]\" +\n          \"}}\";\n      client.request(CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, commands));\n\n      /*** take defaults for cluster preferences\n      String cmd = \"{\" +\n          \" 'set-cluster-preferences': [\" +\n          // \"      {'cores':'<100', 'node':'#ANY'},\" +\n          \"      {minimize:cores}\" +\n          \"    ]\" +\n          \"}\";\n\n      SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, cmd);\n      client.request(req);\n       ***/\n    }\n\n    /***\n    SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.GET, null);\n    SolrResponse response = req.process(client);\n    log.info(\"######### AUTOSCALE {}\", response);\n     ***/\n\n\n    byte[] data = client.getZkStateReader().getZkClient().getData(\"/autoscaling.json\", null, null, true);\n    if (log.isInfoEnabled()) {\n      log.info(\"AUTOSCALE DATA: {}\", new String(data, \"UTF-8\"));\n    }\n\n    final AtomicInteger collectionNum = new AtomicInteger();\n    Thread[] indexThreads = new Thread[nThreads];\n\n    for (int i=0; i<nThreads; i++) {\n      indexThreads[i] = new Thread(() -> {\n        try {\n          for (int j=0; j<createsPerThread; j++) {\n            int num = collectionNum.incrementAndGet();\n            // Thread.sleep(num*1000);\n            String collectionName = \"collection\" + num;\n            CollectionAdminRequest.Create createReq = CollectionAdminRequest\n                .createCollection(collectionName, \"conf\", nShards, repFactor)\n                // .setMaxShardsPerNode(1) // should be default\n                ;\n            createReq.setWaitForFinalState(false);\n            if (useCollectionPolicy) {\n              createReq.setPolicy(\"policy1\");\n            }\n            createReq.setAutoAddReplicas(true);\n\n            createReq.process(client);\n            // cluster.waitForActiveCollection(collectionName, 1, repFactor);\n            // Thread.sleep(10000);\n          }\n        } catch (Exception e) {\n          fail(e.getMessage());\n        }\n      });\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.start();\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    int expectedTotalReplicas = unbalancedSize + nThreads * createsPerThread * nShards * repFactor;\n    int expectedPerNode = expectedTotalReplicas / NODES;\n    boolean expectBalanced = (expectedPerNode * NODES == expectedTotalReplicas);\n\n    Map<String,List<Replica>> replicaMap = new HashMap<>();\n    ClusterState cstate = client.getZkStateReader().getClusterState();\n    for (DocCollection collection : cstate.getCollectionsMap().values()) {\n      for (Replica replica : collection.getReplicas()) {\n        String url = replica.getBaseUrl();\n        List<Replica> replicas = replicaMap.get(url);\n        if (replicas == null) {\n          replicas = new ArrayList<>();\n          replicaMap.put(url, replicas);\n        }\n        replicas.add(replica);\n      }\n    }\n\n    // check if nodes are balanced\n    boolean failed = false;\n    for (List<Replica> replicas : replicaMap.values()) {\n      if (replicas.size() != expectedPerNode ) {\n        if (expectBalanced) {\n          failed = true;\n        }\n        log.error(\"UNBALANCED CLUSTER: expected replicas per node {} but got {}\", expectedPerNode, replicas.size());\n      }\n    }\n\n    // check if there were multiple replicas of the same shard placed on the same node\n    for (DocCollection collection : cstate.getCollectionsMap().values()) {\n      for (Slice slice : collection.getSlices()) {\n        Map<String, Replica> nodeToReplica = new HashMap<>();\n        for (Replica replica : slice.getReplicas()) {\n          Replica prev = nodeToReplica.put(replica.getBaseUrl(), replica);\n          if (prev != null) {\n            failed = true;\n            // NOTE: with a replication factor > 2, this will print multiple times per bad slice.\n            log.error(\"MULTIPLE REPLICAS OF SINGLE SHARD ON SAME NODE: r1={} r2={}\", prev, replica);\n          }\n        }\n      }\n    }\n\n    if (failed) {\n      log.error(\"Cluster state {}\", cstate.getCollectionsMap());\n    }\n\n    assertEquals(replicaMap.size(),  NODES);  // make sure something was created\n\n    assertTrue(!failed);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3f504512a03d978990cbff30db0522b354e846db","date":1595247421,"type":4,"author":"Ishan Chattopadhyaya","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/ConcurrentCreateCollectionTest#testConcurrentCreatePlacement().mjava","sourceNew":null,"sourceOld":"  public void testConcurrentCreatePlacement() throws Exception {\n    final int nThreads = 2;\n    final int createsPerThread = 1;\n    final int nShards = 1;\n    final int repFactor = 2;\n    final boolean useClusterPolicy = false;\n    final boolean useCollectionPolicy = true;\n    final boolean startUnbalanced = true; // can help make a smaller test that can still reproduce an issue.\n    final int unbalancedSize = 1; // the number of replicas to create first\n    final boolean stopNode = false;  // only applicable when startUnbalanced==true... stops a node during first collection creation, then restarts\n\n    final CloudSolrClient client = cluster.getSolrClient();\n\n\n    if (startUnbalanced) {\n      /*** This produces a failure (multiple replicas of single shard on same node) when run with NODES=4 and\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 2;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = true;\n       // NOTE: useClusterPolicy=true seems to fix it! So does putting both creates in a single thread!\n       // NOTE: even creating a single replica to start with causes failure later on.\n\n       Also reproduced with smaller cluster: NODES=2 and\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 1;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = true;\n\n       Also, with NODES=3:\n       final int nThreads = 2;\n       final int createsPerThread = 1;\n       final int nShards = 1;\n       final int repFactor = 2;\n       final boolean useClusterPolicy = false;\n       final boolean useCollectionPolicy = true;\n       final boolean startUnbalanced = false;\n\n       // Also succeeded in replicating a bug where all 5 replicas were on a single node: CORES=5, nThreads=5, repFactor=5,\n       //    unbalancedSize = 16 (4 replicas on each of the up nodes), stopNode=true\n       ***/\n\n\n      JettySolrRunner downJetty = cluster.getJettySolrRunners().get(0);\n      if (stopNode) {\n        cluster.stopJettySolrRunner(downJetty);\n      }\n\n      String cname = \"STARTCOLLECTION\";\n      CollectionAdminRequest.Create creq = CollectionAdminRequest\n          //  .createCollection(cname, \"conf\", NODES - 1, NODES - 1)\n          .createCollection(cname, \"conf\", unbalancedSize, 1);\n      creq.setWaitForFinalState(true);\n      // creq.setAutoAddReplicas(true);\n      if (useCollectionPolicy) { creq.setPolicy(\"policy1\"); }\n      creq.process(client);\n\n      if (stopNode) {\n        // this will start it with a new port.... does it matter?\n        cluster.startJettySolrRunner(downJetty);\n      }\n    }\n\n\n\n    if (useClusterPolicy) {\n      String setClusterPolicyCommand = \"{\" +\n          \" 'set-cluster-policy': [\" +\n          // \"      {'cores':'<100', 'node':'#ANY'},\" +\n          \"      {'replica':'<2', 'shard': '#EACH', 'node': '#ANY'},\" +\n          // \"      {'replica':'<2', 'node': '#ANY'},\" +\n          \"    ]\" +\n          \"}\";\n\n      @SuppressWarnings({\"rawtypes\"})\n      SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, setClusterPolicyCommand);\n      client.request(req);\n    }\n\n    if (useCollectionPolicy) {\n      // NOTE: the mere act of setting this named policy prevents LegacyAssignStrategy from being used, even if the policy is\n      // not used during collection creation.\n      String commands =  \"{set-policy : {\" +\n          \" policy1 : [{replica:'<2' , node:'#ANY'}]\" +\n          \",policy2 : [{replica:'<2' , shard:'#EACH', node:'#ANY'}]\" +\n          \"}}\";\n      client.request(CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, commands));\n\n      /*** take defaults for cluster preferences\n      String cmd = \"{\" +\n          \" 'set-cluster-preferences': [\" +\n          // \"      {'cores':'<100', 'node':'#ANY'},\" +\n          \"      {minimize:cores}\" +\n          \"    ]\" +\n          \"}\";\n\n      SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.POST, cmd);\n      client.request(req);\n       ***/\n    }\n\n    /***\n    SolrRequest req = CloudTestUtils.AutoScalingRequest.create(SolrRequest.METHOD.GET, null);\n    SolrResponse response = req.process(client);\n    log.info(\"######### AUTOSCALE {}\", response);\n     ***/\n\n\n    byte[] data = client.getZkStateReader().getZkClient().getData(\"/autoscaling.json\", null, null, true);\n    if (log.isInfoEnabled()) {\n      log.info(\"AUTOSCALE DATA: {}\", new String(data, \"UTF-8\"));\n    }\n\n    final AtomicInteger collectionNum = new AtomicInteger();\n    Thread[] indexThreads = new Thread[nThreads];\n\n    for (int i=0; i<nThreads; i++) {\n      indexThreads[i] = new Thread(() -> {\n        try {\n          for (int j=0; j<createsPerThread; j++) {\n            int num = collectionNum.incrementAndGet();\n            // Thread.sleep(num*1000);\n            String collectionName = \"collection\" + num;\n            CollectionAdminRequest.Create createReq = CollectionAdminRequest\n                .createCollection(collectionName, \"conf\", nShards, repFactor)\n                // .setMaxShardsPerNode(1) // should be default\n                ;\n            createReq.setWaitForFinalState(false);\n            if (useCollectionPolicy) {\n              createReq.setPolicy(\"policy1\");\n            }\n            createReq.setAutoAddReplicas(true);\n\n            createReq.process(client);\n            // cluster.waitForActiveCollection(collectionName, 1, repFactor);\n            // Thread.sleep(10000);\n          }\n        } catch (Exception e) {\n          fail(e.getMessage());\n        }\n      });\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.start();\n    }\n\n    for (Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    int expectedTotalReplicas = unbalancedSize + nThreads * createsPerThread * nShards * repFactor;\n    int expectedPerNode = expectedTotalReplicas / NODES;\n    boolean expectBalanced = (expectedPerNode * NODES == expectedTotalReplicas);\n\n    Map<String,List<Replica>> replicaMap = new HashMap<>();\n    ClusterState cstate = client.getZkStateReader().getClusterState();\n    for (DocCollection collection : cstate.getCollectionsMap().values()) {\n      for (Replica replica : collection.getReplicas()) {\n        String url = replica.getBaseUrl();\n        List<Replica> replicas = replicaMap.get(url);\n        if (replicas == null) {\n          replicas = new ArrayList<>();\n          replicaMap.put(url, replicas);\n        }\n        replicas.add(replica);\n      }\n    }\n\n    // check if nodes are balanced\n    boolean failed = false;\n    for (List<Replica> replicas : replicaMap.values()) {\n      if (replicas.size() != expectedPerNode ) {\n        if (expectBalanced) {\n          failed = true;\n        }\n        log.error(\"UNBALANCED CLUSTER: expected replicas per node {} but got {}\", expectedPerNode, replicas.size());\n      }\n    }\n\n    // check if there were multiple replicas of the same shard placed on the same node\n    for (DocCollection collection : cstate.getCollectionsMap().values()) {\n      for (Slice slice : collection.getSlices()) {\n        Map<String, Replica> nodeToReplica = new HashMap<>();\n        for (Replica replica : slice.getReplicas()) {\n          Replica prev = nodeToReplica.put(replica.getBaseUrl(), replica);\n          if (prev != null) {\n            failed = true;\n            // NOTE: with a replication factor > 2, this will print multiple times per bad slice.\n            log.error(\"MULTIPLE REPLICAS OF SINGLE SHARD ON SAME NODE: r1={} r2={}\", prev, replica);\n          }\n        }\n      }\n    }\n\n    if (failed) {\n      log.error(\"Cluster state {}\", cstate.getCollectionsMap());\n    }\n\n    assertEquals(replicaMap.size(),  NODES);  // make sure something was created\n\n    assertTrue(!failed);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"aa2585c33d5d66a1c837c312221eb55ddb3c4300":["fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4"],"3f504512a03d978990cbff30db0522b354e846db":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"],"baf0c78c7a5f19bf2f5648cdadaab16181019d72":["2c7f2abe6ac4f6d10bf9e8378b81b87cf40e9b64"],"fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4":["67a8d31cfc3fe149dba4b571b310a8d7b9c2a227"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"2c7f2abe6ac4f6d10bf9e8378b81b87cf40e9b64":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"06595b0c22c7d3075c4104d3820cccf95d9d8a43":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","67a8d31cfc3fe149dba4b571b310a8d7b9c2a227"],"67a8d31cfc3fe149dba4b571b310a8d7b9c2a227":["1983c3206274e794a3d58501a0c4c3d4fd56555f"],"1983c3206274e794a3d58501a0c4c3d4fd56555f":["baf0c78c7a5f19bf2f5648cdadaab16181019d72"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["aa2585c33d5d66a1c837c312221eb55ddb3c4300"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3f504512a03d978990cbff30db0522b354e846db"]},"commit2Childs":{"aa2585c33d5d66a1c837c312221eb55ddb3c4300":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"],"3f504512a03d978990cbff30db0522b354e846db":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"baf0c78c7a5f19bf2f5648cdadaab16181019d72":["1983c3206274e794a3d58501a0c4c3d4fd56555f"],"fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4":["aa2585c33d5d66a1c837c312221eb55ddb3c4300"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["2c7f2abe6ac4f6d10bf9e8378b81b87cf40e9b64","06595b0c22c7d3075c4104d3820cccf95d9d8a43"],"2c7f2abe6ac4f6d10bf9e8378b81b87cf40e9b64":["baf0c78c7a5f19bf2f5648cdadaab16181019d72"],"06595b0c22c7d3075c4104d3820cccf95d9d8a43":[],"67a8d31cfc3fe149dba4b571b310a8d7b9c2a227":["fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4","06595b0c22c7d3075c4104d3820cccf95d9d8a43"],"1983c3206274e794a3d58501a0c4c3d4fd56555f":["67a8d31cfc3fe149dba4b571b310a8d7b9c2a227"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["3f504512a03d978990cbff30db0522b354e846db"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["06595b0c22c7d3075c4104d3820cccf95d9d8a43","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}