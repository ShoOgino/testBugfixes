{"path":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","commits":[{"id":"a0a506fe165b26e024afa1aec8a4a7d758e837ff","date":1410971446,"type":1,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,DirContext).mjava","sourceNew":"  @Override\n  protected Directory create(String path, LockFactory lockFactory, DirContext dirContext) throws IOException {\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = new Metrics(conf);\n    }\n    \n    boolean blockCacheEnabled = params.getBool(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = params.getBool(BLOCKCACHE_GLOBAL, false); // default to false for back compat\n    boolean blockCacheReadEnabled = params.getBool(BLOCKCACHE_READ_ENABLED, true);\n    boolean blockCacheWriteEnabled = params.getBool(BLOCKCACHE_WRITE_ENABLED, false);\n    \n    if (blockCacheWriteEnabled) {\n      LOG.warn(\"Using \" + BLOCKCACHE_WRITE_ENABLED + \" is currently buggy and can result in readers seeing a corrupted view of the index.\");\n    }\n    \n    final Directory dir;\n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = params.getInt(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = params.getInt(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = params.getBool(\n          BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bufferSize, bufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      HdfsDirectory hdfsDirectory = new HdfsDirectory(new Path(path), lockFactory, conf);\n      dir = new BlockDirectory(path, hdfsDirectory, cache, null,\n          blockCacheReadEnabled, blockCacheWriteEnabled);\n    } else {\n      dir = new HdfsDirectory(new Path(path), lockFactory, conf);\n    }\n    \n    boolean nrtCachingDirectory = params.getBool(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = params.getInt(\n          NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = params.getInt(NRTCACHINGDIRECTORY_MAXCACHEMB,\n          192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB,\n          nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","sourceOld":"  @Override\n  protected Directory create(String path, DirContext dirContext)\n      throws IOException {\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = new Metrics(conf);\n    }\n    \n    boolean blockCacheEnabled = params.getBool(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = params.getBool(BLOCKCACHE_GLOBAL, false); // default to false for back compat\n    boolean blockCacheReadEnabled = params.getBool(BLOCKCACHE_READ_ENABLED, true);\n    boolean blockCacheWriteEnabled = params.getBool(BLOCKCACHE_WRITE_ENABLED, false);\n    \n    if (blockCacheWriteEnabled) {\n      LOG.warn(\"Using \" + BLOCKCACHE_WRITE_ENABLED + \" is currently buggy and can result in readers seeing a corrupted view of the index.\");\n    }\n    Directory dir = null;\n    \n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = params.getInt(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = params.getInt(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = params.getBool(\n          BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bufferSize, bufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      HdfsDirectory hdfsDirectory = new HdfsDirectory(new Path(path), conf);\n      dir = new BlockDirectory(path, hdfsDirectory, cache, null,\n          blockCacheReadEnabled, blockCacheWriteEnabled);\n    } else {\n      dir = new HdfsDirectory(new Path(path), conf);\n    }\n    \n    boolean nrtCachingDirectory = params.getBool(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = params.getInt(\n          NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = params.getInt(NRTCACHINGDIRECTORY_MAXCACHEMB,\n          192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB,\n          nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"26bd56bd7f06194390617d646d6b9a24a7a472dd","date":1420576157,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","sourceNew":"  @Override\n  protected Directory create(String path, LockFactory lockFactory, DirContext dirContext) throws IOException {\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = MetricsHolder.metrics;\n    }\n    \n    boolean blockCacheEnabled = params.getBool(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = params.getBool(BLOCKCACHE_GLOBAL, false); // default to false for back compat\n    boolean blockCacheReadEnabled = params.getBool(BLOCKCACHE_READ_ENABLED, true);\n    boolean blockCacheWriteEnabled = params.getBool(BLOCKCACHE_WRITE_ENABLED, false);\n    \n    if (blockCacheWriteEnabled) {\n      LOG.warn(\"Using \" + BLOCKCACHE_WRITE_ENABLED + \" is currently buggy and can result in readers seeing a corrupted view of the index.\");\n    }\n    \n    final Directory dir;\n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = params.getInt(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = params.getInt(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = params.getBool(\n          BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bufferSize, bufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      HdfsDirectory hdfsDirectory = new HdfsDirectory(new Path(path), lockFactory, conf);\n      dir = new BlockDirectory(path, hdfsDirectory, cache, null,\n          blockCacheReadEnabled, blockCacheWriteEnabled);\n    } else {\n      dir = new HdfsDirectory(new Path(path), lockFactory, conf);\n    }\n    \n    boolean nrtCachingDirectory = params.getBool(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = params.getInt(\n          NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = params.getInt(NRTCACHINGDIRECTORY_MAXCACHEMB,\n          192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB,\n          nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","sourceOld":"  @Override\n  protected Directory create(String path, LockFactory lockFactory, DirContext dirContext) throws IOException {\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = new Metrics(conf);\n    }\n    \n    boolean blockCacheEnabled = params.getBool(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = params.getBool(BLOCKCACHE_GLOBAL, false); // default to false for back compat\n    boolean blockCacheReadEnabled = params.getBool(BLOCKCACHE_READ_ENABLED, true);\n    boolean blockCacheWriteEnabled = params.getBool(BLOCKCACHE_WRITE_ENABLED, false);\n    \n    if (blockCacheWriteEnabled) {\n      LOG.warn(\"Using \" + BLOCKCACHE_WRITE_ENABLED + \" is currently buggy and can result in readers seeing a corrupted view of the index.\");\n    }\n    \n    final Directory dir;\n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = params.getInt(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = params.getInt(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = params.getBool(\n          BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bufferSize, bufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      HdfsDirectory hdfsDirectory = new HdfsDirectory(new Path(path), lockFactory, conf);\n      dir = new BlockDirectory(path, hdfsDirectory, cache, null,\n          blockCacheReadEnabled, blockCacheWriteEnabled);\n    } else {\n      dir = new HdfsDirectory(new Path(path), lockFactory, conf);\n    }\n    \n    boolean nrtCachingDirectory = params.getBool(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = params.getInt(\n          NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = params.getInt(NRTCACHINGDIRECTORY_MAXCACHEMB,\n          192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB,\n          nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","bugFix":["849494cf2f3a96af5c8c84995108ddd8456fcd04"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0ce6c1f997c135ab2e3d211580d089de539d7e20","date":1421161966,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","sourceNew":"  @Override\n  protected Directory create(String path, LockFactory lockFactory, DirContext dirContext) throws IOException {\n    assert params != null : \"init must be called before create\";\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = MetricsHolder.metrics;\n    }\n    \n    boolean blockCacheEnabled = getConfig(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = getConfig(BLOCKCACHE_GLOBAL, false); // default to false for back compat\n    boolean blockCacheReadEnabled = getConfig(BLOCKCACHE_READ_ENABLED, true);\n    \n    final Directory dir;\n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = getConfig(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = getConfig(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = getConfig(BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = getConfig(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = getConfig(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bufferSize, bufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      HdfsDirectory hdfsDirectory = new HdfsDirectory(new Path(path), lockFactory, conf);\n      dir = new BlockDirectory(path, hdfsDirectory, cache, null, blockCacheReadEnabled, false);\n    } else {\n      dir = new HdfsDirectory(new Path(path), lockFactory, conf);\n    }\n    \n    boolean nrtCachingDirectory = getConfig(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = getConfig(NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = getConfig(NRTCACHINGDIRECTORY_MAXCACHEMB, 192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB, nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","sourceOld":"  @Override\n  protected Directory create(String path, LockFactory lockFactory, DirContext dirContext) throws IOException {\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = MetricsHolder.metrics;\n    }\n    \n    boolean blockCacheEnabled = params.getBool(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = params.getBool(BLOCKCACHE_GLOBAL, false); // default to false for back compat\n    boolean blockCacheReadEnabled = params.getBool(BLOCKCACHE_READ_ENABLED, true);\n    boolean blockCacheWriteEnabled = params.getBool(BLOCKCACHE_WRITE_ENABLED, false);\n    \n    if (blockCacheWriteEnabled) {\n      LOG.warn(\"Using \" + BLOCKCACHE_WRITE_ENABLED + \" is currently buggy and can result in readers seeing a corrupted view of the index.\");\n    }\n    \n    final Directory dir;\n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = params.getInt(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = params.getInt(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = params.getBool(\n          BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bufferSize, bufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      HdfsDirectory hdfsDirectory = new HdfsDirectory(new Path(path), lockFactory, conf);\n      dir = new BlockDirectory(path, hdfsDirectory, cache, null,\n          blockCacheReadEnabled, blockCacheWriteEnabled);\n    } else {\n      dir = new HdfsDirectory(new Path(path), lockFactory, conf);\n    }\n    \n    boolean nrtCachingDirectory = params.getBool(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = params.getInt(\n          NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = params.getInt(NRTCACHINGDIRECTORY_MAXCACHEMB,\n          192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB,\n          nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","bugFix":null,"bugIntro":["fd47c5508b60610cbccd0cbb31e9cabc01d08d42"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1ce8283f367b946e5dd6300887294d7d115f2b9f","date":1433955116,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","sourceNew":"  @Override\n  protected Directory create(String path, LockFactory lockFactory, DirContext dirContext) throws IOException {\n    assert params != null : \"init must be called before create\";\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = MetricsHolder.metrics;\n    }\n    \n    boolean blockCacheEnabled = getConfig(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = getConfig(BLOCKCACHE_GLOBAL, false); // default to false for back compat\n    boolean blockCacheReadEnabled = getConfig(BLOCKCACHE_READ_ENABLED, true);\n    \n    final HdfsDirectory hdfsDir;\n    final Directory dir;\n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = getConfig(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = getConfig(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = getConfig(BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = getConfig(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = getConfig(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bufferSize, bufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf);\n      dir = new BlockDirectory(path, hdfsDir, cache, null, blockCacheReadEnabled, false);\n    } else {\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf);\n      dir = hdfsDir;\n    }\n    \n    LocalityHolder.reporter.registerDirectory(hdfsDir);\n\n    boolean nrtCachingDirectory = getConfig(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = getConfig(NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = getConfig(NRTCACHINGDIRECTORY_MAXCACHEMB, 192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB, nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","sourceOld":"  @Override\n  protected Directory create(String path, LockFactory lockFactory, DirContext dirContext) throws IOException {\n    assert params != null : \"init must be called before create\";\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = MetricsHolder.metrics;\n    }\n    \n    boolean blockCacheEnabled = getConfig(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = getConfig(BLOCKCACHE_GLOBAL, false); // default to false for back compat\n    boolean blockCacheReadEnabled = getConfig(BLOCKCACHE_READ_ENABLED, true);\n    \n    final Directory dir;\n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = getConfig(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = getConfig(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = getConfig(BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = getConfig(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = getConfig(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bufferSize, bufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      HdfsDirectory hdfsDirectory = new HdfsDirectory(new Path(path), lockFactory, conf);\n      dir = new BlockDirectory(path, hdfsDirectory, cache, null, blockCacheReadEnabled, false);\n    } else {\n      dir = new HdfsDirectory(new Path(path), lockFactory, conf);\n    }\n    \n    boolean nrtCachingDirectory = getConfig(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = getConfig(NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = getConfig(NRTCACHINGDIRECTORY_MAXCACHEMB, 192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB, nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","bugFix":null,"bugIntro":["87df8312996aa71322478e54c0c1c43233e0b491"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"87df8312996aa71322478e54c0c1c43233e0b491","date":1443723405,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","sourceNew":"  @Override\n  protected Directory create(String path, LockFactory lockFactory, DirContext dirContext) throws IOException {\n    assert params != null : \"init must be called before create\";\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = MetricsHolder.metrics;\n    }\n    \n    boolean blockCacheEnabled = getConfig(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = getConfig(BLOCKCACHE_GLOBAL, false); // default to false for back compat\n    boolean blockCacheReadEnabled = getConfig(BLOCKCACHE_READ_ENABLED, true);\n    \n    final HdfsDirectory hdfsDir;\n    final Directory dir;\n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = getConfig(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = getConfig(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = getConfig(BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = getConfig(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = getConfig(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bufferSize, bufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf);\n      dir = new BlockDirectory(path, hdfsDir, cache, null, blockCacheReadEnabled, false);\n    } else {\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf);\n      dir = hdfsDir;\n    }\n    if (params.getBool(LOCALITYMETRICS_ENABLED, false)) {\n      LocalityHolder.reporter.registerDirectory(hdfsDir);\n    }\n\n    boolean nrtCachingDirectory = getConfig(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = getConfig(NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = getConfig(NRTCACHINGDIRECTORY_MAXCACHEMB, 192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB, nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","sourceOld":"  @Override\n  protected Directory create(String path, LockFactory lockFactory, DirContext dirContext) throws IOException {\n    assert params != null : \"init must be called before create\";\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = MetricsHolder.metrics;\n    }\n    \n    boolean blockCacheEnabled = getConfig(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = getConfig(BLOCKCACHE_GLOBAL, false); // default to false for back compat\n    boolean blockCacheReadEnabled = getConfig(BLOCKCACHE_READ_ENABLED, true);\n    \n    final HdfsDirectory hdfsDir;\n    final Directory dir;\n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = getConfig(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = getConfig(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = getConfig(BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = getConfig(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = getConfig(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bufferSize, bufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf);\n      dir = new BlockDirectory(path, hdfsDir, cache, null, blockCacheReadEnabled, false);\n    } else {\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf);\n      dir = hdfsDir;\n    }\n    \n    LocalityHolder.reporter.registerDirectory(hdfsDir);\n\n    boolean nrtCachingDirectory = getConfig(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = getConfig(NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = getConfig(NRTCACHINGDIRECTORY_MAXCACHEMB, 192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB, nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","bugFix":["849494cf2f3a96af5c8c84995108ddd8456fcd04","1ce8283f367b946e5dd6300887294d7d115f2b9f"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fd47c5508b60610cbccd0cbb31e9cabc01d08d42","date":1444142412,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","sourceNew":"  @Override\n  protected Directory create(String path, LockFactory lockFactory, DirContext dirContext) throws IOException {\n    assert params != null : \"init must be called before create\";\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = MetricsHolder.metrics;\n    }\n    \n    boolean blockCacheEnabled = getConfig(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = getConfig(BLOCKCACHE_GLOBAL, true);\n    boolean blockCacheReadEnabled = getConfig(BLOCKCACHE_READ_ENABLED, true);\n    \n    final HdfsDirectory hdfsDir;\n    final Directory dir;\n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = getConfig(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = getConfig(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = getConfig(BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = getConfig(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = getConfig(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bufferSize, bufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf);\n      dir = new BlockDirectory(path, hdfsDir, cache, null, blockCacheReadEnabled, false);\n    } else {\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf);\n      dir = hdfsDir;\n    }\n    if (params.getBool(LOCALITYMETRICS_ENABLED, false)) {\n      LocalityHolder.reporter.registerDirectory(hdfsDir);\n    }\n\n    boolean nrtCachingDirectory = getConfig(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = getConfig(NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = getConfig(NRTCACHINGDIRECTORY_MAXCACHEMB, 192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB, nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","sourceOld":"  @Override\n  protected Directory create(String path, LockFactory lockFactory, DirContext dirContext) throws IOException {\n    assert params != null : \"init must be called before create\";\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = MetricsHolder.metrics;\n    }\n    \n    boolean blockCacheEnabled = getConfig(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = getConfig(BLOCKCACHE_GLOBAL, false); // default to false for back compat\n    boolean blockCacheReadEnabled = getConfig(BLOCKCACHE_READ_ENABLED, true);\n    \n    final HdfsDirectory hdfsDir;\n    final Directory dir;\n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = getConfig(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = getConfig(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = getConfig(BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = getConfig(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = getConfig(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bufferSize, bufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf);\n      dir = new BlockDirectory(path, hdfsDir, cache, null, blockCacheReadEnabled, false);\n    } else {\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf);\n      dir = hdfsDir;\n    }\n    if (params.getBool(LOCALITYMETRICS_ENABLED, false)) {\n      LocalityHolder.reporter.registerDirectory(hdfsDir);\n    }\n\n    boolean nrtCachingDirectory = getConfig(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = getConfig(NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = getConfig(NRTCACHINGDIRECTORY_MAXCACHEMB, 192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB, nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","bugFix":["0ce6c1f997c135ab2e3d211580d089de539d7e20"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f064be057b13c0e9885962e9e5bae10317371f1b","date":1458702313,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","sourceNew":"  @Override\n  protected Directory create(String path, LockFactory lockFactory, DirContext dirContext) throws IOException {\n    assert params != null : \"init must be called before create\";\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = MetricsHolder.metrics;\n    }\n    \n    boolean blockCacheEnabled = getConfig(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = getConfig(BLOCKCACHE_GLOBAL, true);\n    boolean blockCacheReadEnabled = getConfig(BLOCKCACHE_READ_ENABLED, true);\n    \n    final HdfsDirectory hdfsDir;\n    final Directory dir;\n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = getConfig(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = getConfig(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = getConfig(BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = getConfig(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = getConfig(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bufferSize, bufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf);\n      dir = new BlockDirectory(path, hdfsDir, cache, null, blockCacheReadEnabled, false, cacheMerges, cacheReadOnce);\n    } else {\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf);\n      dir = hdfsDir;\n    }\n    if (params.getBool(LOCALITYMETRICS_ENABLED, false)) {\n      LocalityHolder.reporter.registerDirectory(hdfsDir);\n    }\n\n    boolean nrtCachingDirectory = getConfig(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = getConfig(NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = getConfig(NRTCACHINGDIRECTORY_MAXCACHEMB, 192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB, nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","sourceOld":"  @Override\n  protected Directory create(String path, LockFactory lockFactory, DirContext dirContext) throws IOException {\n    assert params != null : \"init must be called before create\";\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = MetricsHolder.metrics;\n    }\n    \n    boolean blockCacheEnabled = getConfig(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = getConfig(BLOCKCACHE_GLOBAL, true);\n    boolean blockCacheReadEnabled = getConfig(BLOCKCACHE_READ_ENABLED, true);\n    \n    final HdfsDirectory hdfsDir;\n    final Directory dir;\n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = getConfig(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = getConfig(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = getConfig(BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = getConfig(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = getConfig(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bufferSize, bufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf);\n      dir = new BlockDirectory(path, hdfsDir, cache, null, blockCacheReadEnabled, false);\n    } else {\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf);\n      dir = hdfsDir;\n    }\n    if (params.getBool(LOCALITYMETRICS_ENABLED, false)) {\n      LocalityHolder.reporter.registerDirectory(hdfsDir);\n    }\n\n    boolean nrtCachingDirectory = getConfig(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = getConfig(NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = getConfig(NRTCACHINGDIRECTORY_MAXCACHEMB, 192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB, nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"61c6fb105d0ce095d846502a1d7a2a4bbf8fdecd","date":1466528770,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","sourceNew":"  @Override\n  protected Directory create(String path, LockFactory lockFactory, DirContext dirContext) throws IOException {\n    assert params != null : \"init must be called before create\";\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = MetricsHolder.metrics;\n    }\n    \n    boolean blockCacheEnabled = getConfig(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = getConfig(BLOCKCACHE_GLOBAL, true);\n    boolean blockCacheReadEnabled = getConfig(BLOCKCACHE_READ_ENABLED, true);\n    \n    final HdfsDirectory hdfsDir;\n    final Directory dir;\n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = getConfig(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = getConfig(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = getConfig(BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bsBufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", blockSize);\n      int bsBufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 0); // this is actually total size\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bsBufferSize, bsBufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      int readBufferSize = params.getInt(\"solr.hdfs.blockcache.read.buffersize\", blockSize);\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf, readBufferSize);\n      dir = new BlockDirectory(path, hdfsDir, cache, null, blockCacheReadEnabled, false, cacheMerges, cacheReadOnce);\n    } else {\n      hdfsDir = new HdfsDirectory(new Path(path), conf);\n      dir = hdfsDir;\n    }\n    if (params.getBool(LOCALITYMETRICS_ENABLED, false)) {\n      LocalityHolder.reporter.registerDirectory(hdfsDir);\n    }\n\n    boolean nrtCachingDirectory = getConfig(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = getConfig(NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = getConfig(NRTCACHINGDIRECTORY_MAXCACHEMB, 192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB, nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","sourceOld":"  @Override\n  protected Directory create(String path, LockFactory lockFactory, DirContext dirContext) throws IOException {\n    assert params != null : \"init must be called before create\";\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = MetricsHolder.metrics;\n    }\n    \n    boolean blockCacheEnabled = getConfig(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = getConfig(BLOCKCACHE_GLOBAL, true);\n    boolean blockCacheReadEnabled = getConfig(BLOCKCACHE_READ_ENABLED, true);\n    \n    final HdfsDirectory hdfsDir;\n    final Directory dir;\n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = getConfig(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = getConfig(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = getConfig(BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = getConfig(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = getConfig(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bufferSize, bufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf);\n      dir = new BlockDirectory(path, hdfsDir, cache, null, blockCacheReadEnabled, false, cacheMerges, cacheReadOnce);\n    } else {\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf);\n      dir = hdfsDir;\n    }\n    if (params.getBool(LOCALITYMETRICS_ENABLED, false)) {\n      LocalityHolder.reporter.registerDirectory(hdfsDir);\n    }\n\n    boolean nrtCachingDirectory = getConfig(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = getConfig(NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = getConfig(NRTCACHINGDIRECTORY_MAXCACHEMB, 192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB, nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","sourceNew":"  @Override\n  protected Directory create(String path, LockFactory lockFactory, DirContext dirContext) throws IOException {\n    assert params != null : \"init must be called before create\";\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = MetricsHolder.metrics;\n    }\n    \n    boolean blockCacheEnabled = getConfig(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = getConfig(BLOCKCACHE_GLOBAL, true);\n    boolean blockCacheReadEnabled = getConfig(BLOCKCACHE_READ_ENABLED, true);\n    \n    final HdfsDirectory hdfsDir;\n    final Directory dir;\n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = getConfig(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = getConfig(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = getConfig(BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bsBufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", blockSize);\n      int bsBufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 0); // this is actually total size\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bsBufferSize, bsBufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      int readBufferSize = params.getInt(\"solr.hdfs.blockcache.read.buffersize\", blockSize);\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf, readBufferSize);\n      dir = new BlockDirectory(path, hdfsDir, cache, null, blockCacheReadEnabled, false, cacheMerges, cacheReadOnce);\n    } else {\n      hdfsDir = new HdfsDirectory(new Path(path), conf);\n      dir = hdfsDir;\n    }\n    if (params.getBool(LOCALITYMETRICS_ENABLED, false)) {\n      LocalityHolder.reporter.registerDirectory(hdfsDir);\n    }\n\n    boolean nrtCachingDirectory = getConfig(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = getConfig(NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = getConfig(NRTCACHINGDIRECTORY_MAXCACHEMB, 192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB, nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","sourceOld":"  @Override\n  protected Directory create(String path, LockFactory lockFactory, DirContext dirContext) throws IOException {\n    assert params != null : \"init must be called before create\";\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = MetricsHolder.metrics;\n    }\n    \n    boolean blockCacheEnabled = getConfig(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = getConfig(BLOCKCACHE_GLOBAL, true);\n    boolean blockCacheReadEnabled = getConfig(BLOCKCACHE_READ_ENABLED, true);\n    \n    final HdfsDirectory hdfsDir;\n    final Directory dir;\n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = getConfig(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = getConfig(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = getConfig(BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = getConfig(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = getConfig(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bufferSize, bufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf);\n      dir = new BlockDirectory(path, hdfsDir, cache, null, blockCacheReadEnabled, false, cacheMerges, cacheReadOnce);\n    } else {\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf);\n      dir = hdfsDir;\n    }\n    if (params.getBool(LOCALITYMETRICS_ENABLED, false)) {\n      LocalityHolder.reporter.registerDirectory(hdfsDir);\n    }\n\n    boolean nrtCachingDirectory = getConfig(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = getConfig(NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = getConfig(NRTCACHINGDIRECTORY_MAXCACHEMB, 192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB, nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f15af35d55d70c34451f9df5edeaeff6b31f8cbe","date":1519625627,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","sourceNew":"  @Override\n  @SuppressWarnings(\"resource\")\n  protected Directory create(String path, LockFactory lockFactory, DirContext dirContext) throws IOException {\n    assert params != null : \"init must be called before create\";\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = MetricsHolder.metrics;\n    }\n    \n    boolean blockCacheEnabled = getConfig(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = getConfig(BLOCKCACHE_GLOBAL, true);\n    boolean blockCacheReadEnabled = getConfig(BLOCKCACHE_READ_ENABLED, true);\n    \n    final HdfsDirectory hdfsDir;\n\n    final Directory dir;\n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = getConfig(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = getConfig(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = getConfig(BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bsBufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", blockSize);\n      int bsBufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 0); // this is actually total size\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bsBufferSize, bsBufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      int readBufferSize = params.getInt(\"solr.hdfs.blockcache.read.buffersize\", blockSize);\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf, readBufferSize);\n      dir = new BlockDirectory(path, hdfsDir, cache, null, blockCacheReadEnabled, false, cacheMerges, cacheReadOnce);\n    } else {\n      hdfsDir = new HdfsDirectory(new Path(path), conf);\n      dir = hdfsDir;\n    }\n    if (params.getBool(LOCALITYMETRICS_ENABLED, false)) {\n      LocalityHolder.reporter.registerDirectory(hdfsDir);\n    }\n\n    boolean nrtCachingDirectory = getConfig(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = getConfig(NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = getConfig(NRTCACHINGDIRECTORY_MAXCACHEMB, 192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB, nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","sourceOld":"  @Override\n  protected Directory create(String path, LockFactory lockFactory, DirContext dirContext) throws IOException {\n    assert params != null : \"init must be called before create\";\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = MetricsHolder.metrics;\n    }\n    \n    boolean blockCacheEnabled = getConfig(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = getConfig(BLOCKCACHE_GLOBAL, true);\n    boolean blockCacheReadEnabled = getConfig(BLOCKCACHE_READ_ENABLED, true);\n    \n    final HdfsDirectory hdfsDir;\n    final Directory dir;\n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = getConfig(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = getConfig(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = getConfig(BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bsBufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", blockSize);\n      int bsBufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 0); // this is actually total size\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bsBufferSize, bsBufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      int readBufferSize = params.getInt(\"solr.hdfs.blockcache.read.buffersize\", blockSize);\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf, readBufferSize);\n      dir = new BlockDirectory(path, hdfsDir, cache, null, blockCacheReadEnabled, false, cacheMerges, cacheReadOnce);\n    } else {\n      hdfsDir = new HdfsDirectory(new Path(path), conf);\n      dir = hdfsDir;\n    }\n    if (params.getBool(LOCALITYMETRICS_ENABLED, false)) {\n      LocalityHolder.reporter.registerDirectory(hdfsDir);\n    }\n\n    boolean nrtCachingDirectory = getConfig(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = getConfig(NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = getConfig(NRTCACHINGDIRECTORY_MAXCACHEMB, 192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB, nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e9c81f7e703d7ccca5bc78beb61253f0a8a22afd","date":1534976797,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","sourceNew":"  @Override\n  @SuppressWarnings(\"resource\")\n  protected Directory create(String path, LockFactory lockFactory, DirContext dirContext) throws IOException {\n    assert params != null : \"init must be called before create\";\n    log.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = MetricsHolder.metrics;\n    }\n    \n    boolean blockCacheEnabled = getConfig(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = getConfig(BLOCKCACHE_GLOBAL, true);\n    boolean blockCacheReadEnabled = getConfig(BLOCKCACHE_READ_ENABLED, true);\n    \n    final HdfsDirectory hdfsDir;\n\n    final Directory dir;\n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = getConfig(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = getConfig(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = getConfig(BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      log.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      log.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bsBufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", blockSize);\n      int bsBufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 0); // this is actually total size\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bsBufferSize, bsBufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      int readBufferSize = params.getInt(\"solr.hdfs.blockcache.read.buffersize\", blockSize);\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf, readBufferSize);\n      dir = new BlockDirectory(path, hdfsDir, cache, null, blockCacheReadEnabled, false, cacheMerges, cacheReadOnce);\n    } else {\n      hdfsDir = new HdfsDirectory(new Path(path), conf);\n      dir = hdfsDir;\n    }\n    if (params.getBool(LOCALITYMETRICS_ENABLED, false)) {\n      LocalityHolder.reporter.registerDirectory(hdfsDir);\n    }\n\n    boolean nrtCachingDirectory = getConfig(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = getConfig(NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = getConfig(NRTCACHINGDIRECTORY_MAXCACHEMB, 192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB, nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","sourceOld":"  @Override\n  @SuppressWarnings(\"resource\")\n  protected Directory create(String path, LockFactory lockFactory, DirContext dirContext) throws IOException {\n    assert params != null : \"init must be called before create\";\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = MetricsHolder.metrics;\n    }\n    \n    boolean blockCacheEnabled = getConfig(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = getConfig(BLOCKCACHE_GLOBAL, true);\n    boolean blockCacheReadEnabled = getConfig(BLOCKCACHE_READ_ENABLED, true);\n    \n    final HdfsDirectory hdfsDir;\n\n    final Directory dir;\n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = getConfig(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = getConfig(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = getConfig(BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bsBufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", blockSize);\n      int bsBufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 0); // this is actually total size\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bsBufferSize, bsBufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      int readBufferSize = params.getInt(\"solr.hdfs.blockcache.read.buffersize\", blockSize);\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf, readBufferSize);\n      dir = new BlockDirectory(path, hdfsDir, cache, null, blockCacheReadEnabled, false, cacheMerges, cacheReadOnce);\n    } else {\n      hdfsDir = new HdfsDirectory(new Path(path), conf);\n      dir = hdfsDir;\n    }\n    if (params.getBool(LOCALITYMETRICS_ENABLED, false)) {\n      LocalityHolder.reporter.registerDirectory(hdfsDir);\n    }\n\n    boolean nrtCachingDirectory = getConfig(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = getConfig(NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = getConfig(NRTCACHINGDIRECTORY_MAXCACHEMB, 192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB, nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"870bbea2a1d8085b48b52a1480ac95db389476c1","date":1553970360,"type":3,"author":"Kevin Risden","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","sourceNew":"  @Override\n  @SuppressWarnings(\"resource\")\n  protected Directory create(String path, LockFactory lockFactory, DirContext dirContext) throws IOException {\n    assert params != null : \"init must be called before create\";\n    log.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf(new Path(path));\n    \n    if (metrics == null) {\n      metrics = MetricsHolder.metrics;\n    }\n    \n    boolean blockCacheEnabled = getConfig(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = getConfig(BLOCKCACHE_GLOBAL, true);\n    boolean blockCacheReadEnabled = getConfig(BLOCKCACHE_READ_ENABLED, true);\n    \n    final HdfsDirectory hdfsDir;\n\n    final Directory dir;\n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = getConfig(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = getConfig(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = getConfig(BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      log.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      log.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bsBufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", blockSize);\n      int bsBufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 0); // this is actually total size\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bsBufferSize, bsBufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      int readBufferSize = params.getInt(\"solr.hdfs.blockcache.read.buffersize\", blockSize);\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf, readBufferSize);\n      dir = new BlockDirectory(path, hdfsDir, cache, null, blockCacheReadEnabled, false, cacheMerges, cacheReadOnce);\n    } else {\n      hdfsDir = new HdfsDirectory(new Path(path), conf);\n      dir = hdfsDir;\n    }\n    if (params.getBool(LOCALITYMETRICS_ENABLED, false)) {\n      LocalityHolder.reporter.registerDirectory(hdfsDir);\n    }\n\n    boolean nrtCachingDirectory = getConfig(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = getConfig(NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = getConfig(NRTCACHINGDIRECTORY_MAXCACHEMB, 192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB, nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","sourceOld":"  @Override\n  @SuppressWarnings(\"resource\")\n  protected Directory create(String path, LockFactory lockFactory, DirContext dirContext) throws IOException {\n    assert params != null : \"init must be called before create\";\n    log.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = MetricsHolder.metrics;\n    }\n    \n    boolean blockCacheEnabled = getConfig(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = getConfig(BLOCKCACHE_GLOBAL, true);\n    boolean blockCacheReadEnabled = getConfig(BLOCKCACHE_READ_ENABLED, true);\n    \n    final HdfsDirectory hdfsDir;\n\n    final Directory dir;\n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = getConfig(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = getConfig(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = getConfig(BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      log.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      log.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bsBufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", blockSize);\n      int bsBufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 0); // this is actually total size\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bsBufferSize, bsBufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      int readBufferSize = params.getInt(\"solr.hdfs.blockcache.read.buffersize\", blockSize);\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf, readBufferSize);\n      dir = new BlockDirectory(path, hdfsDir, cache, null, blockCacheReadEnabled, false, cacheMerges, cacheReadOnce);\n    } else {\n      hdfsDir = new HdfsDirectory(new Path(path), conf);\n      dir = hdfsDir;\n    }\n    if (params.getBool(LOCALITYMETRICS_ENABLED, false)) {\n      LocalityHolder.reporter.registerDirectory(hdfsDir);\n    }\n\n    boolean nrtCachingDirectory = getConfig(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = getConfig(NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = getConfig(NRTCACHINGDIRECTORY_MAXCACHEMB, 192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB, nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"140be51d03394488536f4aacedace29f9b318347","date":1587170432,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","sourceNew":"  @Override\n  @SuppressWarnings(\"resource\")\n  protected Directory create(String path, LockFactory lockFactory, DirContext dirContext) throws IOException {\n    assert params != null : \"init must be called before create\";\n    log.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf(new Path(path));\n    \n    if (metrics == null) {\n      metrics = MetricsHolder.metrics;\n    }\n    \n    boolean blockCacheEnabled = getConfig(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = getConfig(BLOCKCACHE_GLOBAL, true);\n    boolean blockCacheReadEnabled = getConfig(BLOCKCACHE_READ_ENABLED, true);\n    \n    final HdfsDirectory hdfsDir;\n\n    final Directory dir;\n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = getConfig(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = getConfig(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = getConfig(BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      log.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      if (log.isInfoEnabled()) {\n        log.info(\n            \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n            new Object[]{slabSize, bankCount,\n                ((long) bankCount * (long) slabSize)});\n      }\n      \n      int bsBufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", blockSize);\n      int bsBufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 0); // this is actually total size\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bsBufferSize, bsBufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      int readBufferSize = params.getInt(\"solr.hdfs.blockcache.read.buffersize\", blockSize);\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf, readBufferSize);\n      dir = new BlockDirectory(path, hdfsDir, cache, null, blockCacheReadEnabled, false, cacheMerges, cacheReadOnce);\n    } else {\n      hdfsDir = new HdfsDirectory(new Path(path), conf);\n      dir = hdfsDir;\n    }\n    if (params.getBool(LOCALITYMETRICS_ENABLED, false)) {\n      LocalityHolder.reporter.registerDirectory(hdfsDir);\n    }\n\n    boolean nrtCachingDirectory = getConfig(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = getConfig(NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = getConfig(NRTCACHINGDIRECTORY_MAXCACHEMB, 192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB, nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","sourceOld":"  @Override\n  @SuppressWarnings(\"resource\")\n  protected Directory create(String path, LockFactory lockFactory, DirContext dirContext) throws IOException {\n    assert params != null : \"init must be called before create\";\n    log.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf(new Path(path));\n    \n    if (metrics == null) {\n      metrics = MetricsHolder.metrics;\n    }\n    \n    boolean blockCacheEnabled = getConfig(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = getConfig(BLOCKCACHE_GLOBAL, true);\n    boolean blockCacheReadEnabled = getConfig(BLOCKCACHE_READ_ENABLED, true);\n    \n    final HdfsDirectory hdfsDir;\n\n    final Directory dir;\n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = getConfig(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = getConfig(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = getConfig(BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      log.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      log.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bsBufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", blockSize);\n      int bsBufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 0); // this is actually total size\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bsBufferSize, bsBufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      int readBufferSize = params.getInt(\"solr.hdfs.blockcache.read.buffersize\", blockSize);\n      hdfsDir = new HdfsDirectory(new Path(path), lockFactory, conf, readBufferSize);\n      dir = new BlockDirectory(path, hdfsDir, cache, null, blockCacheReadEnabled, false, cacheMerges, cacheReadOnce);\n    } else {\n      hdfsDir = new HdfsDirectory(new Path(path), conf);\n      dir = hdfsDir;\n    }\n    if (params.getBool(LOCALITYMETRICS_ENABLED, false)) {\n      LocalityHolder.reporter.registerDirectory(hdfsDir);\n    }\n\n    boolean nrtCachingDirectory = getConfig(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = getConfig(NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = getConfig(NRTCACHINGDIRECTORY_MAXCACHEMB, 192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB, nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0a506fe165b26e024afa1aec8a4a7d758e837ff":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"870bbea2a1d8085b48b52a1480ac95db389476c1":["e9c81f7e703d7ccca5bc78beb61253f0a8a22afd"],"f064be057b13c0e9885962e9e5bae10317371f1b":["fd47c5508b60610cbccd0cbb31e9cabc01d08d42"],"61c6fb105d0ce095d846502a1d7a2a4bbf8fdecd":["f064be057b13c0e9885962e9e5bae10317371f1b"],"87df8312996aa71322478e54c0c1c43233e0b491":["1ce8283f367b946e5dd6300887294d7d115f2b9f"],"0ce6c1f997c135ab2e3d211580d089de539d7e20":["26bd56bd7f06194390617d646d6b9a24a7a472dd"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["f064be057b13c0e9885962e9e5bae10317371f1b","61c6fb105d0ce095d846502a1d7a2a4bbf8fdecd"],"26bd56bd7f06194390617d646d6b9a24a7a472dd":["a0a506fe165b26e024afa1aec8a4a7d758e837ff"],"140be51d03394488536f4aacedace29f9b318347":["870bbea2a1d8085b48b52a1480ac95db389476c1"],"fd47c5508b60610cbccd0cbb31e9cabc01d08d42":["87df8312996aa71322478e54c0c1c43233e0b491"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"f15af35d55d70c34451f9df5edeaeff6b31f8cbe":["61c6fb105d0ce095d846502a1d7a2a4bbf8fdecd"],"1ce8283f367b946e5dd6300887294d7d115f2b9f":["0ce6c1f997c135ab2e3d211580d089de539d7e20"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["140be51d03394488536f4aacedace29f9b318347"],"e9c81f7e703d7ccca5bc78beb61253f0a8a22afd":["f15af35d55d70c34451f9df5edeaeff6b31f8cbe"]},"commit2Childs":{"a0a506fe165b26e024afa1aec8a4a7d758e837ff":["26bd56bd7f06194390617d646d6b9a24a7a472dd"],"870bbea2a1d8085b48b52a1480ac95db389476c1":["140be51d03394488536f4aacedace29f9b318347"],"f064be057b13c0e9885962e9e5bae10317371f1b":["61c6fb105d0ce095d846502a1d7a2a4bbf8fdecd","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"61c6fb105d0ce095d846502a1d7a2a4bbf8fdecd":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","f15af35d55d70c34451f9df5edeaeff6b31f8cbe"],"87df8312996aa71322478e54c0c1c43233e0b491":["fd47c5508b60610cbccd0cbb31e9cabc01d08d42"],"0ce6c1f997c135ab2e3d211580d089de539d7e20":["1ce8283f367b946e5dd6300887294d7d115f2b9f"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"26bd56bd7f06194390617d646d6b9a24a7a472dd":["0ce6c1f997c135ab2e3d211580d089de539d7e20"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a0a506fe165b26e024afa1aec8a4a7d758e837ff"],"fd47c5508b60610cbccd0cbb31e9cabc01d08d42":["f064be057b13c0e9885962e9e5bae10317371f1b"],"140be51d03394488536f4aacedace29f9b318347":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"f15af35d55d70c34451f9df5edeaeff6b31f8cbe":["e9c81f7e703d7ccca5bc78beb61253f0a8a22afd"],"1ce8283f367b946e5dd6300887294d7d115f2b9f":["87df8312996aa71322478e54c0c1c43233e0b491"],"e9c81f7e703d7ccca5bc78beb61253f0a8a22afd":["870bbea2a1d8085b48b52a1480ac95db389476c1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}