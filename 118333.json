{"path":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        TermAttribute termAtt = ts.addAttribute(TermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.term();\n        \tif(!processedTerms.contains(term))\n        \t{\n        \t\tprocessedTerms.add(term);\n                ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                float minScore=0;\n                Term startTerm=internSavingTemplateTerm.createTerm(term);\n                FuzzyTermEnum fe=new FuzzyTermEnum(reader,startTerm,f.minSimilarity,f.prefixLength);\n                TermEnum origEnum = reader.terms(startTerm);\n                int df=0;\n                if(startTerm.equals(origEnum.term()))\n                {\n                    df=origEnum.docFreq(); //store the df so all variants use same idf\n                }\n                int numVariants=0;\n                int totalVariantDocFreqs=0;\n                do\n                {\n                    Term possibleMatch=fe.term();\n                    if(possibleMatch!=null)\n                    {\n    \t                numVariants++;\n    \t                totalVariantDocFreqs+=fe.docFreq();\n    \t                float score=fe.difference();\n    \t                if(variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n    \t                    ScoreTerm st=new ScoreTerm(possibleMatch,score,startTerm);                    \n    \t                    variantsQ.insertWithOverflow(st);\n    \t                    minScore = variantsQ.top().score; // maintain minScore\n    \t                }\n                    }\n                }\n                while(fe.next());\n                if(numVariants>0)\n                {\n\t                int avgDf=totalVariantDocFreqs/numVariants;\n\t                if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n\t                // take the top variants (scored by edit distance) and reset the score\n\t                // to include an IDF factor then add to the global queue for ranking \n\t                // overall top query terms\n\t                int size = variantsQ.size();\n\t                for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        TermAttribute termAtt = ts.addAttribute(TermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.term();\n        \tif(!processedTerms.contains(term))\n        \t{\n        \t\tprocessedTerms.add(term);\n                ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                float minScore=0;\n                Term startTerm=internSavingTemplateTerm.createTerm(term);\n                FuzzyTermEnum fe=new FuzzyTermEnum(reader,startTerm,f.minSimilarity,f.prefixLength);\n                TermEnum origEnum = reader.terms(startTerm);\n                int df=0;\n                if(startTerm.equals(origEnum.term()))\n                {\n                    df=origEnum.docFreq(); //store the df so all variants use same idf\n                }\n                int numVariants=0;\n                int totalVariantDocFreqs=0;\n                do\n                {\n                    Term possibleMatch=fe.term();\n                    if(possibleMatch!=null)\n                    {\n    \t                numVariants++;\n    \t                totalVariantDocFreqs+=fe.docFreq();\n    \t                float score=fe.difference();\n    \t                if(variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n    \t                    ScoreTerm st=new ScoreTerm(possibleMatch,score,startTerm);                    \n    \t                    variantsQ.insertWithOverflow(st);\n    \t                    minScore = variantsQ.top().score; // maintain minScore\n    \t                }\n                    }\n                }\n                while(fe.next());\n                if(numVariants>0)\n                {\n\t                int avgDf=totalVariantDocFreqs/numVariants;\n\t                if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n\t                // take the top variants (scored by edit distance) and reset the score\n\t                // to include an IDF factor then add to the global queue for ranking \n\t                // overall top query terms\n\t                int size = variantsQ.size();\n\t                for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"955c32f886db6f6356c9fcdea6b1f1cb4effda24","date":1270581567,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        TermAttribute termAtt = ts.addAttribute(TermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.term();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(reader, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  MultiTermQuery.BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(MultiTermQuery.BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      if (possibleMatch!=null) {\n                        numVariants++;\n                        totalVariantDocFreqs+=fe.docFreq();\n                        float score=boostAtt.getBoost();\n                        if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                          ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), possibleMatch.utf8ToString()),score,startTerm);                    \n                          variantsQ.insertWithOverflow(st);\n                          minScore = variantsQ.top().score; // maintain minScore\n                        }\n                      }\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        TermAttribute termAtt = ts.addAttribute(TermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.term();\n        \tif(!processedTerms.contains(term))\n        \t{\n        \t\tprocessedTerms.add(term);\n                ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                float minScore=0;\n                Term startTerm=internSavingTemplateTerm.createTerm(term);\n                FuzzyTermEnum fe=new FuzzyTermEnum(reader,startTerm,f.minSimilarity,f.prefixLength);\n                TermEnum origEnum = reader.terms(startTerm);\n                int df=0;\n                if(startTerm.equals(origEnum.term()))\n                {\n                    df=origEnum.docFreq(); //store the df so all variants use same idf\n                }\n                int numVariants=0;\n                int totalVariantDocFreqs=0;\n                do\n                {\n                    Term possibleMatch=fe.term();\n                    if(possibleMatch!=null)\n                    {\n    \t                numVariants++;\n    \t                totalVariantDocFreqs+=fe.docFreq();\n    \t                float score=fe.difference();\n    \t                if(variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n    \t                    ScoreTerm st=new ScoreTerm(possibleMatch,score,startTerm);                    \n    \t                    variantsQ.insertWithOverflow(st);\n    \t                    minScore = variantsQ.top().score; // maintain minScore\n    \t                }\n                    }\n                }\n                while(fe.next());\n                if(numVariants>0)\n                {\n\t                int avgDf=totalVariantDocFreqs/numVariants;\n\t                if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n\t                // take the top variants (scored by edit distance) and reset the score\n\t                // to include an IDF factor then add to the global queue for ranking \n\t                // overall top query terms\n\t                int size = variantsQ.size();\n\t                for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","bugFix":null,"bugIntro":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a7347509fad0711ac30cb15a746e9a3830a38ebd","date":1275388513,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(reader, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  MultiTermQuery.BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(MultiTermQuery.BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      if (possibleMatch!=null) {\n                        numVariants++;\n                        totalVariantDocFreqs+=fe.docFreq();\n                        float score=boostAtt.getBoost();\n                        if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                          ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), possibleMatch.utf8ToString()),score,startTerm);                    \n                          variantsQ.insertWithOverflow(st);\n                          minScore = variantsQ.top().score; // maintain minScore\n                        }\n                      }\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        TermAttribute termAtt = ts.addAttribute(TermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.term();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(reader, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  MultiTermQuery.BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(MultiTermQuery.BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      if (possibleMatch!=null) {\n                        numVariants++;\n                        totalVariantDocFreqs+=fe.docFreq();\n                        float score=boostAtt.getBoost();\n                        if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                          ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), possibleMatch.utf8ToString()),score,startTerm);                    \n                          variantsQ.insertWithOverflow(st);\n                          minScore = variantsQ.top().score; // maintain minScore\n                        }\n                      }\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4f29ba80b723649f5feb7e37afe1a558dd2c1304","date":1278318805,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(reader, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  MultiTermQuery.BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(MultiTermQuery.BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      if (possibleMatch!=null) {\n                        numVariants++;\n                        totalVariantDocFreqs+=fe.docFreq();\n                        float score=boostAtt.getBoost();\n                        if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                          ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                          variantsQ.insertWithOverflow(st);\n                          minScore = variantsQ.top().score; // maintain minScore\n                        }\n                      }\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(reader, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  MultiTermQuery.BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(MultiTermQuery.BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      if (possibleMatch!=null) {\n                        numVariants++;\n                        totalVariantDocFreqs+=fe.docFreq();\n                        float score=boostAtt.getBoost();\n                        if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                          ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), possibleMatch.utf8ToString()),score,startTerm);                    \n                          variantsQ.insertWithOverflow(st);\n                          minScore = variantsQ.top().score; // maintain minScore\n                        }\n                      }\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(reader, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  MultiTermQuery.BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(MultiTermQuery.BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      if (possibleMatch!=null) {\n                        numVariants++;\n                        totalVariantDocFreqs+=fe.docFreq();\n                        float score=boostAtt.getBoost();\n                        if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                          ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                          variantsQ.insertWithOverflow(st);\n                          minScore = variantsQ.top().score; // maintain minScore\n                        }\n                      }\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(reader, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  MultiTermQuery.BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(MultiTermQuery.BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      if (possibleMatch!=null) {\n                        numVariants++;\n                        totalVariantDocFreqs+=fe.docFreq();\n                        float score=boostAtt.getBoost();\n                        if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                          ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), possibleMatch.utf8ToString()),score,startTerm);                    \n                          variantsQ.insertWithOverflow(st);\n                          minScore = variantsQ.top().score; // maintain minScore\n                        }\n                      }\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e744dea5151ae430f7b84db4a2beb3eba003a114","date":1286826225,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(reader, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  MultiTermQuery.BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(MultiTermQuery.BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      if (possibleMatch!=null) {\n                        numVariants++;\n                        totalVariantDocFreqs+=fe.docFreq();\n                        float score=boostAtt.getBoost();\n                        if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                          ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                          variantsQ.insertWithOverflow(st);\n                          minScore = variantsQ.top().score; // maintain minScore\n                        }\n                        boostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                      }\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(reader, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  MultiTermQuery.BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(MultiTermQuery.BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      if (possibleMatch!=null) {\n                        numVariants++;\n                        totalVariantDocFreqs+=fe.docFreq();\n                        float score=boostAtt.getBoost();\n                        if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                          ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                          variantsQ.insertWithOverflow(st);\n                          minScore = variantsQ.top().score; // maintain minScore\n                        }\n                      }\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"33a8b1f99104f4144f210f5d068411c297cd7163","date":1287152748,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  AttributeSource atts = new AttributeSource();\n                  MultiTermQuery.MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MultiTermQuery.MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(reader, atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  MultiTermQuery.BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(MultiTermQuery.BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      if (possibleMatch!=null) {\n                        numVariants++;\n                        totalVariantDocFreqs+=fe.docFreq();\n                        float score=boostAtt.getBoost();\n                        if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                          ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                          variantsQ.insertWithOverflow(st);\n                          minScore = variantsQ.top().score; // maintain minScore\n                        }\n                        maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                      }\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(reader, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  MultiTermQuery.BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(MultiTermQuery.BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      if (possibleMatch!=null) {\n                        numVariants++;\n                        totalVariantDocFreqs+=fe.docFreq();\n                        float score=boostAtt.getBoost();\n                        if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                          ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                          variantsQ.insertWithOverflow(st);\n                          minScore = variantsQ.top().score; // maintain minScore\n                        }\n                        boostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                      }\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4ecea1664e8617d82eca3b8055a3c37cb4da8511","date":1287578668,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  AttributeSource atts = new AttributeSource();\n                  MultiTermQuery.MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MultiTermQuery.MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(reader, atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  MultiTermQuery.BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(MultiTermQuery.BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      if (possibleMatch!=null) {\n                        numVariants++;\n                        totalVariantDocFreqs+=fe.docFreq();\n                        float score=boostAtt.getBoost();\n                        if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                          ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                          variantsQ.insertWithOverflow(st);\n                          minScore = variantsQ.top().score; // maintain minScore\n                        }\n                        maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                      }\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(reader, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  MultiTermQuery.BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(MultiTermQuery.BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      if (possibleMatch!=null) {\n                        numVariants++;\n                        totalVariantDocFreqs+=fe.docFreq();\n                        float score=boostAtt.getBoost();\n                        if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                          ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                          variantsQ.insertWithOverflow(st);\n                          minScore = variantsQ.top().score; // maintain minScore\n                        }\n                        boostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                      }\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"744486748bc5bee772100e49230e5bca39bac99a","date":1289776426,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(reader, atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      if (possibleMatch!=null) {\n                        numVariants++;\n                        totalVariantDocFreqs+=fe.docFreq();\n                        float score=boostAtt.getBoost();\n                        if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                          ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                          variantsQ.insertWithOverflow(st);\n                          minScore = variantsQ.top().score; // maintain minScore\n                        }\n                        maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                      }\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  AttributeSource atts = new AttributeSource();\n                  MultiTermQuery.MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MultiTermQuery.MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(reader, atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  MultiTermQuery.BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(MultiTermQuery.BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      if (possibleMatch!=null) {\n                        numVariants++;\n                        totalVariantDocFreqs+=fe.docFreq();\n                        float score=boostAtt.getBoost();\n                        if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                          ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                          variantsQ.insertWithOverflow(st);\n                          minScore = variantsQ.top().score; // maintain minScore\n                        }\n                        maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                      }\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9ab1f5591dc05f1f2b5407d809c9699f75554a32","date":1290008586,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(reader, atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      if (possibleMatch!=null) {\n                        numVariants++;\n                        totalVariantDocFreqs+=fe.docFreq();\n                        float score=boostAtt.getBoost();\n                        if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                          ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                          variantsQ.insertWithOverflow(st);\n                          minScore = variantsQ.top().score; // maintain minScore\n                        }\n                        maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                      }\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  AttributeSource atts = new AttributeSource();\n                  MultiTermQuery.MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MultiTermQuery.MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(reader, atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  MultiTermQuery.BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(MultiTermQuery.BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      if (possibleMatch!=null) {\n                        numVariants++;\n                        totalVariantDocFreqs+=fe.docFreq();\n                        float score=boostAtt.getBoost();\n                        if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                          ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                          variantsQ.insertWithOverflow(st);\n                          minScore = variantsQ.top().score; // maintain minScore\n                        }\n                        maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                      }\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e8dd2a69747e9f2922fd8b6970bd1661b26a692d","date":1291080135,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()).iterator(), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      if (possibleMatch!=null) {\n                        numVariants++;\n                        totalVariantDocFreqs+=fe.docFreq();\n                        float score=boostAtt.getBoost();\n                        if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                          ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                          variantsQ.insertWithOverflow(st);\n                          minScore = variantsQ.top().score; // maintain minScore\n                        }\n                        maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                      }\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(reader, atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      if (possibleMatch!=null) {\n                        numVariants++;\n                        totalVariantDocFreqs+=fe.docFreq();\n                        float score=boostAtt.getBoost();\n                        if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                          ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                          variantsQ.insertWithOverflow(st);\n                          minScore = variantsQ.top().score; // maintain minScore\n                        }\n                        maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                      }\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3bb13258feba31ab676502787ab2e1779f129b7a","date":1291596436,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()).iterator(), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      if (possibleMatch!=null) {\n                        numVariants++;\n                        totalVariantDocFreqs+=fe.docFreq();\n                        float score=boostAtt.getBoost();\n                        if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                          ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                          variantsQ.insertWithOverflow(st);\n                          minScore = variantsQ.top().score; // maintain minScore\n                        }\n                        maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                      }\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(reader, atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      if (possibleMatch!=null) {\n                        numVariants++;\n                        totalVariantDocFreqs+=fe.docFreq();\n                        float score=boostAtt.getBoost();\n                        if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                          ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                          variantsQ.insertWithOverflow(st);\n                          minScore = variantsQ.top().score; // maintain minScore\n                        }\n                        maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                      }\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()).iterator(), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      if (possibleMatch!=null) {\n                        numVariants++;\n                        totalVariantDocFreqs+=fe.docFreq();\n                        float score=boostAtt.getBoost();\n                        if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                          ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                          variantsQ.insertWithOverflow(st);\n                          minScore = variantsQ.top().score; // maintain minScore\n                        }\n                        maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                      }\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(reader, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  MultiTermQuery.BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(MultiTermQuery.BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      if (possibleMatch!=null) {\n                        numVariants++;\n                        totalVariantDocFreqs+=fe.docFreq();\n                        float score=boostAtt.getBoost();\n                        if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                          ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                          variantsQ.insertWithOverflow(st);\n                          minScore = variantsQ.top().score; // maintain minScore\n                        }\n                      }\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a711c18e3ed869a2d918559dad45c2d25e758da9","date":1304708259,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()).iterator(), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()).iterator(), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      if (possibleMatch!=null) {\n                        numVariants++;\n                        totalVariantDocFreqs+=fe.docFreq();\n                        float score=boostAtt.getBoost();\n                        if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                          ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                          variantsQ.insertWithOverflow(st);\n                          minScore = variantsQ.top().score; // maintain minScore\n                        }\n                        maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                      }\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4","date":1305207152,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()).iterator(), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()).iterator(), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","bugFix":null,"bugIntro":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c700f8d0842d3e52bb2bdfbfdc046a137e836edb","date":1305285499,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()).iterator(), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()).iterator(), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      if (possibleMatch!=null) {\n                        numVariants++;\n                        totalVariantDocFreqs+=fe.docFreq();\n                        float score=boostAtt.getBoost();\n                        if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                          ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                          variantsQ.insertWithOverflow(st);\n                          minScore = variantsQ.top().score; // maintain minScore\n                        }\n                        maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                      }\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"00746ad002a54281629e3b6f3eb39833a33f093e","date":1305306799,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.reusableTokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()).iterator(), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()).iterator(), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c3a8a449466c1ff7ce2274fe73dab487256964b4","date":1305735867,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.reusableTokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()).iterator(), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()).iterator(), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.reusableTokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()).iterator(), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()).iterator(), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      if (possibleMatch!=null) {\n                        numVariants++;\n                        totalVariantDocFreqs+=fe.docFreq();\n                        float score=boostAtt.getBoost();\n                        if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                          ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                          variantsQ.insertWithOverflow(st);\n                          minScore = variantsQ.top().score; // maintain minScore\n                        }\n                        maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                      }\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fafef7c83fe8e0b3ca9298d5d75d6b943dc28153","date":1308670974,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.reusableTokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=new Term(f.fieldName, term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()).iterator(), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.reusableTokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()).iterator(), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2553b00f699380c64959ccb27991289aae87be2e","date":1309290151,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.reusableTokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=new Term(f.fieldName, term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()).iterator(), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.reusableTokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()).iterator(), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.reusableTokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=new Term(f.fieldName, term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()).iterator(), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.reusableTokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        Term internSavingTemplateTerm =new Term(f.fieldName); //optimization to avoid constructing new Term() objects\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=internSavingTemplateTerm.createTerm(term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()).iterator(), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f9fdc0777b84633cc8cfa8995ff5b0d411e4515b","date":1313816278,"type":5,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","pathOld":"lucene/contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.reusableTokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=new Term(f.fieldName, term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()).iterator(), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","sourceOld":"    private void addTerms(IndexReader reader,FieldVals f) throws IOException\n    {\n        if(f.queryString==null) return;\n        TokenStream ts=analyzer.reusableTokenStream(f.fieldName,new StringReader(f.queryString));\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        \n        int corpusNumDocs=reader.numDocs();\n        HashSet<String> processedTerms=new HashSet<String>();\n        ts.reset();\n        while (ts.incrementToken()) \n        {\n                String term = termAtt.toString();\n        \tif(!processedTerms.contains(term))\n        \t{\n                  processedTerms.add(term);\n                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n                  float minScore=0;\n                  Term startTerm=new Term(f.fieldName, term);\n                  AttributeSource atts = new AttributeSource();\n                  MaxNonCompetitiveBoostAttribute maxBoostAtt =\n                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n                  FuzzyTermsEnum fe = new FuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()).iterator(), atts, startTerm, f.minSimilarity, f.prefixLength);\n                  //store the df so all variants use same idf\n                  int df = reader.docFreq(startTerm);\n                  int numVariants=0;\n                  int totalVariantDocFreqs=0;\n                  BytesRef possibleMatch;\n                  BoostAttribute boostAtt =\n                    fe.attributes().addAttribute(BoostAttribute.class);\n                  while ((possibleMatch = fe.next()) != null) {\n                      numVariants++;\n                      totalVariantDocFreqs+=fe.docFreq();\n                      float score=boostAtt.getBoost();\n                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){\n                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), new BytesRef(possibleMatch)),score,startTerm);                    \n                        variantsQ.insertWithOverflow(st);\n                        minScore = variantsQ.top().score; // maintain minScore\n                      }\n                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n                    }\n\n                  if(numVariants>0)\n                    {\n                      int avgDf=totalVariantDocFreqs/numVariants;\n                      if(df==0)//no direct match we can use as df for all variants \n\t                {\n\t                    df=avgDf; //use avg df of all variants\n\t                }\n\t                \n                    // take the top variants (scored by edit distance) and reset the score\n                    // to include an IDF factor then add to the global queue for ranking \n                    // overall top query terms\n                    int size = variantsQ.size();\n                    for(int i = 0; i < size; i++)\n\t                {\n\t                  ScoreTerm st = variantsQ.pop();\n\t                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);\n\t                  q.insertWithOverflow(st);\n\t                }                            \n                }\n        \t}\n        }\n        ts.end();\n        ts.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"fafef7c83fe8e0b3ca9298d5d75d6b943dc28153":["00746ad002a54281629e3b6f3eb39833a33f093e"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["5f4e87790277826a2aea119328600dfb07761f32","e8dd2a69747e9f2922fd8b6970bd1661b26a692d"],"e744dea5151ae430f7b84db4a2beb3eba003a114":["4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["3bb13258feba31ab676502787ab2e1779f129b7a","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"744486748bc5bee772100e49230e5bca39bac99a":["33a8b1f99104f4144f210f5d068411c297cd7163"],"a7347509fad0711ac30cb15a746e9a3830a38ebd":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4f29ba80b723649f5feb7e37afe1a558dd2c1304":["a7347509fad0711ac30cb15a746e9a3830a38ebd"],"9ab1f5591dc05f1f2b5407d809c9699f75554a32":["4ecea1664e8617d82eca3b8055a3c37cb4da8511","744486748bc5bee772100e49230e5bca39bac99a"],"a711c18e3ed869a2d918559dad45c2d25e758da9":["e8dd2a69747e9f2922fd8b6970bd1661b26a692d"],"5f4e87790277826a2aea119328600dfb07761f32":["a7347509fad0711ac30cb15a746e9a3830a38ebd","4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"2553b00f699380c64959ccb27991289aae87be2e":["a3776dccca01c11e7046323cfad46a3b4a471233","fafef7c83fe8e0b3ca9298d5d75d6b943dc28153"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["00746ad002a54281629e3b6f3eb39833a33f093e","fafef7c83fe8e0b3ca9298d5d75d6b943dc28153"],"33a8b1f99104f4144f210f5d068411c297cd7163":["e744dea5151ae430f7b84db4a2beb3eba003a114"],"f9fdc0777b84633cc8cfa8995ff5b0d411e4515b":["fafef7c83fe8e0b3ca9298d5d75d6b943dc28153"],"c3a8a449466c1ff7ce2274fe73dab487256964b4":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb","00746ad002a54281629e3b6f3eb39833a33f093e"],"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4":["a711c18e3ed869a2d918559dad45c2d25e758da9"],"a3776dccca01c11e7046323cfad46a3b4a471233":["e8dd2a69747e9f2922fd8b6970bd1661b26a692d","00746ad002a54281629e3b6f3eb39833a33f093e"],"00746ad002a54281629e3b6f3eb39833a33f093e":["e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"4ecea1664e8617d82eca3b8055a3c37cb4da8511":["e744dea5151ae430f7b84db4a2beb3eba003a114","33a8b1f99104f4144f210f5d068411c297cd7163"],"e8dd2a69747e9f2922fd8b6970bd1661b26a692d":["744486748bc5bee772100e49230e5bca39bac99a"],"3bb13258feba31ab676502787ab2e1779f129b7a":["9ab1f5591dc05f1f2b5407d809c9699f75554a32","e8dd2a69747e9f2922fd8b6970bd1661b26a692d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["f9fdc0777b84633cc8cfa8995ff5b0d411e4515b"]},"commit2Childs":{"fafef7c83fe8e0b3ca9298d5d75d6b943dc28153":["2553b00f699380c64959ccb27991289aae87be2e","d083e83f225b11e5fdd900e83d26ddb385b6955c","f9fdc0777b84633cc8cfa8995ff5b0d411e4515b"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["a7347509fad0711ac30cb15a746e9a3830a38ebd"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":[],"e744dea5151ae430f7b84db4a2beb3eba003a114":["33a8b1f99104f4144f210f5d068411c297cd7163","4ecea1664e8617d82eca3b8055a3c37cb4da8511"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["c3a8a449466c1ff7ce2274fe73dab487256964b4"],"744486748bc5bee772100e49230e5bca39bac99a":["9ab1f5591dc05f1f2b5407d809c9699f75554a32","e8dd2a69747e9f2922fd8b6970bd1661b26a692d"],"a7347509fad0711ac30cb15a746e9a3830a38ebd":["4f29ba80b723649f5feb7e37afe1a558dd2c1304","5f4e87790277826a2aea119328600dfb07761f32"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"4f29ba80b723649f5feb7e37afe1a558dd2c1304":["e744dea5151ae430f7b84db4a2beb3eba003a114","5f4e87790277826a2aea119328600dfb07761f32"],"9ab1f5591dc05f1f2b5407d809c9699f75554a32":["3bb13258feba31ab676502787ab2e1779f129b7a"],"a711c18e3ed869a2d918559dad45c2d25e758da9":["e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"5f4e87790277826a2aea119328600dfb07761f32":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"2553b00f699380c64959ccb27991289aae87be2e":[],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"33a8b1f99104f4144f210f5d068411c297cd7163":["744486748bc5bee772100e49230e5bca39bac99a","4ecea1664e8617d82eca3b8055a3c37cb4da8511"],"f9fdc0777b84633cc8cfa8995ff5b0d411e4515b":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c3a8a449466c1ff7ce2274fe73dab487256964b4":[],"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb","00746ad002a54281629e3b6f3eb39833a33f093e"],"a3776dccca01c11e7046323cfad46a3b4a471233":["2553b00f699380c64959ccb27991289aae87be2e"],"00746ad002a54281629e3b6f3eb39833a33f093e":["fafef7c83fe8e0b3ca9298d5d75d6b943dc28153","d083e83f225b11e5fdd900e83d26ddb385b6955c","c3a8a449466c1ff7ce2274fe73dab487256964b4","a3776dccca01c11e7046323cfad46a3b4a471233"],"4ecea1664e8617d82eca3b8055a3c37cb4da8511":["9ab1f5591dc05f1f2b5407d809c9699f75554a32"],"e8dd2a69747e9f2922fd8b6970bd1661b26a692d":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","a711c18e3ed869a2d918559dad45c2d25e758da9","a3776dccca01c11e7046323cfad46a3b4a471233","3bb13258feba31ab676502787ab2e1779f129b7a"],"3bb13258feba31ab676502787ab2e1779f129b7a":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","2553b00f699380c64959ccb27991289aae87be2e","d083e83f225b11e5fdd900e83d26ddb385b6955c","c3a8a449466c1ff7ce2274fe73dab487256964b4","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}