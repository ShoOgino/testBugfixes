{"path":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#doTest().mjava","commits":[{"id":"f13b9d4c228e77327b284419c8cafd16913a7a19","date":1405437837,"type":0,"author":"Steven Rowe","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#doTest().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    setupHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrServer().getZkStateReader().getClusterState()\n      .getActiveSlices(\"collection1\").size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrServer ss = clients.get(random().nextInt(clients.size() + 1));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      ss.add(docs);\n    }\n    ss.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    for (RestTestHarness client : restTestHarnesses) {\n      String request = \"/schema/fields?wt=xml\";\n      String response = client.query(request);\n      String result = BaseTestHarness.validateXPath(response, expectedFields);\n      if (result != null) {\n        String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n        log.error(msg);\n        fail(msg);\n      }\n    }\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        ss.add(docs);\n        ss.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["344b0840364d990b29b97467bfcc766ff8325d11"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"39d4fde6822d0be5a5a6cc14de37a614181dbd82","date":1405486956,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    setupHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrServer().getZkStateReader().getClusterState()\n      .getActiveSlices(\"collection1\").size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrServer ss = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      ss.add(docs);\n    }\n    ss.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    for (RestTestHarness client : restTestHarnesses) {\n      String request = \"/schema/fields?wt=xml\";\n      String response = client.query(request);\n      String result = BaseTestHarness.validateXPath(response, expectedFields);\n      if (result != null) {\n        String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n        log.error(msg);\n        fail(msg);\n      }\n    }\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        ss.add(docs);\n        ss.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    setupHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrServer().getZkStateReader().getClusterState()\n      .getActiveSlices(\"collection1\").size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrServer ss = clients.get(random().nextInt(clients.size() + 1));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      ss.add(docs);\n    }\n    ss.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    for (RestTestHarness client : restTestHarnesses) {\n      String request = \"/schema/fields?wt=xml\";\n      String response = client.query(request);\n      String result = BaseTestHarness.validateXPath(response, expectedFields);\n      if (result != null) {\n        String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n        log.error(msg);\n        fail(msg);\n      }\n    }\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        ss.add(docs);\n        ss.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bafca15d8e408346a67f4282ad1143b88023893b","date":1420034748,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    setupHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getActiveSlices(\"collection1\").size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    for (RestTestHarness client : restTestHarnesses) {\n      String request = \"/schema/fields?wt=xml\";\n      String response = client.query(request);\n      String result = BaseTestHarness.validateXPath(response, expectedFields);\n      if (result != null) {\n        String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n        log.error(msg);\n        fail(msg);\n      }\n    }\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        randomClient.add(docs);\n        randomClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    setupHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrServer().getZkStateReader().getClusterState()\n      .getActiveSlices(\"collection1\").size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrServer ss = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      ss.add(docs);\n    }\n    ss.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    for (RestTestHarness client : restTestHarnesses) {\n      String request = \"/schema/fields?wt=xml\";\n      String response = client.query(request);\n      String result = BaseTestHarness.validateXPath(response, expectedFields);\n      if (result != null) {\n        String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n        log.error(msg);\n        fail(msg);\n      }\n    }\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        ss.add(docs);\n        ss.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"abb23fcc2461782ab204e61213240feb77d355aa","date":1422029612,"type":5,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#doTest().mjava","sourceNew":"  @Test\n  @ShardsFixed(num = 8)\n  public void test() throws Exception {\n    setupHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getActiveSlices(\"collection1\").size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    for (RestTestHarness client : restTestHarnesses) {\n      String request = \"/schema/fields?wt=xml\";\n      String response = client.query(request);\n      String result = BaseTestHarness.validateXPath(response, expectedFields);\n      if (result != null) {\n        String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n        log.error(msg);\n        fail(msg);\n      }\n    }\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        randomClient.add(docs);\n        randomClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    setupHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getActiveSlices(\"collection1\").size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    for (RestTestHarness client : restTestHarnesses) {\n      String request = \"/schema/fields?wt=xml\";\n      String response = client.query(request);\n      String result = BaseTestHarness.validateXPath(response, expectedFields);\n      if (result != null) {\n        String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n        log.error(msg);\n        fail(msg);\n      }\n    }\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        randomClient.add(docs);\n        randomClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"abb23fcc2461782ab204e61213240feb77d355aa":["bafca15d8e408346a67f4282ad1143b88023893b"],"39d4fde6822d0be5a5a6cc14de37a614181dbd82":["f13b9d4c228e77327b284419c8cafd16913a7a19"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"bafca15d8e408346a67f4282ad1143b88023893b":["39d4fde6822d0be5a5a6cc14de37a614181dbd82"],"f13b9d4c228e77327b284419c8cafd16913a7a19":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["abb23fcc2461782ab204e61213240feb77d355aa"]},"commit2Childs":{"abb23fcc2461782ab204e61213240feb77d355aa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"39d4fde6822d0be5a5a6cc14de37a614181dbd82":["bafca15d8e408346a67f4282ad1143b88023893b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["f13b9d4c228e77327b284419c8cafd16913a7a19"],"bafca15d8e408346a67f4282ad1143b88023893b":["abb23fcc2461782ab204e61213240feb77d355aa"],"f13b9d4c228e77327b284419c8cafd16913a7a19":["39d4fde6822d0be5a5a6cc14de37a614181dbd82"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}