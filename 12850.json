{"path":"lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer#createComponents(String,Reader).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer#createComponents(String,Reader).mjava","pathOld":"lucene/src/test-framework/java/org/apache/lucene/analysis/MockAnalyzer#createComponents(String,Reader).mjava","sourceNew":"  @Override\n  public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n    MockTokenizer tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);\n    tokenizer.setEnableChecks(enableChecks);\n    TokenFilter filt = new MockTokenFilter(tokenizer, filter, enablePositionIncrements);\n    return new TokenStreamComponents(tokenizer, maybePayload(filt, fieldName));\n  }\n\n","sourceOld":"  @Override\n  public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n    MockTokenizer tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);\n    tokenizer.setEnableChecks(enableChecks);\n    TokenFilter filt = new MockTokenFilter(tokenizer, filter, enablePositionIncrements);\n    return new TokenStreamComponents(tokenizer, maybePayload(filt, fieldName));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5eb3bae48a736f81ed96f4c26b6e5e7e18504187","date":1332294742,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer#createComponents(String,Reader).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer#createComponents(String,Reader).mjava","sourceNew":"  @Override\n  public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n    MockTokenizer tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase, maxTokenLength);\n    tokenizer.setEnableChecks(enableChecks);\n    TokenFilter filt = new MockTokenFilter(tokenizer, filter, enablePositionIncrements);\n    return new TokenStreamComponents(tokenizer, maybePayload(filt, fieldName));\n  }\n\n","sourceOld":"  @Override\n  public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n    MockTokenizer tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);\n    tokenizer.setEnableChecks(enableChecks);\n    TokenFilter filt = new MockTokenFilter(tokenizer, filter, enablePositionIncrements);\n    return new TokenStreamComponents(tokenizer, maybePayload(filt, fieldName));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5a92b21feea3b1b4d7ad5a06439333c4f757318f","date":1333977928,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer#createComponents(String,Reader).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer#createComponents(String,Reader).mjava","sourceNew":"  @Override\n  public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n    MockTokenizer tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase, maxTokenLength);\n    tokenizer.setEnableChecks(enableChecks);\n    MockTokenFilter filt = new MockTokenFilter(tokenizer, filter);\n    filt.setEnablePositionIncrements(enablePositionIncrements);\n    return new TokenStreamComponents(tokenizer, maybePayload(filt, fieldName));\n  }\n\n","sourceOld":"  @Override\n  public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n    MockTokenizer tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase, maxTokenLength);\n    tokenizer.setEnableChecks(enableChecks);\n    TokenFilter filt = new MockTokenFilter(tokenizer, filter, enablePositionIncrements);\n    return new TokenStreamComponents(tokenizer, maybePayload(filt, fieldName));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e","date":1334174049,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer#createComponents(String,Reader).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer#createComponents(String,Reader).mjava","sourceNew":"  @Override\n  public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n    MockTokenizer tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase, maxTokenLength);\n    tokenizer.setEnableChecks(enableChecks);\n    MockTokenFilter filt = new MockTokenFilter(tokenizer, filter);\n    filt.setEnablePositionIncrements(enablePositionIncrements);\n    return new TokenStreamComponents(tokenizer, maybePayload(filt, fieldName));\n  }\n\n","sourceOld":"  @Override\n  public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n    MockTokenizer tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase, maxTokenLength);\n    tokenizer.setEnableChecks(enableChecks);\n    TokenFilter filt = new MockTokenFilter(tokenizer, filter, enablePositionIncrements);\n    return new TokenStreamComponents(tokenizer, maybePayload(filt, fieldName));\n  }\n\n","bugFix":["53ae89cd75b0acbdfb8890710c6742f3fb80e65d"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eafa8c5eabc3dacd34680054e6a33bda024080ac","date":1367691488,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer#createComponents(String,Reader).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer#createComponents(String,Reader).mjava","sourceNew":"  @Override\n  public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n    MockTokenizer tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase, maxTokenLength);\n    tokenizer.setEnableChecks(enableChecks);\n    MockTokenFilter filt = new MockTokenFilter(tokenizer, filter);\n    return new TokenStreamComponents(tokenizer, maybePayload(filt, fieldName));\n  }\n\n","sourceOld":"  @Override\n  public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n    MockTokenizer tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase, maxTokenLength);\n    tokenizer.setEnableChecks(enableChecks);\n    MockTokenFilter filt = new MockTokenFilter(tokenizer, filter);\n    filt.setEnablePositionIncrements(enablePositionIncrements);\n    return new TokenStreamComponents(tokenizer, maybePayload(filt, fieldName));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer#createComponents(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer#createComponents(String,Reader).mjava","sourceNew":"  @Override\n  public TokenStreamComponents createComponents(String fieldName) {\n    MockTokenizer tokenizer = new MockTokenizer(runAutomaton, lowerCase, maxTokenLength);\n    tokenizer.setEnableChecks(enableChecks);\n    MockTokenFilter filt = new MockTokenFilter(tokenizer, filter);\n    return new TokenStreamComponents(tokenizer, maybePayload(filt, fieldName));\n  }\n\n","sourceOld":"  @Override\n  public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n    MockTokenizer tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase, maxTokenLength);\n    tokenizer.setEnableChecks(enableChecks);\n    MockTokenFilter filt = new MockTokenFilter(tokenizer, filter);\n    return new TokenStreamComponents(tokenizer, maybePayload(filt, fieldName));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["eafa8c5eabc3dacd34680054e6a33bda024080ac"],"ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e":["5eb3bae48a736f81ed96f4c26b6e5e7e18504187","5a92b21feea3b1b4d7ad5a06439333c4f757318f"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"eafa8c5eabc3dacd34680054e6a33bda024080ac":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"5eb3bae48a736f81ed96f4c26b6e5e7e18504187":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"5a92b21feea3b1b4d7ad5a06439333c4f757318f":["5eb3bae48a736f81ed96f4c26b6e5e7e18504187"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"]},"commit2Childs":{"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e":["eafa8c5eabc3dacd34680054e6a33bda024080ac"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["5eb3bae48a736f81ed96f4c26b6e5e7e18504187"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"eafa8c5eabc3dacd34680054e6a33bda024080ac":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"5eb3bae48a736f81ed96f4c26b6e5e7e18504187":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e","5a92b21feea3b1b4d7ad5a06439333c4f757318f"],"5a92b21feea3b1b4d7ad5a06439333c4f757318f":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}