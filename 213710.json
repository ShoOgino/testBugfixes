{"path":"contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map,String).mjava","commits":[{"id":"d3c3c2404d1200c39220fa15054fae854db4e1ee","date":1140827958,"type":0,"author":"Mark Harwood","isMerge":false,"pathNew":"contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map,String).mjava","pathOld":"/dev/null","sourceNew":"\t/**\r\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\r\n\t * @param r a source of text to be tokenized\r\n\t * @param termFreqMap a Map of terms and their frequencies\r\n\t * @param fieldName Used by analyzer for any special per-field analysis\r\n\t */\r\n\tprivate void addTermFrequencies(Reader r, Map termFreqMap, String fieldName)\r\n\t\tthrows IOException\r\n\t{\r\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\r\n\t\t\torg.apache.lucene.analysis.Token token;\r\n\t\t\tint tokenCount=0;\r\n\t\t\twhile ((token = ts.next()) != null) { // for every token\r\n\t\t\t\tString word = token.termText();\r\n\t\t\t\ttokenCount++;\r\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\r\n\t\t\t\t{\r\n\t\t\t\t\tbreak;\r\n\t\t\t\t}\r\n\t\t\t\tif(isNoiseWord(word)){\r\n\t\t\t\t\tcontinue;\r\n\t\t\t\t}\r\n\t\t\t\t\r\n\t\t\t\t// increment frequency\r\n\t\t\t\tInt cnt = (Int) termFreqMap.get(word);\r\n\t\t\t\tif (cnt == null) {\r\n\t\t\t\t\ttermFreqMap.put(word, new Int());\r\n\t\t\t\t}\r\n\t\t\t\telse {\r\n\t\t\t\t\tcnt.x++;\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t}\r\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e2cb543b41c145f33390f460ee743d6693c9c6c","date":1219243087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map,String).mjava","pathOld":"contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map,String).mjava","sourceNew":"\t/**\r\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\r\n\t * @param r a source of text to be tokenized\r\n\t * @param termFreqMap a Map of terms and their frequencies\r\n\t * @param fieldName Used by analyzer for any special per-field analysis\r\n\t */\r\n\tprivate void addTermFrequencies(Reader r, Map termFreqMap, String fieldName)\r\n\t\tthrows IOException\r\n\t{\r\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\r\n\t\t\tint tokenCount=0;\r\n\t\t\t// for every token\r\n                        final Token reusableToken = new Token();\r\n\t\t\tfor (Token nextToken = ts.next(reusableToken); nextToken != null; nextToken = ts.next(reusableToken)) {\r\n\t\t\t\tString word = nextToken.term();\r\n\t\t\t\ttokenCount++;\r\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\r\n\t\t\t\t{\r\n\t\t\t\t\tbreak;\r\n\t\t\t\t}\r\n\t\t\t\tif(isNoiseWord(word)){\r\n\t\t\t\t\tcontinue;\r\n\t\t\t\t}\r\n\t\t\t\t\r\n\t\t\t\t// increment frequency\r\n\t\t\t\tInt cnt = (Int) termFreqMap.get(word);\r\n\t\t\t\tif (cnt == null) {\r\n\t\t\t\t\ttermFreqMap.put(word, new Int());\r\n\t\t\t\t}\r\n\t\t\t\telse {\r\n\t\t\t\t\tcnt.x++;\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t}\r\n\n","sourceOld":"\t/**\r\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\r\n\t * @param r a source of text to be tokenized\r\n\t * @param termFreqMap a Map of terms and their frequencies\r\n\t * @param fieldName Used by analyzer for any special per-field analysis\r\n\t */\r\n\tprivate void addTermFrequencies(Reader r, Map termFreqMap, String fieldName)\r\n\t\tthrows IOException\r\n\t{\r\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\r\n\t\t\torg.apache.lucene.analysis.Token token;\r\n\t\t\tint tokenCount=0;\r\n\t\t\twhile ((token = ts.next()) != null) { // for every token\r\n\t\t\t\tString word = token.termText();\r\n\t\t\t\ttokenCount++;\r\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\r\n\t\t\t\t{\r\n\t\t\t\t\tbreak;\r\n\t\t\t\t}\r\n\t\t\t\tif(isNoiseWord(word)){\r\n\t\t\t\t\tcontinue;\r\n\t\t\t\t}\r\n\t\t\t\t\r\n\t\t\t\t// increment frequency\r\n\t\t\t\tInt cnt = (Int) termFreqMap.get(word);\r\n\t\t\t\tif (cnt == null) {\r\n\t\t\t\t\ttermFreqMap.put(word, new Int());\r\n\t\t\t\t}\r\n\t\t\t\telse {\r\n\t\t\t\t\tcnt.x++;\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t}\r\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6a361a621b184d9b73c9c9a37323a9845b8f8260","date":1226370946,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map,String).mjava","pathOld":"contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map,String).mjava","sourceNew":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n                        final Token reusableToken = new Token();\n\t\t\tfor (Token nextToken = ts.next(reusableToken); nextToken != null; nextToken = ts.next(reusableToken)) {\n\t\t\t\tString word = nextToken.term();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = (Int) termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t}\n\n","sourceOld":"\t/**\r\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\r\n\t * @param r a source of text to be tokenized\r\n\t * @param termFreqMap a Map of terms and their frequencies\r\n\t * @param fieldName Used by analyzer for any special per-field analysis\r\n\t */\r\n\tprivate void addTermFrequencies(Reader r, Map termFreqMap, String fieldName)\r\n\t\tthrows IOException\r\n\t{\r\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\r\n\t\t\tint tokenCount=0;\r\n\t\t\t// for every token\r\n                        final Token reusableToken = new Token();\r\n\t\t\tfor (Token nextToken = ts.next(reusableToken); nextToken != null; nextToken = ts.next(reusableToken)) {\r\n\t\t\t\tString word = nextToken.term();\r\n\t\t\t\ttokenCount++;\r\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\r\n\t\t\t\t{\r\n\t\t\t\t\tbreak;\r\n\t\t\t\t}\r\n\t\t\t\tif(isNoiseWord(word)){\r\n\t\t\t\t\tcontinue;\r\n\t\t\t\t}\r\n\t\t\t\t\r\n\t\t\t\t// increment frequency\r\n\t\t\t\tInt cnt = (Int) termFreqMap.get(word);\r\n\t\t\t\tif (cnt == null) {\r\n\t\t\t\t\ttermFreqMap.put(word, new Int());\r\n\t\t\t\t}\r\n\t\t\t\telse {\r\n\t\t\t\t\tcnt.x++;\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t}\r\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9b5756469957918cac40a831acec9cf01c8c2bb3","date":1249167152,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map,String).mjava","pathOld":"contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map,String).mjava","sourceNew":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n\t\t\tTermAttribute termAtt = (TermAttribute) ts.addAttribute(TermAttribute.class);\n\t\t\t\n\t\t\twhile (ts.incrementToken()) {\n\t\t\t\tString word = termAtt.term();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = (Int) termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t}\n\n","sourceOld":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n                        final Token reusableToken = new Token();\n\t\t\tfor (Token nextToken = ts.next(reusableToken); nextToken != null; nextToken = ts.next(reusableToken)) {\n\t\t\t\tString word = nextToken.term();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = (Int) termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8d78f014fded44fbde905f4f84cdc21907b371e8","date":1254383623,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map,String).mjava","pathOld":"contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map,String).mjava","sourceNew":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n\t\t\tTermAttribute termAtt = ts.addAttribute(TermAttribute.class);\n\t\t\t\n\t\t\twhile (ts.incrementToken()) {\n\t\t\t\tString word = termAtt.term();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = (Int) termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t}\n\n","sourceOld":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n\t\t\tTermAttribute termAtt = (TermAttribute) ts.addAttribute(TermAttribute.class);\n\t\t\t\n\t\t\twhile (ts.incrementToken()) {\n\t\t\t\tString word = termAtt.term();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = (Int) termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"60cdc0e643184821eb066795a8791cd82559f46e","date":1257941914,"type":5,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","pathOld":"contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map,String).mjava","sourceNew":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map<String,Int> termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n\t\t\tTermAttribute termAtt = ts.addAttribute(TermAttribute.class);\n\t\t\t\n\t\t\twhile (ts.incrementToken()) {\n\t\t\t\tString word = termAtt.term();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t}\n\n","sourceOld":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n\t\t\tTermAttribute termAtt = ts.addAttribute(TermAttribute.class);\n\t\t\t\n\t\t\twhile (ts.incrementToken()) {\n\t\t\t\tString word = termAtt.term();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = (Int) termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"60cdc0e643184821eb066795a8791cd82559f46e":["8d78f014fded44fbde905f4f84cdc21907b371e8"],"7e2cb543b41c145f33390f460ee743d6693c9c6c":["d3c3c2404d1200c39220fa15054fae854db4e1ee"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9b5756469957918cac40a831acec9cf01c8c2bb3":["6a361a621b184d9b73c9c9a37323a9845b8f8260"],"6a361a621b184d9b73c9c9a37323a9845b8f8260":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"d3c3c2404d1200c39220fa15054fae854db4e1ee":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"8d78f014fded44fbde905f4f84cdc21907b371e8":["9b5756469957918cac40a831acec9cf01c8c2bb3"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["60cdc0e643184821eb066795a8791cd82559f46e"]},"commit2Childs":{"60cdc0e643184821eb066795a8791cd82559f46e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"7e2cb543b41c145f33390f460ee743d6693c9c6c":["6a361a621b184d9b73c9c9a37323a9845b8f8260"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d3c3c2404d1200c39220fa15054fae854db4e1ee"],"9b5756469957918cac40a831acec9cf01c8c2bb3":["8d78f014fded44fbde905f4f84cdc21907b371e8"],"d3c3c2404d1200c39220fa15054fae854db4e1ee":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"6a361a621b184d9b73c9c9a37323a9845b8f8260":["9b5756469957918cac40a831acec9cf01c8c2bb3"],"8d78f014fded44fbde905f4f84cdc21907b371e8":["60cdc0e643184821eb066795a8791cd82559f46e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}