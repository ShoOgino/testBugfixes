{"path":"contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","commits":[{"id":"60cdc0e643184821eb066795a8791cd82559f46e","date":1257941914,"type":1,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","pathOld":"contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map,String).mjava","sourceNew":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map<String,Int> termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n\t\t\tTermAttribute termAtt = ts.addAttribute(TermAttribute.class);\n\t\t\t\n\t\t\twhile (ts.incrementToken()) {\n\t\t\t\tString word = termAtt.term();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t}\n\n","sourceOld":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n\t\t\tTermAttribute termAtt = ts.addAttribute(TermAttribute.class);\n\t\t\t\n\t\t\twhile (ts.incrementToken()) {\n\t\t\t\tString word = termAtt.term();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = (Int) termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","pathOld":"contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","sourceNew":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map<String,Int> termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n\t\t\tTermAttribute termAtt = ts.addAttribute(TermAttribute.class);\n\t\t\t\n\t\t\twhile (ts.incrementToken()) {\n\t\t\t\tString word = termAtt.term();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t}\n\n","sourceOld":"\t/**\n\t * Adds term frequencies found by tokenizing text from reader into the Map words\n\t * @param r a source of text to be tokenized\n\t * @param termFreqMap a Map of terms and their frequencies\n\t * @param fieldName Used by analyzer for any special per-field analysis\n\t */\n\tprivate void addTermFrequencies(Reader r, Map<String,Int> termFreqMap, String fieldName)\n\t\tthrows IOException\n\t{\n\t\t   TokenStream ts = analyzer.tokenStream(fieldName, r);\n\t\t\tint tokenCount=0;\n\t\t\t// for every token\n\t\t\tTermAttribute termAtt = ts.addAttribute(TermAttribute.class);\n\t\t\t\n\t\t\twhile (ts.incrementToken()) {\n\t\t\t\tString word = termAtt.term();\n\t\t\t\ttokenCount++;\n\t\t\t\tif(tokenCount>maxNumTokensParsed)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif(isNoiseWord(word)){\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t// increment frequency\n\t\t\t\tInt cnt = termFreqMap.get(word);\n\t\t\t\tif (cnt == null) {\n\t\t\t\t\ttermFreqMap.put(word, new Int());\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tcnt.x++;\n\t\t\t\t}\n\t\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"60cdc0e643184821eb066795a8791cd82559f46e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["60cdc0e643184821eb066795a8791cd82559f46e"]},"commit2Childs":{"60cdc0e643184821eb066795a8791cd82559f46e":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["60cdc0e643184821eb066795a8791cd82559f46e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}