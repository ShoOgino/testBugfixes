{"path":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[]).mjava","commits":[{"id":"0984ad47974c2d5d354519ddb2aa8358973a6271","date":1330868053,"type":0,"author":"Christian Moen","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[]).mjava","pathOld":"/dev/null","sourceNew":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[]) throws IOException {\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["c83d6c4335f31cae14f625a222bc842f20073dcd"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","date":1331075828,"type":0,"author":"Ryan McKinley","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[]).mjava","pathOld":"/dev/null","sourceNew":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[]) throws IOException {\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c83d6c4335f31cae14f625a222bc842f20073dcd","date":1373306148,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[]).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[]).mjava","sourceNew":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[]) throws IOException {\n    assertTokenStreamContents(a.tokenStream(\"dummy\", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length());\n  }\n\n","sourceOld":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[]) throws IOException {\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length());\n  }\n\n","bugFix":["0984ad47974c2d5d354519ddb2aa8358973a6271"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[]).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[]).mjava","sourceNew":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[]) throws IOException {\n    assertTokenStreamContents(a.tokenStream(\"dummy\", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length());\n  }\n\n","sourceOld":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[]) throws IOException {\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"df1b735b811bfe6055a98336ee8dfd1e43cf2dc0","date":1379858263,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[]).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[]).mjava","sourceNew":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[]) throws IOException {\n    checkResetException(a, input);\n    assertTokenStreamContents(a.tokenStream(\"dummy\", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length());\n  }\n\n","sourceOld":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[]) throws IOException {\n    assertTokenStreamContents(a.tokenStream(\"dummy\", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"24a98f5fdd23e04f85819dbc63b47a12f7c44311","date":1482439157,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[]).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[]).mjava","sourceNew":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[]) throws IOException {\n    checkResetException(a, input);\n    checkAnalysisConsistency(random(), a, true, input);\n    assertTokenStreamContents(a.tokenStream(\"dummy\", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length());\n  }\n\n","sourceOld":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[]) throws IOException {\n    checkResetException(a, input);\n    assertTokenStreamContents(a.tokenStream(\"dummy\", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f03e4bed5023ec3ef93a771b8888cae991cf448d","date":1483469262,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[]).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[]).mjava","sourceNew":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[]) throws IOException {\n    checkResetException(a, input);\n    checkAnalysisConsistency(random(), a, true, input);\n    assertTokenStreamContents(a.tokenStream(\"dummy\", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length());\n  }\n\n","sourceOld":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[]) throws IOException {\n    checkResetException(a, input);\n    assertTokenStreamContents(a.tokenStream(\"dummy\", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1c85bcc0cb48e35688c792a172bed271a9836d6b","date":1571776257,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[]).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[]).mjava","sourceNew":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[]) throws IOException {\n    assertTokenStreamContents(a.tokenStream(\"dummy\", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length());\n    checkResetException(a, input);\n    checkAnalysisConsistency(random(), a, true, input);\n  }\n\n","sourceOld":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[]) throws IOException {\n    checkResetException(a, input);\n    checkAnalysisConsistency(random(), a, true, input);\n    assertTokenStreamContents(a.tokenStream(\"dummy\", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"df1b735b811bfe6055a98336ee8dfd1e43cf2dc0":["c83d6c4335f31cae14f625a222bc842f20073dcd"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["0984ad47974c2d5d354519ddb2aa8358973a6271","c83d6c4335f31cae14f625a222bc842f20073dcd"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","0984ad47974c2d5d354519ddb2aa8358973a6271"],"c83d6c4335f31cae14f625a222bc842f20073dcd":["0984ad47974c2d5d354519ddb2aa8358973a6271"],"0984ad47974c2d5d354519ddb2aa8358973a6271":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"24a98f5fdd23e04f85819dbc63b47a12f7c44311":["df1b735b811bfe6055a98336ee8dfd1e43cf2dc0"],"1c85bcc0cb48e35688c792a172bed271a9836d6b":["24a98f5fdd23e04f85819dbc63b47a12f7c44311"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["1c85bcc0cb48e35688c792a172bed271a9836d6b"],"f03e4bed5023ec3ef93a771b8888cae991cf448d":["df1b735b811bfe6055a98336ee8dfd1e43cf2dc0","24a98f5fdd23e04f85819dbc63b47a12f7c44311"]},"commit2Childs":{"df1b735b811bfe6055a98336ee8dfd1e43cf2dc0":["24a98f5fdd23e04f85819dbc63b47a12f7c44311","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":[],"c83d6c4335f31cae14f625a222bc842f20073dcd":["df1b735b811bfe6055a98336ee8dfd1e43cf2dc0","37a0f60745e53927c4c876cfe5b5a58170f0646c"],"0984ad47974c2d5d354519ddb2aa8358973a6271":["37a0f60745e53927c4c876cfe5b5a58170f0646c","9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","c83d6c4335f31cae14f625a222bc842f20073dcd"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","0984ad47974c2d5d354519ddb2aa8358973a6271"],"24a98f5fdd23e04f85819dbc63b47a12f7c44311":["1c85bcc0cb48e35688c792a172bed271a9836d6b","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"1c85bcc0cb48e35688c792a172bed271a9836d6b":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"f03e4bed5023ec3ef93a771b8888cae991cf448d":[]},"heads":["37a0f60745e53927c4c876cfe5b5a58170f0646c","9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","cd5edd1f2b162a5cfa08efd17851a07373a96817","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}