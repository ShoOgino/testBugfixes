{"path":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random, new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.fnx\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 4, dir.listAll().length);\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random, new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.fnx\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 4, dir.listAll().length);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.fnx\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 4, dir.listAll().length);\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random, new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.fnx\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 4, dir.listAll().length);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4356000e349e38c9fb48034695b7c309abd54557","date":1337460341,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.si\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 5, dir.listAll().length);\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.fnx\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 4, dir.listAll().length);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"615ddbd81799980d0fdd95e0238e1c498b6f47b0","date":1338233290,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.si\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 5, dir.listAll().length);\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.fnx\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 4, dir.listAll().length);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","date":1338430031,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.si\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 5, dir.listAll().length);\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { IndexReader.open(dirs[0]), IndexReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.si\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 5, dir.listAll().length);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e7b103c0ed2ba7edf422d1ccb5489815dc6acb84","date":1345973500,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.si\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 5, dir.listAll().length);\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.si\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 5, dir.listAll().length);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"05a14b2611ead08655a2b2bdc61632eb31316e57","date":1346366621,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.si\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 5, dir.listAll().length);\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    lmp.setUseCompoundFile(true);\n    lmp.setNoCFSRatio(1.0); // Force creation of CFS\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.si\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 5, dir.listAll().length);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"088a7ef694fd43d5d9a4d200c4005865f773d1e7","date":1371136274,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.si\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 5, dir.listAll().length);\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.si\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 5, dir.listAll().length);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"19275ba31e621f6da1b83bf13af75233876fd3d4","date":1374846698,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.si\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 5, dir.listAll().length);\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.si\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 5, dir.listAll().length);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","date":1376366778,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.si\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 5, dir.listAll().length);\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.si\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 5, dir.listAll().length);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.shutdown();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.shutdown();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.si\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 5, dir.listAll().length);\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.si\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 5, dir.listAll().length);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.si\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 5, dir.listAll().length);\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.shutdown();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.shutdown();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.si\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 5, dir.listAll().length);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"98d2deb8c96c79ebef084a1f8e5a1a6c08608f13","date":1409346855,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    dir.setEnableVirusScanner(false); // we check for specific list of files\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.si\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 5, dir.listAll().length);\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    Directory dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.si\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 5, dir.listAll().length);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"46e4a8bdfbafda795ef9c39a2bc2d47095770299","date":1410411846,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    dir.setEnableVirusScanner(false); // we check for specific list of files\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // _Y.cfs,_Y.cfe, _Z.si\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 4, dir.listAll().length);\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    dir.setEnableVirusScanner(false); // we check for specific list of files\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // segments.gen,_Y.cfs,_Y.cfe, _Z.si\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 5, dir.listAll().length);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"41307b73b6c5ab4779490d54afb6393c80ba5a3b","date":1412433761,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    dir.setEnableVirusScanner(false); // we check for specific list of files\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // _Y.cfs,_Y.cfe, _Z.si\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(\"Only one compound segment should exist\", 1, sis.size());\n    assertTrue(sis.info(0).info.getUseCompoundFile());\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    dir.setEnableVirusScanner(false); // we check for specific list of files\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // _Y.cfs,_Y.cfe, _Z.si\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 4, dir.listAll().length);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9bb9a29a5e71a90295f175df8919802993142c9a","date":1412517673,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    dir.setEnableVirusScanner(false); // we check for specific list of files\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // _Y.cfs,_Y.cfe, _Z.si\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(\"Only one compound segment should exist\", 1, sis.size());\n    assertTrue(sis.info(0).info.getUseCompoundFile());\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    dir.setEnableVirusScanner(false); // we check for specific list of files\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // _Y.cfs,_Y.cfe, _Z.si\n    assertEquals(\"Only one compound segment should exist, but got: \" + Arrays.toString(dir.listAll()), 4, dir.listAll().length);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3384e6013a93e4d11b7d75388693f8d0388602bf","date":1413951663,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    dir.setEnableVirusScanner(false); // we check for specific list of files\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // _Y.cfs,_Y.cfe, _Z.si\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    assertEquals(\"Only one compound segment should exist\", 1, sis.size());\n    assertTrue(sis.info(0).info.getUseCompoundFile());\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    dir.setEnableVirusScanner(false); // we check for specific list of files\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // _Y.cfs,_Y.cfe, _Z.si\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(\"Only one compound segment should exist\", 1, sis.size());\n    assertTrue(sis.info(0).info.getUseCompoundFile());\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"db68c63cbfaa8698b9c4475f75ed2b9c9696d238","date":1414118621,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    dir.setEnableVirusScanner(false); // we check for specific list of files\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // _Y.cfs,_Y.cfe, _Z.si\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    assertEquals(\"Only one compound segment should exist\", 1, sis.size());\n    assertTrue(sis.info(0).info.getUseCompoundFile());\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    dir.setEnableVirusScanner(false); // we check for specific list of files\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // _Y.cfs,_Y.cfe, _Z.si\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(\"Only one compound segment should exist\", 1, sis.size());\n    assertTrue(sis.info(0).info.getUseCompoundFile());\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa","date":1420599177,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    DirectoryReader[] readers = new DirectoryReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    dir.setEnableVirusScanner(false); // we check for specific list of files\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    TestUtil.addIndexesSlowly(w3, readers);\n    w3.close();\n    // we should now see segments_X,\n    // _Y.cfs,_Y.cfe, _Z.si\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    assertEquals(\"Only one compound segment should exist\", 1, sis.size());\n    assertTrue(sis.info(0).info.getUseCompoundFile());\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    IndexReader[] readers = new IndexReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    dir.setEnableVirusScanner(false); // we check for specific list of files\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    w3.addIndexes(readers);\n    w3.close();\n    // we should now see segments_X,\n    // _Y.cfs,_Y.cfe, _Z.si\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    assertEquals(\"Only one compound segment should exist\", 1, sis.size());\n    assertTrue(sis.info(0).info.getUseCompoundFile());\n    dir.close();\n  }\n\n","bugFix":["38a62612cfa4e104080d89d7751a8f1a258ac335","e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6bfe104fc023fadc9e709f8d17403d2cc61133fe","date":1454446396,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    DirectoryReader[] readers = new DirectoryReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    TestUtil.addIndexesSlowly(w3, readers);\n    w3.close();\n    // we should now see segments_X,\n    // _Y.cfs,_Y.cfe, _Z.si\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    assertEquals(\"Only one compound segment should exist\", 1, sis.size());\n    assertTrue(sis.info(0).info.getUseCompoundFile());\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    DirectoryReader[] readers = new DirectoryReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    dir.setEnableVirusScanner(false); // we check for specific list of files\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    TestUtil.addIndexesSlowly(w3, readers);\n    w3.close();\n    // we should now see segments_X,\n    // _Y.cfs,_Y.cfe, _Z.si\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    assertEquals(\"Only one compound segment should exist\", 1, sis.size());\n    assertTrue(sis.info(0).info.getUseCompoundFile());\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b470f36a9372c97283360b1304eacbde22df6c0d","date":1454765175,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    DirectoryReader[] readers = new DirectoryReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    TestUtil.addIndexesSlowly(w3, readers);\n    w3.close();\n    // we should now see segments_X,\n    // _Y.cfs,_Y.cfe, _Z.si\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    assertEquals(\"Only one compound segment should exist\", 1, sis.size());\n    assertTrue(sis.info(0).info.getUseCompoundFile());\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    DirectoryReader[] readers = new DirectoryReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    dir.setEnableVirusScanner(false); // we check for specific list of files\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    TestUtil.addIndexesSlowly(w3, readers);\n    w3.close();\n    // we should now see segments_X,\n    // _Y.cfs,_Y.cfe, _Z.si\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    assertEquals(\"Only one compound segment should exist\", 1, sis.size());\n    assertTrue(sis.info(0).info.getUseCompoundFile());\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1e6acbaae7af722f17204ceccf0f7db5753eccf3","date":1454775255,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    DirectoryReader[] readers = new DirectoryReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    TestUtil.addIndexesSlowly(w3, readers);\n    w3.close();\n    // we should now see segments_X,\n    // _Y.cfs,_Y.cfe, _Z.si\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    assertEquals(\"Only one compound segment should exist\", 1, sis.size());\n    assertTrue(sis.info(0).info.getUseCompoundFile());\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    DirectoryReader[] readers = new DirectoryReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    dir.setEnableVirusScanner(false); // we check for specific list of files\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    TestUtil.addIndexesSlowly(w3, readers);\n    w3.close();\n    // we should now see segments_X,\n    // _Y.cfs,_Y.cfe, _Z.si\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    assertEquals(\"Only one compound segment should exist\", 1, sis.size());\n    assertTrue(sis.info(0).info.getUseCompoundFile());\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5a207d19eac354d649c3f0e2cce070017c78125e","date":1454776470,"type":3,"author":"Erick Erickson","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    DirectoryReader[] readers = new DirectoryReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    TestUtil.addIndexesSlowly(w3, readers);\n    w3.close();\n    // we should now see segments_X,\n    // _Y.cfs,_Y.cfe, _Z.si\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    assertEquals(\"Only one compound segment should exist\", 1, sis.size());\n    assertTrue(sis.info(0).info.getUseCompoundFile());\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    DirectoryReader[] readers = new DirectoryReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    dir.setEnableVirusScanner(false); // we check for specific list of files\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    TestUtil.addIndexesSlowly(w3, readers);\n    w3.close();\n    // we should now see segments_X,\n    // _Y.cfs,_Y.cfe, _Z.si\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    assertEquals(\"Only one compound segment should exist\", 1, sis.size());\n    assertTrue(sis.info(0).info.getUseCompoundFile());\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d77dafd89756a5161d244985903e3487ca109182","date":1548679743,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestAddIndexes#testNonCFSLeftovers().mjava","sourceNew":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new ByteBuffersDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    DirectoryReader[] readers = new DirectoryReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new ByteBuffersDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    TestUtil.addIndexesSlowly(w3, readers);\n    w3.close();\n    // we should now see segments_X,\n    // _Y.cfs,_Y.cfe, _Z.si\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    assertEquals(\"Only one compound segment should exist\", 1, sis.size());\n    assertTrue(sis.info(0).info.getUseCompoundFile());\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2790: tests that the non CFS files were deleted by addIndexes\n  public void testNonCFSLeftovers() throws Exception {\n    Directory[] dirs = new Directory[2];\n    for (int i = 0; i < dirs.length; i++) {\n      dirs[i] = new RAMDirectory();\n      IndexWriter w = new IndexWriter(dirs[i], new IndexWriterConfig(new MockAnalyzer(random())));\n      Document d = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      d.add(new Field(\"c\", \"v\", customType));\n      w.addDocument(d);\n      w.close();\n    }\n    \n    DirectoryReader[] readers = new DirectoryReader[] { DirectoryReader.open(dirs[0]), DirectoryReader.open(dirs[1]) };\n    \n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory());\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy(true));\n    MergePolicy lmp = conf.getMergePolicy();\n    // Force creation of CFS:\n    lmp.setNoCFSRatio(1.0);\n    lmp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    IndexWriter w3 = new IndexWriter(dir, conf);\n    TestUtil.addIndexesSlowly(w3, readers);\n    w3.close();\n    // we should now see segments_X,\n    // _Y.cfs,_Y.cfe, _Z.si\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    assertEquals(\"Only one compound segment should exist\", 1, sis.size());\n    assertTrue(sis.info(0).info.getUseCompoundFile());\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"41307b73b6c5ab4779490d54afb6393c80ba5a3b":["46e4a8bdfbafda795ef9c39a2bc2d47095770299"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"5a207d19eac354d649c3f0e2cce070017c78125e":["0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa","b470f36a9372c97283360b1304eacbde22df6c0d"],"98d2deb8c96c79ebef084a1f8e5a1a6c08608f13":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"19275ba31e621f6da1b83bf13af75233876fd3d4":["088a7ef694fd43d5d9a4d200c4005865f773d1e7"],"4356000e349e38c9fb48034695b7c309abd54557":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"9bb9a29a5e71a90295f175df8919802993142c9a":["46e4a8bdfbafda795ef9c39a2bc2d47095770299","41307b73b6c5ab4779490d54afb6393c80ba5a3b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["088a7ef694fd43d5d9a4d200c4005865f773d1e7"],"6bfe104fc023fadc9e709f8d17403d2cc61133fe":["0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["19275ba31e621f6da1b83bf13af75233876fd3d4"],"1e6acbaae7af722f17204ceccf0f7db5753eccf3":["0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa","b470f36a9372c97283360b1304eacbde22df6c0d"],"0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"d77dafd89756a5161d244985903e3487ca109182":["5a207d19eac354d649c3f0e2cce070017c78125e"],"3384e6013a93e4d11b7d75388693f8d0388602bf":["41307b73b6c5ab4779490d54afb6393c80ba5a3b"],"05a14b2611ead08655a2b2bdc61632eb31316e57":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","e7b103c0ed2ba7edf422d1ccb5489815dc6acb84"],"46e4a8bdfbafda795ef9c39a2bc2d47095770299":["98d2deb8c96c79ebef084a1f8e5a1a6c08608f13"],"088a7ef694fd43d5d9a4d200c4005865f773d1e7":["e7b103c0ed2ba7edf422d1ccb5489815dc6acb84"],"e7b103c0ed2ba7edf422d1ccb5489815dc6acb84":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["629c38c4ae4e303d0617e05fbfe508140b32f0a3","4356000e349e38c9fb48034695b7c309abd54557"],"b470f36a9372c97283360b1304eacbde22df6c0d":["0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa","6bfe104fc023fadc9e709f8d17403d2cc61133fe"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"db68c63cbfaa8698b9c4475f75ed2b9c9696d238":["9bb9a29a5e71a90295f175df8919802993142c9a","3384e6013a93e4d11b7d75388693f8d0388602bf"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d77dafd89756a5161d244985903e3487ca109182"]},"commit2Childs":{"41307b73b6c5ab4779490d54afb6393c80ba5a3b":["9bb9a29a5e71a90295f175df8919802993142c9a","3384e6013a93e4d11b7d75388693f8d0388602bf"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"5a207d19eac354d649c3f0e2cce070017c78125e":["d77dafd89756a5161d244985903e3487ca109182"],"98d2deb8c96c79ebef084a1f8e5a1a6c08608f13":["46e4a8bdfbafda795ef9c39a2bc2d47095770299"],"19275ba31e621f6da1b83bf13af75233876fd3d4":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"4356000e349e38c9fb48034695b7c309abd54557":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["05a14b2611ead08655a2b2bdc61632eb31316e57","e7b103c0ed2ba7edf422d1ccb5489815dc6acb84"],"9bb9a29a5e71a90295f175df8919802993142c9a":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":[],"6bfe104fc023fadc9e709f8d17403d2cc61133fe":["b470f36a9372c97283360b1304eacbde22df6c0d"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"1e6acbaae7af722f17204ceccf0f7db5753eccf3":[],"0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa":["5a207d19eac354d649c3f0e2cce070017c78125e","6bfe104fc023fadc9e709f8d17403d2cc61133fe","1e6acbaae7af722f17204ceccf0f7db5753eccf3","b470f36a9372c97283360b1304eacbde22df6c0d"],"d77dafd89756a5161d244985903e3487ca109182":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3384e6013a93e4d11b7d75388693f8d0388602bf":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"05a14b2611ead08655a2b2bdc61632eb31316e57":[],"46e4a8bdfbafda795ef9c39a2bc2d47095770299":["41307b73b6c5ab4779490d54afb6393c80ba5a3b","9bb9a29a5e71a90295f175df8919802993142c9a"],"088a7ef694fd43d5d9a4d200c4005865f773d1e7":["19275ba31e621f6da1b83bf13af75233876fd3d4","8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"e7b103c0ed2ba7edf422d1ccb5489815dc6acb84":["05a14b2611ead08655a2b2bdc61632eb31316e57","088a7ef694fd43d5d9a4d200c4005865f773d1e7"],"b470f36a9372c97283360b1304eacbde22df6c0d":["5a207d19eac354d649c3f0e2cce070017c78125e","1e6acbaae7af722f17204ceccf0f7db5753eccf3"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["98d2deb8c96c79ebef084a1f8e5a1a6c08608f13"],"db68c63cbfaa8698b9c4475f75ed2b9c9696d238":["0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["4356000e349e38c9fb48034695b7c309abd54557","615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","1e6acbaae7af722f17204ceccf0f7db5753eccf3","05a14b2611ead08655a2b2bdc61632eb31316e57","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}