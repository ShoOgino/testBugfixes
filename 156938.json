{"path":"sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/fetcher/FetcherMain#FetcherMain(int,String).mjava","commits":[{"id":"2b24dfe829236e25670a0fb6bd671abc4e2a91f9","date":1035299107,"type":1,"author":"cmarschner","isMerge":false,"pathNew":"sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/fetcher/FetcherMain#FetcherMain(int,String).mjava","pathOld":"sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/fetcher/FetcherMain#FetcherMain(int).mjava","sourceNew":"    /**\n     * initializes all classes and registers anonymous adapter classes as\n     * listeners for fetcher events.\n     *\n     * @param nrThreads  number of fetcher threads to be created\n     */\n    public FetcherMain(int nrThreads, String hostResolverFile) throws Exception\n    {\n        // to make things clear, this method is commented a bit better than\n        // the rest of the program...\n\n        // this is the main message queue. handlers are registered with\n        // the queue, and whenever a message is put in it, the message is passed to the\n        // filters in a \"chain of responibility\" manner. Every listener can decide\n        // to throw the message away\n        messageHandler = new MessageHandler();\n\n        // the storage is the class which saves a WebDocument somewhere, no\n        // matter how it does it, whether it's in a file, in a database or\n        // whatever\n\n        // example for the (very slow) SQL Server storage:\n        // this.storage = new SQLServerStorage(\"sun.jdbc.odbc.JdbcOdbcDriver\",\"jdbc:odbc:search\",\"sa\",\"...\",nrThreads);\n\n        // the LogStorage used here does extensive logging. It logs all links and\n        // document information.\n        // it also saves all documents to page files.\n        File logsDir = new File(\"logs\");\n        logsDir.mkdir();    // ensure log directory exists\n\n        // in this experimental implementation, the crawler is pretty verbose\n        // the SimpleLogger, however, is a FlyWeight logger which is buffered and\n        // not thread safe by default\n        SimpleLogger storeLog = new SimpleLogger(\"store\", /* add date/time? */ false);\n        SimpleLogger visitedLog = new SimpleLogger(\"URLVisitedFilter\", /* add date/time? */ false);\n        SimpleLogger scopeLog = new SimpleLogger(\"URLScopeFilter\", /* add date/time? */ false);\n        SimpleLogger pathsLog = new SimpleLogger(\"KnownPathsFilter\", /* add date/time? */ false);\n        SimpleLogger linksLog = new SimpleLogger(\"links\", /* add date/time? */ false);\n        SimpleLogger lengthLog = new SimpleLogger(\"length\", /* add date/time? */ false);\n\n        StoragePipeline storage = new StoragePipeline();\n\n\n        // in the default configuration, the crawler will only save the document\n        // information to store.log and the link information to links.log\n        // The contents of the files are _not_ saved. If you set\n        // \"save in page files\" to \"true\", they will be saved in \"page files\",\n        // binary files each containing a set of documents. Here, the\n        // maximum file size is ~50 MB (crawled files won't be split up into different\n        // files). The logs/store.log file contains pointers to these files: a page\n        // file number, the offset within that file, and the document's length\n\n        // FIXME: default constructor for all storages + bean access methods\n        storage.addDocStorage(new LogStorage(storeLog, /* save in page files? */ false,\n                                             /* page file prefix */ \"logs/pagefile\"));\n        storage.addLinkStorage(new LinkLogStorage(linksLog));\n        storage.addLinkStorage(messageHandler);\n        /*\n        // experimental Lucene storage. will slow the crawler down *a lot*\n        LuceneStorage luceneStorage = new LuceneStorage();\n        luceneStorage.setAnalyzer(new org.apache.lucene.analysis.de.GermanAnalyzer());\n        luceneStorage.setCreate(true);\n\t// FIXME: index name and path need to be configurable\n        luceneStorage.setIndexName(\"luceneIndex\");\n        // the field names come from URLMessage.java and WebDocument.java. See\n        // LuceneStorage source for details\n        luceneStorage.setFieldInfo(\"url\", LuceneStorage.INDEX | LuceneStorage.STORE);\n        luceneStorage.setFieldInfo(\"content\", LuceneStorage.INDEX | LuceneStorage.STORE | LuceneStorage.TOKEN);\n        storage.addDocStorage(luceneStorage);\n        */\n\n        storage.open();\n\n        //storage.addStorage(new JMSStorage(...));\n\n        // create the filters and add them to the message queue\n        urlScopeFilter = new URLScopeFilter(scopeLog);\n\n        // dnsResolver = new DNSResolver();\n        hostManager = new HostManager(1000);\n        hostResolver = new HostResolver();\n        hostResolver.initFromFile(hostResolverFile);\n        hostManager.setHostResolver(hostResolver);\n\n//        hostManager.addSynonym(\"www.fachsprachen.uni-muenchen.de\", \"www.fremdsprachen.uni-muenchen.de\");\n//        hostManager.addSynonym(\"www.uni-muenchen.de\", \"www.lmu.de\");\n//        hostManager.addSynonym(\"www.uni-muenchen.de\", \"uni-muenchen.de\");\n//        hostManager.addSynonym(\"webinfo.uni-muenchen.de\", \"www.webinfo.uni-muenchen.de\");\n//        hostManager.addSynonym(\"webinfo.uni-muenchen.de\", \"webinfo.campus.lmu.de\");\n//        hostManager.addSynonym(\"www.s-a.uni-muenchen.de\", \"s-a.uni-muenchen.de\");\n\n        reFilter = new RobotExclusionFilter(hostManager);\n\n        fetcher = new Fetcher(nrThreads, storage, storage, hostManager);\n\n        // prevent message box popups\n        HTTPConnection.setDefaultAllowUserInteraction(false);\n\n        // prevent GZipped files from being decoded\n        HTTPConnection.removeDefaultModule(HTTPClient.ContentEncodingModule.class);\n\n        urlVisitedFilter = new URLVisitedFilter(visitedLog, 100000);\n\n        // initialize the threads\n        fetcher.init();\n\n        // the thread monitor watches the thread pool.\n\n        monitor = new ThreadMonitor(urlLengthFilter,\n                urlVisitedFilter,\n                urlScopeFilter,\n                /*dnsResolver,*/\n                reFilter,\n                messageHandler,\n                fetcher.getThreadPool(),\n                hostManager,\n                5000        // wake up every 5 seconds\n                );\n\n\n        // add all filters to the handler.\n        messageHandler.addListener(urlLengthFilter);\n        messageHandler.addListener(urlScopeFilter);\n        messageHandler.addListener(reFilter);\n        messageHandler.addListener(urlVisitedFilter);\n        messageHandler.addListener(knownPathsFilter);\n\n        messageHandler.addListener(fetcher);\n\n         //uncomment this to enable HTTPClient logging\n        /*\n        try\n        {\n            HTTPClient.Log.setLogWriter(new java.io.OutputStreamWriter(System.out) //new java.io.FileWriter(\"logs/HttpClient.log\")\n            ,false);\n            HTTPClient.Log.setLogging(HTTPClient.Log.ALL, true);\n        }\n        catch (Exception e)\n        {\n            e.printStackTrace();\n        }\n        */\n\n    }\n\n","sourceOld":"    /**\n     * initializes all classes and registers anonymous adapter classes as\n     * listeners for fetcher events.\n     *\n     * @param nrThreads  number of fetcher threads to be created\n     */\n    private FetcherMain(int nrThreads)\n    {\n        // to make things clear, this method is commented a bit better than\n        // the rest of the program...\n\n        // this is the main message queue. handlers are registered with\n        // the queue, and whenever a message is put in it, the message is passed to the\n        // filters in a \"chain of responibility\" manner. Every listener can decide\n        // to throw the message away\n        messageHandler = new MessageHandler();\n\n        // the storage is the class which saves a WebDocument somewhere, no\n        // matter how it does it, whether it's in a file, in a database or\n        // whatever\n\n        // example for the (very slow) SQL Server storage:\n        // this.storage = new SQLServerStorage(\"sun.jdbc.odbc.JdbcOdbcDriver\",\"jdbc:odbc:search\",\"sa\",\"...\",nrThreads);\n\n        // the LogStorage used here does extensive logging. It logs all links and\n        // document information.\n        // it also saves all documents to page files. Probably this single storage\n        // could also be replaced by a pipeline; or even incorporated into the\n        // existing message pipeline\n        SimpleLogger storeLog = new SimpleLogger(\"store\", false);\n        SimpleLogger linksLog = new SimpleLogger(\"links\", false);\n\n\n        StoragePipeline storage = new StoragePipeline();\n        //storage.addDocStorage(new LogStorage(storeLog, /* save in page files? */ false, /* logfile prefix */ \"logs/pagefile\"));\n        storage.addLinkStorage(new LinkLogStorage(linksLog));\n        storage.addLinkStorage(messageHandler);\n\n        LuceneStorage luceneStorage = new LuceneStorage();\n        luceneStorage.setAnalyzer(new org.apache.lucene.analysis.de.GermanAnalyzer());\n        luceneStorage.setCreate(true);\n\t// FIXME: index name and path need to be configurable\n        luceneStorage.setIndexName(\"luceneIndex\");\n        luceneStorage.setFieldInfo(\"url\", LuceneStorage.INDEX | LuceneStorage.STORE);\n        luceneStorage.setFieldInfo(\"content\", LuceneStorage.INDEX | LuceneStorage.STORE | LuceneStorage.TOKEN);\n        storage.addDocStorage(luceneStorage);\n        storage.open();\n\n        //storage.addStorage(new JMSStorage(...));\n\n        // a third example would be the NullStorage, which converts the documents into\n        // heat, which evaporates above the processor\n        // NullStorage();\n\n        hostManager = new HostManager(1000);\n\n        // create the filters and add them to the message queue\n        reFilter = new RobotExclusionFilter(hostManager);\n        urlScopeFilter = new URLScopeFilter();\n        urlVisitedFilter = new URLVisitedFilter(100000);\n        knownPathsFilter = new KnownPathsFilter();\n        urlLengthFilter = new URLLengthFilter(255);\n\n        // dnsResolver = new DNSResolver();\n        fetcher = new Fetcher(nrThreads, storage, storage, hostManager);\n\n        // prevent message box popups\n        HTTPConnection.setDefaultAllowUserInteraction(false);\n\n        // prevent GZipped files from being decoded\n        HTTPConnection.removeDefaultModule(HTTPClient.ContentEncodingModule.class);\n\n        // initialize the threads\n        fetcher.init();\n\n        // the thread monitor watches the thread pool.\n\n        monitor = new ThreadMonitor(urlLengthFilter,\n                urlVisitedFilter,\n                urlScopeFilter,\n                /*dnsResolver,*/\n                reFilter,\n                messageHandler,\n                fetcher.getThreadPool(),\n                hostManager,\n                5000        // wake up every 5 seconds\n                );\n\n\n        // add all filters to the handler.\n        messageHandler.addListener(urlLengthFilter);\n        messageHandler.addListener(urlScopeFilter);\n        messageHandler.addListener(reFilter);\n        messageHandler.addListener(urlVisitedFilter);\n        messageHandler.addListener(knownPathsFilter);\n        messageHandler.addListener(fetcher);\n\n        /* uncomment this to enable HTTPClient logging\n        try\n        {\n            HTTPClient.Log.setLogWriter(new java.io.FileWriter(\"logs/HttpClient.log\"),false);\n            HTTPClient.Log.setLogging(HTTPClient.Log.ALL, true);\n        }\n        catch (Exception e)\n        {\n            e.printStackTrace();\n        }\n        */\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"00827ccb785ffa5db0167c3ae7dc48969c3ba4f9","date":1050068621,"type":3,"author":"cmarschner","isMerge":false,"pathNew":"sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/fetcher/FetcherMain#FetcherMain(int,String).mjava","pathOld":"sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/fetcher/FetcherMain#FetcherMain(int,String).mjava","sourceNew":"    /**\n     * initializes all classes and registers anonymous adapter classes as\n     * listeners for fetcher events.\n     *\n     * @param nrThreads  number of fetcher threads to be created\n     */\n    public FetcherMain(int nrThreads, String hostResolverFile) throws Exception\n    {\n        // to make things clear, this method is commented a bit better than\n        // the rest of the program...\n\n        // this is the main message queue. handlers are registered with\n        // the queue, and whenever a message is put in it, the message is passed to the\n        // filters in a \"chain of responibility\" manner. Every listener can decide\n        // to throw the message away\n        messageHandler = new MessageHandler();\n\n        // the storage is the class which saves a WebDocument somewhere, no\n        // matter how it does it, whether it's in a file, in a database or\n        // whatever\n\n        // example for the (very slow) SQL Server storage:\n        // this.storage = new SQLServerStorage(\"sun.jdbc.odbc.JdbcOdbcDriver\",\"jdbc:odbc:search\",\"sa\",\"...\",nrThreads);\n\n        // the LogStorage used here does extensive logging. It logs all links and\n        // document information.\n        // it also saves all documents to page files.\n        File logsDir = new File(\"logs\");\n        logsDir.mkdir();    // ensure log directory exists\n\n        // in this experimental implementation, the crawler is pretty verbose\n        // the SimpleLogger, however, is a FlyWeight logger which is buffered and\n        // not thread safe by default\n        SimpleLogger storeLog = new SimpleLogger(\"store\", /* add date/time? */ false);\n        SimpleLogger visitedLog = new SimpleLogger(\"URLVisitedFilter\", /* add date/time? */ false);\n        SimpleLogger scopeLog = new SimpleLogger(\"URLScopeFilter\", /* add date/time? */ false);\n        SimpleLogger pathsLog = new SimpleLogger(\"KnownPathsFilter\", /* add date/time? */ false);\n        SimpleLogger linksLog = new SimpleLogger(\"links\", /* add date/time? */ false);\n        SimpleLogger lengthLog = new SimpleLogger(\"length\", /* add date/time? */ false);\n\n        StoragePipeline storage = new StoragePipeline();\n\n\n        // in the default configuration, the crawler will only save the document\n        // information to store.log and the link information to links.log\n        // The contents of the files are _not_ saved. If you set\n        // \"save in page files\" to \"true\", they will be saved in \"page files\",\n        // binary files each containing a set of documents. Here, the\n        // maximum file size is ~50 MB (crawled files won't be split up into different\n        // files). The logs/store.log file contains pointers to these files: a page\n        // file number, the offset within that file, and the document's length\n\n        // FIXME: default constructor for all storages + bean access methods\n        storage.addDocStorage(new LogStorage(storeLog, /* save in page files? */ true,\n                                             /* page file prefix */ \"logs/pagefile\"));\n        storage.addLinkStorage(new LinkLogStorage(linksLog));\n        storage.addLinkStorage(messageHandler);\n        /*\n        // experimental Lucene storage. will slow the crawler down *a lot*\n        LuceneStorage luceneStorage = new LuceneStorage();\n        luceneStorage.setAnalyzer(new org.apache.lucene.analysis.de.GermanAnalyzer());\n        luceneStorage.setCreate(true);\n\t// FIXME: index name and path need to be configurable\n        luceneStorage.setIndexName(\"luceneIndex\");\n        // the field names come from URLMessage.java and WebDocument.java. See\n        // LuceneStorage source for details\n        luceneStorage.setFieldInfo(\"url\", LuceneStorage.INDEX | LuceneStorage.STORE);\n        luceneStorage.setFieldInfo(\"content\", LuceneStorage.INDEX | LuceneStorage.STORE | LuceneStorage.TOKEN);\n        storage.addDocStorage(luceneStorage);\n        */\n\n        storage.open();\n\n        //storage.addStorage(new JMSStorage(...));\n\n        // create the filters and add them to the message queue\n        urlScopeFilter = new URLScopeFilter(scopeLog);\n\n        // dnsResolver = new DNSResolver();\n        hostManager = new HostManager(1000);\n        hostResolver = new HostResolver();\n        if(hostResolverFile != null && !\"\".equals(hostResolverFile))\n        {\n            hostResolver.initFromFile(hostResolverFile);\n        }\n        hostManager.setHostResolver(hostResolver);\n\n//        hostManager.addSynonym(\"www.fachsprachen.uni-muenchen.de\", \"www.fremdsprachen.uni-muenchen.de\");\n//        hostManager.addSynonym(\"www.uni-muenchen.de\", \"www.lmu.de\");\n//        hostManager.addSynonym(\"www.uni-muenchen.de\", \"uni-muenchen.de\");\n//        hostManager.addSynonym(\"webinfo.uni-muenchen.de\", \"www.webinfo.uni-muenchen.de\");\n//        hostManager.addSynonym(\"webinfo.uni-muenchen.de\", \"webinfo.campus.lmu.de\");\n//        hostManager.addSynonym(\"www.s-a.uni-muenchen.de\", \"s-a.uni-muenchen.de\");\n\n        reFilter = new RobotExclusionFilter(hostManager);\n\n        fetcher = new Fetcher(nrThreads, storage, storage, hostManager);\n\n        urlLengthFilter = new URLLengthFilter(500, lengthLog);\n        \n        //knownPathsFilter = new KnownPathsFilter()\n        \n        // prevent message box popups\n        HTTPConnection.setDefaultAllowUserInteraction(false);\n\n        // prevent GZipped files from being decoded\n        HTTPConnection.removeDefaultModule(HTTPClient.ContentEncodingModule.class);\n\n        urlVisitedFilter = new URLVisitedFilter(visitedLog, 100000);\n\n        // initialize the threads\n        fetcher.init();\n\n        // the thread monitor watches the thread pool.\n\n        monitor = new ThreadMonitor(urlLengthFilter,\n                urlVisitedFilter,\n                urlScopeFilter,\n                /*dnsResolver,*/\n                reFilter,\n                messageHandler,\n                fetcher.getThreadPool(),\n                hostManager,\n                5000        // wake up every 5 seconds\n                );\n\n\n        // add all filters to the handler.\n        messageHandler.addListener(urlLengthFilter);\n        messageHandler.addListener(urlScopeFilter);\n        messageHandler.addListener(reFilter);\n        messageHandler.addListener(urlVisitedFilter);\n        //messageHandler.addListener(knownPathsFilter);\n\n        messageHandler.addListener(fetcher);\n\n         //uncomment this to enable HTTPClient logging\n        /*\n        try\n        {\n            HTTPClient.Log.setLogWriter(new java.io.OutputStreamWriter(System.out) //new java.io.FileWriter(\"logs/HttpClient.log\")\n            ,false);\n            HTTPClient.Log.setLogging(HTTPClient.Log.ALL, true);\n        }\n        catch (Exception e)\n        {\n            e.printStackTrace();\n        }\n        */\n\n    }\n\n","sourceOld":"    /**\n     * initializes all classes and registers anonymous adapter classes as\n     * listeners for fetcher events.\n     *\n     * @param nrThreads  number of fetcher threads to be created\n     */\n    public FetcherMain(int nrThreads, String hostResolverFile) throws Exception\n    {\n        // to make things clear, this method is commented a bit better than\n        // the rest of the program...\n\n        // this is the main message queue. handlers are registered with\n        // the queue, and whenever a message is put in it, the message is passed to the\n        // filters in a \"chain of responibility\" manner. Every listener can decide\n        // to throw the message away\n        messageHandler = new MessageHandler();\n\n        // the storage is the class which saves a WebDocument somewhere, no\n        // matter how it does it, whether it's in a file, in a database or\n        // whatever\n\n        // example for the (very slow) SQL Server storage:\n        // this.storage = new SQLServerStorage(\"sun.jdbc.odbc.JdbcOdbcDriver\",\"jdbc:odbc:search\",\"sa\",\"...\",nrThreads);\n\n        // the LogStorage used here does extensive logging. It logs all links and\n        // document information.\n        // it also saves all documents to page files.\n        File logsDir = new File(\"logs\");\n        logsDir.mkdir();    // ensure log directory exists\n\n        // in this experimental implementation, the crawler is pretty verbose\n        // the SimpleLogger, however, is a FlyWeight logger which is buffered and\n        // not thread safe by default\n        SimpleLogger storeLog = new SimpleLogger(\"store\", /* add date/time? */ false);\n        SimpleLogger visitedLog = new SimpleLogger(\"URLVisitedFilter\", /* add date/time? */ false);\n        SimpleLogger scopeLog = new SimpleLogger(\"URLScopeFilter\", /* add date/time? */ false);\n        SimpleLogger pathsLog = new SimpleLogger(\"KnownPathsFilter\", /* add date/time? */ false);\n        SimpleLogger linksLog = new SimpleLogger(\"links\", /* add date/time? */ false);\n        SimpleLogger lengthLog = new SimpleLogger(\"length\", /* add date/time? */ false);\n\n        StoragePipeline storage = new StoragePipeline();\n\n\n        // in the default configuration, the crawler will only save the document\n        // information to store.log and the link information to links.log\n        // The contents of the files are _not_ saved. If you set\n        // \"save in page files\" to \"true\", they will be saved in \"page files\",\n        // binary files each containing a set of documents. Here, the\n        // maximum file size is ~50 MB (crawled files won't be split up into different\n        // files). The logs/store.log file contains pointers to these files: a page\n        // file number, the offset within that file, and the document's length\n\n        // FIXME: default constructor for all storages + bean access methods\n        storage.addDocStorage(new LogStorage(storeLog, /* save in page files? */ false,\n                                             /* page file prefix */ \"logs/pagefile\"));\n        storage.addLinkStorage(new LinkLogStorage(linksLog));\n        storage.addLinkStorage(messageHandler);\n        /*\n        // experimental Lucene storage. will slow the crawler down *a lot*\n        LuceneStorage luceneStorage = new LuceneStorage();\n        luceneStorage.setAnalyzer(new org.apache.lucene.analysis.de.GermanAnalyzer());\n        luceneStorage.setCreate(true);\n\t// FIXME: index name and path need to be configurable\n        luceneStorage.setIndexName(\"luceneIndex\");\n        // the field names come from URLMessage.java and WebDocument.java. See\n        // LuceneStorage source for details\n        luceneStorage.setFieldInfo(\"url\", LuceneStorage.INDEX | LuceneStorage.STORE);\n        luceneStorage.setFieldInfo(\"content\", LuceneStorage.INDEX | LuceneStorage.STORE | LuceneStorage.TOKEN);\n        storage.addDocStorage(luceneStorage);\n        */\n\n        storage.open();\n\n        //storage.addStorage(new JMSStorage(...));\n\n        // create the filters and add them to the message queue\n        urlScopeFilter = new URLScopeFilter(scopeLog);\n\n        // dnsResolver = new DNSResolver();\n        hostManager = new HostManager(1000);\n        hostResolver = new HostResolver();\n        hostResolver.initFromFile(hostResolverFile);\n        hostManager.setHostResolver(hostResolver);\n\n//        hostManager.addSynonym(\"www.fachsprachen.uni-muenchen.de\", \"www.fremdsprachen.uni-muenchen.de\");\n//        hostManager.addSynonym(\"www.uni-muenchen.de\", \"www.lmu.de\");\n//        hostManager.addSynonym(\"www.uni-muenchen.de\", \"uni-muenchen.de\");\n//        hostManager.addSynonym(\"webinfo.uni-muenchen.de\", \"www.webinfo.uni-muenchen.de\");\n//        hostManager.addSynonym(\"webinfo.uni-muenchen.de\", \"webinfo.campus.lmu.de\");\n//        hostManager.addSynonym(\"www.s-a.uni-muenchen.de\", \"s-a.uni-muenchen.de\");\n\n        reFilter = new RobotExclusionFilter(hostManager);\n\n        fetcher = new Fetcher(nrThreads, storage, storage, hostManager);\n\n        // prevent message box popups\n        HTTPConnection.setDefaultAllowUserInteraction(false);\n\n        // prevent GZipped files from being decoded\n        HTTPConnection.removeDefaultModule(HTTPClient.ContentEncodingModule.class);\n\n        urlVisitedFilter = new URLVisitedFilter(visitedLog, 100000);\n\n        // initialize the threads\n        fetcher.init();\n\n        // the thread monitor watches the thread pool.\n\n        monitor = new ThreadMonitor(urlLengthFilter,\n                urlVisitedFilter,\n                urlScopeFilter,\n                /*dnsResolver,*/\n                reFilter,\n                messageHandler,\n                fetcher.getThreadPool(),\n                hostManager,\n                5000        // wake up every 5 seconds\n                );\n\n\n        // add all filters to the handler.\n        messageHandler.addListener(urlLengthFilter);\n        messageHandler.addListener(urlScopeFilter);\n        messageHandler.addListener(reFilter);\n        messageHandler.addListener(urlVisitedFilter);\n        messageHandler.addListener(knownPathsFilter);\n\n        messageHandler.addListener(fetcher);\n\n         //uncomment this to enable HTTPClient logging\n        /*\n        try\n        {\n            HTTPClient.Log.setLogWriter(new java.io.OutputStreamWriter(System.out) //new java.io.FileWriter(\"logs/HttpClient.log\")\n            ,false);\n            HTTPClient.Log.setLogging(HTTPClient.Log.ALL, true);\n        }\n        catch (Exception e)\n        {\n            e.printStackTrace();\n        }\n        */\n\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"afc16d717d9ed1a8e45371668ca6de674164d624","date":1103345442,"type":4,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"/dev/null","pathOld":"sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/fetcher/FetcherMain#FetcherMain(int,String).mjava","sourceNew":null,"sourceOld":"    /**\n     * initializes all classes and registers anonymous adapter classes as\n     * listeners for fetcher events.\n     *\n     * @param nrThreads  number of fetcher threads to be created\n     */\n    public FetcherMain(int nrThreads, String hostResolverFile) throws Exception\n    {\n        // to make things clear, this method is commented a bit better than\n        // the rest of the program...\n\n        // this is the main message queue. handlers are registered with\n        // the queue, and whenever a message is put in it, the message is passed to the\n        // filters in a \"chain of responibility\" manner. Every listener can decide\n        // to throw the message away\n        messageHandler = new MessageHandler();\n\n        // the storage is the class which saves a WebDocument somewhere, no\n        // matter how it does it, whether it's in a file, in a database or\n        // whatever\n\n        // example for the (very slow) SQL Server storage:\n        // this.storage = new SQLServerStorage(\"sun.jdbc.odbc.JdbcOdbcDriver\",\"jdbc:odbc:search\",\"sa\",\"...\",nrThreads);\n\n        // the LogStorage used here does extensive logging. It logs all links and\n        // document information.\n        // it also saves all documents to page files.\n        File logsDir = new File(\"logs\");\n        logsDir.mkdir();    // ensure log directory exists\n\n        // in this experimental implementation, the crawler is pretty verbose\n        // the SimpleLogger, however, is a FlyWeight logger which is buffered and\n        // not thread safe by default\n        SimpleLogger storeLog = new SimpleLogger(\"store\", /* add date/time? */ false);\n        SimpleLogger visitedLog = new SimpleLogger(\"URLVisitedFilter\", /* add date/time? */ false);\n        SimpleLogger scopeLog = new SimpleLogger(\"URLScopeFilter\", /* add date/time? */ false);\n        SimpleLogger pathsLog = new SimpleLogger(\"KnownPathsFilter\", /* add date/time? */ false);\n        SimpleLogger linksLog = new SimpleLogger(\"links\", /* add date/time? */ false);\n        SimpleLogger lengthLog = new SimpleLogger(\"length\", /* add date/time? */ false);\n\n        StoragePipeline storage = new StoragePipeline();\n\n\n        // in the default configuration, the crawler will only save the document\n        // information to store.log and the link information to links.log\n        // The contents of the files are _not_ saved. If you set\n        // \"save in page files\" to \"true\", they will be saved in \"page files\",\n        // binary files each containing a set of documents. Here, the\n        // maximum file size is ~50 MB (crawled files won't be split up into different\n        // files). The logs/store.log file contains pointers to these files: a page\n        // file number, the offset within that file, and the document's length\n\n        // FIXME: default constructor for all storages + bean access methods\n        storage.addDocStorage(new LogStorage(storeLog, /* save in page files? */ true,\n                                             /* page file prefix */ \"logs/pagefile\"));\n        storage.addLinkStorage(new LinkLogStorage(linksLog));\n        storage.addLinkStorage(messageHandler);\n        /*\n        // experimental Lucene storage. will slow the crawler down *a lot*\n        LuceneStorage luceneStorage = new LuceneStorage();\n        luceneStorage.setAnalyzer(new org.apache.lucene.analysis.de.GermanAnalyzer());\n        luceneStorage.setCreate(true);\n\t// FIXME: index name and path need to be configurable\n        luceneStorage.setIndexName(\"luceneIndex\");\n        // the field names come from URLMessage.java and WebDocument.java. See\n        // LuceneStorage source for details\n        luceneStorage.setFieldInfo(\"url\", LuceneStorage.INDEX | LuceneStorage.STORE);\n        luceneStorage.setFieldInfo(\"content\", LuceneStorage.INDEX | LuceneStorage.STORE | LuceneStorage.TOKEN);\n        storage.addDocStorage(luceneStorage);\n        */\n\n        storage.open();\n\n        //storage.addStorage(new JMSStorage(...));\n\n        // create the filters and add them to the message queue\n        urlScopeFilter = new URLScopeFilter(scopeLog);\n\n        // dnsResolver = new DNSResolver();\n        hostManager = new HostManager(1000);\n        hostResolver = new HostResolver();\n        if(hostResolverFile != null && !\"\".equals(hostResolverFile))\n        {\n            hostResolver.initFromFile(hostResolverFile);\n        }\n        hostManager.setHostResolver(hostResolver);\n\n//        hostManager.addSynonym(\"www.fachsprachen.uni-muenchen.de\", \"www.fremdsprachen.uni-muenchen.de\");\n//        hostManager.addSynonym(\"www.uni-muenchen.de\", \"www.lmu.de\");\n//        hostManager.addSynonym(\"www.uni-muenchen.de\", \"uni-muenchen.de\");\n//        hostManager.addSynonym(\"webinfo.uni-muenchen.de\", \"www.webinfo.uni-muenchen.de\");\n//        hostManager.addSynonym(\"webinfo.uni-muenchen.de\", \"webinfo.campus.lmu.de\");\n//        hostManager.addSynonym(\"www.s-a.uni-muenchen.de\", \"s-a.uni-muenchen.de\");\n\n        reFilter = new RobotExclusionFilter(hostManager);\n\n        fetcher = new Fetcher(nrThreads, storage, storage, hostManager);\n\n        urlLengthFilter = new URLLengthFilter(500, lengthLog);\n        \n        //knownPathsFilter = new KnownPathsFilter()\n        \n        // prevent message box popups\n        HTTPConnection.setDefaultAllowUserInteraction(false);\n\n        // prevent GZipped files from being decoded\n        HTTPConnection.removeDefaultModule(HTTPClient.ContentEncodingModule.class);\n\n        urlVisitedFilter = new URLVisitedFilter(visitedLog, 100000);\n\n        // initialize the threads\n        fetcher.init();\n\n        // the thread monitor watches the thread pool.\n\n        monitor = new ThreadMonitor(urlLengthFilter,\n                urlVisitedFilter,\n                urlScopeFilter,\n                /*dnsResolver,*/\n                reFilter,\n                messageHandler,\n                fetcher.getThreadPool(),\n                hostManager,\n                5000        // wake up every 5 seconds\n                );\n\n\n        // add all filters to the handler.\n        messageHandler.addListener(urlLengthFilter);\n        messageHandler.addListener(urlScopeFilter);\n        messageHandler.addListener(reFilter);\n        messageHandler.addListener(urlVisitedFilter);\n        //messageHandler.addListener(knownPathsFilter);\n\n        messageHandler.addListener(fetcher);\n\n         //uncomment this to enable HTTPClient logging\n        /*\n        try\n        {\n            HTTPClient.Log.setLogWriter(new java.io.OutputStreamWriter(System.out) //new java.io.FileWriter(\"logs/HttpClient.log\")\n            ,false);\n            HTTPClient.Log.setLogging(HTTPClient.Log.ALL, true);\n        }\n        catch (Exception e)\n        {\n            e.printStackTrace();\n        }\n        */\n\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"2b24dfe829236e25670a0fb6bd671abc4e2a91f9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"afc16d717d9ed1a8e45371668ca6de674164d624":["00827ccb785ffa5db0167c3ae7dc48969c3ba4f9"],"00827ccb785ffa5db0167c3ae7dc48969c3ba4f9":["2b24dfe829236e25670a0fb6bd671abc4e2a91f9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["afc16d717d9ed1a8e45371668ca6de674164d624"]},"commit2Childs":{"2b24dfe829236e25670a0fb6bd671abc4e2a91f9":["00827ccb785ffa5db0167c3ae7dc48969c3ba4f9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["2b24dfe829236e25670a0fb6bd671abc4e2a91f9"],"afc16d717d9ed1a8e45371668ca6de674164d624":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"00827ccb785ffa5db0167c3ae7dc48969c3ba4f9":["afc16d717d9ed1a8e45371668ca6de674164d624"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}