{"path":"src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","commits":[{"id":"5df1793b9dbc0f17ba1d1dddb8a15748fdc3aaf3","date":1263013040,"type":0,"author":"Koji Sekiguchi","isMerge":false,"pathNew":"src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","pathOld":"/dev/null","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    if (docTexts == null) return;\n    \n    SolrIndexSearcher searcher = req.getSearcher();\n    IndexSchema schema = searcher.getSchema();\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n                   \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        tstream = new CachingTokenFilter(tstream);\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["c45bbf26db88631f7a389cbff0f4eab70f55ec64","815287248ca7a77db68038baad5698c5767f36a7","815287248ca7a77db68038baad5698c5767f36a7","815287248ca7a77db68038baad5698c5767f36a7","815287248ca7a77db68038baad5698c5767f36a7","815287248ca7a77db68038baad5698c5767f36a7","815287248ca7a77db68038baad5698c5767f36a7"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c597e25fc624dace57b3eb1bd2c4b780d404a3fd","date":1263119348,"type":3,"author":"Koji Sekiguchi","isMerge":false,"pathNew":"src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","pathOld":"src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    SolrIndexSearcher searcher = req.getSearcher();\n    IndexSchema schema = searcher.getSchema();\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n                   \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        tstream = new CachingTokenFilter(tstream);\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    if (docTexts == null) return;\n    \n    SolrIndexSearcher searcher = req.getSearcher();\n    IndexSchema schema = searcher.getSchema();\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n                   \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        tstream = new CachingTokenFilter(tstream);\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","pathOld":"src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    SolrIndexSearcher searcher = req.getSearcher();\n    IndexSchema schema = searcher.getSchema();\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n                   \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        tstream = new CachingTokenFilter(tstream);\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    SolrIndexSearcher searcher = req.getSearcher();\n    IndexSchema schema = searcher.getSchema();\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n                   \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        tstream = new CachingTokenFilter(tstream);\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"c597e25fc624dace57b3eb1bd2c4b780d404a3fd":["5df1793b9dbc0f17ba1d1dddb8a15748fdc3aaf3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"ad94625fb8d088209f46650c8097196fec67f00c":["c597e25fc624dace57b3eb1bd2c4b780d404a3fd"],"5df1793b9dbc0f17ba1d1dddb8a15748fdc3aaf3":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"c597e25fc624dace57b3eb1bd2c4b780d404a3fd":["ad94625fb8d088209f46650c8097196fec67f00c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["5df1793b9dbc0f17ba1d1dddb8a15748fdc3aaf3"],"ad94625fb8d088209f46650c8097196fec67f00c":[],"5df1793b9dbc0f17ba1d1dddb8a15748fdc3aaf3":["c597e25fc624dace57b3eb1bd2c4b780d404a3fd"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ad94625fb8d088209f46650c8097196fec67f00c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"pathCommit":null}