{"path":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#writePoints(SegmentWriteState,Sorter.DocMap).mjava","commits":[{"id":"86a0a50d2d14aaee1e635bbec914468551f7f9a2","date":1482234306,"type":1,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#writePoints(SegmentWriteState,Sorter.DocMap).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#writePoints(SegmentWriteState).mjava","sourceNew":"  /** Writes all buffered points. */\n  private void writePoints(SegmentWriteState state, Sorter.DocMap sortMap) throws IOException {\n    PointsWriter pointsWriter = null;\n    boolean success = false;\n    try {\n      for (int i=0;i<fieldHash.length;i++) {\n        PerField perField = fieldHash[i];\n        while (perField != null) {\n          if (perField.pointValuesWriter != null) {\n            if (perField.fieldInfo.getPointDimensionCount() == 0) {\n              // BUG\n              throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has no points but wrote them\");\n            }\n            if (pointsWriter == null) {\n              // lazy init\n              PointsFormat fmt = state.segmentInfo.getCodec().pointsFormat();\n              if (fmt == null) {\n                throw new IllegalStateException(\"field=\\\"\" + perField.fieldInfo.name + \"\\\" was indexed as points but codec does not support points\");\n              }\n              pointsWriter = fmt.fieldsWriter(state);\n            }\n\n            perField.pointValuesWriter.flush(state, sortMap, pointsWriter);\n            perField.pointValuesWriter = null;\n          } else if (perField.fieldInfo.getPointDimensionCount() != 0) {\n            // BUG\n            throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has points but did not write them\");\n          }\n          perField = perField.next;\n        }\n      }\n      if (pointsWriter != null) {\n        pointsWriter.finish();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(pointsWriter);\n      } else {\n        IOUtils.closeWhileHandlingException(pointsWriter);\n      }\n    }\n  }\n\n","sourceOld":"  /** Writes all buffered points. */\n  private void writePoints(SegmentWriteState state) throws IOException {\n    PointsWriter pointsWriter = null;\n    boolean success = false;\n    try {\n      for (int i=0;i<fieldHash.length;i++) {\n        PerField perField = fieldHash[i];\n        while (perField != null) {\n          if (perField.pointValuesWriter != null) {\n            if (perField.fieldInfo.getPointDimensionCount() == 0) {\n              // BUG\n              throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has no points but wrote them\");\n            }\n            if (pointsWriter == null) {\n              // lazy init\n              PointsFormat fmt = state.segmentInfo.getCodec().pointsFormat();\n              if (fmt == null) {\n                throw new IllegalStateException(\"field=\\\"\" + perField.fieldInfo.name + \"\\\" was indexed as points but codec does not support points\");\n              }\n              pointsWriter = fmt.fieldsWriter(state);\n            }\n\n            perField.pointValuesWriter.flush(state, pointsWriter);\n            perField.pointValuesWriter = null;\n          } else if (perField.fieldInfo.getPointDimensionCount() != 0) {\n            // BUG\n            throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has points but did not write them\");\n          }\n          perField = perField.next;\n        }\n      }\n      if (pointsWriter != null) {\n        pointsWriter.finish();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(pointsWriter);\n      } else {\n        IOUtils.closeWhileHandlingException(pointsWriter);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","date":1482251961,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#writePoints(SegmentWriteState,Sorter.DocMap).mjava","pathOld":"/dev/null","sourceNew":"  /** Writes all buffered points. */\n  private void writePoints(SegmentWriteState state, Sorter.DocMap sortMap) throws IOException {\n    PointsWriter pointsWriter = null;\n    boolean success = false;\n    try {\n      for (int i=0;i<fieldHash.length;i++) {\n        PerField perField = fieldHash[i];\n        while (perField != null) {\n          if (perField.pointValuesWriter != null) {\n            if (perField.fieldInfo.getPointDimensionCount() == 0) {\n              // BUG\n              throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has no points but wrote them\");\n            }\n            if (pointsWriter == null) {\n              // lazy init\n              PointsFormat fmt = state.segmentInfo.getCodec().pointsFormat();\n              if (fmt == null) {\n                throw new IllegalStateException(\"field=\\\"\" + perField.fieldInfo.name + \"\\\" was indexed as points but codec does not support points\");\n              }\n              pointsWriter = fmt.fieldsWriter(state);\n            }\n\n            perField.pointValuesWriter.flush(state, sortMap, pointsWriter);\n            perField.pointValuesWriter = null;\n          } else if (perField.fieldInfo.getPointDimensionCount() != 0) {\n            // BUG\n            throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has points but did not write them\");\n          }\n          perField = perField.next;\n        }\n      }\n      if (pointsWriter != null) {\n        pointsWriter.finish();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(pointsWriter);\n      } else {\n        IOUtils.closeWhileHandlingException(pointsWriter);\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f6652c943595e92c187ee904c382863013eae28f","date":1539042663,"type":3,"author":"Nicholas Knize","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#writePoints(SegmentWriteState,Sorter.DocMap).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#writePoints(SegmentWriteState,Sorter.DocMap).mjava","sourceNew":"  /** Writes all buffered points. */\n  private void writePoints(SegmentWriteState state, Sorter.DocMap sortMap) throws IOException {\n    PointsWriter pointsWriter = null;\n    boolean success = false;\n    try {\n      for (int i=0;i<fieldHash.length;i++) {\n        PerField perField = fieldHash[i];\n        while (perField != null) {\n          if (perField.pointValuesWriter != null) {\n            if (perField.fieldInfo.getPointDataDimensionCount() == 0) {\n              // BUG\n              throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has no points but wrote them\");\n            }\n            if (pointsWriter == null) {\n              // lazy init\n              PointsFormat fmt = state.segmentInfo.getCodec().pointsFormat();\n              if (fmt == null) {\n                throw new IllegalStateException(\"field=\\\"\" + perField.fieldInfo.name + \"\\\" was indexed as points but codec does not support points\");\n              }\n              pointsWriter = fmt.fieldsWriter(state);\n            }\n\n            perField.pointValuesWriter.flush(state, sortMap, pointsWriter);\n            perField.pointValuesWriter = null;\n          } else if (perField.fieldInfo.getPointDataDimensionCount() != 0) {\n            // BUG\n            throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has points but did not write them\");\n          }\n          perField = perField.next;\n        }\n      }\n      if (pointsWriter != null) {\n        pointsWriter.finish();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(pointsWriter);\n      } else {\n        IOUtils.closeWhileHandlingException(pointsWriter);\n      }\n    }\n  }\n\n","sourceOld":"  /** Writes all buffered points. */\n  private void writePoints(SegmentWriteState state, Sorter.DocMap sortMap) throws IOException {\n    PointsWriter pointsWriter = null;\n    boolean success = false;\n    try {\n      for (int i=0;i<fieldHash.length;i++) {\n        PerField perField = fieldHash[i];\n        while (perField != null) {\n          if (perField.pointValuesWriter != null) {\n            if (perField.fieldInfo.getPointDimensionCount() == 0) {\n              // BUG\n              throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has no points but wrote them\");\n            }\n            if (pointsWriter == null) {\n              // lazy init\n              PointsFormat fmt = state.segmentInfo.getCodec().pointsFormat();\n              if (fmt == null) {\n                throw new IllegalStateException(\"field=\\\"\" + perField.fieldInfo.name + \"\\\" was indexed as points but codec does not support points\");\n              }\n              pointsWriter = fmt.fieldsWriter(state);\n            }\n\n            perField.pointValuesWriter.flush(state, sortMap, pointsWriter);\n            perField.pointValuesWriter = null;\n          } else if (perField.fieldInfo.getPointDimensionCount() != 0) {\n            // BUG\n            throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has points but did not write them\");\n          }\n          perField = perField.next;\n        }\n      }\n      if (pointsWriter != null) {\n        pointsWriter.finish();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(pointsWriter);\n      } else {\n        IOUtils.closeWhileHandlingException(pointsWriter);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"59ed8c026ba85e3c42fb89605b2032dc6f9cc241","date":1581113294,"type":3,"author":"Nicholas Knize","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#writePoints(SegmentWriteState,Sorter.DocMap).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#writePoints(SegmentWriteState,Sorter.DocMap).mjava","sourceNew":"  /** Writes all buffered points. */\n  private void writePoints(SegmentWriteState state, Sorter.DocMap sortMap) throws IOException {\n    PointsWriter pointsWriter = null;\n    boolean success = false;\n    try {\n      for (int i=0;i<fieldHash.length;i++) {\n        PerField perField = fieldHash[i];\n        while (perField != null) {\n          if (perField.pointValuesWriter != null) {\n            if (perField.fieldInfo.getPointDimensionCount() == 0) {\n              // BUG\n              throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has no points but wrote them\");\n            }\n            if (pointsWriter == null) {\n              // lazy init\n              PointsFormat fmt = state.segmentInfo.getCodec().pointsFormat();\n              if (fmt == null) {\n                throw new IllegalStateException(\"field=\\\"\" + perField.fieldInfo.name + \"\\\" was indexed as points but codec does not support points\");\n              }\n              pointsWriter = fmt.fieldsWriter(state);\n            }\n\n            perField.pointValuesWriter.flush(state, sortMap, pointsWriter);\n            perField.pointValuesWriter = null;\n          } else if (perField.fieldInfo.getPointDimensionCount() != 0) {\n            // BUG\n            throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has points but did not write them\");\n          }\n          perField = perField.next;\n        }\n      }\n      if (pointsWriter != null) {\n        pointsWriter.finish();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(pointsWriter);\n      } else {\n        IOUtils.closeWhileHandlingException(pointsWriter);\n      }\n    }\n  }\n\n","sourceOld":"  /** Writes all buffered points. */\n  private void writePoints(SegmentWriteState state, Sorter.DocMap sortMap) throws IOException {\n    PointsWriter pointsWriter = null;\n    boolean success = false;\n    try {\n      for (int i=0;i<fieldHash.length;i++) {\n        PerField perField = fieldHash[i];\n        while (perField != null) {\n          if (perField.pointValuesWriter != null) {\n            if (perField.fieldInfo.getPointDataDimensionCount() == 0) {\n              // BUG\n              throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has no points but wrote them\");\n            }\n            if (pointsWriter == null) {\n              // lazy init\n              PointsFormat fmt = state.segmentInfo.getCodec().pointsFormat();\n              if (fmt == null) {\n                throw new IllegalStateException(\"field=\\\"\" + perField.fieldInfo.name + \"\\\" was indexed as points but codec does not support points\");\n              }\n              pointsWriter = fmt.fieldsWriter(state);\n            }\n\n            perField.pointValuesWriter.flush(state, sortMap, pointsWriter);\n            perField.pointValuesWriter = null;\n          } else if (perField.fieldInfo.getPointDataDimensionCount() != 0) {\n            // BUG\n            throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has points but did not write them\");\n          }\n          perField = perField.next;\n        }\n      }\n      if (pointsWriter != null) {\n        pointsWriter.finish();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(pointsWriter);\n      } else {\n        IOUtils.closeWhileHandlingException(pointsWriter);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7a6f8af01d9b3067b143bbdc0a492720e2af97cf","date":1600157724,"type":5,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexingChain#writePoints(SegmentWriteState,Sorter.DocMap).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#writePoints(SegmentWriteState,Sorter.DocMap).mjava","sourceNew":"  /** Writes all buffered points. */\n  private void writePoints(SegmentWriteState state, Sorter.DocMap sortMap) throws IOException {\n    PointsWriter pointsWriter = null;\n    boolean success = false;\n    try {\n      for (int i=0;i<fieldHash.length;i++) {\n        PerField perField = fieldHash[i];\n        while (perField != null) {\n          if (perField.pointValuesWriter != null) {\n            if (perField.fieldInfo.getPointDimensionCount() == 0) {\n              // BUG\n              throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has no points but wrote them\");\n            }\n            if (pointsWriter == null) {\n              // lazy init\n              PointsFormat fmt = state.segmentInfo.getCodec().pointsFormat();\n              if (fmt == null) {\n                throw new IllegalStateException(\"field=\\\"\" + perField.fieldInfo.name + \"\\\" was indexed as points but codec does not support points\");\n              }\n              pointsWriter = fmt.fieldsWriter(state);\n            }\n\n            perField.pointValuesWriter.flush(state, sortMap, pointsWriter);\n            perField.pointValuesWriter = null;\n          } else if (perField.fieldInfo.getPointDimensionCount() != 0) {\n            // BUG\n            throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has points but did not write them\");\n          }\n          perField = perField.next;\n        }\n      }\n      if (pointsWriter != null) {\n        pointsWriter.finish();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(pointsWriter);\n      } else {\n        IOUtils.closeWhileHandlingException(pointsWriter);\n      }\n    }\n  }\n\n","sourceOld":"  /** Writes all buffered points. */\n  private void writePoints(SegmentWriteState state, Sorter.DocMap sortMap) throws IOException {\n    PointsWriter pointsWriter = null;\n    boolean success = false;\n    try {\n      for (int i=0;i<fieldHash.length;i++) {\n        PerField perField = fieldHash[i];\n        while (perField != null) {\n          if (perField.pointValuesWriter != null) {\n            if (perField.fieldInfo.getPointDimensionCount() == 0) {\n              // BUG\n              throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has no points but wrote them\");\n            }\n            if (pointsWriter == null) {\n              // lazy init\n              PointsFormat fmt = state.segmentInfo.getCodec().pointsFormat();\n              if (fmt == null) {\n                throw new IllegalStateException(\"field=\\\"\" + perField.fieldInfo.name + \"\\\" was indexed as points but codec does not support points\");\n              }\n              pointsWriter = fmt.fieldsWriter(state);\n            }\n\n            perField.pointValuesWriter.flush(state, sortMap, pointsWriter);\n            perField.pointValuesWriter = null;\n          } else if (perField.fieldInfo.getPointDimensionCount() != 0) {\n            // BUG\n            throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has points but did not write them\");\n          }\n          perField = perField.next;\n        }\n      }\n      if (pointsWriter != null) {\n        pointsWriter.finish();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(pointsWriter);\n      } else {\n        IOUtils.closeWhileHandlingException(pointsWriter);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"680b6449f09827f58fe987aff279e014c311d966","date":1600247985,"type":5,"author":"noblepaul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexingChain#writePoints(SegmentWriteState,Sorter.DocMap).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#writePoints(SegmentWriteState,Sorter.DocMap).mjava","sourceNew":"  /** Writes all buffered points. */\n  private void writePoints(SegmentWriteState state, Sorter.DocMap sortMap) throws IOException {\n    PointsWriter pointsWriter = null;\n    boolean success = false;\n    try {\n      for (int i=0;i<fieldHash.length;i++) {\n        PerField perField = fieldHash[i];\n        while (perField != null) {\n          if (perField.pointValuesWriter != null) {\n            if (perField.fieldInfo.getPointDimensionCount() == 0) {\n              // BUG\n              throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has no points but wrote them\");\n            }\n            if (pointsWriter == null) {\n              // lazy init\n              PointsFormat fmt = state.segmentInfo.getCodec().pointsFormat();\n              if (fmt == null) {\n                throw new IllegalStateException(\"field=\\\"\" + perField.fieldInfo.name + \"\\\" was indexed as points but codec does not support points\");\n              }\n              pointsWriter = fmt.fieldsWriter(state);\n            }\n\n            perField.pointValuesWriter.flush(state, sortMap, pointsWriter);\n            perField.pointValuesWriter = null;\n          } else if (perField.fieldInfo.getPointDimensionCount() != 0) {\n            // BUG\n            throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has points but did not write them\");\n          }\n          perField = perField.next;\n        }\n      }\n      if (pointsWriter != null) {\n        pointsWriter.finish();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(pointsWriter);\n      } else {\n        IOUtils.closeWhileHandlingException(pointsWriter);\n      }\n    }\n  }\n\n","sourceOld":"  /** Writes all buffered points. */\n  private void writePoints(SegmentWriteState state, Sorter.DocMap sortMap) throws IOException {\n    PointsWriter pointsWriter = null;\n    boolean success = false;\n    try {\n      for (int i=0;i<fieldHash.length;i++) {\n        PerField perField = fieldHash[i];\n        while (perField != null) {\n          if (perField.pointValuesWriter != null) {\n            if (perField.fieldInfo.getPointDimensionCount() == 0) {\n              // BUG\n              throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has no points but wrote them\");\n            }\n            if (pointsWriter == null) {\n              // lazy init\n              PointsFormat fmt = state.segmentInfo.getCodec().pointsFormat();\n              if (fmt == null) {\n                throw new IllegalStateException(\"field=\\\"\" + perField.fieldInfo.name + \"\\\" was indexed as points but codec does not support points\");\n              }\n              pointsWriter = fmt.fieldsWriter(state);\n            }\n\n            perField.pointValuesWriter.flush(state, sortMap, pointsWriter);\n            perField.pointValuesWriter = null;\n          } else if (perField.fieldInfo.getPointDimensionCount() != 0) {\n            // BUG\n            throw new AssertionError(\"segment=\" + state.segmentInfo + \": field=\\\"\" + perField.fieldInfo.name + \"\\\" has points but did not write them\");\n          }\n          perField = perField.next;\n        }\n      }\n      if (pointsWriter != null) {\n        pointsWriter.finish();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(pointsWriter);\n      } else {\n        IOUtils.closeWhileHandlingException(pointsWriter);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"680b6449f09827f58fe987aff279e014c311d966":["59ed8c026ba85e3c42fb89605b2032dc6f9cc241","7a6f8af01d9b3067b143bbdc0a492720e2af97cf"],"7a6f8af01d9b3067b143bbdc0a492720e2af97cf":["59ed8c026ba85e3c42fb89605b2032dc6f9cc241"],"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"59ed8c026ba85e3c42fb89605b2032dc6f9cc241":["f6652c943595e92c187ee904c382863013eae28f"],"f6652c943595e92c187ee904c382863013eae28f":["86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"86a0a50d2d14aaee1e635bbec914468551f7f9a2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["680b6449f09827f58fe987aff279e014c311d966"]},"commit2Childs":{"680b6449f09827f58fe987aff279e014c311d966":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"7a6f8af01d9b3067b143bbdc0a492720e2af97cf":["680b6449f09827f58fe987aff279e014c311d966"],"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf":[],"59ed8c026ba85e3c42fb89605b2032dc6f9cc241":["680b6449f09827f58fe987aff279e014c311d966","7a6f8af01d9b3067b143bbdc0a492720e2af97cf"],"f6652c943595e92c187ee904c382863013eae28f":["59ed8c026ba85e3c42fb89605b2032dc6f9cc241"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"86a0a50d2d14aaee1e635bbec914468551f7f9a2":["5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","f6652c943595e92c187ee904c382863013eae28f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}