{"path":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/SynonymTokenizer[HighlighterTest].TestHighlightRunner#doStandardHighlights(Analyzer,IndexSearcher,TopDocs,Query,Formatter,boolean).mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/SynonymTokenizer[HighlighterTest].TestHighlightRunner#doStandardHighlights(Analyzer,IndexSearcher,TopDocs,Query,Formatter,boolean).mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/SynonymTokenizer[HighlighterTest].TestHighlightRunner#doStandardHighlights(Analyzer,IndexSearcher,TopDocs,Query,Formatter,boolean).mjava","sourceNew":"    void doStandardHighlights(Analyzer analyzer, IndexSearcher searcher, TopDocs hits, Query query, Formatter formatter, boolean expandMT)\n        throws Exception {\n\n      for (int i = 0; i < hits.totalHits; i++) {\n        String text = searcher.doc(hits.scoreDocs[i].doc).get(HighlighterTest.FIELD_NAME);\n        int maxNumFragmentsRequired = 2;\n        String fragmentSeparator = \"...\";\n        Scorer scorer = null;\n        TokenStream tokenStream = analyzer.tokenStream(HighlighterTest.FIELD_NAME, new StringReader(text));\n        if (mode == QUERY) {\n          scorer = new QueryScorer(query);\n        } else if (mode == QUERY_TERM) {\n          scorer = new QueryTermScorer(query);\n        }\n        Highlighter highlighter = new Highlighter(formatter, scorer);\n        highlighter.setTextFragmenter(frag);\n\n        String result = highlighter.getBestFragments(tokenStream, text, maxNumFragmentsRequired,\n            fragmentSeparator);\n        if (LuceneTestCase.VERBOSE) System.out.println(\"\\t\" + result);\n      }\n    }\n\n","sourceOld":"    void doStandardHighlights(Analyzer analyzer, IndexSearcher searcher, TopDocs hits, Query query, Formatter formatter, boolean expandMT)\n        throws Exception {\n\n      for (int i = 0; i < hits.totalHits; i++) {\n        String text = searcher.doc(hits.scoreDocs[i].doc).get(HighlighterTest.FIELD_NAME);\n        int maxNumFragmentsRequired = 2;\n        String fragmentSeparator = \"...\";\n        Scorer scorer = null;\n        TokenStream tokenStream = analyzer.tokenStream(HighlighterTest.FIELD_NAME, new StringReader(text));\n        if (mode == QUERY) {\n          scorer = new QueryScorer(query);\n        } else if (mode == QUERY_TERM) {\n          scorer = new QueryTermScorer(query);\n        }\n        Highlighter highlighter = new Highlighter(formatter, scorer);\n        highlighter.setTextFragmenter(frag);\n\n        String result = highlighter.getBestFragments(tokenStream, text, maxNumFragmentsRequired,\n            fragmentSeparator);\n        if (LuceneTestCase.VERBOSE) System.out.println(\"\\t\" + result);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c83d6c4335f31cae14f625a222bc842f20073dcd","date":1373306148,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/SynonymTokenizer[HighlighterTest].TestHighlightRunner#doStandardHighlights(Analyzer,IndexSearcher,TopDocs,Query,Formatter,boolean).mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/SynonymTokenizer[HighlighterTest].TestHighlightRunner#doStandardHighlights(Analyzer,IndexSearcher,TopDocs,Query,Formatter,boolean).mjava","sourceNew":"    void doStandardHighlights(Analyzer analyzer, IndexSearcher searcher, TopDocs hits, Query query, Formatter formatter, boolean expandMT)\n        throws Exception {\n\n      for (int i = 0; i < hits.totalHits; i++) {\n        String text = searcher.doc(hits.scoreDocs[i].doc).get(HighlighterTest.FIELD_NAME);\n        int maxNumFragmentsRequired = 2;\n        String fragmentSeparator = \"...\";\n        Scorer scorer = null;\n        TokenStream tokenStream = analyzer.tokenStream(HighlighterTest.FIELD_NAME, text);\n        if (mode == QUERY) {\n          scorer = new QueryScorer(query);\n        } else if (mode == QUERY_TERM) {\n          scorer = new QueryTermScorer(query);\n        }\n        Highlighter highlighter = new Highlighter(formatter, scorer);\n        highlighter.setTextFragmenter(frag);\n\n        String result = highlighter.getBestFragments(tokenStream, text, maxNumFragmentsRequired,\n            fragmentSeparator);\n        if (LuceneTestCase.VERBOSE) System.out.println(\"\\t\" + result);\n      }\n    }\n\n","sourceOld":"    void doStandardHighlights(Analyzer analyzer, IndexSearcher searcher, TopDocs hits, Query query, Formatter formatter, boolean expandMT)\n        throws Exception {\n\n      for (int i = 0; i < hits.totalHits; i++) {\n        String text = searcher.doc(hits.scoreDocs[i].doc).get(HighlighterTest.FIELD_NAME);\n        int maxNumFragmentsRequired = 2;\n        String fragmentSeparator = \"...\";\n        Scorer scorer = null;\n        TokenStream tokenStream = analyzer.tokenStream(HighlighterTest.FIELD_NAME, new StringReader(text));\n        if (mode == QUERY) {\n          scorer = new QueryScorer(query);\n        } else if (mode == QUERY_TERM) {\n          scorer = new QueryTermScorer(query);\n        }\n        Highlighter highlighter = new Highlighter(formatter, scorer);\n        highlighter.setTextFragmenter(frag);\n\n        String result = highlighter.getBestFragments(tokenStream, text, maxNumFragmentsRequired,\n            fragmentSeparator);\n        if (LuceneTestCase.VERBOSE) System.out.println(\"\\t\" + result);\n      }\n    }\n\n","bugFix":["69e043c521d4e8db770cc140c63f5ef51f03426a"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/SynonymTokenizer[HighlighterTest].TestHighlightRunner#doStandardHighlights(Analyzer,IndexSearcher,TopDocs,Query,Formatter,boolean).mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/SynonymTokenizer[HighlighterTest].TestHighlightRunner#doStandardHighlights(Analyzer,IndexSearcher,TopDocs,Query,Formatter,boolean).mjava","sourceNew":"    void doStandardHighlights(Analyzer analyzer, IndexSearcher searcher, TopDocs hits, Query query, Formatter formatter, boolean expandMT)\n        throws Exception {\n\n      for (int i = 0; i < hits.totalHits; i++) {\n        String text = searcher.doc(hits.scoreDocs[i].doc).get(HighlighterTest.FIELD_NAME);\n        int maxNumFragmentsRequired = 2;\n        String fragmentSeparator = \"...\";\n        Scorer scorer = null;\n        TokenStream tokenStream = analyzer.tokenStream(HighlighterTest.FIELD_NAME, text);\n        if (mode == QUERY) {\n          scorer = new QueryScorer(query);\n        } else if (mode == QUERY_TERM) {\n          scorer = new QueryTermScorer(query);\n        }\n        Highlighter highlighter = new Highlighter(formatter, scorer);\n        highlighter.setTextFragmenter(frag);\n\n        String result = highlighter.getBestFragments(tokenStream, text, maxNumFragmentsRequired,\n            fragmentSeparator);\n        if (LuceneTestCase.VERBOSE) System.out.println(\"\\t\" + result);\n      }\n    }\n\n","sourceOld":"    void doStandardHighlights(Analyzer analyzer, IndexSearcher searcher, TopDocs hits, Query query, Formatter formatter, boolean expandMT)\n        throws Exception {\n\n      for (int i = 0; i < hits.totalHits; i++) {\n        String text = searcher.doc(hits.scoreDocs[i].doc).get(HighlighterTest.FIELD_NAME);\n        int maxNumFragmentsRequired = 2;\n        String fragmentSeparator = \"...\";\n        Scorer scorer = null;\n        TokenStream tokenStream = analyzer.tokenStream(HighlighterTest.FIELD_NAME, new StringReader(text));\n        if (mode == QUERY) {\n          scorer = new QueryScorer(query);\n        } else if (mode == QUERY_TERM) {\n          scorer = new QueryTermScorer(query);\n        }\n        Highlighter highlighter = new Highlighter(formatter, scorer);\n        highlighter.setTextFragmenter(frag);\n\n        String result = highlighter.getBestFragments(tokenStream, text, maxNumFragmentsRequired,\n            fragmentSeparator);\n        if (LuceneTestCase.VERBOSE) System.out.println(\"\\t\" + result);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aeebe27bce18b879b80f68494c52cda1021b5705","date":1417792137,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/SynonymTokenizer[HighlighterTest].TestHighlightRunner#doStandardHighlights(Analyzer,IndexSearcher,TopDocs,Query,Formatter,boolean).mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/SynonymTokenizer[HighlighterTest].TestHighlightRunner#doStandardHighlights(Analyzer,IndexSearcher,TopDocs,Query,Formatter,boolean).mjava","sourceNew":"    void doStandardHighlights(Analyzer analyzer, IndexSearcher searcher, TopDocs hits, Query query, Formatter formatter, boolean expandMT)\n        throws Exception {\n\n      for (int i = 0; i < hits.totalHits; i++) {\n        final StoredDocument doc = searcher.doc(hits.scoreDocs[i].doc);\n        String text = doc.get(HighlighterTest.FIELD_NAME);\n        int maxNumFragmentsRequired = 2;\n        String fragmentSeparator = \"...\";\n        Scorer scorer = null;\n        TokenStream tokenStream = TokenSources.getAnyTokenStream(searcher.getIndexReader(),\n            hits.scoreDocs[i].doc, HighlighterTest.FIELD_NAME, doc, analyzer);\n        if (mode == QUERY) {\n          scorer = new QueryScorer(query);\n        } else if (mode == QUERY_TERM) {\n          scorer = new QueryTermScorer(query);\n        }\n        Highlighter highlighter = new Highlighter(formatter, scorer);\n        highlighter.setTextFragmenter(frag);\n        String result = highlighter.getBestFragments(tokenStream, text, maxNumFragmentsRequired,\n            fragmentSeparator);\n        if (LuceneTestCase.VERBOSE) System.out.println(\"\\t\" + result);\n      }\n    }\n\n","sourceOld":"    void doStandardHighlights(Analyzer analyzer, IndexSearcher searcher, TopDocs hits, Query query, Formatter formatter, boolean expandMT)\n        throws Exception {\n\n      for (int i = 0; i < hits.totalHits; i++) {\n        String text = searcher.doc(hits.scoreDocs[i].doc).get(HighlighterTest.FIELD_NAME);\n        int maxNumFragmentsRequired = 2;\n        String fragmentSeparator = \"...\";\n        Scorer scorer = null;\n        TokenStream tokenStream = analyzer.tokenStream(HighlighterTest.FIELD_NAME, text);\n        if (mode == QUERY) {\n          scorer = new QueryScorer(query);\n        } else if (mode == QUERY_TERM) {\n          scorer = new QueryTermScorer(query);\n        }\n        Highlighter highlighter = new Highlighter(formatter, scorer);\n        highlighter.setTextFragmenter(frag);\n\n        String result = highlighter.getBestFragments(tokenStream, text, maxNumFragmentsRequired,\n            fragmentSeparator);\n        if (LuceneTestCase.VERBOSE) System.out.println(\"\\t\" + result);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5d62e4938659e263e96ae8188e11aea8a940aea5","date":1430230314,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/SynonymTokenizer[HighlighterTest].TestHighlightRunner#doStandardHighlights(Analyzer,IndexSearcher,TopDocs,Query,Formatter,boolean).mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/SynonymTokenizer[HighlighterTest].TestHighlightRunner#doStandardHighlights(Analyzer,IndexSearcher,TopDocs,Query,Formatter,boolean).mjava","sourceNew":"    void doStandardHighlights(Analyzer analyzer, IndexSearcher searcher, TopDocs hits, Query query, Formatter formatter, boolean expandMT)\n        throws Exception {\n\n      for (int i = 0; i < hits.totalHits; i++) {\n        final int docId = hits.scoreDocs[i].doc;\n        final StoredDocument doc = searcher.doc(docId);\n        String text = doc.get(HighlighterTest.FIELD_NAME);\n        int maxNumFragmentsRequired = 2;\n        String fragmentSeparator = \"...\";\n        Scorer scorer = null;\n        TokenStream tokenStream =\n            TokenSources.getTokenStream(HighlighterTest.FIELD_NAME,\n                searcher.getIndexReader().getTermVectors(docId), text, analyzer, -1);\n        if (mode == QUERY) {\n          scorer = new QueryScorer(query);\n        } else if (mode == QUERY_TERM) {\n          scorer = new QueryTermScorer(query);\n        }\n        Highlighter highlighter = new Highlighter(formatter, scorer);\n        highlighter.setTextFragmenter(frag);\n        String result = highlighter.getBestFragments(tokenStream, text, maxNumFragmentsRequired,\n            fragmentSeparator);\n        if (LuceneTestCase.VERBOSE) System.out.println(\"\\t\" + result);\n      }\n    }\n\n","sourceOld":"    void doStandardHighlights(Analyzer analyzer, IndexSearcher searcher, TopDocs hits, Query query, Formatter formatter, boolean expandMT)\n        throws Exception {\n\n      for (int i = 0; i < hits.totalHits; i++) {\n        final StoredDocument doc = searcher.doc(hits.scoreDocs[i].doc);\n        String text = doc.get(HighlighterTest.FIELD_NAME);\n        int maxNumFragmentsRequired = 2;\n        String fragmentSeparator = \"...\";\n        Scorer scorer = null;\n        TokenStream tokenStream = TokenSources.getAnyTokenStream(searcher.getIndexReader(),\n            hits.scoreDocs[i].doc, HighlighterTest.FIELD_NAME, doc, analyzer);\n        if (mode == QUERY) {\n          scorer = new QueryScorer(query);\n        } else if (mode == QUERY_TERM) {\n          scorer = new QueryTermScorer(query);\n        }\n        Highlighter highlighter = new Highlighter(formatter, scorer);\n        highlighter.setTextFragmenter(frag);\n        String result = highlighter.getBestFragments(tokenStream, text, maxNumFragmentsRequired,\n            fragmentSeparator);\n        if (LuceneTestCase.VERBOSE) System.out.println(\"\\t\" + result);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","date":1453060490,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/SynonymTokenizer[HighlighterTest].TestHighlightRunner#doStandardHighlights(Analyzer,IndexSearcher,TopDocs,Query,Formatter,boolean).mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/SynonymTokenizer[HighlighterTest].TestHighlightRunner#doStandardHighlights(Analyzer,IndexSearcher,TopDocs,Query,Formatter,boolean).mjava","sourceNew":"    void doStandardHighlights(Analyzer analyzer, IndexSearcher searcher, TopDocs hits, Query query, Formatter formatter, boolean expandMT)\n        throws Exception {\n\n      for (int i = 0; i < hits.totalHits; i++) {\n        final int docId = hits.scoreDocs[i].doc;\n        final Document doc = searcher.doc(docId);\n        String text = doc.get(HighlighterTest.FIELD_NAME);\n        int maxNumFragmentsRequired = 2;\n        String fragmentSeparator = \"...\";\n        Scorer scorer = null;\n        TokenStream tokenStream =\n            TokenSources.getTokenStream(HighlighterTest.FIELD_NAME,\n                searcher.getIndexReader().getTermVectors(docId), text, analyzer, -1);\n        if (mode == QUERY) {\n          scorer = new QueryScorer(query);\n        } else if (mode == QUERY_TERM) {\n          scorer = new QueryTermScorer(query);\n        }\n        Highlighter highlighter = new Highlighter(formatter, scorer);\n        highlighter.setTextFragmenter(frag);\n        String result = highlighter.getBestFragments(tokenStream, text, maxNumFragmentsRequired,\n            fragmentSeparator);\n        if (LuceneTestCase.VERBOSE) System.out.println(\"\\t\" + result);\n      }\n    }\n\n","sourceOld":"    void doStandardHighlights(Analyzer analyzer, IndexSearcher searcher, TopDocs hits, Query query, Formatter formatter, boolean expandMT)\n        throws Exception {\n\n      for (int i = 0; i < hits.totalHits; i++) {\n        final int docId = hits.scoreDocs[i].doc;\n        final StoredDocument doc = searcher.doc(docId);\n        String text = doc.get(HighlighterTest.FIELD_NAME);\n        int maxNumFragmentsRequired = 2;\n        String fragmentSeparator = \"...\";\n        Scorer scorer = null;\n        TokenStream tokenStream =\n            TokenSources.getTokenStream(HighlighterTest.FIELD_NAME,\n                searcher.getIndexReader().getTermVectors(docId), text, analyzer, -1);\n        if (mode == QUERY) {\n          scorer = new QueryScorer(query);\n        } else if (mode == QUERY_TERM) {\n          scorer = new QueryTermScorer(query);\n        }\n        Highlighter highlighter = new Highlighter(formatter, scorer);\n        highlighter.setTextFragmenter(frag);\n        String result = highlighter.getBestFragments(tokenStream, text, maxNumFragmentsRequired,\n            fragmentSeparator);\n        if (LuceneTestCase.VERBOSE) System.out.println(\"\\t\" + result);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"83788ad129a5154d5c6562c4e8ce3db48793aada","date":1532961485,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/SynonymTokenizer[HighlighterTest].TestHighlightRunner#doStandardHighlights(Analyzer,IndexSearcher,TopDocs,Query,Formatter,boolean).mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/SynonymTokenizer[HighlighterTest].TestHighlightRunner#doStandardHighlights(Analyzer,IndexSearcher,TopDocs,Query,Formatter,boolean).mjava","sourceNew":"    void doStandardHighlights(Analyzer analyzer, IndexSearcher searcher, TopDocs hits, Query query, Formatter formatter, boolean expandMT)\n        throws Exception {\n\n      for (int i = 0; i < hits.totalHits.value; i++) {\n        final int docId = hits.scoreDocs[i].doc;\n        final Document doc = searcher.doc(docId);\n        String text = doc.get(HighlighterTest.FIELD_NAME);\n        int maxNumFragmentsRequired = 2;\n        String fragmentSeparator = \"...\";\n        Scorer scorer = null;\n        TokenStream tokenStream =\n            TokenSources.getTokenStream(HighlighterTest.FIELD_NAME,\n                searcher.getIndexReader().getTermVectors(docId), text, analyzer, -1);\n        if (mode == QUERY) {\n          scorer = new QueryScorer(query);\n        } else if (mode == QUERY_TERM) {\n          scorer = new QueryTermScorer(query);\n        }\n        Highlighter highlighter = new Highlighter(formatter, scorer);\n        highlighter.setTextFragmenter(frag);\n        String result = highlighter.getBestFragments(tokenStream, text, maxNumFragmentsRequired,\n            fragmentSeparator);\n        if (LuceneTestCase.VERBOSE) System.out.println(\"\\t\" + result);\n      }\n    }\n\n","sourceOld":"    void doStandardHighlights(Analyzer analyzer, IndexSearcher searcher, TopDocs hits, Query query, Formatter formatter, boolean expandMT)\n        throws Exception {\n\n      for (int i = 0; i < hits.totalHits; i++) {\n        final int docId = hits.scoreDocs[i].doc;\n        final Document doc = searcher.doc(docId);\n        String text = doc.get(HighlighterTest.FIELD_NAME);\n        int maxNumFragmentsRequired = 2;\n        String fragmentSeparator = \"...\";\n        Scorer scorer = null;\n        TokenStream tokenStream =\n            TokenSources.getTokenStream(HighlighterTest.FIELD_NAME,\n                searcher.getIndexReader().getTermVectors(docId), text, analyzer, -1);\n        if (mode == QUERY) {\n          scorer = new QueryScorer(query);\n        } else if (mode == QUERY_TERM) {\n          scorer = new QueryTermScorer(query);\n        }\n        Highlighter highlighter = new Highlighter(formatter, scorer);\n        highlighter.setTextFragmenter(frag);\n        String result = highlighter.getBestFragments(tokenStream, text, maxNumFragmentsRequired,\n            fragmentSeparator);\n        if (LuceneTestCase.VERBOSE) System.out.println(\"\\t\" + result);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["b89678825b68eccaf09e6ab71675fc0b0af1e099","c83d6c4335f31cae14f625a222bc842f20073dcd"],"c83d6c4335f31cae14f625a222bc842f20073dcd":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5d62e4938659e263e96ae8188e11aea8a940aea5":["aeebe27bce18b879b80f68494c52cda1021b5705"],"83788ad129a5154d5c6562c4e8ce3db48793aada":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["83788ad129a5154d5c6562c4e8ce3db48793aada"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["5d62e4938659e263e96ae8188e11aea8a940aea5"],"aeebe27bce18b879b80f68494c52cda1021b5705":["c83d6c4335f31cae14f625a222bc842f20073dcd"]},"commit2Childs":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["37a0f60745e53927c4c876cfe5b5a58170f0646c","c83d6c4335f31cae14f625a222bc842f20073dcd"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"c83d6c4335f31cae14f625a222bc842f20073dcd":["37a0f60745e53927c4c876cfe5b5a58170f0646c","aeebe27bce18b879b80f68494c52cda1021b5705"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"5d62e4938659e263e96ae8188e11aea8a940aea5":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"83788ad129a5154d5c6562c4e8ce3db48793aada":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"aeebe27bce18b879b80f68494c52cda1021b5705":["5d62e4938659e263e96ae8188e11aea8a940aea5"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["83788ad129a5154d5c6562c4e8ce3db48793aada"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["37a0f60745e53927c4c876cfe5b5a58170f0646c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}