{"path":"lucene/core/src/java/org/apache/lucene/index/IndexingChain#flush(SegmentWriteState).mjava","commits":[{"id":"7a6f8af01d9b3067b143bbdc0a492720e2af97cf","date":1600157724,"type":1,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  Sorter.DocMap flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    SegmentReadState readState = new SegmentReadState(state.directory, state.segmentInfo, state.fieldInfos, IOContext.READ, state.segmentSuffix);\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    try (NormsProducer norms = readState.fieldInfos.hasNorms()\n        ? state.segmentInfo.getCodec().normsFormat().normsProducer(readState)\n        : null) {\n      NormsProducer normsMergeInstance = null;\n      if (norms != null) {\n        // Use the merge instance in order to reuse the same IndexInput for all terms\n        normsMergeInstance = norms.getMergeInstance();\n      }\n      termsHash.flush(fieldsToFlush, state, sortMap, normsMergeInstance);\n    }\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    indexWriterConfig.getCodec().fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","sourceOld":"  @Override\n  public Sorter.DocMap flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    SegmentReadState readState = new SegmentReadState(state.directory, state.segmentInfo, state.fieldInfos, IOContext.READ, state.segmentSuffix);\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    try (NormsProducer norms = readState.fieldInfos.hasNorms()\n        ? state.segmentInfo.getCodec().normsFormat().normsProducer(readState)\n        : null) {\n      NormsProducer normsMergeInstance = null;\n      if (norms != null) {\n        // Use the merge instance in order to reuse the same IndexInput for all terms\n        normsMergeInstance = norms.getMergeInstance();\n      }\n      termsHash.flush(fieldsToFlush, state, sortMap, normsMergeInstance);\n    }\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    indexWriterConfig.getCodec().fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"680b6449f09827f58fe987aff279e014c311d966","date":1600247985,"type":1,"author":"noblepaul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","sourceNew":"  Sorter.DocMap flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    SegmentReadState readState = new SegmentReadState(state.directory, state.segmentInfo, state.fieldInfos, IOContext.READ, state.segmentSuffix);\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    try (NormsProducer norms = readState.fieldInfos.hasNorms()\n        ? state.segmentInfo.getCodec().normsFormat().normsProducer(readState)\n        : null) {\n      NormsProducer normsMergeInstance = null;\n      if (norms != null) {\n        // Use the merge instance in order to reuse the same IndexInput for all terms\n        normsMergeInstance = norms.getMergeInstance();\n      }\n      termsHash.flush(fieldsToFlush, state, sortMap, normsMergeInstance);\n    }\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    indexWriterConfig.getCodec().fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","sourceOld":"  @Override\n  public Sorter.DocMap flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n    Sorter.DocMap sortMap = maybeSortSegment(state);\n    int maxDoc = state.segmentInfo.maxDoc();\n    long t0 = System.nanoTime();\n    writeNorms(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write norms\");\n    }\n    SegmentReadState readState = new SegmentReadState(state.directory, state.segmentInfo, state.fieldInfos, IOContext.READ, state.segmentSuffix);\n    \n    t0 = System.nanoTime();\n    writeDocValues(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write docValues\");\n    }\n\n    t0 = System.nanoTime();\n    writePoints(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write points\");\n    }\n    \n    // it's possible all docs hit non-aborting exceptions...\n    t0 = System.nanoTime();\n    storedFieldsConsumer.finish(maxDoc);\n    storedFieldsConsumer.flush(state, sortMap);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to finish stored fields\");\n    }\n\n    t0 = System.nanoTime();\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    try (NormsProducer norms = readState.fieldInfos.hasNorms()\n        ? state.segmentInfo.getCodec().normsFormat().normsProducer(readState)\n        : null) {\n      NormsProducer normsMergeInstance = null;\n      if (norms != null) {\n        // Use the merge instance in order to reuse the same IndexInput for all terms\n        normsMergeInstance = norms.getMergeInstance();\n      }\n      termsHash.flush(fieldsToFlush, state, sortMap, normsMergeInstance);\n    }\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write postings and finish vectors\");\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    t0 = System.nanoTime();\n    indexWriterConfig.getCodec().fieldInfosFormat().write(state.directory, state.segmentInfo, \"\", state.fieldInfos, IOContext.DEFAULT);\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", ((System.nanoTime()-t0)/1000000) + \" msec to write fieldInfos\");\n    }\n\n    return sortMap;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"680b6449f09827f58fe987aff279e014c311d966":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","7a6f8af01d9b3067b143bbdc0a492720e2af97cf"],"7a6f8af01d9b3067b143bbdc0a492720e2af97cf":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["680b6449f09827f58fe987aff279e014c311d966"]},"commit2Childs":{"680b6449f09827f58fe987aff279e014c311d966":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"7a6f8af01d9b3067b143bbdc0a492720e2af97cf":["680b6449f09827f58fe987aff279e014c311d966"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["680b6449f09827f58fe987aff279e014c311d966","7a6f8af01d9b3067b143bbdc0a492720e2af97cf"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}