{"path":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","commits":[{"id":"52c7e49be259508735752fba88085255014a6ecf","date":1398706273,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"/dev/null","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      // if the field omits norms, the boost cannot be indexed.\n      if (fieldType.omitNorms() && field.boost() != 1.0f) {\n        throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      int lastStartOffset = 0;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = field.tokenStream(docState.analyzer)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        if (invertState.attributeSource != stream) {\n          // EmptyTokenStream gets angry otherwise:\n          if (stream.hasAttribute(TermToBytesRefAttribute.class)) {\n            invertState.termAttribute = stream.getAttribute(TermToBytesRefAttribute.class);\n          } else {\n            invertState.termAttribute = null;\n          }\n          invertState.posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n          invertState.offsetAttribute = stream.addAttribute(OffsetAttribute.class);\n          if (stream.hasAttribute(PayloadAttribute.class)) {\n            invertState.payloadAttribute = stream.getAttribute(PayloadAttribute.class);\n          } else {\n            invertState.payloadAttribute = null;\n          }\n          invertState.attributeSource = stream;\n        }\n\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          final int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          if (posIncr < 0) {\n            throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          if (invertState.position == 0 && posIncr == 0) {\n            throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n          }\n          int position = invertState.position + posIncr;\n          if (position > 0) {\n            // NOTE: confusing: this \"mirrors\" the\n            // position++ we do below\n            position--;\n          } else if (position < 0) {\n            throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n          }\n              \n          // position is legal, we can safely place it in invertState now.\n          // not sure if anything will use invertState after non-aborting exc...\n          invertState.position = position;\n\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < 0 || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n            }\n            if (startOffset < lastStartOffset) {\n              throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                                                 + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            lastStartOffset = startOffset;\n          }\n\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n\n          invertState.length++;\n          invertState.position++;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...'\";\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["bdc1a53703bb3d96d108c76a4321c3fac506b341","b1e9f55cd548ce59174edc2a6e3df3bd8eece7c8","7b0a60ef1cfb11efc4c3ca24b5a894b0eaf65101"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"63241596de245e96a0a3c36c7b03eb92130b81db","date":1398708795,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      // if the field omits norms, the boost cannot be indexed.\n      if (fieldType.omitNorms() && field.boost() != 1.0f) {\n        throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      int lastStartOffset = 0;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = field.tokenStream(docState.analyzer)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        if (invertState.attributeSource != stream) {\n          // EmptyTokenStream gets angry otherwise:\n          invertState.termAttribute = stream.getAttribute(TermToBytesRefAttribute.class);\n          invertState.posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n          invertState.offsetAttribute = stream.addAttribute(OffsetAttribute.class);\n          invertState.payloadAttribute = stream.getAttribute(PayloadAttribute.class);\n          invertState.attributeSource = stream;\n        }\n\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          final int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          if (posIncr < 0) {\n            throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          if (invertState.position == 0 && posIncr == 0) {\n            throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n          }\n          int position = invertState.position + posIncr;\n          if (position > 0) {\n            // NOTE: confusing: this \"mirrors\" the\n            // position++ we do below\n            position--;\n          } else if (position < 0) {\n            throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n          }\n              \n          // position is legal, we can safely place it in invertState now.\n          // not sure if anything will use invertState after non-aborting exc...\n          invertState.position = position;\n\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < 0 || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n            }\n            if (startOffset < lastStartOffset) {\n              throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                                                 + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            lastStartOffset = startOffset;\n          }\n\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n\n          invertState.length++;\n          invertState.position++;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...'\";\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      // if the field omits norms, the boost cannot be indexed.\n      if (fieldType.omitNorms() && field.boost() != 1.0f) {\n        throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      int lastStartOffset = 0;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = field.tokenStream(docState.analyzer)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        if (invertState.attributeSource != stream) {\n          // EmptyTokenStream gets angry otherwise:\n          if (stream.hasAttribute(TermToBytesRefAttribute.class)) {\n            invertState.termAttribute = stream.getAttribute(TermToBytesRefAttribute.class);\n          } else {\n            invertState.termAttribute = null;\n          }\n          invertState.posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n          invertState.offsetAttribute = stream.addAttribute(OffsetAttribute.class);\n          if (stream.hasAttribute(PayloadAttribute.class)) {\n            invertState.payloadAttribute = stream.getAttribute(PayloadAttribute.class);\n          } else {\n            invertState.payloadAttribute = null;\n          }\n          invertState.attributeSource = stream;\n        }\n\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          final int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          if (posIncr < 0) {\n            throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          if (invertState.position == 0 && posIncr == 0) {\n            throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n          }\n          int position = invertState.position + posIncr;\n          if (position > 0) {\n            // NOTE: confusing: this \"mirrors\" the\n            // position++ we do below\n            position--;\n          } else if (position < 0) {\n            throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n          }\n              \n          // position is legal, we can safely place it in invertState now.\n          // not sure if anything will use invertState after non-aborting exc...\n          invertState.position = position;\n\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < 0 || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n            }\n            if (startOffset < lastStartOffset) {\n              throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                                                 + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            lastStartOffset = startOffset;\n          }\n\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n\n          invertState.length++;\n          invertState.position++;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...'\";\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"148086fe4538b1522e0536870a0c90e762a5ab5f","date":1398711337,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      // if the field omits norms, the boost cannot be indexed.\n      if (fieldType.omitNorms() && field.boost() != 1.0f) {\n        throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      int lastStartOffset = 0;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = field.tokenStream(docState.analyzer)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          final int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          if (posIncr < 0) {\n            throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          if (invertState.position == 0 && posIncr == 0) {\n            throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n          }\n          int position = invertState.position + posIncr;\n          if (position > 0) {\n            // NOTE: confusing: this \"mirrors\" the\n            // position++ we do below\n            position--;\n          } else if (position < 0) {\n            throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n          }\n              \n          // position is legal, we can safely place it in invertState now.\n          // not sure if anything will use invertState after non-aborting exc...\n          invertState.position = position;\n\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < 0 || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n            }\n            if (startOffset < lastStartOffset) {\n              throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                                                 + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            lastStartOffset = startOffset;\n          }\n\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n\n          invertState.length++;\n          invertState.position++;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...'\";\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      // if the field omits norms, the boost cannot be indexed.\n      if (fieldType.omitNorms() && field.boost() != 1.0f) {\n        throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      int lastStartOffset = 0;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = field.tokenStream(docState.analyzer)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n\n        if (invertState.attributeSource != stream) {\n          // EmptyTokenStream gets angry otherwise:\n          invertState.termAttribute = stream.getAttribute(TermToBytesRefAttribute.class);\n          invertState.posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n          invertState.offsetAttribute = stream.addAttribute(OffsetAttribute.class);\n          invertState.payloadAttribute = stream.getAttribute(PayloadAttribute.class);\n          invertState.attributeSource = stream;\n        }\n\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          final int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          if (posIncr < 0) {\n            throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          if (invertState.position == 0 && posIncr == 0) {\n            throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n          }\n          int position = invertState.position + posIncr;\n          if (position > 0) {\n            // NOTE: confusing: this \"mirrors\" the\n            // position++ we do below\n            position--;\n          } else if (position < 0) {\n            throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n          }\n              \n          // position is legal, we can safely place it in invertState now.\n          // not sure if anything will use invertState after non-aborting exc...\n          invertState.position = position;\n\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < 0 || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n            }\n            if (startOffset < lastStartOffset) {\n              throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                                                 + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            lastStartOffset = startOffset;\n          }\n\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n\n          invertState.length++;\n          invertState.position++;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...'\";\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3394716f52b34ab259ad5247e7595d9f9db6e935","date":1398791921,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"/dev/null","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      // if the field omits norms, the boost cannot be indexed.\n      if (fieldType.omitNorms() && field.boost() != 1.0f) {\n        throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      int lastStartOffset = 0;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = field.tokenStream(docState.analyzer)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          final int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          if (posIncr < 0) {\n            throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          if (invertState.position == 0 && posIncr == 0) {\n            throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n          }\n          int position = invertState.position + posIncr;\n          if (position > 0) {\n            // NOTE: confusing: this \"mirrors\" the\n            // position++ we do below\n            position--;\n          } else if (position < 0) {\n            throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n          }\n              \n          // position is legal, we can safely place it in invertState now.\n          // not sure if anything will use invertState after non-aborting exc...\n          invertState.position = position;\n\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < 0 || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n            }\n            if (startOffset < lastStartOffset) {\n              throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                                                 + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            lastStartOffset = startOffset;\n          }\n\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n\n          invertState.length++;\n          invertState.position++;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...'\";\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","date":1398844771,"type":0,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"/dev/null","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      // if the field omits norms, the boost cannot be indexed.\n      if (fieldType.omitNorms() && field.boost() != 1.0f) {\n        throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      int lastStartOffset = 0;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = field.tokenStream(docState.analyzer)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          final int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          if (posIncr < 0) {\n            throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          if (invertState.position == 0 && posIncr == 0) {\n            throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n          }\n          int position = invertState.position + posIncr;\n          if (position > 0) {\n            // NOTE: confusing: this \"mirrors\" the\n            // position++ we do below\n            position--;\n          } else if (position < 0) {\n            throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n          }\n              \n          // position is legal, we can safely place it in invertState now.\n          // not sure if anything will use invertState after non-aborting exc...\n          invertState.position = position;\n\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < 0 || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n            }\n            if (startOffset < lastStartOffset) {\n              throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                                                 + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            lastStartOffset = startOffset;\n          }\n\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n\n          invertState.length++;\n          invertState.position++;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...'\";\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"090a3a6b4b32e55f8fe1eab3359dbe628a208a0c","date":1399054058,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      // if the field omits norms, the boost cannot be indexed.\n      if (fieldType.omitNorms() && field.boost() != 1.0f) {\n        throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      int lastStartOffset = 0;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          final int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          if (posIncr < 0) {\n            throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          if (invertState.position == 0 && posIncr == 0) {\n            throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n          }\n          int position = invertState.position + posIncr;\n          if (position > 0) {\n            // NOTE: confusing: this \"mirrors\" the\n            // position++ we do below\n            position--;\n          } else if (position < 0) {\n            throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n          }\n              \n          // position is legal, we can safely place it in invertState now.\n          // not sure if anything will use invertState after non-aborting exc...\n          invertState.position = position;\n\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < 0 || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n            }\n            if (startOffset < lastStartOffset) {\n              throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                                                 + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            lastStartOffset = startOffset;\n          }\n\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n\n          invertState.length++;\n          invertState.position++;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...'\";\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      // if the field omits norms, the boost cannot be indexed.\n      if (fieldType.omitNorms() && field.boost() != 1.0f) {\n        throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      int lastStartOffset = 0;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = field.tokenStream(docState.analyzer)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          final int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          if (posIncr < 0) {\n            throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          if (invertState.position == 0 && posIncr == 0) {\n            throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n          }\n          int position = invertState.position + posIncr;\n          if (position > 0) {\n            // NOTE: confusing: this \"mirrors\" the\n            // position++ we do below\n            position--;\n          } else if (position < 0) {\n            throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n          }\n              \n          // position is legal, we can safely place it in invertState now.\n          // not sure if anything will use invertState after non-aborting exc...\n          invertState.position = position;\n\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < 0 || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n            }\n            if (startOffset < lastStartOffset) {\n              throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                                                 + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            lastStartOffset = startOffset;\n          }\n\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n\n          invertState.length++;\n          invertState.position++;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...'\";\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bdc1a53703bb3d96d108c76a4321c3fac506b341","date":1400331928,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      int lastStartOffset = 0;\n      int lastPosition = 0;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            }\n            throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            lastStartOffset = startOffset;\n          }\n\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n\n          invertState.length++;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...'\";\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      // if the field omits norms, the boost cannot be indexed.\n      if (fieldType.omitNorms() && field.boost() != 1.0f) {\n        throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      int lastStartOffset = 0;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          final int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          if (posIncr < 0) {\n            throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          if (invertState.position == 0 && posIncr == 0) {\n            throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n          }\n          int position = invertState.position + posIncr;\n          if (position > 0) {\n            // NOTE: confusing: this \"mirrors\" the\n            // position++ we do below\n            position--;\n          } else if (position < 0) {\n            throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n          }\n              \n          // position is legal, we can safely place it in invertState now.\n          // not sure if anything will use invertState after non-aborting exc...\n          invertState.position = position;\n\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < 0 || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n            }\n            if (startOffset < lastStartOffset) {\n              throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                                                 + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            lastStartOffset = startOffset;\n          }\n\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n\n          invertState.length++;\n          invertState.position++;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...'\";\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","bugFix":["52c7e49be259508735752fba88085255014a6ecf"],"bugIntro":["b1e9f55cd548ce59174edc2a6e3df3bd8eece7c8"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"56572ec06f1407c066d6b7399413178b33176cd8","date":1400495675,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      int lastStartOffset = 0;\n      int lastPosition = 0;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            }\n            throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            lastStartOffset = startOffset;\n          }\n\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n\n          invertState.length++;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...'\";\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      // if the field omits norms, the boost cannot be indexed.\n      if (fieldType.omitNorms() && field.boost() != 1.0f) {\n        throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      int lastStartOffset = 0;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          final int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          if (posIncr < 0) {\n            throw new IllegalArgumentException(\"position increment must be >=0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          if (invertState.position == 0 && posIncr == 0) {\n            throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n          }\n          int position = invertState.position + posIncr;\n          if (position > 0) {\n            // NOTE: confusing: this \"mirrors\" the\n            // position++ we do below\n            position--;\n          } else if (position < 0) {\n            throw new IllegalArgumentException(\"position overflow for field '\" + field.name() + \"'\");\n          }\n              \n          // position is legal, we can safely place it in invertState now.\n          // not sure if anything will use invertState after non-aborting exc...\n          invertState.position = position;\n\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < 0 || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \" for field '\" + field.name() + \"'\");\n            }\n            if (startOffset < lastStartOffset) {\n              throw new IllegalArgumentException(\"offsets must not go backwards startOffset=\" \n                                                 + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            lastStartOffset = startOffset;\n          }\n\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n\n          invertState.length++;\n          invertState.position++;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...'\";\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5aec33329f2e1af95159eee0f97947f322fb3e22","date":1401366505,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      int lastStartOffset = 0;\n      int lastPosition = 0;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            }\n            throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            lastStartOffset = startOffset;\n          }\n\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n\n          invertState.length++;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg, e);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      int lastStartOffset = 0;\n      int lastPosition = 0;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            }\n            throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            lastStartOffset = startOffset;\n          }\n\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n\n          invertState.length++;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...'\";\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b1e9f55cd548ce59174edc2a6e3df3bd8eece7c8","date":1412000835,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            }\n            throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n\n          invertState.length++;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg, e);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      int lastStartOffset = 0;\n      int lastPosition = 0;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            }\n            throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            lastStartOffset = startOffset;\n          }\n\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n\n          invertState.length++;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg, e);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","bugFix":["bdc1a53703bb3d96d108c76a4321c3fac506b341","52c7e49be259508735752fba88085255014a6ecf"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d9a47902d6207303f5ed3e7aaca62ca33433af66","date":1412435312,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            }\n            throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n\n          invertState.length++;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg, e);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      int lastStartOffset = 0;\n      int lastPosition = 0;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            }\n            throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            lastStartOffset = startOffset;\n          }\n\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n\n          invertState.length++;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg, e);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7b0a60ef1cfb11efc4c3ca24b5a894b0eaf65101","date":1412673344,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            }\n            throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg, e);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            }\n            throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n\n          invertState.length++;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg, e);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","bugFix":["52c7e49be259508735752fba88085255014a6ecf"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"55980207f1977bd1463465de1659b821347e2fa8","date":1413336386,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            }\n            throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg, e);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            }\n            throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n\n          invertState.length++;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg, e);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3184874f7f3aca850248483485b4995343066875","date":1413876758,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            }\n            throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg, e);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            }\n            throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg, e);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a22eafe3f72a4c2945eaad9547e6c78816978f4","date":1413956657,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            }\n            throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg, e);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            }\n            throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg, e);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eac6ccb51c439bec7f67cb0e299d3cb77b62b87e","date":1415435053,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            }\n            throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg, e);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            }\n            throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg, e);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9299079153fd7895bf3cf6835cf7019af2ba89b3","date":1417813477,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            }\n            throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw new AbortingException(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean aborting = false;\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            }\n            throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          aborting = true;\n          termsHashPerField.add();\n          aborting = false;\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } catch (MaxBytesLengthExceededException e) {\n        aborting = false;\n        byte[] prefix = new byte[30];\n        BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n        String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n        if (docState.infoStream.isEnabled(\"IW\")) {\n          docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n        }\n        // Document will be deleted above:\n        throw new IllegalArgumentException(msg, e);\n      } finally {\n        if (succeededInProcessingField == false && aborting) {\n          docState.docWriter.setAborting();\n        }\n\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9d2879042d8cb5738afcc7dc5f8f11f9d3006642","date":1418137170,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            }\n            throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            }\n            throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw new AbortingException(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1","date":1419400138,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            }\n            throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            }\n            throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, its already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"35fb92999b9df7df2ce2b35b83a044cbede61f45","date":1429037650,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            }\n            throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"20516936173630d2dead65d176e1ee649d6162a7","date":1475573241,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1967bed916cc89da82a1c2085f27976da6d08cbd","date":1475588750,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position increments (and gaps) must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"762c80e29fe0c3bb83aabe2e64af6379273cec7b","date":1484347562,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n        CharTermAttribute termAtt = tokenStream.getAttribute(CharTermAttribute.class);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","bugFix":null,"bugIntro":["14162b1f8f2266547b5e1059f37c2efcf8981ea2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"507e7decdf00981d09a74632ea30299a4ce6ba72","date":1484600874,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n        CharTermAttribute termAtt = tokenStream.getAttribute(CharTermAttribute.class);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      // only bother checking offsets if something will consume them.\n      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.\n      final boolean checkOffsets = indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;\n\n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          if (checkOffsets) {\n            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n              throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                                 + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n            }\n            invertState.lastStartOffset = startOffset;\n          }\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"acd9883560fd89e6448b2b447302fe543040cd4f","date":1488478696,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n        CharTermAttribute termAtt = tokenStream.getAttribute(CharTermAttribute.class);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n        CharTermAttribute termAtt = tokenStream.getAttribute(CharTermAttribute.class);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n\n      invertState.boost *= field.boost();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e72e3ade782716457071fee4033f18689acc4c4f","date":1496770651,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n        CharTermAttribute termAtt = tokenStream.getAttribute(CharTermAttribute.class);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n        CharTermAttribute termAtt = tokenStream.getAttribute(CharTermAttribute.class);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1d6a8624f79f6c119e91fd7c88895145bc6e65cf","date":1496864636,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n        CharTermAttribute termAtt = tokenStream.getAttribute(CharTermAttribute.class);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          try {\n            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          } catch (ArithmeticException ae) {\n            throw new IllegalArgumentException(\"too many tokens for field \\\"\" + field.name() + \"\\\"\");\n          }\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n        CharTermAttribute termAtt = tokenStream.getAttribute(CharTermAttribute.class);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f344bb33ca91f48e99c061980115b46fa84fc8f5","date":1496903283,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n        CharTermAttribute termAtt = tokenStream.getAttribute(CharTermAttribute.class);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          try {\n            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          } catch (ArithmeticException ae) {\n            throw new IllegalArgumentException(\"too many tokens for field \\\"\" + field.name() + \"\\\"\");\n          }\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n        CharTermAttribute termAtt = tokenStream.getAttribute(CharTermAttribute.class);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","date":1498028748,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n        CharTermAttribute termAtt = tokenStream.getAttribute(CharTermAttribute.class);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          try {\n            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          } catch (ArithmeticException ae) {\n            throw new IllegalArgumentException(\"too many tokens for field \\\"\" + field.name() + \"\\\"\");\n          }\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n        CharTermAttribute termAtt = tokenStream.getAttribute(CharTermAttribute.class);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n        CharTermAttribute termAtt = tokenStream.getAttribute(CharTermAttribute.class);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          try {\n            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          } catch (ArithmeticException ae) {\n            throw new IllegalArgumentException(\"too many tokens for field \\\"\" + field.name() + \"\\\"\");\n          }\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n        CharTermAttribute termAtt = tokenStream.getAttribute(CharTermAttribute.class);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          invertState.length++;\n          if (invertState.length < 0) {\n            throw new IllegalArgumentException(\"too many tokens in field '\" + field.name() + \"'\");\n          }\n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"14162b1f8f2266547b5e1059f37c2efcf8981ea2","date":1504885152,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          try {\n            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          } catch (ArithmeticException ae) {\n            throw new IllegalArgumentException(\"too many tokens for field \\\"\" + field.name() + \"\\\"\");\n          }\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n        CharTermAttribute termAtt = tokenStream.getAttribute(CharTermAttribute.class);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          try {\n            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          } catch (ArithmeticException ae) {\n            throw new IllegalArgumentException(\"too many tokens for field \\\"\" + field.name() + \"\\\"\");\n          }\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","bugFix":["762c80e29fe0c3bb83aabe2e64af6379273cec7b"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dd68c90c39731aaa564d6995e5dd4a4c2388e13e","date":1504887539,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          try {\n            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          } catch (ArithmeticException ae) {\n            throw new IllegalArgumentException(\"too many tokens for field \\\"\" + field.name() + \"\\\"\");\n          }\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n        CharTermAttribute termAtt = tokenStream.getAttribute(CharTermAttribute.class);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          try {\n            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          } catch (ArithmeticException ae) {\n            throw new IllegalArgumentException(\"too many tokens for field \\\"\" + field.name() + \"\\\"\");\n          }\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"685bd38810c206c93e9058f3c2cfa9827c086c27","date":1505751821,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          try {\n            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          } catch (ArithmeticException ae) {\n            throw new IllegalArgumentException(\"too many tokens for field \\\"\" + field.name() + \"\\\"\");\n          }\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n        CharTermAttribute termAtt = tokenStream.getAttribute(CharTermAttribute.class);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          try {\n            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          } catch (ArithmeticException ae) {\n            throw new IllegalArgumentException(\"too many tokens for field \\\"\" + field.name() + \"\\\"\");\n          }\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"845b760a99e5f369fcd0a5d723a87b8def6a3f56","date":1521117993,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          try {\n            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          } catch (ArithmeticException ae) {\n            throw new IllegalArgumentException(\"too many tokens for field \\\"\" + field.name() + \"\\\"\");\n          }\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            docWriter.onAbortingException(th);\n            throw th;\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException, AbortingException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          try {\n            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          } catch (ArithmeticException ae) {\n            throw new IllegalArgumentException(\"too many tokens for field \\\"\" + field.name() + \"\\\"\");\n          }\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3cc3fa1ecad75b99ec55169e44628808f9866ad","date":1592311545,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          try {\n            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          } catch (ArithmeticException ae) {\n            throw new IllegalArgumentException(\"too many tokens for field \\\"\" + field.name() + \"\\\"\");\n          }\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add(invertState.termAttribute.getBytesRef(), docState.docID);\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            docWriter.onAbortingException(th);\n            throw th;\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          try {\n            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          } catch (ArithmeticException ae) {\n            throw new IllegalArgumentException(\"too many tokens for field \\\"\" + field.name() + \"\\\"\");\n          }\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add();\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            docWriter.onAbortingException(th);\n            throw th;\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f97270426d92300e08ac1bd1a4ef499ae02e88b7","date":1592503330,"type":5,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(int,IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(int docID, IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          try {\n            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          } catch (ArithmeticException ae) {\n            throw new IllegalArgumentException(\"too many tokens for field \\\"\" + field.name() + \"\\\"\");\n          }\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add(invertState.termAttribute.getBytesRef(), docID);\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            docWriter.onAbortingException(th);\n            throw th;\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          try {\n            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          } catch (ArithmeticException ae) {\n            throw new IllegalArgumentException(\"too many tokens for field \\\"\" + field.name() + \"\\\"\");\n          }\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add(invertState.termAttribute.getBytesRef(), docState.docID);\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            docWriter.onAbortingException(th);\n            throw th;\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"bdc1a53703bb3d96d108c76a4321c3fac506b341":["090a3a6b4b32e55f8fe1eab3359dbe628a208a0c"],"845b760a99e5f369fcd0a5d723a87b8def6a3f56":["dd68c90c39731aaa564d6995e5dd4a4c2388e13e"],"1967bed916cc89da82a1c2085f27976da6d08cbd":["35fb92999b9df7df2ce2b35b83a044cbede61f45","20516936173630d2dead65d176e1ee649d6162a7"],"1d6a8624f79f6c119e91fd7c88895145bc6e65cf":["e72e3ade782716457071fee4033f18689acc4c4f"],"20516936173630d2dead65d176e1ee649d6162a7":["35fb92999b9df7df2ce2b35b83a044cbede61f45"],"56572ec06f1407c066d6b7399413178b33176cd8":["090a3a6b4b32e55f8fe1eab3359dbe628a208a0c","bdc1a53703bb3d96d108c76a4321c3fac506b341"],"148086fe4538b1522e0536870a0c90e762a5ab5f":["63241596de245e96a0a3c36c7b03eb92130b81db"],"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1":["9d2879042d8cb5738afcc7dc5f8f11f9d3006642"],"35fb92999b9df7df2ce2b35b83a044cbede61f45":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1"],"e72e3ade782716457071fee4033f18689acc4c4f":["acd9883560fd89e6448b2b447302fe543040cd4f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"eac6ccb51c439bec7f67cb0e299d3cb77b62b87e":["3184874f7f3aca850248483485b4995343066875"],"d3cc3fa1ecad75b99ec55169e44628808f9866ad":["845b760a99e5f369fcd0a5d723a87b8def6a3f56"],"5aec33329f2e1af95159eee0f97947f322fb3e22":["bdc1a53703bb3d96d108c76a4321c3fac506b341"],"55980207f1977bd1463465de1659b821347e2fa8":["d9a47902d6207303f5ed3e7aaca62ca33433af66","7b0a60ef1cfb11efc4c3ca24b5a894b0eaf65101"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3394716f52b34ab259ad5247e7595d9f9db6e935"],"d9a47902d6207303f5ed3e7aaca62ca33433af66":["5aec33329f2e1af95159eee0f97947f322fb3e22","b1e9f55cd548ce59174edc2a6e3df3bd8eece7c8"],"f344bb33ca91f48e99c061980115b46fa84fc8f5":["acd9883560fd89e6448b2b447302fe543040cd4f","1d6a8624f79f6c119e91fd7c88895145bc6e65cf"],"507e7decdf00981d09a74632ea30299a4ce6ba72":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","762c80e29fe0c3bb83aabe2e64af6379273cec7b"],"3184874f7f3aca850248483485b4995343066875":["7b0a60ef1cfb11efc4c3ca24b5a894b0eaf65101"],"28288370235ed02234a64753cdbf0c6ec096304a":["acd9883560fd89e6448b2b447302fe543040cd4f","f344bb33ca91f48e99c061980115b46fa84fc8f5"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["35fb92999b9df7df2ce2b35b83a044cbede61f45","1967bed916cc89da82a1c2085f27976da6d08cbd"],"3394716f52b34ab259ad5247e7595d9f9db6e935":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","148086fe4538b1522e0536870a0c90e762a5ab5f"],"685bd38810c206c93e9058f3c2cfa9827c086c27":["2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","dd68c90c39731aaa564d6995e5dd4a4c2388e13e"],"0a22eafe3f72a4c2945eaad9547e6c78816978f4":["55980207f1977bd1463465de1659b821347e2fa8","3184874f7f3aca850248483485b4995343066875"],"63241596de245e96a0a3c36c7b03eb92130b81db":["52c7e49be259508735752fba88085255014a6ecf"],"762c80e29fe0c3bb83aabe2e64af6379273cec7b":["1967bed916cc89da82a1c2085f27976da6d08cbd"],"9d2879042d8cb5738afcc7dc5f8f11f9d3006642":["9299079153fd7895bf3cf6835cf7019af2ba89b3"],"14162b1f8f2266547b5e1059f37c2efcf8981ea2":["28288370235ed02234a64753cdbf0c6ec096304a"],"9299079153fd7895bf3cf6835cf7019af2ba89b3":["eac6ccb51c439bec7f67cb0e299d3cb77b62b87e"],"b1e9f55cd548ce59174edc2a6e3df3bd8eece7c8":["5aec33329f2e1af95159eee0f97947f322fb3e22"],"090a3a6b4b32e55f8fe1eab3359dbe628a208a0c":["3394716f52b34ab259ad5247e7595d9f9db6e935"],"dd68c90c39731aaa564d6995e5dd4a4c2388e13e":["28288370235ed02234a64753cdbf0c6ec096304a","14162b1f8f2266547b5e1059f37c2efcf8981ea2"],"7b0a60ef1cfb11efc4c3ca24b5a894b0eaf65101":["b1e9f55cd548ce59174edc2a6e3df3bd8eece7c8"],"f97270426d92300e08ac1bd1a4ef499ae02e88b7":["d3cc3fa1ecad75b99ec55169e44628808f9866ad"],"acd9883560fd89e6448b2b447302fe543040cd4f":["762c80e29fe0c3bb83aabe2e64af6379273cec7b"],"52c7e49be259508735752fba88085255014a6ecf":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":["acd9883560fd89e6448b2b447302fe543040cd4f","f344bb33ca91f48e99c061980115b46fa84fc8f5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["f97270426d92300e08ac1bd1a4ef499ae02e88b7"]},"commit2Childs":{"bdc1a53703bb3d96d108c76a4321c3fac506b341":["56572ec06f1407c066d6b7399413178b33176cd8","5aec33329f2e1af95159eee0f97947f322fb3e22"],"845b760a99e5f369fcd0a5d723a87b8def6a3f56":["d3cc3fa1ecad75b99ec55169e44628808f9866ad"],"1967bed916cc89da82a1c2085f27976da6d08cbd":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","762c80e29fe0c3bb83aabe2e64af6379273cec7b"],"1d6a8624f79f6c119e91fd7c88895145bc6e65cf":["f344bb33ca91f48e99c061980115b46fa84fc8f5"],"20516936173630d2dead65d176e1ee649d6162a7":["1967bed916cc89da82a1c2085f27976da6d08cbd"],"56572ec06f1407c066d6b7399413178b33176cd8":[],"148086fe4538b1522e0536870a0c90e762a5ab5f":["3394716f52b34ab259ad5247e7595d9f9db6e935"],"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1":["35fb92999b9df7df2ce2b35b83a044cbede61f45"],"35fb92999b9df7df2ce2b35b83a044cbede61f45":["1967bed916cc89da82a1c2085f27976da6d08cbd","20516936173630d2dead65d176e1ee649d6162a7","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"e72e3ade782716457071fee4033f18689acc4c4f":["1d6a8624f79f6c119e91fd7c88895145bc6e65cf"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","3394716f52b34ab259ad5247e7595d9f9db6e935","52c7e49be259508735752fba88085255014a6ecf"],"eac6ccb51c439bec7f67cb0e299d3cb77b62b87e":["9299079153fd7895bf3cf6835cf7019af2ba89b3"],"d3cc3fa1ecad75b99ec55169e44628808f9866ad":["f97270426d92300e08ac1bd1a4ef499ae02e88b7"],"5aec33329f2e1af95159eee0f97947f322fb3e22":["d9a47902d6207303f5ed3e7aaca62ca33433af66","b1e9f55cd548ce59174edc2a6e3df3bd8eece7c8"],"55980207f1977bd1463465de1659b821347e2fa8":["0a22eafe3f72a4c2945eaad9547e6c78816978f4"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":[],"d9a47902d6207303f5ed3e7aaca62ca33433af66":["55980207f1977bd1463465de1659b821347e2fa8"],"f344bb33ca91f48e99c061980115b46fa84fc8f5":["28288370235ed02234a64753cdbf0c6ec096304a","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"507e7decdf00981d09a74632ea30299a4ce6ba72":[],"3184874f7f3aca850248483485b4995343066875":["eac6ccb51c439bec7f67cb0e299d3cb77b62b87e","0a22eafe3f72a4c2945eaad9547e6c78816978f4"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["507e7decdf00981d09a74632ea30299a4ce6ba72"],"28288370235ed02234a64753cdbf0c6ec096304a":["14162b1f8f2266547b5e1059f37c2efcf8981ea2","dd68c90c39731aaa564d6995e5dd4a4c2388e13e"],"3394716f52b34ab259ad5247e7595d9f9db6e935":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","090a3a6b4b32e55f8fe1eab3359dbe628a208a0c"],"685bd38810c206c93e9058f3c2cfa9827c086c27":[],"0a22eafe3f72a4c2945eaad9547e6c78816978f4":[],"63241596de245e96a0a3c36c7b03eb92130b81db":["148086fe4538b1522e0536870a0c90e762a5ab5f"],"9d2879042d8cb5738afcc7dc5f8f11f9d3006642":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1"],"762c80e29fe0c3bb83aabe2e64af6379273cec7b":["507e7decdf00981d09a74632ea30299a4ce6ba72","acd9883560fd89e6448b2b447302fe543040cd4f"],"14162b1f8f2266547b5e1059f37c2efcf8981ea2":["dd68c90c39731aaa564d6995e5dd4a4c2388e13e"],"b1e9f55cd548ce59174edc2a6e3df3bd8eece7c8":["d9a47902d6207303f5ed3e7aaca62ca33433af66","7b0a60ef1cfb11efc4c3ca24b5a894b0eaf65101"],"9299079153fd7895bf3cf6835cf7019af2ba89b3":["9d2879042d8cb5738afcc7dc5f8f11f9d3006642"],"090a3a6b4b32e55f8fe1eab3359dbe628a208a0c":["bdc1a53703bb3d96d108c76a4321c3fac506b341","56572ec06f1407c066d6b7399413178b33176cd8"],"dd68c90c39731aaa564d6995e5dd4a4c2388e13e":["845b760a99e5f369fcd0a5d723a87b8def6a3f56","685bd38810c206c93e9058f3c2cfa9827c086c27"],"7b0a60ef1cfb11efc4c3ca24b5a894b0eaf65101":["55980207f1977bd1463465de1659b821347e2fa8","3184874f7f3aca850248483485b4995343066875"],"acd9883560fd89e6448b2b447302fe543040cd4f":["e72e3ade782716457071fee4033f18689acc4c4f","f344bb33ca91f48e99c061980115b46fa84fc8f5","28288370235ed02234a64753cdbf0c6ec096304a","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"52c7e49be259508735752fba88085255014a6ecf":["63241596de245e96a0a3c36c7b03eb92130b81db"],"f97270426d92300e08ac1bd1a4ef499ae02e88b7":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":["685bd38810c206c93e9058f3c2cfa9827c086c27"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["56572ec06f1407c066d6b7399413178b33176cd8","c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","507e7decdf00981d09a74632ea30299a4ce6ba72","685bd38810c206c93e9058f3c2cfa9827c086c27","0a22eafe3f72a4c2945eaad9547e6c78816978f4","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}