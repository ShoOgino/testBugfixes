{"path":"lucene/core/src/test/org/apache/lucene/search/TestTopDocsCollector#testRealisticConcurrentMinimumScore().mjava","commits":[{"id":"9d8f3dc907fb89a6215dc071ea011dd6cd395563","date":1569926650,"type":0,"author":"jimczi","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTopDocsCollector#testRealisticConcurrentMinimumScore().mjava","pathOld":"/dev/null","sourceNew":"  public void testRealisticConcurrentMinimumScore() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    try (LineFileDocs docs = new LineFileDocs(random())) {\n      int numDocs = atLeast(100);\n      for (int i = 0; i < numDocs; i++) {\n        writer.addDocument(docs.nextDoc());\n      }\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    final IndexSearcher s = newSearcher(reader);\n    Terms terms = MultiTerms.getTerms(reader, \"body\");\n    int termCount = 0;\n    TermsEnum termsEnum = terms.iterator();\n    while(termsEnum.next() != null) {\n      termCount++;\n    }\n    assertTrue(termCount > 0);\n\n    // Target ~10 terms to search:\n    double chance = 10.0 / termCount;\n    termsEnum = terms.iterator();\n    while(termsEnum.next() != null) {\n      if (random().nextDouble() <= chance) {\n        BytesRef term = BytesRef.deepCopyOf(termsEnum.term());\n        Query query = new TermQuery(new Term(\"body\", term));\n\n        TopDocsCollector collector = doSearchWithThreshold(5, 0, query, reader);\n        TopDocs tdc = doConcurrentSearchWithThreshold(5, 0, query, reader);\n        TopDocs tdc2 = collector.topDocs();\n\n        CheckHits.checkEqual(query, tdc.scoreDocs, tdc2.scoreDocs);\n      }\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3fc6bc0fbd97ca2f45fd376fa8e733728e665c4f","date":1571662992,"type":1,"author":"Jim Ferenczi","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTopDocsCollector#testRealisticConcurrentMinimumScore().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTopDocsCollector#testGlobalScore().mjava","sourceNew":"  public void testRealisticConcurrentMinimumScore() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    try (LineFileDocs docs = new LineFileDocs(random())) {\n      int numDocs = atLeast(100);\n      for (int i = 0; i < numDocs; i++) {\n        writer.addDocument(docs.nextDoc());\n      }\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    final IndexSearcher s = newSearcher(reader);\n    Terms terms = MultiTerms.getTerms(reader, \"body\");\n    int termCount = 0;\n    TermsEnum termsEnum = terms.iterator();\n    while(termsEnum.next() != null) {\n      termCount++;\n    }\n    assertTrue(termCount > 0);\n\n    // Target ~10 terms to search:\n    double chance = 10.0 / termCount;\n    termsEnum = terms.iterator();\n    while(termsEnum.next() != null) {\n      if (random().nextDouble() <= chance) {\n        BytesRef term = BytesRef.deepCopyOf(termsEnum.term());\n        Query query = new TermQuery(new Term(\"body\", term));\n\n        TopDocsCollector collector = doSearchWithThreshold(5, 0, query, reader);\n        TopDocs tdc = doConcurrentSearchWithThreshold(5, 0, query, reader);\n        TopDocs tdc2 = collector.topDocs();\n\n        CheckHits.checkEqual(query, tdc.scoreDocs, tdc2.scoreDocs);\n      }\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testGlobalScore() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    try (LineFileDocs docs = new LineFileDocs(random())) {\n      int numDocs = atLeast(100);\n      for (int i = 0; i < numDocs; i++) {\n        writer.addDocument(docs.nextDoc());\n      }\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    final IndexSearcher s = newSearcher(reader);\n    Terms terms = MultiTerms.getTerms(reader, \"body\");\n    int termCount = 0;\n    TermsEnum termsEnum = terms.iterator();\n    while(termsEnum.next() != null) {\n      termCount++;\n    }\n    assertTrue(termCount > 0);\n\n    // Target ~10 terms to search:\n    double chance = 10.0 / termCount;\n    termsEnum = terms.iterator();\n    while(termsEnum.next() != null) {\n      if (random().nextDouble() <= chance) {\n        BytesRef term = BytesRef.deepCopyOf(termsEnum.term());\n        Query query = new TermQuery(new Term(\"body\", term));\n\n        TopDocsCollector collector = doSearchWithThreshold(5, 10);\n        TopDocs tdc = doConcurrentSearchWithThreshold(5, 10, reader);\n        TopDocs tdc2 = collector.topDocs();\n\n        CheckHits.checkEqual(query, tdc.scoreDocs, tdc2.scoreDocs);\n      }\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"30c21918ef3ad23e254f39cb0cd6c86851fcb993","date":1580123310,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTopDocsCollector#testRealisticConcurrentMinimumScore().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTopDocsCollector#testRealisticConcurrentMinimumScore().mjava","sourceNew":"  public void testRealisticConcurrentMinimumScore() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    try (LineFileDocs docs = new LineFileDocs(random())) {\n      int numDocs = atLeast(100);\n      for (int i = 0; i < numDocs; i++) {\n        writer.addDocument(docs.nextDoc());\n      }\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    Terms terms = MultiTerms.getTerms(reader, \"body\");\n    int termCount = 0;\n    TermsEnum termsEnum = terms.iterator();\n    while(termsEnum.next() != null) {\n      termCount++;\n    }\n    assertTrue(termCount > 0);\n\n    // Target ~10 terms to search:\n    double chance = 10.0 / termCount;\n    termsEnum = terms.iterator();\n    while(termsEnum.next() != null) {\n      if (random().nextDouble() <= chance) {\n        BytesRef term = BytesRef.deepCopyOf(termsEnum.term());\n        Query query = new TermQuery(new Term(\"body\", term));\n\n        TopDocsCollector<ScoreDoc> collector = doSearchWithThreshold(5, 0, query, reader);\n        TopDocs tdc = doConcurrentSearchWithThreshold(5, 0, query, reader);\n        TopDocs tdc2 = collector.topDocs();\n\n        CheckHits.checkEqual(query, tdc.scoreDocs, tdc2.scoreDocs);\n      }\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRealisticConcurrentMinimumScore() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    try (LineFileDocs docs = new LineFileDocs(random())) {\n      int numDocs = atLeast(100);\n      for (int i = 0; i < numDocs; i++) {\n        writer.addDocument(docs.nextDoc());\n      }\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    final IndexSearcher s = newSearcher(reader);\n    Terms terms = MultiTerms.getTerms(reader, \"body\");\n    int termCount = 0;\n    TermsEnum termsEnum = terms.iterator();\n    while(termsEnum.next() != null) {\n      termCount++;\n    }\n    assertTrue(termCount > 0);\n\n    // Target ~10 terms to search:\n    double chance = 10.0 / termCount;\n    termsEnum = terms.iterator();\n    while(termsEnum.next() != null) {\n      if (random().nextDouble() <= chance) {\n        BytesRef term = BytesRef.deepCopyOf(termsEnum.term());\n        Query query = new TermQuery(new Term(\"body\", term));\n\n        TopDocsCollector collector = doSearchWithThreshold(5, 0, query, reader);\n        TopDocs tdc = doConcurrentSearchWithThreshold(5, 0, query, reader);\n        TopDocs tdc2 = collector.topDocs();\n\n        CheckHits.checkEqual(query, tdc.scoreDocs, tdc2.scoreDocs);\n      }\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3fc6bc0fbd97ca2f45fd376fa8e733728e665c4f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","9d8f3dc907fb89a6215dc071ea011dd6cd395563"],"9d8f3dc907fb89a6215dc071ea011dd6cd395563":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"30c21918ef3ad23e254f39cb0cd6c86851fcb993":["3fc6bc0fbd97ca2f45fd376fa8e733728e665c4f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["30c21918ef3ad23e254f39cb0cd6c86851fcb993"]},"commit2Childs":{"3fc6bc0fbd97ca2f45fd376fa8e733728e665c4f":["30c21918ef3ad23e254f39cb0cd6c86851fcb993"],"9d8f3dc907fb89a6215dc071ea011dd6cd395563":["3fc6bc0fbd97ca2f45fd376fa8e733728e665c4f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3fc6bc0fbd97ca2f45fd376fa8e733728e665c4f","9d8f3dc907fb89a6215dc071ea011dd6cd395563"],"30c21918ef3ad23e254f39cb0cd6c86851fcb993":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}