{"path":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","commits":[{"id":"60596f28be69b10c37a56a303c2dbea07b2ca4ba","date":1425060541,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestLRUFilterCache#testFineGrainedStats().mjava","sourceNew":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<Object, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUQueryCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","sourceOld":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<Object, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUFilterCache filterCache = new LRUFilterCache(2, 10000000) {\n      @Override\n      protected void onHit(Object readerCoreKey, Filter filter) {\n        super.onHit(readerCoreKey, filter);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Filter filter) {\n        super.onMiss(readerCoreKey, filter);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onFilterCache(Filter filter, long ramBytesUsed) {\n        super.onFilterCache(filter, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onFilterEviction(Filter filter, long ramBytesUsed) {\n        super.onFilterEviction(filter, ramBytesUsed);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Filter filter = new QueryWrapperFilter(new TermQuery(new Term(\"color\", \"red\")));\n    final Filter filter2 = new QueryWrapperFilter(new TermQuery(new Term(\"color\", \"blue\")));\n    final Filter filter3 = new QueryWrapperFilter(new TermQuery(new Term(\"color\", \"green\")));\n\n    // search on searcher1\n    Filter cached = filterCache.doCache(filter, FilterCachingPolicy.ALWAYS_CACHE);\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(cached), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    cached = filterCache.doCache(filter2, FilterCachingPolicy.ALWAYS_CACHE);\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(cached), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    cached = filterCache.doCache(filter3, FilterCachingPolicy.ALWAYS_CACHE);\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(cached), 1);\n    }\n    assertEquals(segmentCount1, filterCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(filterCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUFilterCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(filterCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(filterCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    filterCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":0,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","pathOld":"/dev/null","sourceNew":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<Object, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUQueryCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aac61ee5b4492f174e60bd54939aba9539906edf","date":1461245473,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","sourceNew":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<Object, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000, context -> true) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUQueryCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","sourceOld":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<Object, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUQueryCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d211216c83f01894810543d1c107160a9ae3650b","date":1488289605,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","sourceNew":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<IndexReader.CacheKey, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000, context -> true) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUQueryCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","sourceOld":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<Object, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000, context -> true) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUQueryCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"393f9042b18c4281cb212ceb8dc71c31a2220f46","date":1494510970,"type":3,"author":"ChristophKaser","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","sourceNew":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<IndexReader.CacheKey, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000, context -> true) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        assertNotNull(\"cached query is null\", query);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        assertNotNull(\"evicted query is null\", query);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUQueryCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","sourceOld":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<IndexReader.CacheKey, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000, context -> true) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUQueryCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"360b3962efc30aa8b2c39c3087aa36069674bbe7","date":1494557674,"type":3,"author":"Cao Manh Dat","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","sourceNew":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<IndexReader.CacheKey, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000, context -> true) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        assertNotNull(\"cached query is null\", query);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        assertNotNull(\"evicted query is null\", query);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUQueryCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","sourceOld":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<IndexReader.CacheKey, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000, context -> true) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUQueryCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e9017cf144952056066919f1ebc7897ff9bd71b1","date":1496757600,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","sourceNew":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<IndexReader.CacheKey, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000, context -> true) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        assertNotNull(\"cached query is null\", query);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        assertNotNull(\"evicted query is null\", query);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUQueryCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","sourceOld":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<IndexReader.CacheKey, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000, context -> true) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUQueryCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c7732a106554be0db3e03ac5211e46f6e0c285b8","date":1511975378,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","sourceNew":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<IndexReader.CacheKey, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000, context -> true, Float.POSITIVE_INFINITY) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        assertNotNull(\"cached query is null\", query);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        assertNotNull(\"evicted query is null\", query);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUQueryCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","sourceOld":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<IndexReader.CacheKey, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000, context -> true) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        assertNotNull(\"cached query is null\", query);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        assertNotNull(\"evicted query is null\", query);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUQueryCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1aad05eeff7818b0833c02ac6b743aa72054963b","date":1512093122,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","sourceNew":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<IndexReader.CacheKey, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000, context -> true, Float.POSITIVE_INFINITY) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        assertNotNull(\"cached query is null\", query);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        assertNotNull(\"evicted query is null\", query);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUQueryCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","sourceOld":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<IndexReader.CacheKey, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000, context -> true) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        assertNotNull(\"cached query is null\", query);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        assertNotNull(\"evicted query is null\", query);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUQueryCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"789fb338d3c53b4478938723d60f6623e764ca38","date":1521535944,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","sourceNew":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<IndexReader.CacheKey, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000, context -> true) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        assertNotNull(\"cached query is null\", query);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        assertNotNull(\"evicted query is null\", query);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUQueryCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","sourceOld":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<IndexReader.CacheKey, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000, context -> true, Float.POSITIVE_INFINITY) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        assertNotNull(\"cached query is null\", query);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        assertNotNull(\"evicted query is null\", query);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUQueryCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6815b5b5d6334b2245dd7be2f8b6cca949bf7f43","date":1521731438,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","sourceNew":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<IndexReader.CacheKey, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000, context -> true) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        assertNotNull(\"cached query is null\", query);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        assertNotNull(\"evicted query is null\", query);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUQueryCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","sourceOld":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<IndexReader.CacheKey, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000, context -> true, Float.POSITIVE_INFINITY) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        assertNotNull(\"cached query is null\", query);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        assertNotNull(\"evicted query is null\", query);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUQueryCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6c9d00c591703058371b3dc36f4957a6f24ca302","date":1527233410,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","sourceNew":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<IndexReader.CacheKey, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000, context -> true) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        assertNotNull(\"cached query is null\", query);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        assertNotNull(\"evicted query is null\", query);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUQueryCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","sourceOld":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<IndexReader.CacheKey, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000, context -> true) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        assertNotNull(\"cached query is null\", query);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        assertNotNull(\"evicted query is null\", query);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUQueryCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a71ca10e7131e1f01868c80d228f26a855e79dd0","date":1562166223,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","sourceNew":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<IndexReader.CacheKey, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000, context -> true) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        assertNotNull(\"cached query is null\", query);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        assertNotNull(\"evicted query is null\", query);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","sourceOld":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<IndexReader.CacheKey, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000, context -> true) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        assertNotNull(\"cached query is null\", query);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        assertNotNull(\"evicted query is null\", query);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * LRUQueryCache.HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b5754bd6f04f13b67e9575f8b226a0303c31c7d5","date":1573506453,"type":3,"author":"ginger","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testFineGrainedStats().mjava","sourceNew":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<IndexReader.CacheKey, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000, context -> true, 1) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        assertNotNull(\"cached query is null\", query);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        assertNotNull(\"evicted query is null\", query);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","sourceOld":"  public void testFineGrainedStats() throws IOException {\n    Directory dir1 = newDirectory();\n    final RandomIndexWriter w1 = new RandomIndexWriter(random(), dir1);\n    Directory dir2 = newDirectory();\n    final RandomIndexWriter w2 = new RandomIndexWriter(random(), dir2);\n\n    final List<String> colors = Arrays.asList(\"blue\", \"red\", \"green\", \"yellow\");\n\n    Document doc = new Document();\n    StringField f = new StringField(\"color\", \"\", Store.NO);\n    doc.add(f);\n    for (RandomIndexWriter w : Arrays.asList(w1, w2)) {\n      for (int i = 0; i < 10; ++i) {\n        f.setStringValue(RandomPicks.randomFrom(random(), colors));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.getReader().close();\n        }\n      }\n    }\n\n    final DirectoryReader reader1 = w1.getReader();\n    final int segmentCount1 = reader1.leaves().size();\n    final IndexSearcher searcher1 = new IndexSearcher(reader1);\n\n    final DirectoryReader reader2 = w2.getReader();\n    final int segmentCount2 = reader2.leaves().size();\n    final IndexSearcher searcher2 = new IndexSearcher(reader2);\n\n    final Map<IndexReader.CacheKey, Integer> indexId = new HashMap<>();\n    for (LeafReaderContext ctx : reader1.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 1);\n    }\n    for (LeafReaderContext ctx : reader2.leaves()) {\n      indexId.put(ctx.reader().getCoreCacheHelper().getKey(), 2);\n    }\n\n    final AtomicLong hitCount1 = new AtomicLong();\n    final AtomicLong hitCount2 = new AtomicLong();\n    final AtomicLong missCount1 = new AtomicLong();\n    final AtomicLong missCount2 = new AtomicLong();\n\n    final AtomicLong ramBytesUsage = new AtomicLong();\n    final AtomicLong cacheSize = new AtomicLong();\n\n    final LRUQueryCache queryCache = new LRUQueryCache(2, 10000000, context -> true) {\n      @Override\n      protected void onHit(Object readerCoreKey, Query query) {\n        super.onHit(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            hitCount1.incrementAndGet();\n            break;\n          case 2:\n            hitCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onMiss(Object readerCoreKey, Query query) {\n        super.onMiss(readerCoreKey, query);\n        switch(indexId.get(readerCoreKey).intValue()) {\n          case 1:\n            missCount1.incrementAndGet();\n            break;\n          case 2:\n            missCount2.incrementAndGet();\n            break;\n          default:\n            throw new AssertionError();\n        }\n      }\n\n      @Override\n      protected void onQueryCache(Query query, long ramBytesUsed) {\n        super.onQueryCache(query, ramBytesUsed);\n        assertNotNull(\"cached query is null\", query);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n      }\n\n      @Override\n      protected void onQueryEviction(Query query, long ramBytesUsed) {\n        super.onQueryEviction(query, ramBytesUsed);\n        assertNotNull(\"evicted query is null\", query);\n        ramBytesUsage.addAndGet(-ramBytesUsed);\n      }\n\n      @Override\n      protected void onDocIdSetCache(Object readerCoreKey, long ramBytesUsed) {\n        super.onDocIdSetCache(readerCoreKey, ramBytesUsed);\n        ramBytesUsage.addAndGet(ramBytesUsed);\n        cacheSize.incrementAndGet();\n      }\n\n      @Override\n      protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {\n        super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);\n        ramBytesUsage.addAndGet(-sumRamBytesUsed);\n        cacheSize.addAndGet(-numEntries);\n      }\n\n      @Override\n      protected void onClear() {\n        super.onClear();\n        ramBytesUsage.set(0);\n        cacheSize.set(0);\n      }\n    };\n\n    final Query query = new TermQuery(new Term(\"color\", \"red\"));\n    final Query query2 = new TermQuery(new Term(\"color\", \"blue\"));\n    final Query query3 = new TermQuery(new Term(\"color\", \"green\"));\n\n    for (IndexSearcher searcher : Arrays.asList(searcher1, searcher2)) {\n      searcher.setQueryCache(queryCache);\n      searcher.setQueryCachingPolicy(ALWAYS_CACHE);\n    }\n\n    // search on searcher1\n    for (int i = 0; i < 10; ++i) {\n      searcher1.search(new ConstantScoreQuery(query), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(0, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(0, missCount2.longValue());\n\n    // then on searcher2\n    for (int i = 0; i < 20; ++i) {\n      searcher2.search(new ConstantScoreQuery(query2), 1);\n    }\n    assertEquals(9 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // now on searcher1 again to trigger evictions\n    for (int i = 0; i < 30; ++i) {\n      searcher1.search(new ConstantScoreQuery(query3), 1);\n    }\n    assertEquals(segmentCount1, queryCache.getEvictionCount());\n    assertEquals(38 * segmentCount1, hitCount1.longValue());\n    assertEquals(19 * segmentCount2, hitCount2.longValue());\n    assertEquals(2 * segmentCount1, missCount1.longValue());\n    assertEquals(segmentCount2, missCount2.longValue());\n\n    // check that the recomputed stats are the same as those reported by the cache\n    assertEquals(queryCache.ramBytesUsed(), (segmentCount1 + segmentCount2) * HASHTABLE_RAM_BYTES_PER_ENTRY + ramBytesUsage.longValue());\n    assertEquals(queryCache.getCacheSize(), cacheSize.longValue());\n\n    reader1.close();\n    reader2.close();\n    w1.close();\n    w2.close();\n\n    assertEquals(queryCache.ramBytesUsed(), ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    queryCache.clear();\n    assertEquals(0, ramBytesUsage.longValue());\n    assertEquals(0, cacheSize.longValue());\n\n    dir1.close();\n    dir2.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"6815b5b5d6334b2245dd7be2f8b6cca949bf7f43":["1aad05eeff7818b0833c02ac6b743aa72054963b","789fb338d3c53b4478938723d60f6623e764ca38"],"aac61ee5b4492f174e60bd54939aba9539906edf":["60596f28be69b10c37a56a303c2dbea07b2ca4ba"],"6c9d00c591703058371b3dc36f4957a6f24ca302":["6815b5b5d6334b2245dd7be2f8b6cca949bf7f43"],"60596f28be69b10c37a56a303c2dbea07b2ca4ba":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a71ca10e7131e1f01868c80d228f26a855e79dd0":["6c9d00c591703058371b3dc36f4957a6f24ca302"],"b5754bd6f04f13b67e9575f8b226a0303c31c7d5":["a71ca10e7131e1f01868c80d228f26a855e79dd0"],"393f9042b18c4281cb212ceb8dc71c31a2220f46":["d211216c83f01894810543d1c107160a9ae3650b"],"c7732a106554be0db3e03ac5211e46f6e0c285b8":["360b3962efc30aa8b2c39c3087aa36069674bbe7"],"1aad05eeff7818b0833c02ac6b743aa72054963b":["360b3962efc30aa8b2c39c3087aa36069674bbe7","c7732a106554be0db3e03ac5211e46f6e0c285b8"],"e9017cf144952056066919f1ebc7897ff9bd71b1":["d211216c83f01894810543d1c107160a9ae3650b","360b3962efc30aa8b2c39c3087aa36069674bbe7"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","60596f28be69b10c37a56a303c2dbea07b2ca4ba"],"360b3962efc30aa8b2c39c3087aa36069674bbe7":["d211216c83f01894810543d1c107160a9ae3650b","393f9042b18c4281cb212ceb8dc71c31a2220f46"],"d211216c83f01894810543d1c107160a9ae3650b":["aac61ee5b4492f174e60bd54939aba9539906edf"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"789fb338d3c53b4478938723d60f6623e764ca38":["1aad05eeff7818b0833c02ac6b743aa72054963b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b5754bd6f04f13b67e9575f8b226a0303c31c7d5"]},"commit2Childs":{"6815b5b5d6334b2245dd7be2f8b6cca949bf7f43":["6c9d00c591703058371b3dc36f4957a6f24ca302"],"aac61ee5b4492f174e60bd54939aba9539906edf":["d211216c83f01894810543d1c107160a9ae3650b"],"60596f28be69b10c37a56a303c2dbea07b2ca4ba":["aac61ee5b4492f174e60bd54939aba9539906edf","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"6c9d00c591703058371b3dc36f4957a6f24ca302":["a71ca10e7131e1f01868c80d228f26a855e79dd0"],"a71ca10e7131e1f01868c80d228f26a855e79dd0":["b5754bd6f04f13b67e9575f8b226a0303c31c7d5"],"b5754bd6f04f13b67e9575f8b226a0303c31c7d5":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"393f9042b18c4281cb212ceb8dc71c31a2220f46":["360b3962efc30aa8b2c39c3087aa36069674bbe7"],"c7732a106554be0db3e03ac5211e46f6e0c285b8":["1aad05eeff7818b0833c02ac6b743aa72054963b"],"1aad05eeff7818b0833c02ac6b743aa72054963b":["6815b5b5d6334b2245dd7be2f8b6cca949bf7f43","789fb338d3c53b4478938723d60f6623e764ca38"],"e9017cf144952056066919f1ebc7897ff9bd71b1":[],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"360b3962efc30aa8b2c39c3087aa36069674bbe7":["c7732a106554be0db3e03ac5211e46f6e0c285b8","1aad05eeff7818b0833c02ac6b743aa72054963b","e9017cf144952056066919f1ebc7897ff9bd71b1"],"789fb338d3c53b4478938723d60f6623e764ca38":["6815b5b5d6334b2245dd7be2f8b6cca949bf7f43"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["60596f28be69b10c37a56a303c2dbea07b2ca4ba","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"d211216c83f01894810543d1c107160a9ae3650b":["393f9042b18c4281cb212ceb8dc71c31a2220f46","e9017cf144952056066919f1ebc7897ff9bd71b1","360b3962efc30aa8b2c39c3087aa36069674bbe7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["e9017cf144952056066919f1ebc7897ff9bd71b1","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}