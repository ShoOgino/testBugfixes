{"path":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testManySubFiles().mjava","commits":[{"id":"3257de94b910b1c34362d2f90d9407daf63dd68b","date":1412367119,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testManySubFiles().mjava","pathOld":"/dev/null","sourceNew":"  // Make sure we don't somehow use more than 1 descriptor\n  // when reading a CFS with many subs:\n  public void testManySubFiles() throws IOException {\n    final MockDirectoryWrapper dir = newMockFSDirectory(createTempDir(\"CFSManySubFiles\"));\n    \n    final int FILE_COUNT = atLeast(500);\n    \n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      IndexOutput out = dir.createOutput(\"_123.\" + fileIdx, newIOContext(random()));\n      out.writeByte((byte) fileIdx);\n      out.close();\n    }\n    \n    assertEquals(0, dir.getFileHandleCount());\n    \n    SegmentInfo si = newSegmentInfo(dir, \"_123\");\n    si.getCodec().compoundFormat().write(dir, si, Arrays.asList(dir.listAll()), MergeState.CheckAbort.NONE, IOContext.DEFAULT);\n    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);\n    \n    final IndexInput[] ins = new IndexInput[FILE_COUNT];\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      ins[fileIdx] = cfs.openInput(\"file.\" + fileIdx, newIOContext(random()));\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      assertEquals((byte) fileIdx, ins[fileIdx].readByte());\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n    \n    for(int fileIdx=0;fileIdx<FILE_COUNT;fileIdx++) {\n      ins[fileIdx].close();\n    }\n    cfs.close();\n    \n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c761d502ca1f6c5324aa909468ebc4f761c92c1f","date":1412431978,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testManySubFiles().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testManySubFiles().mjava","sourceNew":"  // Make sure we don't somehow use more than 1 descriptor\n  // when reading a CFS with many subs:\n  public void testManySubFiles() throws IOException {\n    final MockDirectoryWrapper dir = newMockFSDirectory(createTempDir(\"CFSManySubFiles\"));\n    \n    final int FILE_COUNT = atLeast(500);\n    \n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      IndexOutput out = dir.createOutput(\"_123.\" + fileIdx, newIOContext(random()));\n      out.writeByte((byte) fileIdx);\n      out.close();\n    }\n    \n    assertEquals(0, dir.getFileHandleCount());\n    \n    SegmentInfo si = newSegmentInfo(dir, \"_123\");\n    si.getCodec().compoundFormat().write(dir, si, Arrays.asList(dir.listAll()), MergeState.CheckAbort.NONE, IOContext.DEFAULT);\n    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);\n    \n    final IndexInput[] ins = new IndexInput[FILE_COUNT];\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      ins[fileIdx] = cfs.openInput(\"_123.\" + fileIdx, newIOContext(random()));\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      assertEquals((byte) fileIdx, ins[fileIdx].readByte());\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n    \n    for(int fileIdx=0;fileIdx<FILE_COUNT;fileIdx++) {\n      ins[fileIdx].close();\n    }\n    cfs.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't somehow use more than 1 descriptor\n  // when reading a CFS with many subs:\n  public void testManySubFiles() throws IOException {\n    final MockDirectoryWrapper dir = newMockFSDirectory(createTempDir(\"CFSManySubFiles\"));\n    \n    final int FILE_COUNT = atLeast(500);\n    \n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      IndexOutput out = dir.createOutput(\"_123.\" + fileIdx, newIOContext(random()));\n      out.writeByte((byte) fileIdx);\n      out.close();\n    }\n    \n    assertEquals(0, dir.getFileHandleCount());\n    \n    SegmentInfo si = newSegmentInfo(dir, \"_123\");\n    si.getCodec().compoundFormat().write(dir, si, Arrays.asList(dir.listAll()), MergeState.CheckAbort.NONE, IOContext.DEFAULT);\n    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);\n    \n    final IndexInput[] ins = new IndexInput[FILE_COUNT];\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      ins[fileIdx] = cfs.openInput(\"file.\" + fileIdx, newIOContext(random()));\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      assertEquals((byte) fileIdx, ins[fileIdx].readByte());\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n    \n    for(int fileIdx=0;fileIdx<FILE_COUNT;fileIdx++) {\n      ins[fileIdx].close();\n    }\n    cfs.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9bb9a29a5e71a90295f175df8919802993142c9a","date":1412517673,"type":1,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testManySubFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestCompoundFile#testManySubFiles().mjava","sourceNew":"  // Make sure we don't somehow use more than 1 descriptor\n  // when reading a CFS with many subs:\n  public void testManySubFiles() throws IOException {\n    final MockDirectoryWrapper dir = newMockFSDirectory(createTempDir(\"CFSManySubFiles\"));\n    \n    final int FILE_COUNT = atLeast(500);\n    \n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      IndexOutput out = dir.createOutput(\"_123.\" + fileIdx, newIOContext(random()));\n      out.writeByte((byte) fileIdx);\n      out.close();\n    }\n    \n    assertEquals(0, dir.getFileHandleCount());\n    \n    SegmentInfo si = newSegmentInfo(dir, \"_123\");\n    si.getCodec().compoundFormat().write(dir, si, Arrays.asList(dir.listAll()), MergeState.CheckAbort.NONE, IOContext.DEFAULT);\n    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);\n    \n    final IndexInput[] ins = new IndexInput[FILE_COUNT];\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      ins[fileIdx] = cfs.openInput(\"_123.\" + fileIdx, newIOContext(random()));\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      assertEquals((byte) fileIdx, ins[fileIdx].readByte());\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n    \n    for(int fileIdx=0;fileIdx<FILE_COUNT;fileIdx++) {\n      ins[fileIdx].close();\n    }\n    cfs.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't somehow use more than 1 descriptor\n  // when reading a CFS with many subs:\n  public void testManySubFiles() throws IOException {\n\n    final Directory d = newFSDirectory(createTempDir(\"CFSManySubFiles\"));\n    final int FILE_COUNT = atLeast(500);\n\n    for(int fileIdx=0;fileIdx<FILE_COUNT;fileIdx++) {\n      IndexOutput out = d.createOutput(\"file.\" + fileIdx, newIOContext(random()));\n      out.writeByte((byte) fileIdx);\n      out.close();\n    }\n    \n    final CompoundFileDirectory cfd = new CompoundFileDirectory(d, \"c.cfs\", newIOContext(random()), true);\n    for(int fileIdx=0;fileIdx<FILE_COUNT;fileIdx++) {\n      final String fileName = \"file.\" + fileIdx;\n      d.copy(cfd, fileName, fileName, newIOContext(random()));\n    }\n    cfd.close();\n\n    final IndexInput[] ins = new IndexInput[FILE_COUNT];\n    final CompoundFileDirectory cfr = new CompoundFileDirectory(d, \"c.cfs\", newIOContext(random()), false);\n    for(int fileIdx=0;fileIdx<FILE_COUNT;fileIdx++) {\n      ins[fileIdx] = cfr.openInput(\"file.\" + fileIdx, newIOContext(random()));\n    }\n\n    for(int fileIdx=0;fileIdx<FILE_COUNT;fileIdx++) {\n      assertEquals((byte) fileIdx, ins[fileIdx].readByte());\n    }\n\n    for(int fileIdx=0;fileIdx<FILE_COUNT;fileIdx++) {\n      ins[fileIdx].close();\n    }\n    cfr.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5faf65b6692f15cca0f87bf8666c87899afc619f","date":1420468108,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testManySubFiles().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testManySubFiles().mjava","sourceNew":"  // Make sure we don't somehow use more than 1 descriptor\n  // when reading a CFS with many subs:\n  public void testManySubFiles() throws IOException {\n    final MockDirectoryWrapper dir = newMockFSDirectory(createTempDir(\"CFSManySubFiles\"));\n    \n    final int FILE_COUNT = atLeast(500);\n    \n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      IndexOutput out = dir.createOutput(\"_123.\" + fileIdx, newIOContext(random()));\n      out.writeByte((byte) fileIdx);\n      out.close();\n    }\n    \n    assertEquals(0, dir.getFileHandleCount());\n    \n    SegmentInfo si = newSegmentInfo(dir, \"_123\");\n    si.getCodec().compoundFormat().write(dir, si, Arrays.asList(dir.listAll()), IOContext.DEFAULT);\n    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);\n    \n    final IndexInput[] ins = new IndexInput[FILE_COUNT];\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      ins[fileIdx] = cfs.openInput(\"_123.\" + fileIdx, newIOContext(random()));\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      assertEquals((byte) fileIdx, ins[fileIdx].readByte());\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n    \n    for(int fileIdx=0;fileIdx<FILE_COUNT;fileIdx++) {\n      ins[fileIdx].close();\n    }\n    cfs.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't somehow use more than 1 descriptor\n  // when reading a CFS with many subs:\n  public void testManySubFiles() throws IOException {\n    final MockDirectoryWrapper dir = newMockFSDirectory(createTempDir(\"CFSManySubFiles\"));\n    \n    final int FILE_COUNT = atLeast(500);\n    \n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      IndexOutput out = dir.createOutput(\"_123.\" + fileIdx, newIOContext(random()));\n      out.writeByte((byte) fileIdx);\n      out.close();\n    }\n    \n    assertEquals(0, dir.getFileHandleCount());\n    \n    SegmentInfo si = newSegmentInfo(dir, \"_123\");\n    si.getCodec().compoundFormat().write(dir, si, Arrays.asList(dir.listAll()), MergeState.CheckAbort.NONE, IOContext.DEFAULT);\n    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);\n    \n    final IndexInput[] ins = new IndexInput[FILE_COUNT];\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      ins[fileIdx] = cfs.openInput(\"_123.\" + fileIdx, newIOContext(random()));\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      assertEquals((byte) fileIdx, ins[fileIdx].readByte());\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n    \n    for(int fileIdx=0;fileIdx<FILE_COUNT;fileIdx++) {\n      ins[fileIdx].close();\n    }\n    cfs.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"75d243fa001c0783996918dbbe60b55cbaeeff46","date":1422502815,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testManySubFiles().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testManySubFiles().mjava","sourceNew":"  // Make sure we don't somehow use more than 1 descriptor\n  // when reading a CFS with many subs:\n  public void testManySubFiles() throws IOException {\n    final MockDirectoryWrapper dir = newMockFSDirectory(createTempDir(\"CFSManySubFiles\"));\n    \n    final int FILE_COUNT = atLeast(500);\n    \n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      IndexOutput out = dir.createOutput(\"_123.\" + fileIdx, newIOContext(random()));\n      out.writeByte((byte) fileIdx);\n      out.close();\n    }\n    \n    assertEquals(0, dir.getFileHandleCount());\n    \n    SegmentInfo si = newSegmentInfo(dir, \"_123\");\n    si.setFiles(Arrays.asList(dir.listAll()));\n    si.getCodec().compoundFormat().write(dir, si, IOContext.DEFAULT);\n    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);\n    \n    final IndexInput[] ins = new IndexInput[FILE_COUNT];\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      ins[fileIdx] = cfs.openInput(\"_123.\" + fileIdx, newIOContext(random()));\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      assertEquals((byte) fileIdx, ins[fileIdx].readByte());\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n    \n    for(int fileIdx=0;fileIdx<FILE_COUNT;fileIdx++) {\n      ins[fileIdx].close();\n    }\n    cfs.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't somehow use more than 1 descriptor\n  // when reading a CFS with many subs:\n  public void testManySubFiles() throws IOException {\n    final MockDirectoryWrapper dir = newMockFSDirectory(createTempDir(\"CFSManySubFiles\"));\n    \n    final int FILE_COUNT = atLeast(500);\n    \n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      IndexOutput out = dir.createOutput(\"_123.\" + fileIdx, newIOContext(random()));\n      out.writeByte((byte) fileIdx);\n      out.close();\n    }\n    \n    assertEquals(0, dir.getFileHandleCount());\n    \n    SegmentInfo si = newSegmentInfo(dir, \"_123\");\n    si.getCodec().compoundFormat().write(dir, si, Arrays.asList(dir.listAll()), IOContext.DEFAULT);\n    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);\n    \n    final IndexInput[] ins = new IndexInput[FILE_COUNT];\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      ins[fileIdx] = cfs.openInput(\"_123.\" + fileIdx, newIOContext(random()));\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      assertEquals((byte) fileIdx, ins[fileIdx].readByte());\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n    \n    for(int fileIdx=0;fileIdx<FILE_COUNT;fileIdx++) {\n      ins[fileIdx].close();\n    }\n    cfs.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"22989c36ff05c657df26dd3377b37c9ad35859bc","date":1424477375,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testManySubFiles().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testManySubFiles().mjava","sourceNew":"  // Make sure we don't somehow use more than 1 descriptor\n  // when reading a CFS with many subs:\n  public void testManySubFiles() throws IOException {\n    final MockDirectoryWrapper dir = newMockFSDirectory(createTempDir(\"CFSManySubFiles\"));\n    \n    final int FILE_COUNT = atLeast(500);\n    \n    List<String> files = new ArrayList<>();\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      String file = \"_123.\" + fileIdx;\n      files.add(file);\n      IndexOutput out = dir.createOutput(file, newIOContext(random()));\n      out.writeByte((byte) fileIdx);\n      out.close();\n    }\n    \n    assertEquals(0, dir.getFileHandleCount());\n    \n    SegmentInfo si = newSegmentInfo(dir, \"_123\");\n    si.setFiles(files);\n    si.getCodec().compoundFormat().write(dir, si, IOContext.DEFAULT);\n    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);\n    \n    final IndexInput[] ins = new IndexInput[FILE_COUNT];\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      ins[fileIdx] = cfs.openInput(\"_123.\" + fileIdx, newIOContext(random()));\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      assertEquals((byte) fileIdx, ins[fileIdx].readByte());\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n    \n    for(int fileIdx=0;fileIdx<FILE_COUNT;fileIdx++) {\n      ins[fileIdx].close();\n    }\n    cfs.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't somehow use more than 1 descriptor\n  // when reading a CFS with many subs:\n  public void testManySubFiles() throws IOException {\n    final MockDirectoryWrapper dir = newMockFSDirectory(createTempDir(\"CFSManySubFiles\"));\n    \n    final int FILE_COUNT = atLeast(500);\n    \n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      IndexOutput out = dir.createOutput(\"_123.\" + fileIdx, newIOContext(random()));\n      out.writeByte((byte) fileIdx);\n      out.close();\n    }\n    \n    assertEquals(0, dir.getFileHandleCount());\n    \n    SegmentInfo si = newSegmentInfo(dir, \"_123\");\n    si.setFiles(Arrays.asList(dir.listAll()));\n    si.getCodec().compoundFormat().write(dir, si, IOContext.DEFAULT);\n    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);\n    \n    final IndexInput[] ins = new IndexInput[FILE_COUNT];\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      ins[fileIdx] = cfs.openInput(\"_123.\" + fileIdx, newIOContext(random()));\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      assertEquals((byte) fileIdx, ins[fileIdx].readByte());\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n    \n    for(int fileIdx=0;fileIdx<FILE_COUNT;fileIdx++) {\n      ins[fileIdx].close();\n    }\n    cfs.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"71488d7f5786ae87541276121ecb69705a11a295","date":1465498138,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testManySubFiles().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testManySubFiles().mjava","sourceNew":"  // Make sure we don't somehow use more than 1 descriptor\n  // when reading a CFS with many subs:\n  public void testManySubFiles() throws IOException {\n    final MockDirectoryWrapper dir = newMockFSDirectory(createTempDir(\"CFSManySubFiles\"));\n    \n    final int FILE_COUNT = atLeast(500);\n    \n    List<String> files = new ArrayList<>();\n    SegmentInfo si = newSegmentInfo(dir, \"_123\");\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      String file = \"_123.\" + fileIdx;\n      files.add(file);\n      try (IndexOutput out = dir.createOutput(file, newIOContext(random()))) {\n        CodecUtil.writeIndexHeader(out, \"Foo\", 0, si.getId(), \"suffix\");\n        out.writeByte((byte) fileIdx);\n        CodecUtil.writeFooter(out);\n      }\n    }\n    \n    assertEquals(0, dir.getFileHandleCount());\n    \n    si.setFiles(files);\n    si.getCodec().compoundFormat().write(dir, si, IOContext.DEFAULT);\n    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);\n    \n    final IndexInput[] ins = new IndexInput[FILE_COUNT];\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      ins[fileIdx] = cfs.openInput(\"_123.\" + fileIdx, newIOContext(random()));\n      CodecUtil.checkIndexHeader(ins[fileIdx], \"Foo\", 0, 0, si.getId(), \"suffix\");\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      assertEquals((byte) fileIdx, ins[fileIdx].readByte());\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n    \n    for(int fileIdx=0;fileIdx<FILE_COUNT;fileIdx++) {\n      ins[fileIdx].close();\n    }\n    cfs.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't somehow use more than 1 descriptor\n  // when reading a CFS with many subs:\n  public void testManySubFiles() throws IOException {\n    final MockDirectoryWrapper dir = newMockFSDirectory(createTempDir(\"CFSManySubFiles\"));\n    \n    final int FILE_COUNT = atLeast(500);\n    \n    List<String> files = new ArrayList<>();\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      String file = \"_123.\" + fileIdx;\n      files.add(file);\n      IndexOutput out = dir.createOutput(file, newIOContext(random()));\n      out.writeByte((byte) fileIdx);\n      out.close();\n    }\n    \n    assertEquals(0, dir.getFileHandleCount());\n    \n    SegmentInfo si = newSegmentInfo(dir, \"_123\");\n    si.setFiles(files);\n    si.getCodec().compoundFormat().write(dir, si, IOContext.DEFAULT);\n    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);\n    \n    final IndexInput[] ins = new IndexInput[FILE_COUNT];\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      ins[fileIdx] = cfs.openInput(\"_123.\" + fileIdx, newIOContext(random()));\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      assertEquals((byte) fileIdx, ins[fileIdx].readByte());\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n    \n    for(int fileIdx=0;fileIdx<FILE_COUNT;fileIdx++) {\n      ins[fileIdx].close();\n    }\n    cfs.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testManySubFiles().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testManySubFiles().mjava","sourceNew":"  // Make sure we don't somehow use more than 1 descriptor\n  // when reading a CFS with many subs:\n  public void testManySubFiles() throws IOException {\n    final MockDirectoryWrapper dir = newMockFSDirectory(createTempDir(\"CFSManySubFiles\"));\n    \n    final int FILE_COUNT = atLeast(500);\n    \n    List<String> files = new ArrayList<>();\n    SegmentInfo si = newSegmentInfo(dir, \"_123\");\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      String file = \"_123.\" + fileIdx;\n      files.add(file);\n      try (IndexOutput out = dir.createOutput(file, newIOContext(random()))) {\n        CodecUtil.writeIndexHeader(out, \"Foo\", 0, si.getId(), \"suffix\");\n        out.writeByte((byte) fileIdx);\n        CodecUtil.writeFooter(out);\n      }\n    }\n    \n    assertEquals(0, dir.getFileHandleCount());\n    \n    si.setFiles(files);\n    si.getCodec().compoundFormat().write(dir, si, IOContext.DEFAULT);\n    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);\n    \n    final IndexInput[] ins = new IndexInput[FILE_COUNT];\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      ins[fileIdx] = cfs.openInput(\"_123.\" + fileIdx, newIOContext(random()));\n      CodecUtil.checkIndexHeader(ins[fileIdx], \"Foo\", 0, 0, si.getId(), \"suffix\");\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      assertEquals((byte) fileIdx, ins[fileIdx].readByte());\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n    \n    for(int fileIdx=0;fileIdx<FILE_COUNT;fileIdx++) {\n      ins[fileIdx].close();\n    }\n    cfs.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't somehow use more than 1 descriptor\n  // when reading a CFS with many subs:\n  public void testManySubFiles() throws IOException {\n    final MockDirectoryWrapper dir = newMockFSDirectory(createTempDir(\"CFSManySubFiles\"));\n    \n    final int FILE_COUNT = atLeast(500);\n    \n    List<String> files = new ArrayList<>();\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      String file = \"_123.\" + fileIdx;\n      files.add(file);\n      IndexOutput out = dir.createOutput(file, newIOContext(random()));\n      out.writeByte((byte) fileIdx);\n      out.close();\n    }\n    \n    assertEquals(0, dir.getFileHandleCount());\n    \n    SegmentInfo si = newSegmentInfo(dir, \"_123\");\n    si.setFiles(files);\n    si.getCodec().compoundFormat().write(dir, si, IOContext.DEFAULT);\n    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);\n    \n    final IndexInput[] ins = new IndexInput[FILE_COUNT];\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      ins[fileIdx] = cfs.openInput(\"_123.\" + fileIdx, newIOContext(random()));\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n\n    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {\n      assertEquals((byte) fileIdx, ins[fileIdx].readByte());\n    }\n    \n    assertEquals(1, dir.getFileHandleCount());\n    \n    for(int fileIdx=0;fileIdx<FILE_COUNT;fileIdx++) {\n      ins[fileIdx].close();\n    }\n    cfs.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"c761d502ca1f6c5324aa909468ebc4f761c92c1f":["3257de94b910b1c34362d2f90d9407daf63dd68b"],"9bb9a29a5e71a90295f175df8919802993142c9a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c761d502ca1f6c5324aa909468ebc4f761c92c1f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3257de94b910b1c34362d2f90d9407daf63dd68b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"71488d7f5786ae87541276121ecb69705a11a295":["22989c36ff05c657df26dd3377b37c9ad35859bc"],"22989c36ff05c657df26dd3377b37c9ad35859bc":["75d243fa001c0783996918dbbe60b55cbaeeff46"],"5faf65b6692f15cca0f87bf8666c87899afc619f":["9bb9a29a5e71a90295f175df8919802993142c9a"],"75d243fa001c0783996918dbbe60b55cbaeeff46":["5faf65b6692f15cca0f87bf8666c87899afc619f"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["22989c36ff05c657df26dd3377b37c9ad35859bc","71488d7f5786ae87541276121ecb69705a11a295"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["71488d7f5786ae87541276121ecb69705a11a295"]},"commit2Childs":{"c761d502ca1f6c5324aa909468ebc4f761c92c1f":["9bb9a29a5e71a90295f175df8919802993142c9a"],"9bb9a29a5e71a90295f175df8919802993142c9a":["5faf65b6692f15cca0f87bf8666c87899afc619f"],"3257de94b910b1c34362d2f90d9407daf63dd68b":["c761d502ca1f6c5324aa909468ebc4f761c92c1f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9bb9a29a5e71a90295f175df8919802993142c9a","3257de94b910b1c34362d2f90d9407daf63dd68b"],"71488d7f5786ae87541276121ecb69705a11a295":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"22989c36ff05c657df26dd3377b37c9ad35859bc":["71488d7f5786ae87541276121ecb69705a11a295","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"5faf65b6692f15cca0f87bf8666c87899afc619f":["75d243fa001c0783996918dbbe60b55cbaeeff46"],"75d243fa001c0783996918dbbe60b55cbaeeff46":["22989c36ff05c657df26dd3377b37c9ad35859bc"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}