{"path":"solr/core/src/java/org/apache/solr/update/processor/DistributedZkUpdateProcessor#doDeleteByQuery(DeleteUpdateCommand).mjava","commits":[{"id":"9d70e774cb25c8a8d2c3e5e84200f235f9168d87","date":1553016391,"type":0,"author":"Bar Rotstein","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedZkUpdateProcessor#doDeleteByQuery(DeleteUpdateCommand).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  protected void doDeleteByQuery(DeleteUpdateCommand cmd) throws IOException {\n    zkCheck();\n\n    // NONE: we are the first to receive this deleteByQuery\n    //       - it must be forwarded to the leader of every shard\n    // TO:   we are a leader receiving a forwarded deleteByQuery... we must:\n    //       - block all updates (use VersionInfo)\n    //       - flush *all* updates going to our replicas\n    //       - forward the DBQ to our replicas and wait for the response\n    //       - log + execute the local DBQ\n    // FROM: we are a replica receiving a DBQ from our leader\n    //       - log + execute the local DBQ\n    DistribPhase phase = DistribPhase.parseParam(req.getParams().get(DISTRIB_UPDATE_PARAM));\n\n    DocCollection coll = zkController.getClusterState().getCollection(collection);\n\n    if (DistribPhase.NONE == phase) {\n      if (rollupReplicationTracker == null) {\n        rollupReplicationTracker = new RollupRequestReplicationTracker();\n      }\n      boolean leaderForAnyShard = false;  // start off by assuming we are not a leader for any shard\n\n      ModifiableSolrParams outParams = new ModifiableSolrParams(filterParams(req.getParams()));\n      outParams.set(DISTRIB_UPDATE_PARAM, DistribPhase.TOLEADER.toString());\n      outParams.set(DISTRIB_FROM, ZkCoreNodeProps.getCoreUrl(\n          zkController.getBaseUrl(), req.getCore().getName()));\n\n      SolrParams params = req.getParams();\n      String route = params.get(ShardParams._ROUTE_);\n      Collection<Slice> slices = coll.getRouter().getSearchSlices(route, params, coll);\n\n      List<SolrCmdDistributor.Node> leaders =  new ArrayList<>(slices.size());\n      for (Slice slice : slices) {\n        String sliceName = slice.getName();\n        Replica leader;\n        try {\n          leader = zkController.getZkStateReader().getLeaderRetry(collection, sliceName);\n        } catch (InterruptedException e) {\n          throw new SolrException(SolrException.ErrorCode.SERVICE_UNAVAILABLE, \"Exception finding leader for shard \" + sliceName, e);\n        }\n\n        // TODO: What if leaders changed in the meantime?\n        // should we send out slice-at-a-time and if a node returns \"hey, I'm not a leader\" (or we get an error because it went down) then look up the new leader?\n\n        // Am I the leader for this slice?\n        ZkCoreNodeProps coreLeaderProps = new ZkCoreNodeProps(leader);\n        String leaderCoreNodeName = leader.getName();\n        String coreNodeName = cloudDesc.getCoreNodeName();\n        isLeader = coreNodeName.equals(leaderCoreNodeName);\n\n        if (isLeader) {\n          // don't forward to ourself\n          leaderForAnyShard = true;\n        } else {\n          leaders.add(new SolrCmdDistributor.ForwardNode(coreLeaderProps, zkController.getZkStateReader(), collection, sliceName, maxRetriesOnForward));\n        }\n      }\n\n      outParams.remove(\"commit\"); // this will be distributed from the local commit\n\n\n      if (params.get(UpdateRequest.MIN_REPFACT) != null) {\n        // TODO: Kept this for rolling upgrades. Remove in Solr 9\n        outParams.add(UpdateRequest.MIN_REPFACT, req.getParams().get(UpdateRequest.MIN_REPFACT));\n      }\n      cmdDistrib.distribDelete(cmd, leaders, outParams, false, rollupReplicationTracker, null);\n\n      if (!leaderForAnyShard) {\n        return;\n      }\n\n      // change the phase to TOLEADER so we look up and forward to our own replicas (if any)\n      phase = DistribPhase.TOLEADER;\n    }\n    List<SolrCmdDistributor.Node> replicas = null;\n\n    if (DistribPhase.TOLEADER == phase) {\n      // This core should be a leader\n      isLeader = true;\n      replicas = setupRequestForDBQ();\n    } else if (DistribPhase.FROMLEADER == phase) {\n      isLeader = false;\n    }\n\n    // check if client has requested minimum replication factor information. will set replicationTracker to null if\n    // we aren't the leader or subShardLeader\n    checkReplicationTracker(cmd);\n    super.doDeleteByQuery(cmd, replicas, coll);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"88922bf68f0b509aba218f1b9e7ef5981b4d13bc","date":1570820823,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedZkUpdateProcessor#doDeleteByQuery(DeleteUpdateCommand).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedZkUpdateProcessor#doDeleteByQuery(DeleteUpdateCommand).mjava","sourceNew":"  @Override\n  protected void doDeleteByQuery(DeleteUpdateCommand cmd) throws IOException {\n    zkCheck();\n\n    // NONE: we are the first to receive this deleteByQuery\n    //       - it must be forwarded to the leader of every shard\n    // TO:   we are a leader receiving a forwarded deleteByQuery... we must:\n    //       - block all updates (use VersionInfo)\n    //       - flush *all* updates going to our replicas\n    //       - forward the DBQ to our replicas and wait for the response\n    //       - log + execute the local DBQ\n    // FROM: we are a replica receiving a DBQ from our leader\n    //       - log + execute the local DBQ\n    DistribPhase phase = DistribPhase.parseParam(req.getParams().get(DISTRIB_UPDATE_PARAM));\n\n    DocCollection coll = clusterState.getCollection(collection);\n\n    if (DistribPhase.NONE == phase) {\n      if (rollupReplicationTracker == null) {\n        rollupReplicationTracker = new RollupRequestReplicationTracker();\n      }\n      boolean leaderForAnyShard = false;  // start off by assuming we are not a leader for any shard\n\n      ModifiableSolrParams outParams = new ModifiableSolrParams(filterParams(req.getParams()));\n      outParams.set(DISTRIB_UPDATE_PARAM, DistribPhase.TOLEADER.toString());\n      outParams.set(DISTRIB_FROM, ZkCoreNodeProps.getCoreUrl(\n          zkController.getBaseUrl(), req.getCore().getName()));\n\n      SolrParams params = req.getParams();\n      String route = params.get(ShardParams._ROUTE_);\n      Collection<Slice> slices = coll.getRouter().getSearchSlices(route, params, coll);\n\n      List<SolrCmdDistributor.Node> leaders =  new ArrayList<>(slices.size());\n      for (Slice slice : slices) {\n        String sliceName = slice.getName();\n        Replica leader;\n        try {\n          leader = zkController.getZkStateReader().getLeaderRetry(collection, sliceName);\n        } catch (InterruptedException e) {\n          throw new SolrException(SolrException.ErrorCode.SERVICE_UNAVAILABLE, \"Exception finding leader for shard \" + sliceName, e);\n        }\n\n        // TODO: What if leaders changed in the meantime?\n        // should we send out slice-at-a-time and if a node returns \"hey, I'm not a leader\" (or we get an error because it went down) then look up the new leader?\n\n        // Am I the leader for this slice?\n        ZkCoreNodeProps coreLeaderProps = new ZkCoreNodeProps(leader);\n        String leaderCoreNodeName = leader.getName();\n        String coreNodeName = cloudDesc.getCoreNodeName();\n        isLeader = coreNodeName.equals(leaderCoreNodeName);\n\n        if (isLeader) {\n          // don't forward to ourself\n          leaderForAnyShard = true;\n        } else {\n          leaders.add(new SolrCmdDistributor.ForwardNode(coreLeaderProps, zkController.getZkStateReader(), collection, sliceName, maxRetriesOnForward));\n        }\n      }\n\n      outParams.remove(\"commit\"); // this will be distributed from the local commit\n\n\n      if (params.get(UpdateRequest.MIN_REPFACT) != null) {\n        // TODO: Kept this for rolling upgrades. Remove in Solr 9\n        outParams.add(UpdateRequest.MIN_REPFACT, req.getParams().get(UpdateRequest.MIN_REPFACT));\n      }\n      cmdDistrib.distribDelete(cmd, leaders, outParams, false, rollupReplicationTracker, null);\n\n      if (!leaderForAnyShard) {\n        return;\n      }\n\n      // change the phase to TOLEADER so we look up and forward to our own replicas (if any)\n      phase = DistribPhase.TOLEADER;\n    }\n    List<SolrCmdDistributor.Node> replicas = null;\n\n    if (DistribPhase.TOLEADER == phase) {\n      // This core should be a leader\n      isLeader = true;\n      replicas = setupRequestForDBQ();\n    } else if (DistribPhase.FROMLEADER == phase) {\n      isLeader = false;\n    }\n\n    // check if client has requested minimum replication factor information. will set replicationTracker to null if\n    // we aren't the leader or subShardLeader\n    checkReplicationTracker(cmd);\n    super.doDeleteByQuery(cmd, replicas, coll);\n  }\n\n","sourceOld":"  @Override\n  protected void doDeleteByQuery(DeleteUpdateCommand cmd) throws IOException {\n    zkCheck();\n\n    // NONE: we are the first to receive this deleteByQuery\n    //       - it must be forwarded to the leader of every shard\n    // TO:   we are a leader receiving a forwarded deleteByQuery... we must:\n    //       - block all updates (use VersionInfo)\n    //       - flush *all* updates going to our replicas\n    //       - forward the DBQ to our replicas and wait for the response\n    //       - log + execute the local DBQ\n    // FROM: we are a replica receiving a DBQ from our leader\n    //       - log + execute the local DBQ\n    DistribPhase phase = DistribPhase.parseParam(req.getParams().get(DISTRIB_UPDATE_PARAM));\n\n    DocCollection coll = zkController.getClusterState().getCollection(collection);\n\n    if (DistribPhase.NONE == phase) {\n      if (rollupReplicationTracker == null) {\n        rollupReplicationTracker = new RollupRequestReplicationTracker();\n      }\n      boolean leaderForAnyShard = false;  // start off by assuming we are not a leader for any shard\n\n      ModifiableSolrParams outParams = new ModifiableSolrParams(filterParams(req.getParams()));\n      outParams.set(DISTRIB_UPDATE_PARAM, DistribPhase.TOLEADER.toString());\n      outParams.set(DISTRIB_FROM, ZkCoreNodeProps.getCoreUrl(\n          zkController.getBaseUrl(), req.getCore().getName()));\n\n      SolrParams params = req.getParams();\n      String route = params.get(ShardParams._ROUTE_);\n      Collection<Slice> slices = coll.getRouter().getSearchSlices(route, params, coll);\n\n      List<SolrCmdDistributor.Node> leaders =  new ArrayList<>(slices.size());\n      for (Slice slice : slices) {\n        String sliceName = slice.getName();\n        Replica leader;\n        try {\n          leader = zkController.getZkStateReader().getLeaderRetry(collection, sliceName);\n        } catch (InterruptedException e) {\n          throw new SolrException(SolrException.ErrorCode.SERVICE_UNAVAILABLE, \"Exception finding leader for shard \" + sliceName, e);\n        }\n\n        // TODO: What if leaders changed in the meantime?\n        // should we send out slice-at-a-time and if a node returns \"hey, I'm not a leader\" (or we get an error because it went down) then look up the new leader?\n\n        // Am I the leader for this slice?\n        ZkCoreNodeProps coreLeaderProps = new ZkCoreNodeProps(leader);\n        String leaderCoreNodeName = leader.getName();\n        String coreNodeName = cloudDesc.getCoreNodeName();\n        isLeader = coreNodeName.equals(leaderCoreNodeName);\n\n        if (isLeader) {\n          // don't forward to ourself\n          leaderForAnyShard = true;\n        } else {\n          leaders.add(new SolrCmdDistributor.ForwardNode(coreLeaderProps, zkController.getZkStateReader(), collection, sliceName, maxRetriesOnForward));\n        }\n      }\n\n      outParams.remove(\"commit\"); // this will be distributed from the local commit\n\n\n      if (params.get(UpdateRequest.MIN_REPFACT) != null) {\n        // TODO: Kept this for rolling upgrades. Remove in Solr 9\n        outParams.add(UpdateRequest.MIN_REPFACT, req.getParams().get(UpdateRequest.MIN_REPFACT));\n      }\n      cmdDistrib.distribDelete(cmd, leaders, outParams, false, rollupReplicationTracker, null);\n\n      if (!leaderForAnyShard) {\n        return;\n      }\n\n      // change the phase to TOLEADER so we look up and forward to our own replicas (if any)\n      phase = DistribPhase.TOLEADER;\n    }\n    List<SolrCmdDistributor.Node> replicas = null;\n\n    if (DistribPhase.TOLEADER == phase) {\n      // This core should be a leader\n      isLeader = true;\n      replicas = setupRequestForDBQ();\n    } else if (DistribPhase.FROMLEADER == phase) {\n      isLeader = false;\n    }\n\n    // check if client has requested minimum replication factor information. will set replicationTracker to null if\n    // we aren't the leader or subShardLeader\n    checkReplicationTracker(cmd);\n    super.doDeleteByQuery(cmd, replicas, coll);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b0b597c65628ca9e73913a07e81691f8229bae35","date":1571224353,"type":3,"author":"jimczi","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedZkUpdateProcessor#doDeleteByQuery(DeleteUpdateCommand).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedZkUpdateProcessor#doDeleteByQuery(DeleteUpdateCommand).mjava","sourceNew":"  @Override\n  protected void doDeleteByQuery(DeleteUpdateCommand cmd) throws IOException {\n    zkCheck();\n\n    // NONE: we are the first to receive this deleteByQuery\n    //       - it must be forwarded to the leader of every shard\n    // TO:   we are a leader receiving a forwarded deleteByQuery... we must:\n    //       - block all updates (use VersionInfo)\n    //       - flush *all* updates going to our replicas\n    //       - forward the DBQ to our replicas and wait for the response\n    //       - log + execute the local DBQ\n    // FROM: we are a replica receiving a DBQ from our leader\n    //       - log + execute the local DBQ\n    DistribPhase phase = DistribPhase.parseParam(req.getParams().get(DISTRIB_UPDATE_PARAM));\n\n    DocCollection coll = clusterState.getCollection(collection);\n\n    if (DistribPhase.NONE == phase) {\n      if (rollupReplicationTracker == null) {\n        rollupReplicationTracker = new RollupRequestReplicationTracker();\n      }\n      boolean leaderForAnyShard = false;  // start off by assuming we are not a leader for any shard\n\n      ModifiableSolrParams outParams = new ModifiableSolrParams(filterParams(req.getParams()));\n      outParams.set(DISTRIB_UPDATE_PARAM, DistribPhase.TOLEADER.toString());\n      outParams.set(DISTRIB_FROM, ZkCoreNodeProps.getCoreUrl(\n          zkController.getBaseUrl(), req.getCore().getName()));\n\n      SolrParams params = req.getParams();\n      String route = params.get(ShardParams._ROUTE_);\n      Collection<Slice> slices = coll.getRouter().getSearchSlices(route, params, coll);\n\n      List<SolrCmdDistributor.Node> leaders =  new ArrayList<>(slices.size());\n      for (Slice slice : slices) {\n        String sliceName = slice.getName();\n        Replica leader;\n        try {\n          leader = zkController.getZkStateReader().getLeaderRetry(collection, sliceName);\n        } catch (InterruptedException e) {\n          throw new SolrException(SolrException.ErrorCode.SERVICE_UNAVAILABLE, \"Exception finding leader for shard \" + sliceName, e);\n        }\n\n        // TODO: What if leaders changed in the meantime?\n        // should we send out slice-at-a-time and if a node returns \"hey, I'm not a leader\" (or we get an error because it went down) then look up the new leader?\n\n        // Am I the leader for this slice?\n        ZkCoreNodeProps coreLeaderProps = new ZkCoreNodeProps(leader);\n        String leaderCoreNodeName = leader.getName();\n        String coreNodeName = cloudDesc.getCoreNodeName();\n        isLeader = coreNodeName.equals(leaderCoreNodeName);\n\n        if (isLeader) {\n          // don't forward to ourself\n          leaderForAnyShard = true;\n        } else {\n          leaders.add(new SolrCmdDistributor.ForwardNode(coreLeaderProps, zkController.getZkStateReader(), collection, sliceName, maxRetriesOnForward));\n        }\n      }\n\n      outParams.remove(\"commit\"); // this will be distributed from the local commit\n\n\n      if (params.get(UpdateRequest.MIN_REPFACT) != null) {\n        // TODO: Kept this for rolling upgrades. Remove in Solr 9\n        outParams.add(UpdateRequest.MIN_REPFACT, req.getParams().get(UpdateRequest.MIN_REPFACT));\n      }\n      cmdDistrib.distribDelete(cmd, leaders, outParams, false, rollupReplicationTracker, null);\n\n      if (!leaderForAnyShard) {\n        return;\n      }\n\n      // change the phase to TOLEADER so we look up and forward to our own replicas (if any)\n      phase = DistribPhase.TOLEADER;\n    }\n    List<SolrCmdDistributor.Node> replicas = null;\n\n    if (DistribPhase.TOLEADER == phase) {\n      // This core should be a leader\n      isLeader = true;\n      replicas = setupRequestForDBQ();\n    } else if (DistribPhase.FROMLEADER == phase) {\n      isLeader = false;\n    }\n\n    // check if client has requested minimum replication factor information. will set replicationTracker to null if\n    // we aren't the leader or subShardLeader\n    checkReplicationTracker(cmd);\n    super.doDeleteByQuery(cmd, replicas, coll);\n  }\n\n","sourceOld":"  @Override\n  protected void doDeleteByQuery(DeleteUpdateCommand cmd) throws IOException {\n    zkCheck();\n\n    // NONE: we are the first to receive this deleteByQuery\n    //       - it must be forwarded to the leader of every shard\n    // TO:   we are a leader receiving a forwarded deleteByQuery... we must:\n    //       - block all updates (use VersionInfo)\n    //       - flush *all* updates going to our replicas\n    //       - forward the DBQ to our replicas and wait for the response\n    //       - log + execute the local DBQ\n    // FROM: we are a replica receiving a DBQ from our leader\n    //       - log + execute the local DBQ\n    DistribPhase phase = DistribPhase.parseParam(req.getParams().get(DISTRIB_UPDATE_PARAM));\n\n    DocCollection coll = zkController.getClusterState().getCollection(collection);\n\n    if (DistribPhase.NONE == phase) {\n      if (rollupReplicationTracker == null) {\n        rollupReplicationTracker = new RollupRequestReplicationTracker();\n      }\n      boolean leaderForAnyShard = false;  // start off by assuming we are not a leader for any shard\n\n      ModifiableSolrParams outParams = new ModifiableSolrParams(filterParams(req.getParams()));\n      outParams.set(DISTRIB_UPDATE_PARAM, DistribPhase.TOLEADER.toString());\n      outParams.set(DISTRIB_FROM, ZkCoreNodeProps.getCoreUrl(\n          zkController.getBaseUrl(), req.getCore().getName()));\n\n      SolrParams params = req.getParams();\n      String route = params.get(ShardParams._ROUTE_);\n      Collection<Slice> slices = coll.getRouter().getSearchSlices(route, params, coll);\n\n      List<SolrCmdDistributor.Node> leaders =  new ArrayList<>(slices.size());\n      for (Slice slice : slices) {\n        String sliceName = slice.getName();\n        Replica leader;\n        try {\n          leader = zkController.getZkStateReader().getLeaderRetry(collection, sliceName);\n        } catch (InterruptedException e) {\n          throw new SolrException(SolrException.ErrorCode.SERVICE_UNAVAILABLE, \"Exception finding leader for shard \" + sliceName, e);\n        }\n\n        // TODO: What if leaders changed in the meantime?\n        // should we send out slice-at-a-time and if a node returns \"hey, I'm not a leader\" (or we get an error because it went down) then look up the new leader?\n\n        // Am I the leader for this slice?\n        ZkCoreNodeProps coreLeaderProps = new ZkCoreNodeProps(leader);\n        String leaderCoreNodeName = leader.getName();\n        String coreNodeName = cloudDesc.getCoreNodeName();\n        isLeader = coreNodeName.equals(leaderCoreNodeName);\n\n        if (isLeader) {\n          // don't forward to ourself\n          leaderForAnyShard = true;\n        } else {\n          leaders.add(new SolrCmdDistributor.ForwardNode(coreLeaderProps, zkController.getZkStateReader(), collection, sliceName, maxRetriesOnForward));\n        }\n      }\n\n      outParams.remove(\"commit\"); // this will be distributed from the local commit\n\n\n      if (params.get(UpdateRequest.MIN_REPFACT) != null) {\n        // TODO: Kept this for rolling upgrades. Remove in Solr 9\n        outParams.add(UpdateRequest.MIN_REPFACT, req.getParams().get(UpdateRequest.MIN_REPFACT));\n      }\n      cmdDistrib.distribDelete(cmd, leaders, outParams, false, rollupReplicationTracker, null);\n\n      if (!leaderForAnyShard) {\n        return;\n      }\n\n      // change the phase to TOLEADER so we look up and forward to our own replicas (if any)\n      phase = DistribPhase.TOLEADER;\n    }\n    List<SolrCmdDistributor.Node> replicas = null;\n\n    if (DistribPhase.TOLEADER == phase) {\n      // This core should be a leader\n      isLeader = true;\n      replicas = setupRequestForDBQ();\n    } else if (DistribPhase.FROMLEADER == phase) {\n      isLeader = false;\n    }\n\n    // check if client has requested minimum replication factor information. will set replicationTracker to null if\n    // we aren't the leader or subShardLeader\n    checkReplicationTracker(cmd);\n    super.doDeleteByQuery(cmd, replicas, coll);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"88922bf68f0b509aba218f1b9e7ef5981b4d13bc":["9d70e774cb25c8a8d2c3e5e84200f235f9168d87"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9d70e774cb25c8a8d2c3e5e84200f235f9168d87":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["88922bf68f0b509aba218f1b9e7ef5981b4d13bc"],"b0b597c65628ca9e73913a07e81691f8229bae35":["9d70e774cb25c8a8d2c3e5e84200f235f9168d87","88922bf68f0b509aba218f1b9e7ef5981b4d13bc"]},"commit2Childs":{"88922bf68f0b509aba218f1b9e7ef5981b4d13bc":["cd5edd1f2b162a5cfa08efd17851a07373a96817","b0b597c65628ca9e73913a07e81691f8229bae35"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9d70e774cb25c8a8d2c3e5e84200f235f9168d87"],"9d70e774cb25c8a8d2c3e5e84200f235f9168d87":["88922bf68f0b509aba218f1b9e7ef5981b4d13bc","b0b597c65628ca9e73913a07e81691f8229bae35"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"b0b597c65628ca9e73913a07e81691f8229bae35":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817","b0b597c65628ca9e73913a07e81691f8229bae35"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}