{"path":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","pathOld":"src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"955c32f886db6f6356c9fcdea6b1f1cb4effda24","date":1270581567,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6c18273ea5b3974d2f30117f46f1ae416c28f727","date":1279708040,"type":5,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":"  @Override\n  public DocumentsWriterPerThread.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n        \n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else {\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n      }\n      \n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterRAMAllocator.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriterPerThread.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriterPerThread.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"01f60198ece724a6e96cd0b45f289cf42ff83d4f","date":1286864103,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n   \n\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      final Fieldable fieldable = perField.fields[0];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n      if(!fieldable.hasFieldAttribute())\n        continue;\n      final AttributeSource attrSource = fieldable.getFieldAttributes();\n      if(!attrSource.hasAttribute(ValuesAttribute.class))\n        continue;\n      final ValuesAttribute attribute = attrSource.getAttribute(ValuesAttribute.class);\n      final DocFieldProcessor.IndexValuesProcessor processor = docFieldProcessor\n          .getProcessor(docState.docWriter.directory,\n              docState.docWriter.segment, fieldable.name(), attribute, perField.fieldInfo);\n      if (processor != null)\n        processor.add(docState.docID, fieldable.name(), attribute);\n    }\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0e28c49f1fb6215a550fdadcf3805aa629b63ec0","date":1288081775,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n   \n\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      final Fieldable fieldable = perField.fields[0];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n     \n      if(!fieldable.hasFieldAttribute())\n        continue;\n      final AttributeSource attrSource = fieldable.getFieldAttributes();\n      if(!attrSource.hasAttribute(ValuesAttribute.class))\n        continue;\n      final ValuesAttribute attribute = attrSource.getAttribute(ValuesAttribute.class);\n      final DocValuesConsumer consumer = docFieldProcessor.docValuesConsumer(docState.docWriter.directory,\n              docState.docWriter.segment, fieldable.name(), attribute, perField.fieldInfo);\n      consumer.add(docState.docID, attribute);\n    }\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n   \n\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      final Fieldable fieldable = perField.fields[0];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n      if(!fieldable.hasFieldAttribute())\n        continue;\n      final AttributeSource attrSource = fieldable.getFieldAttributes();\n      if(!attrSource.hasAttribute(ValuesAttribute.class))\n        continue;\n      final ValuesAttribute attribute = attrSource.getAttribute(ValuesAttribute.class);\n      final DocFieldProcessor.IndexValuesProcessor processor = docFieldProcessor\n          .getProcessor(docState.docWriter.directory,\n              docState.docWriter.segment, fieldable.name(), attribute, perField.fieldInfo);\n      if (processor != null)\n        processor.add(docState.docID, fieldable.name(), attribute);\n    }\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae7aa98ce0c64f3b2b81087d14ff9ae992b4903b","date":1288192616,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ca0ffea399542e8aac8ed7608f34f8ec4cb8904d","date":1288424244,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n   \n\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      final Fieldable fieldable = perField.fields[0];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n     \n      if(!fieldable.hasFieldAttribute())\n        continue;\n      final AttributeSource attrSource = fieldable.getFieldAttributes();\n      if(!attrSource.hasAttribute(ValuesAttribute.class))\n        continue;\n      final ValuesAttribute attribute = attrSource.getAttribute(ValuesAttribute.class);\n      final DocValuesConsumer consumer = docFieldProcessor.docValuesConsumer(docState.docWriter.directory,\n              docState.docWriter.segment, fieldable.name(), attribute, perField.fieldInfo);\n      consumer.add(docState.docID, attribute);\n    }\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n   \n\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      final Fieldable fieldable = perField.fields[0];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n     \n      if(!fieldable.hasFieldAttribute())\n        continue;\n      final AttributeSource attrSource = fieldable.getFieldAttributes();\n      if(!attrSource.hasAttribute(ValuesAttribute.class))\n        continue;\n      final ValuesAttribute attribute = attrSource.getAttribute(ValuesAttribute.class);\n      final DocValuesConsumer consumer = docFieldProcessor.docValuesConsumer(docState.docWriter.directory,\n              docState.docWriter.segment, fieldable.name(), attribute, perField.fieldInfo);\n      consumer.add(docState.docID, attribute);\n    }\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a493e6d0c3ad86bd55c0a1360d110142e948f2bd","date":1289406991,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"85a883878c0af761245ab048babc63d099f835f3","date":1289553330,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n   \n\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      final Fieldable fieldable = perField.fields[0];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n     \n      if(!fieldable.hasFieldAttribute())\n        continue;\n      final AttributeSource attrSource = fieldable.getFieldAttributes();\n      if(!attrSource.hasAttribute(ValuesAttribute.class))\n        continue;\n      final ValuesAttribute attribute = attrSource.getAttribute(ValuesAttribute.class);\n      final DocValuesConsumer consumer = docFieldProcessor.docValuesConsumer(docState.docWriter.directory,\n              docState.docWriter.segment, fieldable.name(), attribute, perField.fieldInfo);\n      consumer.add(docState.docID, attribute);\n    }\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n   \n\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      final Fieldable fieldable = perField.fields[0];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n     \n      if(!fieldable.hasFieldAttribute())\n        continue;\n      final AttributeSource attrSource = fieldable.getFieldAttributes();\n      if(!attrSource.hasAttribute(ValuesAttribute.class))\n        continue;\n      final ValuesAttribute attribute = attrSource.getAttribute(ValuesAttribute.class);\n      final DocValuesConsumer consumer = docFieldProcessor.docValuesConsumer(docState.docWriter.directory,\n              docState.docWriter.segment, fieldable.name(), attribute, perField.fieldInfo);\n      consumer.add(docState.docID, attribute);\n    }\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3bb13258feba31ab676502787ab2e1779f129b7a","date":1291596436,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n   \n\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      final Fieldable fieldable = perField.fields[0];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n     \n      if (!(fieldable instanceof AbstractField)) {\n        continue;\n      }\n      final PerDocFieldValues docValues = ((AbstractField)fieldable).getDocValues();\n      if (docValues == null) {\n        continue;\n      }\n      final DocValuesConsumer consumer = docFieldProcessor.docValuesConsumer(docState.docWriter.directory,\n              docState.docWriter.segment, fieldable.name(), docValues, perField.fieldInfo);\n      consumer.add(docState.docID, docValues);\n    }\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n   \n\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      final Fieldable fieldable = perField.fields[0];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n     \n      if(!fieldable.hasFieldAttribute())\n        continue;\n      final AttributeSource attrSource = fieldable.getFieldAttributes();\n      if(!attrSource.hasAttribute(ValuesAttribute.class))\n        continue;\n      final ValuesAttribute attribute = attrSource.getAttribute(ValuesAttribute.class);\n      final DocValuesConsumer consumer = docFieldProcessor.docValuesConsumer(docState.docWriter.directory,\n              docState.docWriter.segment, fieldable.name(), attribute, perField.fieldInfo);\n      consumer.add(docState.docID, attribute);\n    }\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions(), field.docValuesType());\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n   \n\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      final Fieldable fieldable = perField.fields[0];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n     \n      if (!(fieldable instanceof AbstractField)) {\n        continue;\n      }\n      final PerDocFieldValues docValues = ((AbstractField)fieldable).getDocValues();\n      if (docValues == null) {\n        continue;\n      }\n      final DocValuesConsumer consumer = docFieldProcessor.docValuesConsumer(docState.docWriter.directory,\n              docState.docWriter.segment, fieldable.name(), docValues, perField.fieldInfo);\n      consumer.add(docState.docID, docValues);\n    }\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n   \n\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      final Fieldable fieldable = perField.fields[0];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n     \n      if (!(fieldable instanceof AbstractField)) {\n        continue;\n      }\n      final PerDocFieldValues docValues = ((AbstractField)fieldable).getDocValues();\n      if (docValues == null) {\n        continue;\n      }\n      final DocValuesConsumer consumer = docFieldProcessor.docValuesConsumer(docState.docWriter.directory,\n              docState.docWriter.segment, fieldable.name(), docValues, perField.fieldInfo);\n      consumer.add(docState.docID, docValues);\n    }\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"85130289d2ed101fcc2d8798511c7c5b020ffab4","date":1297239859,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions(), field.docValuesType());\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n   \n\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      final Fieldable fieldable = perField.fields[0];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n      final PerDocFieldValues docValues = fieldable.getDocValues();\n      if (docValues == null) {\n        continue;\n      }\n      final DocValuesConsumer consumer = docFieldProcessor.docValuesConsumer(docState.docWriter.directory,\n              docState.docWriter.segment, fieldable.name(), docValues, perField.fieldInfo);\n      consumer.add(docState.docID, docValues);\n    }\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions(), field.docValuesType());\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n   \n\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      final Fieldable fieldable = perField.fields[0];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n     \n      if (!(fieldable instanceof AbstractField)) {\n        continue;\n      }\n      final PerDocFieldValues docValues = ((AbstractField)fieldable).getDocValues();\n      if (docValues == null) {\n        continue;\n      }\n      final DocValuesConsumer consumer = docFieldProcessor.docValuesConsumer(docState.docWriter.directory,\n              docState.docWriter.segment, fieldable.name(), docValues, perField.fieldInfo);\n      consumer.add(docState.docID, docValues);\n    }\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b0c7a8f7304b75b1528814c5820fa23a96816c27","date":1298314239,"type":5,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument(FieldInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":"  @Override\n  public DocumentsWriter.DocWriter processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"14ec33385f6fbb6ce172882d14605790418a5d31","date":1298910796,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument(FieldInfos).mjava","sourceNew":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocumentsWriter.DocWriter processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1224a4027481acce15495b03bce9b48b93b42722","date":1300792329,"type":5,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument(FieldInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":"  @Override\n  public DocumentsWriter.DocWriter processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d619839baa8ce5503e496b94a9e42ad6f079293f","date":1301309428,"type":4,"author":"Simon Willnauer","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":null,"sourceOld":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions(), field.docValuesType());\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n   \n\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      final Fieldable fieldable = perField.fields[0];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n      final PerDocFieldValues docValues = fieldable.getDocValues();\n      if (docValues == null) {\n        continue;\n      }\n      final DocValuesConsumer consumer = docFieldProcessor.docValuesConsumer(docState.docWriter.directory,\n              docState.docWriter.segment, fieldable.name(), docValues, perField.fieldInfo);\n      consumer.add(docState.docID, docValues);\n    }\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"6c18273ea5b3974d2f30117f46f1ae416c28f727":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"a493e6d0c3ad86bd55c0a1360d110142e948f2bd":["ae7aa98ce0c64f3b2b81087d14ff9ae992b4903b"],"85a883878c0af761245ab048babc63d099f835f3":["ca0ffea399542e8aac8ed7608f34f8ec4cb8904d","a493e6d0c3ad86bd55c0a1360d110142e948f2bd"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["3bb13258feba31ab676502787ab2e1779f129b7a","a493e6d0c3ad86bd55c0a1360d110142e948f2bd"],"01f60198ece724a6e96cd0b45f289cf42ff83d4f":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"ae7aa98ce0c64f3b2b81087d14ff9ae992b4903b":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"14ec33385f6fbb6ce172882d14605790418a5d31":["b0c7a8f7304b75b1528814c5820fa23a96816c27"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["85130289d2ed101fcc2d8798511c7c5b020ffab4","1224a4027481acce15495b03bce9b48b93b42722"],"b0c7a8f7304b75b1528814c5820fa23a96816c27":["a493e6d0c3ad86bd55c0a1360d110142e948f2bd"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"85130289d2ed101fcc2d8798511c7c5b020ffab4":["ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"ca0ffea399542e8aac8ed7608f34f8ec4cb8904d":["0e28c49f1fb6215a550fdadcf3805aa629b63ec0","ae7aa98ce0c64f3b2b81087d14ff9ae992b4903b"],"0e28c49f1fb6215a550fdadcf3805aa629b63ec0":["01f60198ece724a6e96cd0b45f289cf42ff83d4f"],"3bb13258feba31ab676502787ab2e1779f129b7a":["85a883878c0af761245ab048babc63d099f835f3","a493e6d0c3ad86bd55c0a1360d110142e948f2bd"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["1224a4027481acce15495b03bce9b48b93b42722"],"1224a4027481acce15495b03bce9b48b93b42722":["14ec33385f6fbb6ce172882d14605790418a5d31"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"6c18273ea5b3974d2f30117f46f1ae416c28f727":[],"a493e6d0c3ad86bd55c0a1360d110142e948f2bd":["85a883878c0af761245ab048babc63d099f835f3","ab5cb6a74aefb78aa0569857970b9151dfe2e787","b0c7a8f7304b75b1528814c5820fa23a96816c27","3bb13258feba31ab676502787ab2e1779f129b7a"],"85a883878c0af761245ab048babc63d099f835f3":["3bb13258feba31ab676502787ab2e1779f129b7a"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["6c18273ea5b3974d2f30117f46f1ae416c28f727","01f60198ece724a6e96cd0b45f289cf42ff83d4f","ae7aa98ce0c64f3b2b81087d14ff9ae992b4903b"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["85130289d2ed101fcc2d8798511c7c5b020ffab4"],"01f60198ece724a6e96cd0b45f289cf42ff83d4f":["0e28c49f1fb6215a550fdadcf3805aa629b63ec0"],"ae7aa98ce0c64f3b2b81087d14ff9ae992b4903b":["a493e6d0c3ad86bd55c0a1360d110142e948f2bd","ca0ffea399542e8aac8ed7608f34f8ec4cb8904d"],"14ec33385f6fbb6ce172882d14605790418a5d31":["1224a4027481acce15495b03bce9b48b93b42722"],"d619839baa8ce5503e496b94a9e42ad6f079293f":[],"b0c7a8f7304b75b1528814c5820fa23a96816c27":["14ec33385f6fbb6ce172882d14605790418a5d31"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"ca0ffea399542e8aac8ed7608f34f8ec4cb8904d":["85a883878c0af761245ab048babc63d099f835f3"],"85130289d2ed101fcc2d8798511c7c5b020ffab4":["d619839baa8ce5503e496b94a9e42ad6f079293f"],"0e28c49f1fb6215a550fdadcf3805aa629b63ec0":["ca0ffea399542e8aac8ed7608f34f8ec4cb8904d"],"3bb13258feba31ab676502787ab2e1779f129b7a":["ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"1224a4027481acce15495b03bce9b48b93b42722":["d619839baa8ce5503e496b94a9e42ad6f079293f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["6c18273ea5b3974d2f30117f46f1ae416c28f727","d619839baa8ce5503e496b94a9e42ad6f079293f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}