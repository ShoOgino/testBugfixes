{"path":"solr/core/src/test/org/apache/solr/uninverting/TestNumericTerms64#beforeClass().mjava","commits":[{"id":"a076c3c721f685b7559308fdc2cd72d91bba67e5","date":1464168992,"type":1,"author":"Mike McCandless","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestNumericTerms64#beforeClass().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms64#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LegacyLongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LegacyLongField\n      field8 = new LegacyLongField(\"field8\", 0L, storedLong8),\n      field6 = new LegacyLongField(\"field6\", 0L, storedLong6),\n      field4 = new LegacyLongField(\"field4\", 0L, storedLong4),\n      field2 = new LegacyLongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LEGACY_LONG);\n    map.put(\"field4\", Type.LEGACY_LONG);\n    map.put(\"field6\", Type.LEGACY_LONG);\n    map.put(\"field8\", Type.LEGACY_LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LegacyLongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LegacyLongField\n      field8 = new LegacyLongField(\"field8\", 0L, storedLong8),\n      field6 = new LegacyLongField(\"field6\", 0L, storedLong6),\n      field4 = new LegacyLongField(\"field4\", 0L, storedLong4),\n      field2 = new LegacyLongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LEGACY_LONG);\n    map.put(\"field4\", Type.LEGACY_LONG);\n    map.put(\"field6\", Type.LEGACY_LONG);\n    map.put(\"field8\", Type.LEGACY_LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0e121d43b5a10f2df530f406f935102656e9c4e8","date":1464198131,"type":1,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestNumericTerms64#beforeClass().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms64#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LegacyLongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LegacyLongField\n      field8 = new LegacyLongField(\"field8\", 0L, storedLong8),\n      field6 = new LegacyLongField(\"field6\", 0L, storedLong6),\n      field4 = new LegacyLongField(\"field4\", 0L, storedLong4),\n      field2 = new LegacyLongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LEGACY_LONG);\n    map.put(\"field4\", Type.LEGACY_LONG);\n    map.put(\"field6\", Type.LEGACY_LONG);\n    map.put(\"field8\", Type.LEGACY_LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LegacyLongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LegacyLongField\n      field8 = new LegacyLongField(\"field8\", 0L, storedLong8),\n      field6 = new LegacyLongField(\"field6\", 0L, storedLong6),\n      field4 = new LegacyLongField(\"field4\", 0L, storedLong4),\n      field2 = new LegacyLongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LEGACY_LONG);\n    map.put(\"field4\", Type.LEGACY_LONG);\n    map.put(\"field6\", Type.LEGACY_LONG);\n    map.put(\"field8\", Type.LEGACY_LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"83870855d82aba6819217abeff5a40779dbb28b4","date":1464291012,"type":1,"author":"Mike McCandless","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestNumericTerms64#beforeClass().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms64#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LegacyLongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LegacyLongField\n      field8 = new LegacyLongField(\"field8\", 0L, storedLong8),\n      field6 = new LegacyLongField(\"field6\", 0L, storedLong6),\n      field4 = new LegacyLongField(\"field4\", 0L, storedLong4),\n      field2 = new LegacyLongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LEGACY_LONG);\n    map.put(\"field4\", Type.LEGACY_LONG);\n    map.put(\"field6\", Type.LEGACY_LONG);\n    map.put(\"field8\", Type.LEGACY_LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LegacyLongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LegacyLongField\n      field8 = new LegacyLongField(\"field8\", 0L, storedLong8),\n      field6 = new LegacyLongField(\"field6\", 0L, storedLong6),\n      field4 = new LegacyLongField(\"field4\", 0L, storedLong4),\n      field2 = new LegacyLongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LEGACY_LONG);\n    map.put(\"field4\", Type.LEGACY_LONG);\n    map.put(\"field6\", Type.LEGACY_LONG);\n    map.put(\"field8\", Type.LEGACY_LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5af5ba0166322092193d4c29880b0f7670fc7ca0","date":1471440525,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestNumericTerms64#beforeClass().mjava","pathOld":"solr/core/src/test/org/apache/solr/uninverting/TestNumericTerms64#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final LegacyFieldType storedLong = new LegacyFieldType(LegacyLongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final LegacyFieldType storedLong8 = new LegacyFieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final LegacyFieldType storedLong4 = new LegacyFieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final LegacyFieldType storedLong6 = new LegacyFieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final LegacyFieldType storedLong2 = new LegacyFieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LegacyLongField\n      field8 = new LegacyLongField(\"field8\", 0L, storedLong8),\n      field6 = new LegacyLongField(\"field6\", 0L, storedLong6),\n      field4 = new LegacyLongField(\"field4\", 0L, storedLong4),\n      field2 = new LegacyLongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LEGACY_LONG);\n    map.put(\"field4\", Type.LEGACY_LONG);\n    map.put(\"field6\", Type.LEGACY_LONG);\n    map.put(\"field8\", Type.LEGACY_LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LegacyLongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LegacyLongField\n      field8 = new LegacyLongField(\"field8\", 0L, storedLong8),\n      field6 = new LegacyLongField(\"field6\", 0L, storedLong6),\n      field4 = new LegacyLongField(\"field4\", 0L, storedLong4),\n      field2 = new LegacyLongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LEGACY_LONG);\n    map.put(\"field4\", Type.LEGACY_LONG);\n    map.put(\"field6\", Type.LEGACY_LONG);\n    map.put(\"field8\", Type.LEGACY_LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","date":1471496851,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestNumericTerms64#beforeClass().mjava","pathOld":"solr/core/src/test/org/apache/solr/uninverting/TestNumericTerms64#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final LegacyFieldType storedLong = new LegacyFieldType(LegacyLongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final LegacyFieldType storedLong8 = new LegacyFieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final LegacyFieldType storedLong4 = new LegacyFieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final LegacyFieldType storedLong6 = new LegacyFieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final LegacyFieldType storedLong2 = new LegacyFieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LegacyLongField\n      field8 = new LegacyLongField(\"field8\", 0L, storedLong8),\n      field6 = new LegacyLongField(\"field6\", 0L, storedLong6),\n      field4 = new LegacyLongField(\"field4\", 0L, storedLong4),\n      field2 = new LegacyLongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LEGACY_LONG);\n    map.put(\"field4\", Type.LEGACY_LONG);\n    map.put(\"field6\", Type.LEGACY_LONG);\n    map.put(\"field8\", Type.LEGACY_LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LegacyLongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LegacyLongField\n      field8 = new LegacyLongField(\"field8\", 0L, storedLong8),\n      field6 = new LegacyLongField(\"field6\", 0L, storedLong6),\n      field4 = new LegacyLongField(\"field4\", 0L, storedLong4),\n      field2 = new LegacyLongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LEGACY_LONG);\n    map.put(\"field4\", Type.LEGACY_LONG);\n    map.put(\"field6\", Type.LEGACY_LONG);\n    map.put(\"field8\", Type.LEGACY_LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"403d05f7f8d69b65659157eff1bc1d2717f04c66","date":1471692961,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestNumericTerms64#beforeClass().mjava","pathOld":"solr/core/src/test/org/apache/solr/uninverting/TestNumericTerms64#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final LegacyFieldType storedLong = new LegacyFieldType(LegacyLongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final LegacyFieldType storedLong8 = new LegacyFieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final LegacyFieldType storedLong4 = new LegacyFieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final LegacyFieldType storedLong6 = new LegacyFieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final LegacyFieldType storedLong2 = new LegacyFieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LegacyLongField\n      field8 = new LegacyLongField(\"field8\", 0L, storedLong8),\n      field6 = new LegacyLongField(\"field6\", 0L, storedLong6),\n      field4 = new LegacyLongField(\"field4\", 0L, storedLong4),\n      field2 = new LegacyLongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LEGACY_LONG);\n    map.put(\"field4\", Type.LEGACY_LONG);\n    map.put(\"field6\", Type.LEGACY_LONG);\n    map.put(\"field8\", Type.LEGACY_LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final FieldType storedLong = new FieldType(LegacyLongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final FieldType storedLong8 = new FieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final FieldType storedLong4 = new FieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final FieldType storedLong6 = new FieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final FieldType storedLong2 = new FieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LegacyLongField\n      field8 = new LegacyLongField(\"field8\", 0L, storedLong8),\n      field6 = new LegacyLongField(\"field6\", 0L, storedLong6),\n      field4 = new LegacyLongField(\"field4\", 0L, storedLong4),\n      field2 = new LegacyLongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LEGACY_LONG);\n    map.put(\"field4\", Type.LEGACY_LONG);\n    map.put(\"field6\", Type.LEGACY_LONG);\n    map.put(\"field8\", Type.LEGACY_LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestNumericTerms64#beforeClass().mjava","pathOld":"/dev/null","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    noDocs = atLeast(4096);\n    distance = (1L << 60) / noDocs;\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n        .setMergePolicy(newLogMergePolicy()));\n\n    final LegacyFieldType storedLong = new LegacyFieldType(LegacyLongField.TYPE_NOT_STORED);\n    storedLong.setStored(true);\n    storedLong.freeze();\n\n    final LegacyFieldType storedLong8 = new LegacyFieldType(storedLong);\n    storedLong8.setNumericPrecisionStep(8);\n\n    final LegacyFieldType storedLong4 = new LegacyFieldType(storedLong);\n    storedLong4.setNumericPrecisionStep(4);\n\n    final LegacyFieldType storedLong6 = new LegacyFieldType(storedLong);\n    storedLong6.setNumericPrecisionStep(6);\n\n    final LegacyFieldType storedLong2 = new LegacyFieldType(storedLong);\n    storedLong2.setNumericPrecisionStep(2);\n\n    LegacyLongField\n      field8 = new LegacyLongField(\"field8\", 0L, storedLong8),\n      field6 = new LegacyLongField(\"field6\", 0L, storedLong6),\n      field4 = new LegacyLongField(\"field4\", 0L, storedLong4),\n      field2 = new LegacyLongField(\"field2\", 0L, storedLong2);\n\n    Document doc = new Document();\n    // add fields, that have a distance to test general functionality\n    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);\n    \n    // Add a series of noDocs docs with increasing long values, by updating the fields\n    for (int l=0; l<noDocs; l++) {\n      long val=distance*l+startOffset;\n      field8.setLongValue(val);\n      field6.setLongValue(val);\n      field4.setLongValue(val);\n      field2.setLongValue(val);\n\n      val=l-(noDocs/2);\n      writer.addDocument(doc);\n    }\n    Map<String,Type> map = new HashMap<>();\n    map.put(\"field2\", Type.LEGACY_LONG);\n    map.put(\"field4\", Type.LEGACY_LONG);\n    map.put(\"field6\", Type.LEGACY_LONG);\n    map.put(\"field8\", Type.LEGACY_LONG);\n    reader = UninvertingReader.wrap(writer.getReader(), map);\n    searcher=newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a076c3c721f685b7559308fdc2cd72d91bba67e5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"403d05f7f8d69b65659157eff1bc1d2717f04c66":["0e121d43b5a10f2df530f406f935102656e9c4e8","2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"0e121d43b5a10f2df530f406f935102656e9c4e8":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","a076c3c721f685b7559308fdc2cd72d91bba67e5"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5af5ba0166322092193d4c29880b0f7670fc7ca0":["0e121d43b5a10f2df530f406f935102656e9c4e8"],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["0e121d43b5a10f2df530f406f935102656e9c4e8","5af5ba0166322092193d4c29880b0f7670fc7ca0"],"83870855d82aba6819217abeff5a40779dbb28b4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","0e121d43b5a10f2df530f406f935102656e9c4e8"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","403d05f7f8d69b65659157eff1bc1d2717f04c66"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["403d05f7f8d69b65659157eff1bc1d2717f04c66"]},"commit2Childs":{"a076c3c721f685b7559308fdc2cd72d91bba67e5":["0e121d43b5a10f2df530f406f935102656e9c4e8"],"403d05f7f8d69b65659157eff1bc1d2717f04c66":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"0e121d43b5a10f2df530f406f935102656e9c4e8":["403d05f7f8d69b65659157eff1bc1d2717f04c66","5af5ba0166322092193d4c29880b0f7670fc7ca0","2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","83870855d82aba6819217abeff5a40779dbb28b4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a076c3c721f685b7559308fdc2cd72d91bba67e5","0e121d43b5a10f2df530f406f935102656e9c4e8","83870855d82aba6819217abeff5a40779dbb28b4","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"5af5ba0166322092193d4c29880b0f7670fc7ca0":["2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["403d05f7f8d69b65659157eff1bc1d2717f04c66"],"83870855d82aba6819217abeff5a40779dbb28b4":[],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["83870855d82aba6819217abeff5a40779dbb28b4","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}