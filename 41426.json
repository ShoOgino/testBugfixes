{"path":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","commits":[{"id":"ae695f21c50b03702b5d0fa2543d5af844bb7cd3","date":1331554994,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],IndexWriter.ReadersAndLiveDocs,SegmentReader).mjava","sourceNew":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(null);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes(), false)) {\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, false);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, IndexWriter.ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(null);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes(), false)) {\n        DocsEnum docsEnum = termsEnum.docs(rld.liveDocs, docs, false);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"38e3b736c7ca086d61b7dbb841c905ee115490da","date":1331657018,"type":1,"author":"Ryan McKinley","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],IndexWriter.ReadersAndLiveDocs,SegmentReader).mjava","sourceNew":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(null);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes(), false)) {\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, false);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, IndexWriter.ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(null);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes(), false)) {\n        DocsEnum docsEnum = termsEnum.docs(rld.liveDocs, docs, false);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d08eba3d52b63561ebf936481ce73e6b6a14aa03","date":1333879759,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","sourceNew":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    InvertedFields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(null);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes(), false)) {\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, false);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(null);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes(), false)) {\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, false);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf","date":1333892281,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","sourceNew":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(null);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes(), false)) {\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, false);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    InvertedFields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(null);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes(), false)) {\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, false);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"02331260bb246364779cb6f04919ca47900d01bb","date":1343749884,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","sourceNew":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(null);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes(), false)) {\n        // we don't need term frequencies for this\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, 0);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(null);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes(), false)) {\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, false);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","bugFix":["ae695f21c50b03702b5d0fa2543d5af844bb7cd3"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","date":1343768312,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","sourceNew":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(null);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes(), false)) {\n        // we don't need term frequencies for this\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, 0);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(null);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes(), false)) {\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, false);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","sourceNew":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(null);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes(), false)) {\n        // we don't need term frequencies for this\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, 0);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(null);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes(), false)) {\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, false);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"15250ca94ba8ab3bcdd476daf6bf3f3febb92640","date":1355200097,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","sourceNew":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(null);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes(), false)) {\n        // we don't need term frequencies for this\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, DocsEnum.FLAG_NONE);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(null);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes(), false)) {\n        // we don't need term frequencies for this\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, 0);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","sourceNew":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(null);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes(), false)) {\n        // we don't need term frequencies for this\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, DocsEnum.FLAG_NONE);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(null);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes(), false)) {\n        // we don't need term frequencies for this\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, 0);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eee5f2a24465d2c9a5f86ab84b7c35041a30fda8","date":1373996650,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","sourceNew":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(null);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes())) {\n        // we don't need term frequencies for this\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, DocsEnum.FLAG_NONE);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(null);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes(), false)) {\n        // we don't need term frequencies for this\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, DocsEnum.FLAG_NONE);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","date":1376366778,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","sourceNew":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(null);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes())) {\n        // we don't need term frequencies for this\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, DocsEnum.FLAG_NONE);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(null);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes(), false)) {\n        // we don't need term frequencies for this\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, DocsEnum.FLAG_NONE);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e072d0b1fc19e0533d8ce432eed245196bca6fde","date":1379265112,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","sourceNew":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(null);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes())) {\n        // we don't need term frequencies for this\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, DocsEnum.FLAG_NONE);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(null);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes())) {\n        // we don't need term frequencies for this\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, DocsEnum.FLAG_NONE);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"19cb6a37efbd77a255f20671e94b6d2b08874241","date":1380833281,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","sourceNew":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(termsEnum);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes())) {\n        // we don't need term frequencies for this\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, DocsEnum.FLAG_NONE);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(null);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes())) {\n        // we don't need term frequencies for this\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, DocsEnum.FLAG_NONE);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0","date":1383367127,"type":5,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(Iterable[Term],ReadersAndUpdates,SegmentReader).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream#applyTermDeletes(Iterable[Term],ReadersAndLiveDocs,SegmentReader).mjava","sourceNew":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndUpdates rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(termsEnum);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes())) {\n        // we don't need term frequencies for this\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, DocsEnum.FLAG_NONE);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  // Delete by Term\n  private synchronized long applyTermDeletes(Iterable<Term> termsIter, ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {\n    long delCount = 0;\n    Fields fields = reader.fields();\n    if (fields == null) {\n      // This reader has no postings\n      return 0;\n    }\n\n    TermsEnum termsEnum = null;\n\n    String currentField = null;\n    DocsEnum docs = null;\n\n    assert checkDeleteTerm(null);\n\n    boolean any = false;\n\n    //System.out.println(Thread.currentThread().getName() + \" del terms reader=\" + reader);\n    for (Term term : termsIter) {\n      // Since we visit terms sorted, we gain performance\n      // by re-using the same TermsEnum and seeking only\n      // forwards\n      if (!term.field().equals(currentField)) {\n        assert currentField == null || currentField.compareTo(term.field()) < 0;\n        currentField = term.field();\n        Terms terms = fields.terms(currentField);\n        if (terms != null) {\n          termsEnum = terms.iterator(termsEnum);\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      if (termsEnum == null) {\n        continue;\n      }\n      assert checkDeleteTerm(term);\n\n      // System.out.println(\"  term=\" + term);\n\n      if (termsEnum.seekExact(term.bytes())) {\n        // we don't need term frequencies for this\n        DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, DocsEnum.FLAG_NONE);\n        //System.out.println(\"BDS: got docsEnum=\" + docsEnum);\n\n        if (docsEnum != null) {\n          while (true) {\n            final int docID = docsEnum.nextDoc();\n            //System.out.println(Thread.currentThread().getName() + \" del term=\" + term + \" doc=\" + docID);\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }   \n            if (!any) {\n              rld.initWritableLiveDocs();\n              any = true;\n            }\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"19cb6a37efbd77a255f20671e94b6d2b08874241":["e072d0b1fc19e0533d8ce432eed245196bca6fde"],"e072d0b1fc19e0533d8ce432eed245196bca6fde":["eee5f2a24465d2c9a5f86ab84b7c35041a30fda8"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["02331260bb246364779cb6f04919ca47900d01bb","15250ca94ba8ab3bcdd476daf6bf3f3febb92640"],"38e3b736c7ca086d61b7dbb841c905ee115490da":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","ae695f21c50b03702b5d0fa2543d5af844bb7cd3"],"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["19cb6a37efbd77a255f20671e94b6d2b08874241"],"ae695f21c50b03702b5d0fa2543d5af844bb7cd3":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"d08eba3d52b63561ebf936481ce73e6b6a14aa03":["ae695f21c50b03702b5d0fa2543d5af844bb7cd3"],"15250ca94ba8ab3bcdd476daf6bf3f3febb92640":["02331260bb246364779cb6f04919ca47900d01bb"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf","02331260bb246364779cb6f04919ca47900d01bb"],"e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf":["d08eba3d52b63561ebf936481ce73e6b6a14aa03"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["15250ca94ba8ab3bcdd476daf6bf3f3febb92640"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf","02331260bb246364779cb6f04919ca47900d01bb"],"eee5f2a24465d2c9a5f86ab84b7c35041a30fda8":["15250ca94ba8ab3bcdd476daf6bf3f3febb92640"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"02331260bb246364779cb6f04919ca47900d01bb":["e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf"]},"commit2Childs":{"19cb6a37efbd77a255f20671e94b6d2b08874241":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"e072d0b1fc19e0533d8ce432eed245196bca6fde":["19cb6a37efbd77a255f20671e94b6d2b08874241"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":[],"38e3b736c7ca086d61b7dbb841c905ee115490da":[],"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ae695f21c50b03702b5d0fa2543d5af844bb7cd3":["38e3b736c7ca086d61b7dbb841c905ee115490da","d08eba3d52b63561ebf936481ce73e6b6a14aa03"],"d08eba3d52b63561ebf936481ce73e6b6a14aa03":["e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf"],"15250ca94ba8ab3bcdd476daf6bf3f3febb92640":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","eee5f2a24465d2c9a5f86ab84b7c35041a30fda8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["38e3b736c7ca086d61b7dbb841c905ee115490da","ae695f21c50b03702b5d0fa2543d5af844bb7cd3"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":[],"e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b","02331260bb246364779cb6f04919ca47900d01bb"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":[],"d6f074e73200c07d54f242d3880a8da5a35ff97b":[],"eee5f2a24465d2c9a5f86ab84b7c35041a30fda8":["e072d0b1fc19e0533d8ce432eed245196bca6fde"],"02331260bb246364779cb6f04919ca47900d01bb":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","15250ca94ba8ab3bcdd476daf6bf3f3febb92640","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","38e3b736c7ca086d61b7dbb841c905ee115490da","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","d6f074e73200c07d54f242d3880a8da5a35ff97b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}