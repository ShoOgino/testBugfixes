{"path":"solr/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getTopTerms(IndexReader,Set[String],int,Set[String]).mjava","commits":[{"id":"893269407f5c988a4b2ee13c7ffc01ec43136c1d","date":1268598046,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getTopTerms(IndexReader,Set[String],int,Set[String]).mjava","pathOld":"/dev/null","sourceNew":"  private static Map<String,TopTermQueue> getTopTerms( IndexReader reader, Set<String> fields, int numTerms, Set<String> junkWords ) throws Exception \n  {\n    Map<String,TopTermQueue> info = new HashMap<String, TopTermQueue>();\n    \n    TermEnum terms = null;\n    try{\n      terms = reader.terms();    \n      while (terms.next()) {\n        String field = terms.term().field();\n        String t = terms.term().text();\n  \n        // Compute distinct terms for every field\n        TopTermQueue tiq = info.get( field );\n        if( tiq == null ) {\n          tiq = new TopTermQueue( numTerms+1 );\n          info.put( field, tiq );\n        }\n        tiq.distinctTerms++;\n        tiq.histogram.add( terms.docFreq() );  // add the term to the histogram\n        \n        // Only save the distinct terms for fields we worry about\n        if (fields != null && fields.size() > 0) {\n          if( !fields.contains( field ) ) {\n            continue;\n          }\n        }\n        if( junkWords != null && junkWords.contains( t ) ) {\n          continue;\n        }\n        \n        if( terms.docFreq() > tiq.minFreq ) {\n          tiq.put(new TopTermQueue.TermInfo(terms.term(), terms.docFreq()));\n            if (tiq.size() > numTerms) { // if tiq full\n            tiq.pop(); // remove lowest in tiq\n            tiq.minFreq = ((TopTermQueue.TermInfo)tiq.top()).docFreq; // reset minFreq\n          }\n        }\n      }\n    }\n    finally {\n      if( terms != null ) terms.close();\n    }\n    return info;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8","date":1268599006,"type":4,"author":"Mark Robert Miller","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getTopTerms(IndexReader,Set[String],int,Set[String]).mjava","sourceNew":null,"sourceOld":"  private static Map<String,TopTermQueue> getTopTerms( IndexReader reader, Set<String> fields, int numTerms, Set<String> junkWords ) throws Exception \n  {\n    Map<String,TopTermQueue> info = new HashMap<String, TopTermQueue>();\n    \n    TermEnum terms = null;\n    try{\n      terms = reader.terms();    \n      while (terms.next()) {\n        String field = terms.term().field();\n        String t = terms.term().text();\n  \n        // Compute distinct terms for every field\n        TopTermQueue tiq = info.get( field );\n        if( tiq == null ) {\n          tiq = new TopTermQueue( numTerms+1 );\n          info.put( field, tiq );\n        }\n        tiq.distinctTerms++;\n        tiq.histogram.add( terms.docFreq() );  // add the term to the histogram\n        \n        // Only save the distinct terms for fields we worry about\n        if (fields != null && fields.size() > 0) {\n          if( !fields.contains( field ) ) {\n            continue;\n          }\n        }\n        if( junkWords != null && junkWords.contains( t ) ) {\n          continue;\n        }\n        \n        if( terms.docFreq() > tiq.minFreq ) {\n          tiq.put(new TopTermQueue.TermInfo(terms.term(), terms.docFreq()));\n            if (tiq.size() > numTerms) { // if tiq full\n            tiq.pop(); // remove lowest in tiq\n            tiq.minFreq = ((TopTermQueue.TermInfo)tiq.top()).docFreq; // reset minFreq\n          }\n        }\n      }\n    }\n    finally {\n      if( terms != null ) terms.close();\n    }\n    return info;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getTopTerms(IndexReader,Set[String],int,Set[String]).mjava","pathOld":"src/java/org/apache/solr/handler/admin/LukeRequestHandler#getTopTerms(IndexReader,Set[String],int,Set[String]).mjava","sourceNew":"  private static Map<String,TopTermQueue> getTopTerms( IndexReader reader, Set<String> fields, int numTerms, Set<String> junkWords ) throws Exception \n  {\n    Map<String,TopTermQueue> info = new HashMap<String, TopTermQueue>();\n    \n    TermEnum terms = null;\n    try{\n      terms = reader.terms();    \n      while (terms.next()) {\n        String field = terms.term().field();\n        String t = terms.term().text();\n  \n        // Compute distinct terms for every field\n        TopTermQueue tiq = info.get( field );\n        if( tiq == null ) {\n          tiq = new TopTermQueue( numTerms+1 );\n          info.put( field, tiq );\n        }\n        tiq.distinctTerms++;\n        tiq.histogram.add( terms.docFreq() );  // add the term to the histogram\n        \n        // Only save the distinct terms for fields we worry about\n        if (fields != null && fields.size() > 0) {\n          if( !fields.contains( field ) ) {\n            continue;\n          }\n        }\n        if( junkWords != null && junkWords.contains( t ) ) {\n          continue;\n        }\n        \n        if( terms.docFreq() > tiq.minFreq ) {\n          tiq.add(new TopTermQueue.TermInfo(terms.term(), terms.docFreq()));\n            if (tiq.size() > numTerms) { // if tiq full\n            tiq.pop(); // remove lowest in tiq\n            tiq.minFreq = ((TopTermQueue.TermInfo)tiq.top()).docFreq; // reset minFreq\n          }\n        }\n      }\n    }\n    finally {\n      if( terms != null ) terms.close();\n    }\n    return info;\n  }\n\n","sourceOld":"  private static Map<String,TopTermQueue> getTopTerms( IndexReader reader, Set<String> fields, int numTerms, Set<String> junkWords ) throws Exception \n  {\n    Map<String,TopTermQueue> info = new HashMap<String, TopTermQueue>();\n    \n    TermEnum terms = null;\n    try{\n      terms = reader.terms();    \n      while (terms.next()) {\n        String field = terms.term().field();\n        String t = terms.term().text();\n  \n        // Compute distinct terms for every field\n        TopTermQueue tiq = info.get( field );\n        if( tiq == null ) {\n          tiq = new TopTermQueue( numTerms+1 );\n          info.put( field, tiq );\n        }\n        tiq.distinctTerms++;\n        tiq.histogram.add( terms.docFreq() );  // add the term to the histogram\n        \n        // Only save the distinct terms for fields we worry about\n        if (fields != null && fields.size() > 0) {\n          if( !fields.contains( field ) ) {\n            continue;\n          }\n        }\n        if( junkWords != null && junkWords.contains( t ) ) {\n          continue;\n        }\n        \n        if( terms.docFreq() > tiq.minFreq ) {\n          tiq.add(new TopTermQueue.TermInfo(terms.term(), terms.docFreq()));\n            if (tiq.size() > numTerms) { // if tiq full\n            tiq.pop(); // remove lowest in tiq\n            tiq.minFreq = ((TopTermQueue.TermInfo)tiq.top()).docFreq; // reset minFreq\n          }\n        }\n      }\n    }\n    finally {\n      if( terms != null ) terms.close();\n    }\n    return info;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1da8d55113b689b06716246649de6f62430f15c0","date":1453508340,"type":0,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getTopTerms(IndexReader,Set[String],int,Set[String]).mjava","pathOld":"/dev/null","sourceNew":"  private static Map<String,TopTermQueue> getTopTerms( IndexReader reader, Set<String> fields, int numTerms, Set<String> junkWords ) throws Exception \n  {\n    Map<String,TopTermQueue> info = new HashMap<String, TopTermQueue>();\n    \n    TermEnum terms = null;\n    try{\n      terms = reader.terms();    \n      while (terms.next()) {\n        String field = terms.term().field();\n        String t = terms.term().text();\n  \n        // Compute distinct terms for every field\n        TopTermQueue tiq = info.get( field );\n        if( tiq == null ) {\n          tiq = new TopTermQueue( numTerms+1 );\n          info.put( field, tiq );\n        }\n        tiq.distinctTerms++;\n        tiq.histogram.add( terms.docFreq() );  // add the term to the histogram\n        \n        // Only save the distinct terms for fields we worry about\n        if (fields != null && fields.size() > 0) {\n          if( !fields.contains( field ) ) {\n            continue;\n          }\n        }\n        if( junkWords != null && junkWords.contains( t ) ) {\n          continue;\n        }\n        \n        if( terms.docFreq() > tiq.minFreq ) {\n          tiq.add(new TopTermQueue.TermInfo(terms.term(), terms.docFreq()));\n            if (tiq.size() > numTerms) { // if tiq full\n            tiq.pop(); // remove lowest in tiq\n            tiq.minFreq = ((TopTermQueue.TermInfo)tiq.top()).docFreq; // reset minFreq\n          }\n        }\n      }\n    }\n    finally {\n      if( terms != null ) terms.close();\n    }\n    return info;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28427ef110c4c5bf5b4057731b83110bd1e13724","date":1276701452,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getTopTerms(IndexReader,Set[String],int,Set[String]).mjava","pathOld":"solr/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getTopTerms(IndexReader,Set[String],int,Set[String]).mjava","sourceNew":"  private static Map<String,TopTermQueue> getTopTerms( IndexReader reader, Set<String> fields, int numTerms, Set<String> junkWords ) throws Exception \n  {\n    Map<String,TopTermQueue> info = new HashMap<String, TopTermQueue>();\n\n    Fields fieldsC = MultiFields.getFields(reader);\n    if (fieldsC != null) {\n      FieldsEnum fieldsEnum = fieldsC.iterator();\n      String field;\n      while((field = fieldsEnum.next()) != null) {\n\n        TermsEnum termsEnum = fieldsEnum.terms();\n        BytesRef text;\n        while((text = termsEnum.next()) != null) {\n          String t = text.utf8ToString();\n  \n          // Compute distinct terms for every field\n          TopTermQueue tiq = info.get( field );\n          if( tiq == null ) {\n            tiq = new TopTermQueue( numTerms+1 );\n            info.put( field, tiq );\n          }\n\n          tiq.distinctTerms++;\n          tiq.histogram.add( termsEnum.docFreq() );  // add the term to the histogram\n        \n          // Only save the distinct terms for fields we worry about\n          if (fields != null && fields.size() > 0) {\n            if( !fields.contains( field ) ) {\n              continue;\n            }\n          }\n          if( junkWords != null && junkWords.contains( t ) ) {\n            continue;\n          }\n        \n          if( termsEnum.docFreq() > tiq.minFreq ) {\n            tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n            if (tiq.size() > numTerms) { // if tiq full\n              tiq.pop(); // remove lowest in tiq\n              tiq.minFreq = ((TopTermQueue.TermInfo)tiq.top()).docFreq; // reset minFreq\n            }\n          }\n        }\n      }\n    }\n    return info;\n  }\n\n","sourceOld":"  private static Map<String,TopTermQueue> getTopTerms( IndexReader reader, Set<String> fields, int numTerms, Set<String> junkWords ) throws Exception \n  {\n    Map<String,TopTermQueue> info = new HashMap<String, TopTermQueue>();\n    \n    TermEnum terms = null;\n    try{\n      terms = reader.terms();    \n      while (terms.next()) {\n        String field = terms.term().field();\n        String t = terms.term().text();\n  \n        // Compute distinct terms for every field\n        TopTermQueue tiq = info.get( field );\n        if( tiq == null ) {\n          tiq = new TopTermQueue( numTerms+1 );\n          info.put( field, tiq );\n        }\n        tiq.distinctTerms++;\n        tiq.histogram.add( terms.docFreq() );  // add the term to the histogram\n        \n        // Only save the distinct terms for fields we worry about\n        if (fields != null && fields.size() > 0) {\n          if( !fields.contains( field ) ) {\n            continue;\n          }\n        }\n        if( junkWords != null && junkWords.contains( t ) ) {\n          continue;\n        }\n        \n        if( terms.docFreq() > tiq.minFreq ) {\n          tiq.add(new TopTermQueue.TermInfo(terms.term(), terms.docFreq()));\n            if (tiq.size() > numTerms) { // if tiq full\n            tiq.pop(); // remove lowest in tiq\n            tiq.minFreq = ((TopTermQueue.TermInfo)tiq.top()).docFreq; // reset minFreq\n          }\n        }\n      }\n    }\n    finally {\n      if( terms != null ) terms.close();\n    }\n    return info;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getTopTerms(IndexReader,Set[String],int,Set[String]).mjava","pathOld":"solr/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getTopTerms(IndexReader,Set[String],int,Set[String]).mjava","sourceNew":"  private static Map<String,TopTermQueue> getTopTerms( IndexReader reader, Set<String> fields, int numTerms, Set<String> junkWords ) throws Exception \n  {\n    Map<String,TopTermQueue> info = new HashMap<String, TopTermQueue>();\n\n    Fields fieldsC = MultiFields.getFields(reader);\n    if (fieldsC != null) {\n      FieldsEnum fieldsEnum = fieldsC.iterator();\n      String field;\n      while((field = fieldsEnum.next()) != null) {\n\n        TermsEnum termsEnum = fieldsEnum.terms();\n        BytesRef text;\n        while((text = termsEnum.next()) != null) {\n          String t = text.utf8ToString();\n  \n          // Compute distinct terms for every field\n          TopTermQueue tiq = info.get( field );\n          if( tiq == null ) {\n            tiq = new TopTermQueue( numTerms+1 );\n            info.put( field, tiq );\n          }\n\n          tiq.distinctTerms++;\n          tiq.histogram.add( termsEnum.docFreq() );  // add the term to the histogram\n        \n          // Only save the distinct terms for fields we worry about\n          if (fields != null && fields.size() > 0) {\n            if( !fields.contains( field ) ) {\n              continue;\n            }\n          }\n          if( junkWords != null && junkWords.contains( t ) ) {\n            continue;\n          }\n        \n          if( termsEnum.docFreq() > tiq.minFreq ) {\n            tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n            if (tiq.size() > numTerms) { // if tiq full\n              tiq.pop(); // remove lowest in tiq\n              tiq.minFreq = ((TopTermQueue.TermInfo)tiq.top()).docFreq; // reset minFreq\n            }\n          }\n        }\n      }\n    }\n    return info;\n  }\n\n","sourceOld":"  private static Map<String,TopTermQueue> getTopTerms( IndexReader reader, Set<String> fields, int numTerms, Set<String> junkWords ) throws Exception \n  {\n    Map<String,TopTermQueue> info = new HashMap<String, TopTermQueue>();\n    \n    TermEnum terms = null;\n    try{\n      terms = reader.terms();    \n      while (terms.next()) {\n        String field = terms.term().field();\n        String t = terms.term().text();\n  \n        // Compute distinct terms for every field\n        TopTermQueue tiq = info.get( field );\n        if( tiq == null ) {\n          tiq = new TopTermQueue( numTerms+1 );\n          info.put( field, tiq );\n        }\n        tiq.distinctTerms++;\n        tiq.histogram.add( terms.docFreq() );  // add the term to the histogram\n        \n        // Only save the distinct terms for fields we worry about\n        if (fields != null && fields.size() > 0) {\n          if( !fields.contains( field ) ) {\n            continue;\n          }\n        }\n        if( junkWords != null && junkWords.contains( t ) ) {\n          continue;\n        }\n        \n        if( terms.docFreq() > tiq.minFreq ) {\n          tiq.add(new TopTermQueue.TermInfo(terms.term(), terms.docFreq()));\n            if (tiq.size() > numTerms) { // if tiq full\n            tiq.pop(); // remove lowest in tiq\n            tiq.minFreq = ((TopTermQueue.TermInfo)tiq.top()).docFreq; // reset minFreq\n          }\n        }\n      }\n    }\n    finally {\n      if( terms != null ) terms.close();\n    }\n    return info;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a1b3a24d5d9b47345473ff564f5cc127a7b526b4","date":1306277076,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getTopTerms(IndexReader,Set[String],int,Set[String]).mjava","pathOld":"solr/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getTopTerms(IndexReader,Set[String],int,Set[String]).mjava","sourceNew":"  private static Map<String,TopTermQueue> getTopTerms( IndexReader reader, Set<String> fields, int numTerms, Set<String> junkWords ) throws Exception \n  {\n    Map<String,TopTermQueue> info = new HashMap<String, TopTermQueue>();\n    final CharsRef spare = new CharsRef();\n    Fields fieldsC = MultiFields.getFields(reader);\n    if (fieldsC != null) {\n      FieldsEnum fieldsEnum = fieldsC.iterator();\n      String field;\n      while((field = fieldsEnum.next()) != null) {\n\n        TermsEnum termsEnum = fieldsEnum.terms();\n        BytesRef text;\n        while((text = termsEnum.next()) != null) {\n          String t = text.utf8ToChars(spare).toString();\n  \n          // Compute distinct terms for every field\n          TopTermQueue tiq = info.get( field );\n          if( tiq == null ) {\n            tiq = new TopTermQueue( numTerms+1 );\n            info.put( field, tiq );\n          }\n\n          tiq.distinctTerms++;\n          tiq.histogram.add( termsEnum.docFreq() );  // add the term to the histogram\n        \n          // Only save the distinct terms for fields we worry about\n          if (fields != null && fields.size() > 0) {\n            if( !fields.contains( field ) ) {\n              continue;\n            }\n          }\n          if( junkWords != null && junkWords.contains( t ) ) {\n            continue;\n          }\n        \n          if( termsEnum.docFreq() > tiq.minFreq ) {\n            tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n            if (tiq.size() > numTerms) { // if tiq full\n              tiq.pop(); // remove lowest in tiq\n              tiq.minFreq = ((TopTermQueue.TermInfo)tiq.top()).docFreq; // reset minFreq\n            }\n          }\n        }\n      }\n    }\n    return info;\n  }\n\n","sourceOld":"  private static Map<String,TopTermQueue> getTopTerms( IndexReader reader, Set<String> fields, int numTerms, Set<String> junkWords ) throws Exception \n  {\n    Map<String,TopTermQueue> info = new HashMap<String, TopTermQueue>();\n\n    Fields fieldsC = MultiFields.getFields(reader);\n    if (fieldsC != null) {\n      FieldsEnum fieldsEnum = fieldsC.iterator();\n      String field;\n      while((field = fieldsEnum.next()) != null) {\n\n        TermsEnum termsEnum = fieldsEnum.terms();\n        BytesRef text;\n        while((text = termsEnum.next()) != null) {\n          String t = text.utf8ToString();\n  \n          // Compute distinct terms for every field\n          TopTermQueue tiq = info.get( field );\n          if( tiq == null ) {\n            tiq = new TopTermQueue( numTerms+1 );\n            info.put( field, tiq );\n          }\n\n          tiq.distinctTerms++;\n          tiq.histogram.add( termsEnum.docFreq() );  // add the term to the histogram\n        \n          // Only save the distinct terms for fields we worry about\n          if (fields != null && fields.size() > 0) {\n            if( !fields.contains( field ) ) {\n              continue;\n            }\n          }\n          if( junkWords != null && junkWords.contains( t ) ) {\n            continue;\n          }\n        \n          if( termsEnum.docFreq() > tiq.minFreq ) {\n            tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n            if (tiq.size() > numTerms) { // if tiq full\n              tiq.pop(); // remove lowest in tiq\n              tiq.minFreq = ((TopTermQueue.TermInfo)tiq.top()).docFreq; // reset minFreq\n            }\n          }\n        }\n      }\n    }\n    return info;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","date":1306767085,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getTopTerms(IndexReader,Set[String],int,Set[String]).mjava","pathOld":"solr/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getTopTerms(IndexReader,Set[String],int,Set[String]).mjava","sourceNew":"  private static Map<String,TopTermQueue> getTopTerms( IndexReader reader, Set<String> fields, int numTerms, Set<String> junkWords ) throws Exception \n  {\n    Map<String,TopTermQueue> info = new HashMap<String, TopTermQueue>();\n    final CharsRef spare = new CharsRef();\n    Fields fieldsC = MultiFields.getFields(reader);\n    if (fieldsC != null) {\n      FieldsEnum fieldsEnum = fieldsC.iterator();\n      String field;\n      while((field = fieldsEnum.next()) != null) {\n\n        TermsEnum termsEnum = fieldsEnum.terms();\n        BytesRef text;\n        while((text = termsEnum.next()) != null) {\n          String t = text.utf8ToChars(spare).toString();\n  \n          // Compute distinct terms for every field\n          TopTermQueue tiq = info.get( field );\n          if( tiq == null ) {\n            tiq = new TopTermQueue( numTerms+1 );\n            info.put( field, tiq );\n          }\n\n          tiq.distinctTerms++;\n          tiq.histogram.add( termsEnum.docFreq() );  // add the term to the histogram\n        \n          // Only save the distinct terms for fields we worry about\n          if (fields != null && fields.size() > 0) {\n            if( !fields.contains( field ) ) {\n              continue;\n            }\n          }\n          if( junkWords != null && junkWords.contains( t ) ) {\n            continue;\n          }\n        \n          if( termsEnum.docFreq() > tiq.minFreq ) {\n            tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n            if (tiq.size() > numTerms) { // if tiq full\n              tiq.pop(); // remove lowest in tiq\n              tiq.minFreq = ((TopTermQueue.TermInfo)tiq.top()).docFreq; // reset minFreq\n            }\n          }\n        }\n      }\n    }\n    return info;\n  }\n\n","sourceOld":"  private static Map<String,TopTermQueue> getTopTerms( IndexReader reader, Set<String> fields, int numTerms, Set<String> junkWords ) throws Exception \n  {\n    Map<String,TopTermQueue> info = new HashMap<String, TopTermQueue>();\n\n    Fields fieldsC = MultiFields.getFields(reader);\n    if (fieldsC != null) {\n      FieldsEnum fieldsEnum = fieldsC.iterator();\n      String field;\n      while((field = fieldsEnum.next()) != null) {\n\n        TermsEnum termsEnum = fieldsEnum.terms();\n        BytesRef text;\n        while((text = termsEnum.next()) != null) {\n          String t = text.utf8ToString();\n  \n          // Compute distinct terms for every field\n          TopTermQueue tiq = info.get( field );\n          if( tiq == null ) {\n            tiq = new TopTermQueue( numTerms+1 );\n            info.put( field, tiq );\n          }\n\n          tiq.distinctTerms++;\n          tiq.histogram.add( termsEnum.docFreq() );  // add the term to the histogram\n        \n          // Only save the distinct terms for fields we worry about\n          if (fields != null && fields.size() > 0) {\n            if( !fields.contains( field ) ) {\n              continue;\n            }\n          }\n          if( junkWords != null && junkWords.contains( t ) ) {\n            continue;\n          }\n        \n          if( termsEnum.docFreq() > tiq.minFreq ) {\n            tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n            if (tiq.size() > numTerms) { // if tiq full\n              tiq.pop(); // remove lowest in tiq\n              tiq.minFreq = ((TopTermQueue.TermInfo)tiq.top()).docFreq; // reset minFreq\n            }\n          }\n        }\n      }\n    }\n    return info;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2e10cb22a8bdb44339e282925a29182bb2f3174d","date":1306841137,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getTopTerms(IndexReader,Set[String],int,Set[String]).mjava","pathOld":"solr/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getTopTerms(IndexReader,Set[String],int,Set[String]).mjava","sourceNew":"  private static Map<String,TopTermQueue> getTopTerms( IndexReader reader, Set<String> fields, int numTerms, Set<String> junkWords ) throws Exception \n  {\n    Map<String,TopTermQueue> info = new HashMap<String, TopTermQueue>();\n    final CharsRef spare = new CharsRef();\n    Fields fieldsC = MultiFields.getFields(reader);\n    if (fieldsC != null) {\n      FieldsEnum fieldsEnum = fieldsC.iterator();\n      String field;\n      while((field = fieldsEnum.next()) != null) {\n\n        TermsEnum termsEnum = fieldsEnum.terms();\n        BytesRef text;\n        while((text = termsEnum.next()) != null) {\n          String t = text.utf8ToChars(spare).toString();\n  \n          // Compute distinct terms for every field\n          TopTermQueue tiq = info.get( field );\n          if( tiq == null ) {\n            tiq = new TopTermQueue( numTerms+1 );\n            info.put( field, tiq );\n          }\n\n          tiq.distinctTerms++;\n          tiq.histogram.add( termsEnum.docFreq() );  // add the term to the histogram\n        \n          // Only save the distinct terms for fields we worry about\n          if (fields != null && fields.size() > 0) {\n            if( !fields.contains( field ) ) {\n              continue;\n            }\n          }\n          if( junkWords != null && junkWords.contains( t ) ) {\n            continue;\n          }\n        \n          if( termsEnum.docFreq() > tiq.minFreq ) {\n            tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n            if (tiq.size() > numTerms) { // if tiq full\n              tiq.pop(); // remove lowest in tiq\n              tiq.minFreq = ((TopTermQueue.TermInfo)tiq.top()).docFreq; // reset minFreq\n            }\n          }\n        }\n      }\n    }\n    return info;\n  }\n\n","sourceOld":"  private static Map<String,TopTermQueue> getTopTerms( IndexReader reader, Set<String> fields, int numTerms, Set<String> junkWords ) throws Exception \n  {\n    Map<String,TopTermQueue> info = new HashMap<String, TopTermQueue>();\n\n    Fields fieldsC = MultiFields.getFields(reader);\n    if (fieldsC != null) {\n      FieldsEnum fieldsEnum = fieldsC.iterator();\n      String field;\n      while((field = fieldsEnum.next()) != null) {\n\n        TermsEnum termsEnum = fieldsEnum.terms();\n        BytesRef text;\n        while((text = termsEnum.next()) != null) {\n          String t = text.utf8ToString();\n  \n          // Compute distinct terms for every field\n          TopTermQueue tiq = info.get( field );\n          if( tiq == null ) {\n            tiq = new TopTermQueue( numTerms+1 );\n            info.put( field, tiq );\n          }\n\n          tiq.distinctTerms++;\n          tiq.histogram.add( termsEnum.docFreq() );  // add the term to the histogram\n        \n          // Only save the distinct terms for fields we worry about\n          if (fields != null && fields.size() > 0) {\n            if( !fields.contains( field ) ) {\n              continue;\n            }\n          }\n          if( junkWords != null && junkWords.contains( t ) ) {\n            continue;\n          }\n        \n          if( termsEnum.docFreq() > tiq.minFreq ) {\n            tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n            if (tiq.size() > numTerms) { // if tiq full\n              tiq.pop(); // remove lowest in tiq\n              tiq.minFreq = ((TopTermQueue.TermInfo)tiq.top()).docFreq; // reset minFreq\n            }\n          }\n        }\n      }\n    }\n    return info;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getTopTerms(IndexReader,Set[String],int,Set[String]).mjava","pathOld":"solr/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getTopTerms(IndexReader,Set[String],int,Set[String]).mjava","sourceNew":"  private static Map<String,TopTermQueue> getTopTerms( IndexReader reader, Set<String> fields, int numTerms, Set<String> junkWords ) throws Exception \n  {\n    Map<String,TopTermQueue> info = new HashMap<String, TopTermQueue>();\n    final CharsRef spare = new CharsRef();\n    Fields fieldsC = MultiFields.getFields(reader);\n    if (fieldsC != null) {\n      FieldsEnum fieldsEnum = fieldsC.iterator();\n      String field;\n      while((field = fieldsEnum.next()) != null) {\n\n        TermsEnum termsEnum = fieldsEnum.terms();\n        BytesRef text;\n        while((text = termsEnum.next()) != null) {\n          String t = text.utf8ToChars(spare).toString();\n  \n          // Compute distinct terms for every field\n          TopTermQueue tiq = info.get( field );\n          if( tiq == null ) {\n            tiq = new TopTermQueue( numTerms+1 );\n            info.put( field, tiq );\n          }\n\n          tiq.distinctTerms++;\n          tiq.histogram.add( termsEnum.docFreq() );  // add the term to the histogram\n        \n          // Only save the distinct terms for fields we worry about\n          if (fields != null && fields.size() > 0) {\n            if( !fields.contains( field ) ) {\n              continue;\n            }\n          }\n          if( junkWords != null && junkWords.contains( t ) ) {\n            continue;\n          }\n        \n          if( termsEnum.docFreq() > tiq.minFreq ) {\n            tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n            if (tiq.size() > numTerms) { // if tiq full\n              tiq.pop(); // remove lowest in tiq\n              tiq.minFreq = ((TopTermQueue.TermInfo)tiq.top()).docFreq; // reset minFreq\n            }\n          }\n        }\n      }\n    }\n    return info;\n  }\n\n","sourceOld":"  private static Map<String,TopTermQueue> getTopTerms( IndexReader reader, Set<String> fields, int numTerms, Set<String> junkWords ) throws Exception \n  {\n    Map<String,TopTermQueue> info = new HashMap<String, TopTermQueue>();\n    final CharsRef spare = new CharsRef();\n    Fields fieldsC = MultiFields.getFields(reader);\n    if (fieldsC != null) {\n      FieldsEnum fieldsEnum = fieldsC.iterator();\n      String field;\n      while((field = fieldsEnum.next()) != null) {\n\n        TermsEnum termsEnum = fieldsEnum.terms();\n        BytesRef text;\n        while((text = termsEnum.next()) != null) {\n          String t = text.utf8ToChars(spare).toString();\n  \n          // Compute distinct terms for every field\n          TopTermQueue tiq = info.get( field );\n          if( tiq == null ) {\n            tiq = new TopTermQueue( numTerms+1 );\n            info.put( field, tiq );\n          }\n\n          tiq.distinctTerms++;\n          tiq.histogram.add( termsEnum.docFreq() );  // add the term to the histogram\n        \n          // Only save the distinct terms for fields we worry about\n          if (fields != null && fields.size() > 0) {\n            if( !fields.contains( field ) ) {\n              continue;\n            }\n          }\n          if( junkWords != null && junkWords.contains( t ) ) {\n            continue;\n          }\n        \n          if( termsEnum.docFreq() > tiq.minFreq ) {\n            tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n            if (tiq.size() > numTerms) { // if tiq full\n              tiq.pop(); // remove lowest in tiq\n              tiq.minFreq = ((TopTermQueue.TermInfo)tiq.top()).docFreq; // reset minFreq\n            }\n          }\n        }\n      }\n    }\n    return info;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getTopTerms(IndexReader,Set[String],int,Set[String]).mjava","pathOld":"solr/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getTopTerms(IndexReader,Set[String],int,Set[String]).mjava","sourceNew":"  private static Map<String,TopTermQueue> getTopTerms( IndexReader reader, Set<String> fields, int numTerms, Set<String> junkWords ) throws Exception \n  {\n    Map<String,TopTermQueue> info = new HashMap<String, TopTermQueue>();\n    final CharsRef spare = new CharsRef();\n    Fields fieldsC = MultiFields.getFields(reader);\n    if (fieldsC != null) {\n      FieldsEnum fieldsEnum = fieldsC.iterator();\n      String field;\n      while((field = fieldsEnum.next()) != null) {\n\n        TermsEnum termsEnum = fieldsEnum.terms();\n        BytesRef text;\n        while((text = termsEnum.next()) != null) {\n          String t = text.utf8ToChars(spare).toString();\n  \n          // Compute distinct terms for every field\n          TopTermQueue tiq = info.get( field );\n          if( tiq == null ) {\n            tiq = new TopTermQueue( numTerms+1 );\n            info.put( field, tiq );\n          }\n\n          tiq.distinctTerms++;\n          tiq.histogram.add( termsEnum.docFreq() );  // add the term to the histogram\n        \n          // Only save the distinct terms for fields we worry about\n          if (fields != null && fields.size() > 0) {\n            if( !fields.contains( field ) ) {\n              continue;\n            }\n          }\n          if( junkWords != null && junkWords.contains( t ) ) {\n            continue;\n          }\n        \n          if( termsEnum.docFreq() > tiq.minFreq ) {\n            tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n            if (tiq.size() > numTerms) { // if tiq full\n              tiq.pop(); // remove lowest in tiq\n              tiq.minFreq = ((TopTermQueue.TermInfo)tiq.top()).docFreq; // reset minFreq\n            }\n          }\n        }\n      }\n    }\n    return info;\n  }\n\n","sourceOld":"  private static Map<String,TopTermQueue> getTopTerms( IndexReader reader, Set<String> fields, int numTerms, Set<String> junkWords ) throws Exception \n  {\n    Map<String,TopTermQueue> info = new HashMap<String, TopTermQueue>();\n    final CharsRef spare = new CharsRef();\n    Fields fieldsC = MultiFields.getFields(reader);\n    if (fieldsC != null) {\n      FieldsEnum fieldsEnum = fieldsC.iterator();\n      String field;\n      while((field = fieldsEnum.next()) != null) {\n\n        TermsEnum termsEnum = fieldsEnum.terms();\n        BytesRef text;\n        while((text = termsEnum.next()) != null) {\n          String t = text.utf8ToChars(spare).toString();\n  \n          // Compute distinct terms for every field\n          TopTermQueue tiq = info.get( field );\n          if( tiq == null ) {\n            tiq = new TopTermQueue( numTerms+1 );\n            info.put( field, tiq );\n          }\n\n          tiq.distinctTerms++;\n          tiq.histogram.add( termsEnum.docFreq() );  // add the term to the histogram\n        \n          // Only save the distinct terms for fields we worry about\n          if (fields != null && fields.size() > 0) {\n            if( !fields.contains( field ) ) {\n              continue;\n            }\n          }\n          if( junkWords != null && junkWords.contains( t ) ) {\n            continue;\n          }\n        \n          if( termsEnum.docFreq() > tiq.minFreq ) {\n            tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n            if (tiq.size() > numTerms) { // if tiq full\n              tiq.pop(); // remove lowest in tiq\n              tiq.minFreq = ((TopTermQueue.TermInfo)tiq.top()).docFreq; // reset minFreq\n            }\n          }\n        }\n      }\n    }\n    return info;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":5,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getTopTerms(IndexReader,Set[String],int,Set[String]).mjava","pathOld":"solr/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getTopTerms(IndexReader,Set[String],int,Set[String]).mjava","sourceNew":"  private static Map<String,TopTermQueue> getTopTerms( IndexReader reader, Set<String> fields, int numTerms, Set<String> junkWords ) throws Exception \n  {\n    Map<String,TopTermQueue> info = new HashMap<String, TopTermQueue>();\n    final CharsRef spare = new CharsRef();\n    Fields fieldsC = MultiFields.getFields(reader);\n    if (fieldsC != null) {\n      FieldsEnum fieldsEnum = fieldsC.iterator();\n      String field;\n      while((field = fieldsEnum.next()) != null) {\n\n        TermsEnum termsEnum = fieldsEnum.terms();\n        BytesRef text;\n        while((text = termsEnum.next()) != null) {\n          String t = text.utf8ToChars(spare).toString();\n  \n          // Compute distinct terms for every field\n          TopTermQueue tiq = info.get( field );\n          if( tiq == null ) {\n            tiq = new TopTermQueue( numTerms+1 );\n            info.put( field, tiq );\n          }\n\n          tiq.distinctTerms++;\n          tiq.histogram.add( termsEnum.docFreq() );  // add the term to the histogram\n        \n          // Only save the distinct terms for fields we worry about\n          if (fields != null && fields.size() > 0) {\n            if( !fields.contains( field ) ) {\n              continue;\n            }\n          }\n          if( junkWords != null && junkWords.contains( t ) ) {\n            continue;\n          }\n        \n          if( termsEnum.docFreq() > tiq.minFreq ) {\n            tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n            if (tiq.size() > numTerms) { // if tiq full\n              tiq.pop(); // remove lowest in tiq\n              tiq.minFreq = ((TopTermQueue.TermInfo)tiq.top()).docFreq; // reset minFreq\n            }\n          }\n        }\n      }\n    }\n    return info;\n  }\n\n","sourceOld":"  private static Map<String,TopTermQueue> getTopTerms( IndexReader reader, Set<String> fields, int numTerms, Set<String> junkWords ) throws Exception \n  {\n    Map<String,TopTermQueue> info = new HashMap<String, TopTermQueue>();\n    final CharsRef spare = new CharsRef();\n    Fields fieldsC = MultiFields.getFields(reader);\n    if (fieldsC != null) {\n      FieldsEnum fieldsEnum = fieldsC.iterator();\n      String field;\n      while((field = fieldsEnum.next()) != null) {\n\n        TermsEnum termsEnum = fieldsEnum.terms();\n        BytesRef text;\n        while((text = termsEnum.next()) != null) {\n          String t = text.utf8ToChars(spare).toString();\n  \n          // Compute distinct terms for every field\n          TopTermQueue tiq = info.get( field );\n          if( tiq == null ) {\n            tiq = new TopTermQueue( numTerms+1 );\n            info.put( field, tiq );\n          }\n\n          tiq.distinctTerms++;\n          tiq.histogram.add( termsEnum.docFreq() );  // add the term to the histogram\n        \n          // Only save the distinct terms for fields we worry about\n          if (fields != null && fields.size() > 0) {\n            if( !fields.contains( field ) ) {\n              continue;\n            }\n          }\n          if( junkWords != null && junkWords.contains( t ) ) {\n            continue;\n          }\n        \n          if( termsEnum.docFreq() > tiq.minFreq ) {\n            tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n            if (tiq.size() > numTerms) { // if tiq full\n              tiq.pop(); // remove lowest in tiq\n              tiq.minFreq = ((TopTermQueue.TermInfo)tiq.top()).docFreq; // reset minFreq\n            }\n          }\n        }\n      }\n    }\n    return info;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"893269407f5c988a4b2ee13c7ffc01ec43136c1d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8":["893269407f5c988a4b2ee13c7ffc01ec43136c1d"],"c26f00b574427b55127e869b935845554afde1fa":["a1b3a24d5d9b47345473ff564f5cc127a7b526b4","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"5f4e87790277826a2aea119328600dfb07761f32":["1da8d55113b689b06716246649de6f62430f15c0","28427ef110c4c5bf5b4057731b83110bd1e13724"],"1da8d55113b689b06716246649de6f62430f15c0":["a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8","ad94625fb8d088209f46650c8097196fec67f00c"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["28427ef110c4c5bf5b4057731b83110bd1e13724","a1b3a24d5d9b47345473ff564f5cc127a7b526b4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"28427ef110c4c5bf5b4057731b83110bd1e13724":["1da8d55113b689b06716246649de6f62430f15c0"],"a258fbb26824fd104ed795e5d9033d2d040049ee":["a1b3a24d5d9b47345473ff564f5cc127a7b526b4"],"a1b3a24d5d9b47345473ff564f5cc127a7b526b4":["28427ef110c4c5bf5b4057731b83110bd1e13724"],"ad94625fb8d088209f46650c8097196fec67f00c":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c26f00b574427b55127e869b935845554afde1fa"],"2e10cb22a8bdb44339e282925a29182bb2f3174d":["28427ef110c4c5bf5b4057731b83110bd1e13724","a1b3a24d5d9b47345473ff564f5cc127a7b526b4"]},"commit2Childs":{"893269407f5c988a4b2ee13c7ffc01ec43136c1d":["a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8"],"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8":["1da8d55113b689b06716246649de6f62430f15c0"],"c26f00b574427b55127e869b935845554afde1fa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["ad94625fb8d088209f46650c8097196fec67f00c"],"5f4e87790277826a2aea119328600dfb07761f32":[],"1da8d55113b689b06716246649de6f62430f15c0":["5f4e87790277826a2aea119328600dfb07761f32","28427ef110c4c5bf5b4057731b83110bd1e13724"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["893269407f5c988a4b2ee13c7ffc01ec43136c1d"],"28427ef110c4c5bf5b4057731b83110bd1e13724":["5f4e87790277826a2aea119328600dfb07761f32","5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","a1b3a24d5d9b47345473ff564f5cc127a7b526b4","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"a1b3a24d5d9b47345473ff564f5cc127a7b526b4":["c26f00b574427b55127e869b935845554afde1fa","5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","a258fbb26824fd104ed795e5d9033d2d040049ee","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"ad94625fb8d088209f46650c8097196fec67f00c":["1da8d55113b689b06716246649de6f62430f15c0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"2e10cb22a8bdb44339e282925a29182bb2f3174d":[]},"heads":["5f4e87790277826a2aea119328600dfb07761f32","a258fbb26824fd104ed795e5d9033d2d040049ee","cd5edd1f2b162a5cfa08efd17851a07373a96817","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"roots":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}