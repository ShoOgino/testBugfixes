{"path":"lucene/backwards/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/backwards/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","pathOld":"backwards/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null)\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null)\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6","date":1272983566,"type":4,"author":"Uwe Schindler","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/backwards/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument().mjava","sourceNew":null,"sourceOld":"  @Override\n  public DocumentsWriter.DocWriter processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null)\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}