{"path":"lucene/core/src/java/org/apache/lucene/codecs/temp/TempBlockTermsReader.FieldReader.SegmentTermsEnum.Frame#loadBlock().mjava","commits":[{"id":"0a570bec4aec8585a4eea44849bfb2bc264208c4","date":1371308653,"type":0,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/temp/TempBlockTermsReader.FieldReader.SegmentTermsEnum.Frame#loadBlock().mjava","pathOld":"/dev/null","sourceNew":"        /* Does initial decode of next block of terms; this\n           doesn't actually decode the docFreq, totalTermFreq,\n           postings details (frq/prx offset, etc.) metadata;\n           it just loads them as byte[] blobs which are then      \n           decoded on-demand if the metadata is ever requested\n           for any term in this block.  This enables terms-only\n           intensive consumes (eg certain MTQs, respelling) to\n           not pay the price of decoding metadata they won't\n           use. */\n        void loadBlock() throws IOException {\n\n          // Clone the IndexInput lazily, so that consumers\n          // that just pull a TermsEnum to\n          // seekExact(TermState) don't pay this cost:\n          initIndexInput();\n\n          if (nextEnt != -1) {\n            // Already loaded\n            return;\n          }\n          //System.out.println(\"blc=\" + blockLoadCount);\n\n          in.seek(fp);\n          int code = in.readVInt();\n          entCount = code >>> 1;\n          assert entCount > 0;\n          isLastInFloor = (code & 1) != 0;\n          assert arc == null || (isLastInFloor || isFloor);\n\n          // TODO: if suffixes were stored in random-access\n          // array structure, then we could do binary search\n          // instead of linear scan to find target term; eg\n          // we could have simple array of offsets\n\n          // term suffixes:\n          code = in.readVInt();\n          isLeafBlock = (code & 1) != 0;\n          int numBytes = code >>> 1;\n          if (suffixBytes.length < numBytes) {\n            suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n          }\n          in.readBytes(suffixBytes, 0, numBytes);\n          suffixesReader.reset(suffixBytes, 0, numBytes);\n\n          /*if (DEBUG) {\n            if (arc == null) {\n              System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n            } else {\n              System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n            }\n            }*/\n\n          // stats\n          numBytes = in.readVInt();\n          if (statBytes.length < numBytes) {\n            statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n          }\n          in.readBytes(statBytes, 0, numBytes);\n          statsReader.reset(statBytes, 0, numBytes);\n          metaDataUpto = 0;\n\n          state.termBlockOrd = 0;\n          nextEnt = 0;\n          lastSubFP = -1;\n\n          // TODO: we could skip this if !hasTerms; but\n          // that's rare so won't help much\n          postingsReader.readTermsBlock(in, fieldInfo, state);\n\n          // Sub-blocks of a single floor block are always\n          // written one after another -- tail recurse:\n          fpEnd = in.getFilePointer();\n          // if (DEBUG) {\n          //   System.out.println(\"      fpEnd=\" + fpEnd);\n          // }\n        }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a10f5f1c7f2dcd4a60664dd5c34d803794e023c9","date":1371380031,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/temp/TempBlockTermsReader.FieldReader.SegmentTermsEnum.Frame#loadBlock().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/temp/TempBlockTermsReader.FieldReader.SegmentTermsEnum.Frame#loadBlock().mjava","sourceNew":"        /* Does initial decode of next block of terms; this\n           doesn't actually decode the docFreq, totalTermFreq,\n           postings details (frq/prx offset, etc.) metadata;\n           it just loads them as byte[] blobs which are then      \n           decoded on-demand if the metadata is ever requested\n           for any term in this block.  This enables terms-only\n           intensive consumes (eg certain MTQs, respelling) to\n           not pay the price of decoding metadata they won't\n           use. */\n        void loadBlock() throws IOException {\n\n          // Clone the IndexInput lazily, so that consumers\n          // that just pull a TermsEnum to\n          // seekExact(TermState) don't pay this cost:\n          initIndexInput();\n\n          if (nextEnt != -1) {\n            // Already loaded\n            return;\n          }\n          //System.out.println(\"blc=\" + blockLoadCount);\n\n          in.seek(fp);\n          int code = in.readVInt();\n          entCount = code >>> 1;\n          assert entCount > 0;\n          isLastInFloor = (code & 1) != 0;\n          assert arc == null || (isLastInFloor || isFloor);\n\n          // TODO: if suffixes were stored in random-access\n          // array structure, then we could do binary search\n          // instead of linear scan to find target term; eg\n          // we could have simple array of offsets\n\n          // term suffixes:\n          code = in.readVInt();\n          isLeafBlock = (code & 1) != 0;\n          int numBytes = code >>> 1;\n          if (suffixBytes.length < numBytes) {\n            suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n          }\n          in.readBytes(suffixBytes, 0, numBytes);\n          suffixesReader.reset(suffixBytes, 0, numBytes);\n\n          /*if (DEBUG) {\n            if (arc == null) {\n              System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n            } else {\n              System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n            }\n            }*/\n\n          // stats\n          numBytes = in.readVInt();\n          if (statBytes.length < numBytes) {\n            statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n          }\n          in.readBytes(statBytes, 0, numBytes);\n          statsReader.reset(statBytes, 0, numBytes);\n          metaDataUpto = 0;\n\n          state.termBlockOrd = 0;\n          nextEnt = 0;\n          lastSubFP = -1;\n\n          // TODO: we could skip this if !hasTerms; but\n          // that's rare so won't help much\n          // metadata\n          numBytes = in.readVInt();\n          if (bytes == null) {\n            bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n            bytesReader = new ByteArrayDataInput();\n          } else if (bytes.length < numBytes) {\n            bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n          }\n          in.readBytes(bytes, 0, numBytes);\n          bytesReader.reset(bytes, 0, numBytes);\n\n\n          // Sub-blocks of a single floor block are always\n          // written one after another -- tail recurse:\n          fpEnd = in.getFilePointer();\n          // if (DEBUG) {\n          //   System.out.println(\"      fpEnd=\" + fpEnd);\n          // }\n        }\n\n","sourceOld":"        /* Does initial decode of next block of terms; this\n           doesn't actually decode the docFreq, totalTermFreq,\n           postings details (frq/prx offset, etc.) metadata;\n           it just loads them as byte[] blobs which are then      \n           decoded on-demand if the metadata is ever requested\n           for any term in this block.  This enables terms-only\n           intensive consumes (eg certain MTQs, respelling) to\n           not pay the price of decoding metadata they won't\n           use. */\n        void loadBlock() throws IOException {\n\n          // Clone the IndexInput lazily, so that consumers\n          // that just pull a TermsEnum to\n          // seekExact(TermState) don't pay this cost:\n          initIndexInput();\n\n          if (nextEnt != -1) {\n            // Already loaded\n            return;\n          }\n          //System.out.println(\"blc=\" + blockLoadCount);\n\n          in.seek(fp);\n          int code = in.readVInt();\n          entCount = code >>> 1;\n          assert entCount > 0;\n          isLastInFloor = (code & 1) != 0;\n          assert arc == null || (isLastInFloor || isFloor);\n\n          // TODO: if suffixes were stored in random-access\n          // array structure, then we could do binary search\n          // instead of linear scan to find target term; eg\n          // we could have simple array of offsets\n\n          // term suffixes:\n          code = in.readVInt();\n          isLeafBlock = (code & 1) != 0;\n          int numBytes = code >>> 1;\n          if (suffixBytes.length < numBytes) {\n            suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n          }\n          in.readBytes(suffixBytes, 0, numBytes);\n          suffixesReader.reset(suffixBytes, 0, numBytes);\n\n          /*if (DEBUG) {\n            if (arc == null) {\n              System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n            } else {\n              System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n            }\n            }*/\n\n          // stats\n          numBytes = in.readVInt();\n          if (statBytes.length < numBytes) {\n            statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n          }\n          in.readBytes(statBytes, 0, numBytes);\n          statsReader.reset(statBytes, 0, numBytes);\n          metaDataUpto = 0;\n\n          state.termBlockOrd = 0;\n          nextEnt = 0;\n          lastSubFP = -1;\n\n          // TODO: we could skip this if !hasTerms; but\n          // that's rare so won't help much\n          postingsReader.readTermsBlock(in, fieldInfo, state);\n\n          // Sub-blocks of a single floor block are always\n          // written one after another -- tail recurse:\n          fpEnd = in.getFilePointer();\n          // if (DEBUG) {\n          //   System.out.println(\"      fpEnd=\" + fpEnd);\n          // }\n        }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6d01ed6d1df51191983f4dd157aedf5f4650e2b3","date":1376572546,"type":5,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/temp/TempBlockTreeTermsReader.FieldReader.SegmentTermsEnum.Frame#loadBlock().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/temp/TempBlockTermsReader.FieldReader.SegmentTermsEnum.Frame#loadBlock().mjava","sourceNew":"        /* Does initial decode of next block of terms; this\n           doesn't actually decode the docFreq, totalTermFreq,\n           postings details (frq/prx offset, etc.) metadata;\n           it just loads them as byte[] blobs which are then      \n           decoded on-demand if the metadata is ever requested\n           for any term in this block.  This enables terms-only\n           intensive consumes (eg certain MTQs, respelling) to\n           not pay the price of decoding metadata they won't\n           use. */\n        void loadBlock() throws IOException {\n\n          // Clone the IndexInput lazily, so that consumers\n          // that just pull a TermsEnum to\n          // seekExact(TermState) don't pay this cost:\n          initIndexInput();\n\n          if (nextEnt != -1) {\n            // Already loaded\n            return;\n          }\n          //System.out.println(\"blc=\" + blockLoadCount);\n\n          in.seek(fp);\n          int code = in.readVInt();\n          entCount = code >>> 1;\n          assert entCount > 0;\n          isLastInFloor = (code & 1) != 0;\n          assert arc == null || (isLastInFloor || isFloor);\n\n          // TODO: if suffixes were stored in random-access\n          // array structure, then we could do binary search\n          // instead of linear scan to find target term; eg\n          // we could have simple array of offsets\n\n          // term suffixes:\n          code = in.readVInt();\n          isLeafBlock = (code & 1) != 0;\n          int numBytes = code >>> 1;\n          if (suffixBytes.length < numBytes) {\n            suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n          }\n          in.readBytes(suffixBytes, 0, numBytes);\n          suffixesReader.reset(suffixBytes, 0, numBytes);\n\n          /*if (DEBUG) {\n            if (arc == null) {\n              System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n            } else {\n              System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n            }\n            }*/\n\n          // stats\n          numBytes = in.readVInt();\n          if (statBytes.length < numBytes) {\n            statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n          }\n          in.readBytes(statBytes, 0, numBytes);\n          statsReader.reset(statBytes, 0, numBytes);\n          metaDataUpto = 0;\n\n          state.termBlockOrd = 0;\n          nextEnt = 0;\n          lastSubFP = -1;\n\n          // TODO: we could skip this if !hasTerms; but\n          // that's rare so won't help much\n          // metadata\n          numBytes = in.readVInt();\n          if (bytes == null) {\n            bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n            bytesReader = new ByteArrayDataInput();\n          } else if (bytes.length < numBytes) {\n            bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n          }\n          in.readBytes(bytes, 0, numBytes);\n          bytesReader.reset(bytes, 0, numBytes);\n\n\n          // Sub-blocks of a single floor block are always\n          // written one after another -- tail recurse:\n          fpEnd = in.getFilePointer();\n          // if (DEBUG) {\n          //   System.out.println(\"      fpEnd=\" + fpEnd);\n          // }\n        }\n\n","sourceOld":"        /* Does initial decode of next block of terms; this\n           doesn't actually decode the docFreq, totalTermFreq,\n           postings details (frq/prx offset, etc.) metadata;\n           it just loads them as byte[] blobs which are then      \n           decoded on-demand if the metadata is ever requested\n           for any term in this block.  This enables terms-only\n           intensive consumes (eg certain MTQs, respelling) to\n           not pay the price of decoding metadata they won't\n           use. */\n        void loadBlock() throws IOException {\n\n          // Clone the IndexInput lazily, so that consumers\n          // that just pull a TermsEnum to\n          // seekExact(TermState) don't pay this cost:\n          initIndexInput();\n\n          if (nextEnt != -1) {\n            // Already loaded\n            return;\n          }\n          //System.out.println(\"blc=\" + blockLoadCount);\n\n          in.seek(fp);\n          int code = in.readVInt();\n          entCount = code >>> 1;\n          assert entCount > 0;\n          isLastInFloor = (code & 1) != 0;\n          assert arc == null || (isLastInFloor || isFloor);\n\n          // TODO: if suffixes were stored in random-access\n          // array structure, then we could do binary search\n          // instead of linear scan to find target term; eg\n          // we could have simple array of offsets\n\n          // term suffixes:\n          code = in.readVInt();\n          isLeafBlock = (code & 1) != 0;\n          int numBytes = code >>> 1;\n          if (suffixBytes.length < numBytes) {\n            suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n          }\n          in.readBytes(suffixBytes, 0, numBytes);\n          suffixesReader.reset(suffixBytes, 0, numBytes);\n\n          /*if (DEBUG) {\n            if (arc == null) {\n              System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n            } else {\n              System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n            }\n            }*/\n\n          // stats\n          numBytes = in.readVInt();\n          if (statBytes.length < numBytes) {\n            statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n          }\n          in.readBytes(statBytes, 0, numBytes);\n          statsReader.reset(statBytes, 0, numBytes);\n          metaDataUpto = 0;\n\n          state.termBlockOrd = 0;\n          nextEnt = 0;\n          lastSubFP = -1;\n\n          // TODO: we could skip this if !hasTerms; but\n          // that's rare so won't help much\n          // metadata\n          numBytes = in.readVInt();\n          if (bytes == null) {\n            bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n            bytesReader = new ByteArrayDataInput();\n          } else if (bytes.length < numBytes) {\n            bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n          }\n          in.readBytes(bytes, 0, numBytes);\n          bytesReader.reset(bytes, 0, numBytes);\n\n\n          // Sub-blocks of a single floor block are always\n          // written one after another -- tail recurse:\n          fpEnd = in.getFilePointer();\n          // if (DEBUG) {\n          //   System.out.println(\"      fpEnd=\" + fpEnd);\n          // }\n        }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0a570bec4aec8585a4eea44849bfb2bc264208c4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"6d01ed6d1df51191983f4dd157aedf5f4650e2b3":["a10f5f1c7f2dcd4a60664dd5c34d803794e023c9"],"a10f5f1c7f2dcd4a60664dd5c34d803794e023c9":["0a570bec4aec8585a4eea44849bfb2bc264208c4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"0a570bec4aec8585a4eea44849bfb2bc264208c4":["a10f5f1c7f2dcd4a60664dd5c34d803794e023c9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0a570bec4aec8585a4eea44849bfb2bc264208c4","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"6d01ed6d1df51191983f4dd157aedf5f4650e2b3":[],"a10f5f1c7f2dcd4a60664dd5c34d803794e023c9":["6d01ed6d1df51191983f4dd157aedf5f4650e2b3"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["6d01ed6d1df51191983f4dd157aedf5f4650e2b3","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}