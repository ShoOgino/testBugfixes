{"path":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new SimpleAnalyzer(TEST_VERSION_CURRENT)));\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new SimpleAnalyzer(TEST_VERSION_CURRENT)));\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a775c547c3519b47efd41c09cb47100ddb9604c7","date":1270914087,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new SimpleAnalyzer(TEST_VERSION_CURRENT)));\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new SimpleAnalyzer(TEST_VERSION_CURRENT)));\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d572389229127c297dd1fa5ce4758e1cec41e799","date":1273610938,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer()));\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new SimpleAnalyzer(TEST_VERSION_CURRENT)));\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"28427ef110c4c5bf5b4057731b83110bd1e13724","date":1276701452,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer()));\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getDeletedDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer()));\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer()));\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getDeletedDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer()));\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b21422ff1d1d56499dec481f193b402e5e8def5b","date":1281472367,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(newRandom(), dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getDeletedDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer()));\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getDeletedDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c","date":1281477834,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = new MockRAMDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(newRandom(), dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getDeletedDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(newRandom(), dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getDeletedDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c","date":1281646583,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Random random = newRandom();\n    Directory dir = newDirectory(random);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getDeletedDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = new MockRAMDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(newRandom(), dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getDeletedDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1f653cfcf159baeaafe5d01682a911e95bba4012","date":1284122058,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getDeletedDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Random random = newRandom();\n    Directory dir = newDirectory(random);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getDeletedDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getDeletedDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer()));\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getDeletedDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","date":1309960478,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getDeletedDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"817d8435e9135b756f08ce6710ab0baac51bdf88","date":1309986993,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getDeletedDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getDeletedDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getDeletedDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"31f025ae60076ae95274433f3fe8e6ace2857a87","date":1326669465,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"),\n                                                                          false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"),\n                                                                          false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.append(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new TextField(\"preanalyzed\", stream));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    \n    IndexReader reader = writer.getReader();\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                                          MultiFields.getLiveDocs(reader),\n                                                                          \"preanalyzed\",\n                                                                          new BytesRef(\"term1\"),\n                                                                          false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term2\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions = MultiFields.getTermPositionsEnum(reader,\n                                                     MultiFields.getLiveDocs(reader),\n                                                     \"preanalyzed\",\n                                                     new BytesRef(\"term3\"),\n                                                     false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    writer.close();\n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"d572389229127c297dd1fa5ce4758e1cec41e799":["a775c547c3519b47efd41c09cb47100ddb9604c7"],"5f4e87790277826a2aea119328600dfb07761f32":["d572389229127c297dd1fa5ce4758e1cec41e799","28427ef110c4c5bf5b4057731b83110bd1e13724"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["5f4e87790277826a2aea119328600dfb07761f32","1f653cfcf159baeaafe5d01682a911e95bba4012"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["1f653cfcf159baeaafe5d01682a911e95bba4012","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["1f653cfcf159baeaafe5d01682a911e95bba4012","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"a775c547c3519b47efd41c09cb47100ddb9604c7":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c":["b21422ff1d1d56499dec481f193b402e5e8def5b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"28427ef110c4c5bf5b4057731b83110bd1e13724":["d572389229127c297dd1fa5ce4758e1cec41e799"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"b21422ff1d1d56499dec481f193b402e5e8def5b":["28427ef110c4c5bf5b4057731b83110bd1e13724"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","1509f151d7692d84fae414b2b799ac06ba60fcb4"],"d572389229127c297dd1fa5ce4758e1cec41e799":["5f4e87790277826a2aea119328600dfb07761f32","28427ef110c4c5bf5b4057731b83110bd1e13724"],"5f4e87790277826a2aea119328600dfb07761f32":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":[],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"817d8435e9135b756f08ce6710ab0baac51bdf88":[],"a775c547c3519b47efd41c09cb47100ddb9604c7":["d572389229127c297dd1fa5ce4758e1cec41e799"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88"],"1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"28427ef110c4c5bf5b4057731b83110bd1e13724":["5f4e87790277826a2aea119328600dfb07761f32","b21422ff1d1d56499dec481f193b402e5e8def5b"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"b21422ff1d1d56499dec481f193b402e5e8def5b":["1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a775c547c3519b47efd41c09cb47100ddb9604c7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}