{"path":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","commits":[{"id":"abb23fcc2461782ab204e61213240feb77d355aa","date":1422029612,"type":1,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#doTest().mjava","sourceNew":"  @Test\n  @ShardsFixed(num = 8)\n  public void test() throws Exception {\n    setupHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getActiveSlices(\"collection1\").size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    for (RestTestHarness client : restTestHarnesses) {\n      String request = \"/schema/fields?wt=xml\";\n      String response = client.query(request);\n      String result = BaseTestHarness.validateXPath(response, expectedFields);\n      if (result != null) {\n        String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n        log.error(msg);\n        fail(msg);\n      }\n    }\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        randomClient.add(docs);\n        randomClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    setupHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getActiveSlices(\"collection1\").size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    for (RestTestHarness client : restTestHarnesses) {\n      String request = \"/schema/fields?wt=xml\";\n      String response = client.query(request);\n      String result = BaseTestHarness.validateXPath(response, expectedFields);\n      if (result != null) {\n        String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n        log.error(msg);\n        fail(msg);\n      }\n    }\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        randomClient.add(docs);\n        randomClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"015571db92de1e6f75d89f34faeb520f99f07b97","date":1431466334,"type":3,"author":"Shawn Heisey","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","sourceNew":"  @Test\n  @ShardsFixed(num = 8)\n  public void test() throws Exception {\n    setupHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getActiveSlices(\"collection1\").size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    for (RestTestHarness client : restTestHarnesses) {\n      String request = \"/schema/fields?wt=xml\";\n      String response = client.query(request);\n      String result = BaseTestHarness.validateXPath(response, expectedFields);\n      if (result != null) {\n        String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n        log.error(msg);\n        fail(msg);\n      }\n    }\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        randomClient.add(docs);\n        randomClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n\n      try {\n        CloudSolrClient cloudSolrClient = getCommonCloudSolrClient();\n        cloudSolrClient.add(docs);\n        cloudSolrClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException ex) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode((ex).code()));\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  @ShardsFixed(num = 8)\n  public void test() throws Exception {\n    setupHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getActiveSlices(\"collection1\").size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    for (RestTestHarness client : restTestHarnesses) {\n      String request = \"/schema/fields?wt=xml\";\n      String response = client.query(request);\n      String result = BaseTestHarness.validateXPath(response, expectedFields);\n      if (result != null) {\n        String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n        log.error(msg);\n        fail(msg);\n      }\n    }\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        randomClient.add(docs);\n        randomClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"344b0840364d990b29b97467bfcc766ff8325d11","date":1501574100,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","sourceNew":"  @Test\n  @ShardsFixed(num = 8)\n  public void test() throws Exception {\n    setupHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getCollection(\"collection1\").getActiveSlices().size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    for (RestTestHarness client : restTestHarnesses) {\n      String request = \"/schema/fields?wt=xml\";\n      String response = client.query(request);\n      String result = BaseTestHarness.validateXPath(response, expectedFields);\n      if (result != null) {\n        String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n        log.error(msg);\n        fail(msg);\n      }\n    }\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        randomClient.add(docs);\n        randomClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n\n      try {\n        CloudSolrClient cloudSolrClient = getCommonCloudSolrClient();\n        cloudSolrClient.add(docs);\n        cloudSolrClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException ex) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode((ex).code()));\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  @ShardsFixed(num = 8)\n  public void test() throws Exception {\n    setupHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getActiveSlices(\"collection1\").size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    for (RestTestHarness client : restTestHarnesses) {\n      String request = \"/schema/fields?wt=xml\";\n      String response = client.query(request);\n      String result = BaseTestHarness.validateXPath(response, expectedFields);\n      if (result != null) {\n        String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n        log.error(msg);\n        fail(msg);\n      }\n    }\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        randomClient.add(docs);\n        randomClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n\n      try {\n        CloudSolrClient cloudSolrClient = getCommonCloudSolrClient();\n        cloudSolrClient.add(docs);\n        cloudSolrClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException ex) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode((ex).code()));\n      }\n    }\n  }\n\n","bugFix":["f13b9d4c228e77327b284419c8cafd16913a7a19"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7a23cf16c8fa265dc0a564adcabb55e3f054e0ac","date":1502192746,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","sourceNew":"  @Test\n  @ShardsFixed(num = 8)\n  public void test() throws Exception {\n    setupHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getCollection(\"collection1\").getActiveSlices().size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    for (RestTestHarness client : restTestHarnesses) {\n      String request = \"/schema/fields?wt=xml\";\n      String response = client.query(request);\n      String result = BaseTestHarness.validateXPath(response, expectedFields);\n      if (result != null) {\n        String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n        log.error(msg);\n        fail(msg);\n      }\n    }\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        randomClient.add(docs);\n        randomClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n\n      try {\n        CloudSolrClient cloudSolrClient = getCommonCloudSolrClient();\n        cloudSolrClient.add(docs);\n        cloudSolrClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException ex) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode((ex).code()));\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  @ShardsFixed(num = 8)\n  public void test() throws Exception {\n    setupHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getActiveSlices(\"collection1\").size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    for (RestTestHarness client : restTestHarnesses) {\n      String request = \"/schema/fields?wt=xml\";\n      String response = client.query(request);\n      String result = BaseTestHarness.validateXPath(response, expectedFields);\n      if (result != null) {\n        String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n        log.error(msg);\n        fail(msg);\n      }\n    }\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        randomClient.add(docs);\n        randomClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n\n      try {\n        CloudSolrClient cloudSolrClient = getCommonCloudSolrClient();\n        cloudSolrClient.add(docs);\n        cloudSolrClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException ex) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode((ex).code()));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0853c291617e0f9d0474b720bf609eb350e9225c","date":1514464265,"type":3,"author":"Christine Poerschke","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","sourceNew":"  @Test\n  @ShardsFixed(num = 8)\n  public void test() throws Exception {\n    setupRestTestHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getCollection(\"collection1\").getActiveSlices().size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    forAllRestTestHarnesses( new UnaryOperator<RestTestHarness>() {\n      @Override\n      public RestTestHarness apply(RestTestHarness client) {\n        try {\n          String request = \"/schema/fields?wt=xml\";\n          String response = client.query(request);\n          String result = BaseTestHarness.validateXPath(response, expectedFields);\n          if (result != null) {\n            String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n            log.error(msg);\n            fail(msg);\n          }\n        } catch (Exception ex) {\n          fail(\"Caught exception: \"+ex);\n        }\n        return client;\n      }\n    });\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        randomClient.add(docs);\n        randomClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n\n      try {\n        CloudSolrClient cloudSolrClient = getCommonCloudSolrClient();\n        cloudSolrClient.add(docs);\n        cloudSolrClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException ex) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode((ex).code()));\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  @ShardsFixed(num = 8)\n  public void test() throws Exception {\n    setupHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getCollection(\"collection1\").getActiveSlices().size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    for (RestTestHarness client : restTestHarnesses) {\n      String request = \"/schema/fields?wt=xml\";\n      String response = client.query(request);\n      String result = BaseTestHarness.validateXPath(response, expectedFields);\n      if (result != null) {\n        String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n        log.error(msg);\n        fail(msg);\n      }\n    }\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        randomClient.add(docs);\n        randomClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n\n      try {\n        CloudSolrClient cloudSolrClient = getCommonCloudSolrClient();\n        cloudSolrClient.add(docs);\n        cloudSolrClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException ex) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode((ex).code()));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4bab9eeea60eefbea2957be27b8d1923095df771","date":1525498218,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","sourceNew":"  @Test\n  @ShardsFixed(num = 8)\n  @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 04-May-2018\n  public void test() throws Exception {\n    setupRestTestHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getCollection(\"collection1\").getActiveSlices().size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    forAllRestTestHarnesses( new UnaryOperator<RestTestHarness>() {\n      @Override\n      public RestTestHarness apply(RestTestHarness client) {\n        try {\n          String request = \"/schema/fields?wt=xml\";\n          String response = client.query(request);\n          String result = BaseTestHarness.validateXPath(response, expectedFields);\n          if (result != null) {\n            String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n            log.error(msg);\n            fail(msg);\n          }\n        } catch (Exception ex) {\n          fail(\"Caught exception: \"+ex);\n        }\n        return client;\n      }\n    });\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        randomClient.add(docs);\n        randomClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n\n      try {\n        CloudSolrClient cloudSolrClient = getCommonCloudSolrClient();\n        cloudSolrClient.add(docs);\n        cloudSolrClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException ex) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode((ex).code()));\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  @ShardsFixed(num = 8)\n  public void test() throws Exception {\n    setupRestTestHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getCollection(\"collection1\").getActiveSlices().size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    forAllRestTestHarnesses( new UnaryOperator<RestTestHarness>() {\n      @Override\n      public RestTestHarness apply(RestTestHarness client) {\n        try {\n          String request = \"/schema/fields?wt=xml\";\n          String response = client.query(request);\n          String result = BaseTestHarness.validateXPath(response, expectedFields);\n          if (result != null) {\n            String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n            log.error(msg);\n            fail(msg);\n          }\n        } catch (Exception ex) {\n          fail(\"Caught exception: \"+ex);\n        }\n        return client;\n      }\n    });\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        randomClient.add(docs);\n        randomClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n\n      try {\n        CloudSolrClient cloudSolrClient = getCommonCloudSolrClient();\n        cloudSolrClient.add(docs);\n        cloudSolrClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException ex) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode((ex).code()));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a1c374690db69470f6aa4bffc43dcacf1f4e3e49","date":1529007399,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","sourceNew":"  @Test\n  @ShardsFixed(num = 8)\n  // 12-Jun-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 04-May-2018\n  public void test() throws Exception {\n    setupRestTestHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getCollection(\"collection1\").getActiveSlices().size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    forAllRestTestHarnesses( new UnaryOperator<RestTestHarness>() {\n      @Override\n      public RestTestHarness apply(RestTestHarness client) {\n        try {\n          String request = \"/schema/fields?wt=xml\";\n          String response = client.query(request);\n          String result = BaseTestHarness.validateXPath(response, expectedFields);\n          if (result != null) {\n            String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n            log.error(msg);\n            fail(msg);\n          }\n        } catch (Exception ex) {\n          fail(\"Caught exception: \"+ex);\n        }\n        return client;\n      }\n    });\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        randomClient.add(docs);\n        randomClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n\n      try {\n        CloudSolrClient cloudSolrClient = getCommonCloudSolrClient();\n        cloudSolrClient.add(docs);\n        cloudSolrClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException ex) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode((ex).code()));\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  @ShardsFixed(num = 8)\n  @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 04-May-2018\n  public void test() throws Exception {\n    setupRestTestHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getCollection(\"collection1\").getActiveSlices().size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    forAllRestTestHarnesses( new UnaryOperator<RestTestHarness>() {\n      @Override\n      public RestTestHarness apply(RestTestHarness client) {\n        try {\n          String request = \"/schema/fields?wt=xml\";\n          String response = client.query(request);\n          String result = BaseTestHarness.validateXPath(response, expectedFields);\n          if (result != null) {\n            String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n            log.error(msg);\n            fail(msg);\n          }\n        } catch (Exception ex) {\n          fail(\"Caught exception: \"+ex);\n        }\n        return client;\n      }\n    });\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        randomClient.add(docs);\n        randomClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n\n      try {\n        CloudSolrClient cloudSolrClient = getCommonCloudSolrClient();\n        cloudSolrClient.add(docs);\n        cloudSolrClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException ex) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode((ex).code()));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26","date":1531589977,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","sourceNew":"  @Test\n  @ShardsFixed(num = 8)\n  // 12-Jun-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 04-May-2018\n  public void test() throws Exception {\n    setupRestTestHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getCollection(\"collection1\").getActiveSlices().size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    forAllRestTestHarnesses( new UnaryOperator<RestTestHarness>() {\n      @Override\n      public RestTestHarness apply(RestTestHarness client) {\n        try {\n          String request = \"/schema/fields?wt=xml\";\n          String response = client.query(request);\n          String result = BaseTestHarness.validateXPath(response, expectedFields);\n          if (result != null) {\n            String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n            log.error(msg);\n            fail(msg);\n          }\n        } catch (Exception ex) {\n          fail(\"Caught exception: \"+ex);\n        }\n        return client;\n      }\n    });\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        randomClient.add(docs);\n        randomClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n\n      try {\n        CloudSolrClient cloudSolrClient = getCommonCloudSolrClient();\n        cloudSolrClient.add(docs);\n        cloudSolrClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException ex) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode((ex).code()));\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  @ShardsFixed(num = 8)\n  @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 04-May-2018\n  public void test() throws Exception {\n    setupRestTestHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getCollection(\"collection1\").getActiveSlices().size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    forAllRestTestHarnesses( new UnaryOperator<RestTestHarness>() {\n      @Override\n      public RestTestHarness apply(RestTestHarness client) {\n        try {\n          String request = \"/schema/fields?wt=xml\";\n          String response = client.query(request);\n          String result = BaseTestHarness.validateXPath(response, expectedFields);\n          if (result != null) {\n            String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n            log.error(msg);\n            fail(msg);\n          }\n        } catch (Exception ex) {\n          fail(\"Caught exception: \"+ex);\n        }\n        return client;\n      }\n    });\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        randomClient.add(docs);\n        randomClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n\n      try {\n        CloudSolrClient cloudSolrClient = getCommonCloudSolrClient();\n        cloudSolrClient.add(docs);\n        cloudSolrClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException ex) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode((ex).code()));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","date":1531905561,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","sourceNew":"  @Test\n  @ShardsFixed(num = 8)\n  // 12-Jun-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 04-May-2018\n  public void test() throws Exception {\n    setupRestTestHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getCollection(\"collection1\").getActiveSlices().size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    forAllRestTestHarnesses( new UnaryOperator<RestTestHarness>() {\n      @Override\n      public RestTestHarness apply(RestTestHarness client) {\n        try {\n          String request = \"/schema/fields?wt=xml\";\n          String response = client.query(request);\n          String result = BaseTestHarness.validateXPath(response, expectedFields);\n          if (result != null) {\n            String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n            log.error(msg);\n            fail(msg);\n          }\n        } catch (Exception ex) {\n          fail(\"Caught exception: \"+ex);\n        }\n        return client;\n      }\n    });\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        randomClient.add(docs);\n        randomClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n\n      try {\n        CloudSolrClient cloudSolrClient = getCommonCloudSolrClient();\n        cloudSolrClient.add(docs);\n        cloudSolrClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException ex) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode((ex).code()));\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  @ShardsFixed(num = 8)\n  @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 04-May-2018\n  public void test() throws Exception {\n    setupRestTestHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getCollection(\"collection1\").getActiveSlices().size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    forAllRestTestHarnesses( new UnaryOperator<RestTestHarness>() {\n      @Override\n      public RestTestHarness apply(RestTestHarness client) {\n        try {\n          String request = \"/schema/fields?wt=xml\";\n          String response = client.query(request);\n          String result = BaseTestHarness.validateXPath(response, expectedFields);\n          if (result != null) {\n            String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n            log.error(msg);\n            fail(msg);\n          }\n        } catch (Exception ex) {\n          fail(\"Caught exception: \"+ex);\n        }\n        return client;\n      }\n    });\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        randomClient.add(docs);\n        randomClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n\n      try {\n        CloudSolrClient cloudSolrClient = getCommonCloudSolrClient();\n        cloudSolrClient.add(docs);\n        cloudSolrClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException ex) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode((ex).code()));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d1a307447328c95a00248512b40d7a5ff12ecd6a","date":1564817449,"type":3,"author":"Munendra S N","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","sourceNew":"  @Test\n  @ShardsFixed(num = 8)\n  // 12-Jun-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 04-May-2018\n  public void test() throws Exception {\n    setupRestTestHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getCollection(\"collection1\").getActiveSlices().size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    forAllRestTestHarnesses( new UnaryOperator<RestTestHarness>() {\n      @Override\n      public RestTestHarness apply(RestTestHarness client) {\n        try {\n          String request = \"/schema/fields?wt=xml\";\n          String response = client.query(request);\n          String result = BaseTestHarness.validateXPath(response, expectedFields);\n          if (result != null) {\n            String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n            log.error(msg);\n            fail(msg);\n          }\n        } catch (Exception ex) {\n          fail(\"Caught exception: \"+ex);\n        }\n        return client;\n      }\n    });\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      List<SolrInputDocument> docs = random().nextBoolean()? Arrays.asList(intDoc, dateDoc): Arrays.asList(dateDoc, intDoc);\n\n      SolrException ex = expectThrows(SolrException.class,  () -> {\n        randomClient.add(docs);\n        randomClient.commit();\n      });\n      assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(ex.code()));\n\n      ex = expectThrows(SolrException.class,  () -> {\n        CloudSolrClient cloudSolrClient = getCommonCloudSolrClient();\n        cloudSolrClient.add(docs);\n        cloudSolrClient.commit();\n      });\n      assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(ex.code()));\n    }\n  }\n\n","sourceOld":"  @Test\n  @ShardsFixed(num = 8)\n  // 12-Jun-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 04-May-2018\n  public void test() throws Exception {\n    setupRestTestHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getCollection(\"collection1\").getActiveSlices().size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    forAllRestTestHarnesses( new UnaryOperator<RestTestHarness>() {\n      @Override\n      public RestTestHarness apply(RestTestHarness client) {\n        try {\n          String request = \"/schema/fields?wt=xml\";\n          String response = client.query(request);\n          String result = BaseTestHarness.validateXPath(response, expectedFields);\n          if (result != null) {\n            String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n            log.error(msg);\n            fail(msg);\n          }\n        } catch (Exception ex) {\n          fail(\"Caught exception: \"+ex);\n        }\n        return client;\n      }\n    });\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        randomClient.add(docs);\n        randomClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n\n      try {\n        CloudSolrClient cloudSolrClient = getCommonCloudSolrClient();\n        cloudSolrClient.add(docs);\n        cloudSolrClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException ex) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode((ex).code()));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f8061ddd97f3352007d927dae445884a6f3d857b","date":1564988276,"type":3,"author":"Atri Sharma","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","sourceNew":"  @Test\n  @ShardsFixed(num = 8)\n  // 12-Jun-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 04-May-2018\n  public void test() throws Exception {\n    setupRestTestHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getCollection(\"collection1\").getActiveSlices().size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    forAllRestTestHarnesses( new UnaryOperator<RestTestHarness>() {\n      @Override\n      public RestTestHarness apply(RestTestHarness client) {\n        try {\n          String request = \"/schema/fields?wt=xml\";\n          String response = client.query(request);\n          String result = BaseTestHarness.validateXPath(response, expectedFields);\n          if (result != null) {\n            String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n            log.error(msg);\n            fail(msg);\n          }\n        } catch (Exception ex) {\n          fail(\"Caught exception: \"+ex);\n        }\n        return client;\n      }\n    });\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      List<SolrInputDocument> docs = random().nextBoolean()? Arrays.asList(intDoc, dateDoc): Arrays.asList(dateDoc, intDoc);\n\n      SolrException ex = expectThrows(SolrException.class,  () -> {\n        randomClient.add(docs);\n        randomClient.commit();\n      });\n      assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(ex.code()));\n\n      ex = expectThrows(SolrException.class,  () -> {\n        CloudSolrClient cloudSolrClient = getCommonCloudSolrClient();\n        cloudSolrClient.add(docs);\n        cloudSolrClient.commit();\n      });\n      assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(ex.code()));\n    }\n  }\n\n","sourceOld":"  @Test\n  @ShardsFixed(num = 8)\n  // 12-Jun-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 04-May-2018\n  public void test() throws Exception {\n    setupRestTestHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getCollection(\"collection1\").getActiveSlices().size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    forAllRestTestHarnesses( new UnaryOperator<RestTestHarness>() {\n      @Override\n      public RestTestHarness apply(RestTestHarness client) {\n        try {\n          String request = \"/schema/fields?wt=xml\";\n          String response = client.query(request);\n          String result = BaseTestHarness.validateXPath(response, expectedFields);\n          if (result != null) {\n            String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n            log.error(msg);\n            fail(msg);\n          }\n        } catch (Exception ex) {\n          fail(\"Caught exception: \"+ex);\n        }\n        return client;\n      }\n    });\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        randomClient.add(docs);\n        randomClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n\n      try {\n        CloudSolrClient cloudSolrClient = getCommonCloudSolrClient();\n        cloudSolrClient.add(docs);\n        cloudSolrClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException ex) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode((ex).code()));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d8daa7a1d5d0c033d73962d5ca3bf3f9c9687693","date":1565097295,"type":3,"author":"Jan Høydahl","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","sourceNew":"  @Test\n  @ShardsFixed(num = 8)\n  // 12-Jun-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 04-May-2018\n  public void test() throws Exception {\n    setupRestTestHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getCollection(\"collection1\").getActiveSlices().size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    forAllRestTestHarnesses( new UnaryOperator<RestTestHarness>() {\n      @Override\n      public RestTestHarness apply(RestTestHarness client) {\n        try {\n          String request = \"/schema/fields?wt=xml\";\n          String response = client.query(request);\n          String result = BaseTestHarness.validateXPath(response, expectedFields);\n          if (result != null) {\n            String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n            log.error(msg);\n            fail(msg);\n          }\n        } catch (Exception ex) {\n          fail(\"Caught exception: \"+ex);\n        }\n        return client;\n      }\n    });\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      List<SolrInputDocument> docs = random().nextBoolean()? Arrays.asList(intDoc, dateDoc): Arrays.asList(dateDoc, intDoc);\n\n      SolrException ex = expectThrows(SolrException.class,  () -> {\n        randomClient.add(docs);\n        randomClient.commit();\n      });\n      assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(ex.code()));\n\n      ex = expectThrows(SolrException.class,  () -> {\n        CloudSolrClient cloudSolrClient = getCommonCloudSolrClient();\n        cloudSolrClient.add(docs);\n        cloudSolrClient.commit();\n      });\n      assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(ex.code()));\n    }\n  }\n\n","sourceOld":"  @Test\n  @ShardsFixed(num = 8)\n  // 12-Jun-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 04-May-2018\n  public void test() throws Exception {\n    setupRestTestHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getCollection(\"collection1\").getActiveSlices().size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    forAllRestTestHarnesses( new UnaryOperator<RestTestHarness>() {\n      @Override\n      public RestTestHarness apply(RestTestHarness client) {\n        try {\n          String request = \"/schema/fields?wt=xml\";\n          String response = client.query(request);\n          String result = BaseTestHarness.validateXPath(response, expectedFields);\n          if (result != null) {\n            String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n            log.error(msg);\n            fail(msg);\n          }\n        } catch (Exception ex) {\n          fail(\"Caught exception: \"+ex);\n        }\n        return client;\n      }\n    });\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      List<SolrInputDocument> docs = null;\n\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      if (random().nextBoolean()) {\n        docs = Arrays.asList(intDoc, dateDoc);\n      } else {\n        docs = Arrays.asList(dateDoc, intDoc);\n      }\n\n      try {\n        randomClient.add(docs);\n        randomClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException se) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(se.code()));\n      }\n\n      try {\n        CloudSolrClient cloudSolrClient = getCommonCloudSolrClient();\n        cloudSolrClient.add(docs);\n        cloudSolrClient.commit();\n        fail(\"Expected Bad Request Exception\");\n      } catch (SolrException ex) {\n        assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode((ex).code()));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"42210058b8ae092085fa2658880b69931512e8bf","date":1579289439,"type":3,"author":"Mike","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/schema/TestCloudSchemaless#test().mjava","sourceNew":"  @Test\n  @ShardsFixed(num = 8)\n  // 12-Jun-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 04-May-2018\n  public void test() throws Exception {\n    setupRestTestHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getCollection(\"collection1\").getActiveSlices().size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    forAllRestTestHarnesses(client -> {\n      try {\n        String request = \"/schema/fields?wt=xml\";\n        String response = client.query(request);\n        String result = BaseTestHarness.validateXPath(response, expectedFields);\n        if (result != null) {\n          String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n          log.error(msg);\n          fail(msg);\n        }\n      } catch (Exception ex) {\n        fail(\"Caught exception: \"+ex);\n      }\n    });\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      List<SolrInputDocument> docs = random().nextBoolean()? Arrays.asList(intDoc, dateDoc): Arrays.asList(dateDoc, intDoc);\n\n      SolrException ex = expectThrows(SolrException.class,  () -> {\n        randomClient.add(docs);\n        randomClient.commit();\n      });\n      assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(ex.code()));\n\n      ex = expectThrows(SolrException.class,  () -> {\n        CloudSolrClient cloudSolrClient = getCommonCloudSolrClient();\n        cloudSolrClient.add(docs);\n        cloudSolrClient.commit();\n      });\n      assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(ex.code()));\n    }\n  }\n\n","sourceOld":"  @Test\n  @ShardsFixed(num = 8)\n  // 12-Jun-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 04-May-2018\n  public void test() throws Exception {\n    setupRestTestHarnesses();\n\n    // First, add a bunch of documents in a single update with the same new field.\n    // This tests that the replicas properly handle schema additions.\n\n    int slices =  getCommonCloudSolrClient().getZkStateReader().getClusterState()\n      .getCollection(\"collection1\").getActiveSlices().size();\n    int trials = 50;\n    // generate enough docs so that we can expect at least a doc per slice\n    int numDocsPerTrial = (int)(slices * (Math.log(slices) + 1));\n    SolrClient randomClient = clients.get(random().nextInt(clients.size()));\n    int docNumber = 0;\n    for (int i = 0; i < trials; ++i) {\n      List<SolrInputDocument> docs = new ArrayList<>();\n      for (int j =0; j < numDocsPerTrial; ++j) {\n        SolrInputDocument doc = new SolrInputDocument();\n        doc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n        doc.addField(\"newTestFieldInt\" + docNumber++, \"123\");\n        doc.addField(\"constantField\", \"3.14159\");\n        docs.add(doc);\n      }\n\n      randomClient.add(docs);\n    }\n    randomClient.commit();\n\n    String [] expectedFields = getExpectedFieldResponses(docNumber);\n    // Check that all the fields were added\n    forAllRestTestHarnesses( new UnaryOperator<RestTestHarness>() {\n      @Override\n      public RestTestHarness apply(RestTestHarness client) {\n        try {\n          String request = \"/schema/fields?wt=xml\";\n          String response = client.query(request);\n          String result = BaseTestHarness.validateXPath(response, expectedFields);\n          if (result != null) {\n            String msg = \"QUERY FAILED: xpath=\" + result + \"  request=\" + request + \"  response=\" + response;\n            log.error(msg);\n            fail(msg);\n          }\n        } catch (Exception ex) {\n          fail(\"Caught exception: \"+ex);\n        }\n        return client;\n      }\n    });\n\n    // Now, let's ensure that writing the same field with two different types fails\n    int failTrials = 50;\n    for (int i = 0; i < failTrials; ++i) {\n      SolrInputDocument intDoc = new SolrInputDocument();\n      intDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      intDoc.addField(\"longOrDateField\" + i, \"123\");\n\n      SolrInputDocument dateDoc = new SolrInputDocument();\n      dateDoc.addField(\"id\", Long.toHexString(Double.doubleToLongBits(random().nextDouble())));\n      dateDoc.addField(\"longOrDateField\" + i, \"1995-12-31T23:59:59Z\");\n\n      // randomize the order of the docs\n      List<SolrInputDocument> docs = random().nextBoolean()? Arrays.asList(intDoc, dateDoc): Arrays.asList(dateDoc, intDoc);\n\n      SolrException ex = expectThrows(SolrException.class,  () -> {\n        randomClient.add(docs);\n        randomClient.commit();\n      });\n      assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(ex.code()));\n\n      ex = expectThrows(SolrException.class,  () -> {\n        CloudSolrClient cloudSolrClient = getCommonCloudSolrClient();\n        cloudSolrClient.add(docs);\n        cloudSolrClient.commit();\n      });\n      assertEquals(ErrorCode.BAD_REQUEST, ErrorCode.getErrorCode(ex.code()));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"abb23fcc2461782ab204e61213240feb77d355aa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"344b0840364d990b29b97467bfcc766ff8325d11":["015571db92de1e6f75d89f34faeb520f99f07b97"],"015571db92de1e6f75d89f34faeb520f99f07b97":["abb23fcc2461782ab204e61213240feb77d355aa"],"f8061ddd97f3352007d927dae445884a6f3d857b":["a1c374690db69470f6aa4bffc43dcacf1f4e3e49","d1a307447328c95a00248512b40d7a5ff12ecd6a"],"42210058b8ae092085fa2658880b69931512e8bf":["d1a307447328c95a00248512b40d7a5ff12ecd6a"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":["4bab9eeea60eefbea2957be27b8d1923095df771","a1c374690db69470f6aa4bffc43dcacf1f4e3e49"],"7a23cf16c8fa265dc0a564adcabb55e3f054e0ac":["015571db92de1e6f75d89f34faeb520f99f07b97","344b0840364d990b29b97467bfcc766ff8325d11"],"0853c291617e0f9d0474b720bf609eb350e9225c":["344b0840364d990b29b97467bfcc766ff8325d11"],"4bab9eeea60eefbea2957be27b8d1923095df771":["0853c291617e0f9d0474b720bf609eb350e9225c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d1a307447328c95a00248512b40d7a5ff12ecd6a":["a1c374690db69470f6aa4bffc43dcacf1f4e3e49"],"d8daa7a1d5d0c033d73962d5ca3bf3f9c9687693":["a1c374690db69470f6aa4bffc43dcacf1f4e3e49","d1a307447328c95a00248512b40d7a5ff12ecd6a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["42210058b8ae092085fa2658880b69931512e8bf"],"a1c374690db69470f6aa4bffc43dcacf1f4e3e49":["4bab9eeea60eefbea2957be27b8d1923095df771"],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":["4bab9eeea60eefbea2957be27b8d1923095df771","a1c374690db69470f6aa4bffc43dcacf1f4e3e49"]},"commit2Childs":{"abb23fcc2461782ab204e61213240feb77d355aa":["015571db92de1e6f75d89f34faeb520f99f07b97"],"344b0840364d990b29b97467bfcc766ff8325d11":["7a23cf16c8fa265dc0a564adcabb55e3f054e0ac","0853c291617e0f9d0474b720bf609eb350e9225c"],"015571db92de1e6f75d89f34faeb520f99f07b97":["344b0840364d990b29b97467bfcc766ff8325d11","7a23cf16c8fa265dc0a564adcabb55e3f054e0ac"],"f8061ddd97f3352007d927dae445884a6f3d857b":[],"42210058b8ae092085fa2658880b69931512e8bf":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":[],"7a23cf16c8fa265dc0a564adcabb55e3f054e0ac":[],"0853c291617e0f9d0474b720bf609eb350e9225c":["4bab9eeea60eefbea2957be27b8d1923095df771"],"4bab9eeea60eefbea2957be27b8d1923095df771":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","a1c374690db69470f6aa4bffc43dcacf1f4e3e49","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["abb23fcc2461782ab204e61213240feb77d355aa"],"d1a307447328c95a00248512b40d7a5ff12ecd6a":["f8061ddd97f3352007d927dae445884a6f3d857b","42210058b8ae092085fa2658880b69931512e8bf","d8daa7a1d5d0c033d73962d5ca3bf3f9c9687693"],"d8daa7a1d5d0c033d73962d5ca3bf3f9c9687693":[],"a1c374690db69470f6aa4bffc43dcacf1f4e3e49":["f8061ddd97f3352007d927dae445884a6f3d857b","0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","d1a307447328c95a00248512b40d7a5ff12ecd6a","d8daa7a1d5d0c033d73962d5ca3bf3f9c9687693","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":[]},"heads":["f8061ddd97f3352007d927dae445884a6f3d857b","0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","7a23cf16c8fa265dc0a564adcabb55e3f054e0ac","d8daa7a1d5d0c033d73962d5ca3bf3f9c9687693","cd5edd1f2b162a5cfa08efd17851a07373a96817","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}