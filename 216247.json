{"path":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","commits":[{"id":"abb23fcc2461782ab204e61213240feb77d355aa","date":1422029612,"type":1,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#doTest().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StopableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = new CloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StopableIndexingThread indexThread = new StopableIndexingThread(null, client, \"1\", true, docCount);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StopableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n    \n    for (CloudSolrClient client : cloudClients) {\n      client.shutdown();\n    }\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = ((SolrDispatchFilter) jetty.getDispatchFilter()\n          .getFilter()).getCores();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StopableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = new CloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StopableIndexingThread indexThread = new StopableIndexingThread(null, client, \"1\", true, docCount);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StopableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n    \n    for (CloudSolrClient client : cloudClients) {\n      client.shutdown();\n    }\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = ((SolrDispatchFilter) jetty.getDispatchFilter()\n          .getFilter()).getCores();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cc3b13b430571c2e169f98fe38e1e7666f88522d","date":1422446157,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StopableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = new CloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StopableIndexingThread indexThread = new StopableIndexingThread(null, client, \"1\", true, docCount);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StopableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = ((SolrDispatchFilter) jetty.getDispatchFilter()\n          .getFilter()).getCores();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StopableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = new CloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StopableIndexingThread indexThread = new StopableIndexingThread(null, client, \"1\", true, docCount);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StopableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n    \n    for (CloudSolrClient client : cloudClients) {\n      client.shutdown();\n    }\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = ((SolrDispatchFilter) jetty.getDispatchFilter()\n          .getFilter()).getCores();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6c94d2661bc1c14426980ec7882e951fdcff08d0","date":1427167177,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StopableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = new CloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StopableIndexingThread indexThread = new StopableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StopableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = ((SolrDispatchFilter) jetty.getDispatchFilter()\n          .getFilter()).getCores();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StopableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = new CloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StopableIndexingThread indexThread = new StopableIndexingThread(null, client, \"1\", true, docCount);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StopableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = ((SolrDispatchFilter) jetty.getDispatchFilter()\n          .getFilter()).getCores();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":["bafca15d8e408346a67f4282ad1143b88023893b"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"98287baa2c8d136e801f366a73e27a23285b7b98","date":1427241813,"type":3,"author":"Ramkumar Aiyengar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = new CloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = ((SolrDispatchFilter) jetty.getDispatchFilter()\n          .getFilter()).getCores();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StopableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = new CloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StopableIndexingThread indexThread = new StopableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StopableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = ((SolrDispatchFilter) jetty.getDispatchFilter()\n          .getFilter()).getCores();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = new CloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = ((SolrDispatchFilter) jetty.getDispatchFilter()\n          .getFilter()).getCores();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StopableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = new CloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StopableIndexingThread indexThread = new StopableIndexingThread(null, client, \"1\", true, docCount);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StopableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = ((SolrDispatchFilter) jetty.getDispatchFilter()\n          .getFilter()).getCores();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1088b72b3b4cc45316b7595bd09023c859cd2327","date":1447150009,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = new CloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = new CloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = ((SolrDispatchFilter) jetty.getDispatchFilter()\n          .getFilter()).getCores();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e3c94a8b8bf47db4f968d9ae510ec8bbe1372088","date":1460069869,"type":3,"author":"Anshum Gupta","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = new CloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5bdaf2cee03ff78b0a0cbf23df0095a3590b493b","date":1460110033,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = new CloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0c5635dd6609964d2a79ffdc1b83922d4b31c5e0","date":1465834970,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          Directory dir = core.getDirectoryFactory().get(core.getDataDir(), null, null);\n          try {\n            long dataDirSize = core.getDirectoryFactory().size(dir);\n            FileSystem fileSystem = null;\n            \n            fileSystem = FileSystem.newInstance(\n                new Path(core.getDataDir()).toUri(), new Configuration());\n            long size = fileSystem.getContentSummary(\n                new Path(core.getDataDir())).getLength();\n            assertEquals(size, dataDirSize);\n          } finally {\n            core.getDirectoryFactory().release(dir);\n          }\n          \n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["add53de9835b2cd1a7a80b4e0036afee171c9fdf"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"57dc82c7b33cd580e7ab5179019bc78f3d7f8e79","date":1465913303,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          Directory dir = core.getDirectoryFactory().get(core.getDataDir(), null, null);\n          try {\n            long dataDirSize = core.getDirectoryFactory().size(dir);\n            FileSystem fileSystem = null;\n            \n            fileSystem = FileSystem.newInstance(\n                new Path(core.getDataDir()).toUri(), new Configuration());\n            long size = fileSystem.getContentSummary(\n                new Path(core.getDataDir())).getLength();\n            assertEquals(size, dataDirSize);\n          } finally {\n            core.getDirectoryFactory().release(dir);\n          }\n          \n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          Directory dir = core.getDirectoryFactory().get(core.getDataDir(), null, null);\n          try {\n            long dataDirSize = core.getDirectoryFactory().size(dir);\n            FileSystem fileSystem = null;\n            \n            fileSystem = FileSystem.newInstance(\n                new Path(core.getDataDir()).toUri(), new Configuration());\n            long size = fileSystem.getContentSummary(\n                new Path(core.getDataDir())).getLength();\n            assertEquals(size, dataDirSize);\n          } finally {\n            core.getDirectoryFactory().release(dir);\n          }\n          \n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"edf5bfd92011996df858b4519d3c36705928d01b","date":1486052676,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          DirectoryFactory factory = core.getDirectoryFactory();\n          if (factory instanceof MetricsDirectoryFactory) {\n            factory = ((MetricsDirectoryFactory) factory).getDelegate();\n          }\n          assertTrue(\"Found: \" + core.getDirectoryFactory().getClass().getName(), factory instanceof HdfsDirectoryFactory);\n          Directory dir = factory.get(core.getDataDir(), null, null);\n          try {\n            long dataDirSize = factory.size(dir);\n            FileSystem fileSystem = null;\n            \n            fileSystem = FileSystem.newInstance(\n                new Path(core.getDataDir()).toUri(), new Configuration());\n            long size = fileSystem.getContentSummary(\n                new Path(core.getDataDir())).getLength();\n            assertEquals(size, dataDirSize);\n          } finally {\n            core.getDirectoryFactory().release(dir);\n          }\n          \n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) ((MetricsDirectoryFactory.MetricsDirectory)iw\n                .getDirectory()).getDelegate();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          assertTrue(core.getDirectoryFactory() instanceof HdfsDirectoryFactory);\n          Directory dir = core.getDirectoryFactory().get(core.getDataDir(), null, null);\n          try {\n            long dataDirSize = core.getDirectoryFactory().size(dir);\n            FileSystem fileSystem = null;\n            \n            fileSystem = FileSystem.newInstance(\n                new Path(core.getDataDir()).toUri(), new Configuration());\n            long size = fileSystem.getContentSummary(\n                new Path(core.getDataDir())).getLength();\n            assertEquals(size, dataDirSize);\n          } finally {\n            core.getDirectoryFactory().release(dir);\n          }\n          \n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw\n                .getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e037d4cc589a2acd147ea27ffc29b19d595be53f","date":1488209547,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          DirectoryFactory factory = core.getDirectoryFactory();\n          assertTrue(\"Found: \" + core.getDirectoryFactory().getClass().getName(), factory instanceof HdfsDirectoryFactory);\n          Directory dir = factory.get(core.getDataDir(), null, null);\n          try {\n            long dataDirSize = factory.size(dir);\n            FileSystem fileSystem = null;\n            \n            fileSystem = FileSystem.newInstance(\n                new Path(core.getDataDir()).toUri(), new Configuration());\n            long size = fileSystem.getContentSummary(\n                new Path(core.getDataDir())).getLength();\n            assertEquals(size, dataDirSize);\n          } finally {\n            core.getDirectoryFactory().release(dir);\n          }\n          \n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw.getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          DirectoryFactory factory = core.getDirectoryFactory();\n          if (factory instanceof MetricsDirectoryFactory) {\n            factory = ((MetricsDirectoryFactory) factory).getDelegate();\n          }\n          assertTrue(\"Found: \" + core.getDirectoryFactory().getClass().getName(), factory instanceof HdfsDirectoryFactory);\n          Directory dir = factory.get(core.getDataDir(), null, null);\n          try {\n            long dataDirSize = factory.size(dir);\n            FileSystem fileSystem = null;\n            \n            fileSystem = FileSystem.newInstance(\n                new Path(core.getDataDir()).toUri(), new Configuration());\n            long size = fileSystem.getContentSummary(\n                new Path(core.getDataDir())).getLength();\n            assertEquals(size, dataDirSize);\n          } finally {\n            core.getDirectoryFactory().release(dir);\n          }\n          \n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) ((MetricsDirectoryFactory.MetricsDirectory)iw\n                .getDirectory()).getDelegate();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"197bbedf08450ade98a11f4a0001448059666bec","date":1498534625,"type":3,"author":"Ishan Chattopadhyaya","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, \"conf1\", 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          DirectoryFactory factory = core.getDirectoryFactory();\n          assertTrue(\"Found: \" + core.getDirectoryFactory().getClass().getName(), factory instanceof HdfsDirectoryFactory);\n          Directory dir = factory.get(core.getDataDir(), null, null);\n          try {\n            long dataDirSize = factory.size(dir);\n            FileSystem fileSystem = null;\n            \n            fileSystem = FileSystem.newInstance(\n                new Path(core.getDataDir()).toUri(), new Configuration());\n            long size = fileSystem.getContentSummary(\n                new Path(core.getDataDir())).getLength();\n            assertEquals(size, dataDirSize);\n          } finally {\n            core.getDirectoryFactory().release(dir);\n          }\n          \n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw.getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          DirectoryFactory factory = core.getDirectoryFactory();\n          assertTrue(\"Found: \" + core.getDirectoryFactory().getClass().getName(), factory instanceof HdfsDirectoryFactory);\n          Directory dir = factory.get(core.getDataDir(), null, null);\n          try {\n            long dataDirSize = factory.size(dir);\n            FileSystem fileSystem = null;\n            \n            fileSystem = FileSystem.newInstance(\n                new Path(core.getDataDir()).toUri(), new Configuration());\n            long size = fileSystem.getContentSummary(\n                new Path(core.getDataDir())).getLength();\n            assertEquals(size, dataDirSize);\n          } finally {\n            core.getDirectoryFactory().release(dir);\n          }\n          \n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw.getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4","date":1498540685,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, \"conf1\", 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          DirectoryFactory factory = core.getDirectoryFactory();\n          assertTrue(\"Found: \" + core.getDirectoryFactory().getClass().getName(), factory instanceof HdfsDirectoryFactory);\n          Directory dir = factory.get(core.getDataDir(), null, null);\n          try {\n            long dataDirSize = factory.size(dir);\n            FileSystem fileSystem = null;\n            \n            fileSystem = FileSystem.newInstance(\n                new Path(core.getDataDir()).toUri(), new Configuration());\n            long size = fileSystem.getContentSummary(\n                new Path(core.getDataDir())).getLength();\n            assertEquals(size, dataDirSize);\n          } finally {\n            core.getDirectoryFactory().release(dir);\n          }\n          \n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw.getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          DirectoryFactory factory = core.getDirectoryFactory();\n          assertTrue(\"Found: \" + core.getDirectoryFactory().getClass().getName(), factory instanceof HdfsDirectoryFactory);\n          Directory dir = factory.get(core.getDataDir(), null, null);\n          try {\n            long dataDirSize = factory.size(dir);\n            FileSystem fileSystem = null;\n            \n            fileSystem = FileSystem.newInstance(\n                new Path(core.getDataDir()).toUri(), new Configuration());\n            long size = fileSystem.getContentSummary(\n                new Path(core.getDataDir())).getLength();\n            assertEquals(size, dataDirSize);\n          } finally {\n            core.getDirectoryFactory().release(dir);\n          }\n          \n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw.getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, \"conf1\", 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          DirectoryFactory factory = core.getDirectoryFactory();\n          assertTrue(\"Found: \" + core.getDirectoryFactory().getClass().getName(), factory instanceof HdfsDirectoryFactory);\n          Directory dir = factory.get(core.getDataDir(), null, null);\n          try {\n            long dataDirSize = factory.size(dir);\n            FileSystem fileSystem = null;\n            \n            fileSystem = FileSystem.newInstance(\n                new Path(core.getDataDir()).toUri(), new Configuration());\n            long size = fileSystem.getContentSummary(\n                new Path(core.getDataDir())).getLength();\n            assertEquals(size, dataDirSize);\n          } finally {\n            core.getDirectoryFactory().release(dir);\n          }\n          \n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw.getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          DirectoryFactory factory = core.getDirectoryFactory();\n          assertTrue(\"Found: \" + core.getDirectoryFactory().getClass().getName(), factory instanceof HdfsDirectoryFactory);\n          Directory dir = factory.get(core.getDataDir(), null, null);\n          try {\n            long dataDirSize = factory.size(dir);\n            FileSystem fileSystem = null;\n            \n            fileSystem = FileSystem.newInstance(\n                new Path(core.getDataDir()).toUri(), new Configuration());\n            long size = fileSystem.getContentSummary(\n                new Path(core.getDataDir())).getLength();\n            assertEquals(size, dataDirSize);\n          } finally {\n            core.getDirectoryFactory().release(dir);\n          }\n          \n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw.getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f","date":1552317217,"type":3,"author":"Kevin Risden","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, \"conf1\", 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          DirectoryFactory factory = core.getDirectoryFactory();\n          assertTrue(\"Found: \" + core.getDirectoryFactory().getClass().getName(), factory instanceof HdfsDirectoryFactory);\n          Directory dir = factory.get(core.getDataDir(), null, null);\n          try {\n            long dataDirSize = factory.size(dir);\n            FileSystem fileSystem = null;\n            \n            fileSystem = FileSystem.newInstance(\n                new Path(core.getDataDir()).toUri(), new Configuration());\n            long size = fileSystem.getContentSummary(\n                new Path(core.getDataDir())).getLength();\n            assertEquals(size, dataDirSize);\n          } finally {\n            core.getDirectoryFactory().release(dir);\n          }\n          \n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw.getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(\"solr.hdfs.blockcache.global\")) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, \"conf1\", 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          DirectoryFactory factory = core.getDirectoryFactory();\n          assertTrue(\"Found: \" + core.getDirectoryFactory().getClass().getName(), factory instanceof HdfsDirectoryFactory);\n          Directory dir = factory.get(core.getDataDir(), null, null);\n          try {\n            long dataDirSize = factory.size(dir);\n            FileSystem fileSystem = null;\n            \n            fileSystem = FileSystem.newInstance(\n                new Path(core.getDataDir()).toUri(), new Configuration());\n            long size = fileSystem.getContentSummary(\n                new Path(core.getDataDir())).getLength();\n            assertEquals(size, dataDirSize);\n          } finally {\n            core.getDirectoryFactory().release(dir);\n          }\n          \n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw.getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(SOLR_HDFS_BLOCKCACHE_GLOBAL)) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"add53de9835b2cd1a7a80b4e0036afee171c9fdf","date":1552937136,"type":3,"author":"Kevin Risden","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, \"conf1\", 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          DirectoryFactory factory = core.getDirectoryFactory();\n          assertTrue(\"Found: \" + core.getDirectoryFactory().getClass().getName(), factory instanceof HdfsDirectoryFactory);\n          Directory dir = factory.get(core.getDataDir(), null, null);\n          try {\n            long dataDirSize = factory.size(dir);\n            Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n            FileSystem fileSystem = FileSystem.newInstance(\n                new Path(core.getDataDir()).toUri(), conf);\n            long size = fileSystem.getContentSummary(\n                new Path(core.getDataDir())).getLength();\n            assertEquals(size, dataDirSize);\n          } finally {\n            core.getDirectoryFactory().release(dir);\n          }\n          \n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw.getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(\"solr.hdfs.blockcache.global\")) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, \"conf1\", 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          DirectoryFactory factory = core.getDirectoryFactory();\n          assertTrue(\"Found: \" + core.getDirectoryFactory().getClass().getName(), factory instanceof HdfsDirectoryFactory);\n          Directory dir = factory.get(core.getDataDir(), null, null);\n          try {\n            long dataDirSize = factory.size(dir);\n            FileSystem fileSystem = null;\n            \n            fileSystem = FileSystem.newInstance(\n                new Path(core.getDataDir()).toUri(), new Configuration());\n            long size = fileSystem.getContentSummary(\n                new Path(core.getDataDir())).getLength();\n            assertEquals(size, dataDirSize);\n          } finally {\n            core.getDirectoryFactory().release(dir);\n          }\n          \n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw.getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(\"solr.hdfs.blockcache.global\")) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":["0c5635dd6609964d2a79ffdc1b83922d4b31c5e0"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e","date":1594223844,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, \"conf1\", 2, 2);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          DirectoryFactory factory = core.getDirectoryFactory();\n          assertTrue(\"Found: \" + core.getDirectoryFactory().getClass().getName(), factory instanceof HdfsDirectoryFactory);\n          Directory dir = factory.get(core.getDataDir(), null, null);\n          try {\n            long dataDirSize = factory.size(dir);\n            Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n            FileSystem fileSystem = FileSystem.newInstance(\n                new Path(core.getDataDir()).toUri(), conf);\n            long size = fileSystem.getContentSummary(\n                new Path(core.getDataDir())).getLength();\n            assertEquals(size, dataDirSize);\n          } finally {\n            core.getDirectoryFactory().release(dir);\n          }\n          \n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw.getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(\"solr.hdfs.blockcache.global\")) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    int docCount = random().nextInt(1313) + 1;\n    int cnt = random().nextInt(4) + 1;\n    for (int i = 0; i < cnt; i++) {\n      createCollection(ACOLLECTION + i, \"conf1\", 2, 2, 9);\n    }\n    for (int i = 0; i < cnt; i++) {\n      waitForRecoveriesToFinish(ACOLLECTION + i, false);\n    }\n    List<CloudSolrClient> cloudClients = new ArrayList<>();\n    List<StoppableIndexingThread> threads = new ArrayList<>();\n    for (int i = 0; i < cnt; i++) {\n      CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress());\n      client.setDefaultCollection(ACOLLECTION + i);\n      cloudClients.add(client);\n      StoppableIndexingThread indexThread = new StoppableIndexingThread(null, client, \"1\", true, docCount, 1, true);\n      threads.add(indexThread);\n      indexThread.start();\n    }\n    \n    int addCnt = 0;\n    for (StoppableIndexingThread thread : threads) {\n      thread.join();\n      addCnt += thread.getNumAdds() - thread.getNumDeletes();\n    }\n   \n    long collectionsCount = 0;\n    for (CloudSolrClient client : cloudClients) {\n      client.commit();\n      collectionsCount += client.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n    }\n\n    IOUtils.close(cloudClients);\n\n    assertEquals(addCnt, collectionsCount);\n    \n    BlockCache lastBlockCache = null;\n    // assert that we are using the block directory and that write and read caching are being used\n    for (JettySolrRunner jetty : jettys) {\n      CoreContainer cores = jetty.getCoreContainer();\n      Collection<SolrCore> solrCores = cores.getCores();\n      for (SolrCore core : solrCores) {\n        if (core.getCoreDescriptor().getCloudDescriptor().getCollectionName()\n            .startsWith(ACOLLECTION)) {\n          DirectoryFactory factory = core.getDirectoryFactory();\n          assertTrue(\"Found: \" + core.getDirectoryFactory().getClass().getName(), factory instanceof HdfsDirectoryFactory);\n          Directory dir = factory.get(core.getDataDir(), null, null);\n          try {\n            long dataDirSize = factory.size(dir);\n            Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n            FileSystem fileSystem = FileSystem.newInstance(\n                new Path(core.getDataDir()).toUri(), conf);\n            long size = fileSystem.getContentSummary(\n                new Path(core.getDataDir())).getLength();\n            assertEquals(size, dataDirSize);\n          } finally {\n            core.getDirectoryFactory().release(dir);\n          }\n          \n          RefCounted<IndexWriter> iwRef = core.getUpdateHandler()\n              .getSolrCoreState().getIndexWriter(core);\n          try {\n            IndexWriter iw = iwRef.get();\n            NRTCachingDirectory directory = (NRTCachingDirectory) iw.getDirectory();\n            BlockDirectory blockDirectory = (BlockDirectory) directory\n                .getDelegate();\n            assertTrue(blockDirectory.isBlockCacheReadEnabled());\n            // see SOLR-6424\n            assertFalse(blockDirectory.isBlockCacheWriteEnabled());\n            Cache cache = blockDirectory.getCache();\n            // we know it's a BlockDirectoryCache, but future proof\n            assertTrue(cache instanceof BlockDirectoryCache);\n            BlockCache blockCache = ((BlockDirectoryCache) cache)\n                .getBlockCache();\n            if (lastBlockCache != null) {\n              if (Boolean.getBoolean(\"solr.hdfs.blockcache.global\")) {\n                assertEquals(lastBlockCache, blockCache);\n              } else {\n                assertNotSame(lastBlockCache, blockCache);\n              }\n            }\n            lastBlockCache = blockCache;\n          } finally {\n            iwRef.decref();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f":["28288370235ed02234a64753cdbf0c6ec096304a"],"edf5bfd92011996df858b4519d3c36705928d01b":["57dc82c7b33cd580e7ab5179019bc78f3d7f8e79"],"197bbedf08450ade98a11f4a0001448059666bec":["e037d4cc589a2acd147ea27ffc29b19d595be53f"],"abb23fcc2461782ab204e61213240feb77d355aa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"6c94d2661bc1c14426980ec7882e951fdcff08d0":["cc3b13b430571c2e169f98fe38e1e7666f88522d"],"e3c94a8b8bf47db4f968d9ae510ec8bbe1372088":["1088b72b3b4cc45316b7595bd09023c859cd2327"],"98287baa2c8d136e801f366a73e27a23285b7b98":["6c94d2661bc1c14426980ec7882e951fdcff08d0"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["add53de9835b2cd1a7a80b4e0036afee171c9fdf"],"28288370235ed02234a64753cdbf0c6ec096304a":["e037d4cc589a2acd147ea27ffc29b19d595be53f","197bbedf08450ade98a11f4a0001448059666bec"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["5bdaf2cee03ff78b0a0cbf23df0095a3590b493b","57dc82c7b33cd580e7ab5179019bc78f3d7f8e79"],"add53de9835b2cd1a7a80b4e0036afee171c9fdf":["2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f"],"e037d4cc589a2acd147ea27ffc29b19d595be53f":["edf5bfd92011996df858b4519d3c36705928d01b"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["cc3b13b430571c2e169f98fe38e1e7666f88522d","98287baa2c8d136e801f366a73e27a23285b7b98"],"1088b72b3b4cc45316b7595bd09023c859cd2327":["98287baa2c8d136e801f366a73e27a23285b7b98"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4":["e037d4cc589a2acd147ea27ffc29b19d595be53f","197bbedf08450ade98a11f4a0001448059666bec"],"5bdaf2cee03ff78b0a0cbf23df0095a3590b493b":["1088b72b3b4cc45316b7595bd09023c859cd2327","e3c94a8b8bf47db4f968d9ae510ec8bbe1372088"],"cc3b13b430571c2e169f98fe38e1e7666f88522d":["abb23fcc2461782ab204e61213240feb77d355aa"],"0c5635dd6609964d2a79ffdc1b83922d4b31c5e0":["5bdaf2cee03ff78b0a0cbf23df0095a3590b493b"],"57dc82c7b33cd580e7ab5179019bc78f3d7f8e79":["5bdaf2cee03ff78b0a0cbf23df0095a3590b493b","0c5635dd6609964d2a79ffdc1b83922d4b31c5e0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"]},"commit2Childs":{"2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f":["add53de9835b2cd1a7a80b4e0036afee171c9fdf"],"edf5bfd92011996df858b4519d3c36705928d01b":["e037d4cc589a2acd147ea27ffc29b19d595be53f"],"197bbedf08450ade98a11f4a0001448059666bec":["28288370235ed02234a64753cdbf0c6ec096304a","fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4"],"abb23fcc2461782ab204e61213240feb77d355aa":["cc3b13b430571c2e169f98fe38e1e7666f88522d"],"6c94d2661bc1c14426980ec7882e951fdcff08d0":["98287baa2c8d136e801f366a73e27a23285b7b98"],"e3c94a8b8bf47db4f968d9ae510ec8bbe1372088":["5bdaf2cee03ff78b0a0cbf23df0095a3590b493b"],"98287baa2c8d136e801f366a73e27a23285b7b98":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","1088b72b3b4cc45316b7595bd09023c859cd2327"],"28288370235ed02234a64753cdbf0c6ec096304a":["2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"e037d4cc589a2acd147ea27ffc29b19d595be53f":["197bbedf08450ade98a11f4a0001448059666bec","28288370235ed02234a64753cdbf0c6ec096304a","fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4"],"add53de9835b2cd1a7a80b4e0036afee171c9fdf":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"1088b72b3b4cc45316b7595bd09023c859cd2327":["e3c94a8b8bf47db4f968d9ae510ec8bbe1372088","5bdaf2cee03ff78b0a0cbf23df0095a3590b493b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["abb23fcc2461782ab204e61213240feb77d355aa"],"fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4":[],"5bdaf2cee03ff78b0a0cbf23df0095a3590b493b":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","0c5635dd6609964d2a79ffdc1b83922d4b31c5e0","57dc82c7b33cd580e7ab5179019bc78f3d7f8e79"],"cc3b13b430571c2e169f98fe38e1e7666f88522d":["6c94d2661bc1c14426980ec7882e951fdcff08d0","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"0c5635dd6609964d2a79ffdc1b83922d4b31c5e0":["57dc82c7b33cd580e7ab5179019bc78f3d7f8e79"],"57dc82c7b33cd580e7ab5179019bc78f3d7f8e79":["edf5bfd92011996df858b4519d3c36705928d01b","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}