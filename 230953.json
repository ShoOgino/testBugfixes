{"path":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#testUpgradeOldOptimizedIndexWithAdditions().mjava","commits":[{"id":"536f6911665ce844b868182b8d0ca107ea0eceab","date":1304957860,"type":0,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#testUpgradeOldOptimizedIndexWithAdditions().mjava","pathOld":"/dev/null","sourceNew":"  public void testUpgradeOldOptimizedIndexWithAdditions() throws Exception {\n    for (String name : oldOptimizedNames) {\n      if (VERBOSE) {\n        System.out.println(\"testUpgradeOldOptimizedIndexWithAdditions: index=\" +name);\n      }\n      File oldIndxeDir = _TestUtil.getTempDir(name);\n      _TestUtil.unzip(getDataFile(\"index.\" + name + \".zip\"), oldIndxeDir);\n      Directory dir = newFSDirectory(oldIndxeDir);\n\n      assertEquals(\"Original index must be optimized\", 1, getNumberOfSegments(dir));\n\n      // create a bunch of dummy segments\n      int id = 40;\n      RAMDirectory ramDir = new RAMDirectory();\n      for (int i = 0; i < 3; i++) {\n        // only use Log- or TieredMergePolicy, to make document addition predictable and not suddenly merge:\n        MergePolicy mp = random.nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();\n        IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n          .setMergePolicy(mp);\n        IndexWriter w = new IndexWriter(ramDir, iwc);\n        // add few more docs:\n        for(int j = 0; j < RANDOM_MULTIPLIER * random.nextInt(30); j++) {\n          addDoc(w, id++);\n        }\n        w.close(false);\n      }\n      \n      // add dummy segments (which are all in current version) to optimized index\n      MergePolicy mp = random.nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();\n      IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, null)\n        .setMergePolicy(mp);\n      IndexWriter w = new IndexWriter(dir, iwc);\n      w.setInfoStream(VERBOSE ? System.out : null);\n      w.addIndexes(ramDir);\n      w.close(false);\n      \n      // determine count of segments in modified index\n      final int origSegCount = getNumberOfSegments(dir);\n      \n      new IndexUpgrader(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, null), VERBOSE ? System.out : null, false)\n        .upgrade();\n\n      final int segCount = checkAllSegmentsUpgraded(dir);\n      assertEquals(\"Index must still contain the same number of segments, as only one segment was upgraded and nothing else merged\",\n        origSegCount, segCount);\n      \n      dir.close();\n      _TestUtil.rmDir(oldIndxeDir);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c700f8d0842d3e52bb2bdfbfdc046a137e836edb","date":1305285499,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#testUpgradeOldOptimizedIndexWithAdditions().mjava","pathOld":"/dev/null","sourceNew":"  public void testUpgradeOldOptimizedIndexWithAdditions() throws Exception {\n    for (String name : oldOptimizedNames) {\n      if (VERBOSE) {\n        System.out.println(\"testUpgradeOldOptimizedIndexWithAdditions: index=\" +name);\n      }\n      File oldIndxeDir = _TestUtil.getTempDir(name);\n      _TestUtil.unzip(getDataFile(\"index.\" + name + \".zip\"), oldIndxeDir);\n      Directory dir = newFSDirectory(oldIndxeDir);\n\n      assertEquals(\"Original index must be optimized\", 1, getNumberOfSegments(dir));\n\n      // create a bunch of dummy segments\n      int id = 40;\n      RAMDirectory ramDir = new RAMDirectory();\n      for (int i = 0; i < 3; i++) {\n        // only use Log- or TieredMergePolicy, to make document addition predictable and not suddenly merge:\n        MergePolicy mp = random.nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();\n        IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n          .setMergePolicy(mp);\n        IndexWriter w = new IndexWriter(ramDir, iwc);\n        // add few more docs:\n        for(int j = 0; j < RANDOM_MULTIPLIER * random.nextInt(30); j++) {\n          addDoc(w, id++);\n        }\n        w.close(false);\n      }\n      \n      // add dummy segments (which are all in current version) to optimized index\n      MergePolicy mp = random.nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();\n      IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, null)\n        .setMergePolicy(mp);\n      IndexWriter w = new IndexWriter(dir, iwc);\n      w.setInfoStream(VERBOSE ? System.out : null);\n      w.addIndexes(ramDir);\n      w.close(false);\n      \n      // determine count of segments in modified index\n      final int origSegCount = getNumberOfSegments(dir);\n      \n      new IndexUpgrader(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, null), VERBOSE ? System.out : null, false)\n        .upgrade();\n\n      final int segCount = checkAllSegmentsUpgraded(dir);\n      assertEquals(\"Index must still contain the same number of segments, as only one segment was upgraded and nothing else merged\",\n        origSegCount, segCount);\n      \n      dir.close();\n      _TestUtil.rmDir(oldIndxeDir);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":0,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#testUpgradeOldOptimizedIndexWithAdditions().mjava","pathOld":"/dev/null","sourceNew":"  public void testUpgradeOldOptimizedIndexWithAdditions() throws Exception {\n    for (String name : oldOptimizedNames) {\n      if (VERBOSE) {\n        System.out.println(\"testUpgradeOldOptimizedIndexWithAdditions: index=\" +name);\n      }\n      File oldIndxeDir = _TestUtil.getTempDir(name);\n      _TestUtil.unzip(getDataFile(\"index.\" + name + \".zip\"), oldIndxeDir);\n      Directory dir = newFSDirectory(oldIndxeDir);\n\n      assertEquals(\"Original index must be optimized\", 1, getNumberOfSegments(dir));\n\n      // create a bunch of dummy segments\n      int id = 40;\n      RAMDirectory ramDir = new RAMDirectory();\n      for (int i = 0; i < 3; i++) {\n        // only use Log- or TieredMergePolicy, to make document addition predictable and not suddenly merge:\n        MergePolicy mp = random.nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();\n        IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n          .setMergePolicy(mp);\n        IndexWriter w = new IndexWriter(ramDir, iwc);\n        // add few more docs:\n        for(int j = 0; j < RANDOM_MULTIPLIER * random.nextInt(30); j++) {\n          addDoc(w, id++);\n        }\n        w.close(false);\n      }\n      \n      // add dummy segments (which are all in current version) to optimized index\n      MergePolicy mp = random.nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();\n      IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, null)\n        .setMergePolicy(mp);\n      IndexWriter w = new IndexWriter(dir, iwc);\n      w.setInfoStream(VERBOSE ? System.out : null);\n      w.addIndexes(ramDir);\n      w.close(false);\n      \n      // determine count of segments in modified index\n      final int origSegCount = getNumberOfSegments(dir);\n      \n      new IndexUpgrader(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, null), VERBOSE ? System.out : null, false)\n        .upgrade();\n\n      final int segCount = checkAllSegmentsUpgraded(dir);\n      assertEquals(\"Index must still contain the same number of segments, as only one segment was upgraded and nothing else merged\",\n        origSegCount, segCount);\n      \n      dir.close();\n      _TestUtil.rmDir(oldIndxeDir);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"06584e6e98d592b34e1329b384182f368d2025e8","date":1320850353,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#testUpgradeOldOptimizedIndexWithAdditions().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#testUpgradeOldOptimizedIndexWithAdditions().mjava","sourceNew":"  public void testUpgradeOldOptimizedIndexWithAdditions() throws Exception {\n    for (String name : oldOptimizedNames) {\n      if (VERBOSE) {\n        System.out.println(\"testUpgradeOldOptimizedIndexWithAdditions: index=\" +name);\n      }\n      File oldIndxeDir = _TestUtil.getTempDir(name);\n      _TestUtil.unzip(getDataFile(\"index.\" + name + \".zip\"), oldIndxeDir);\n      Directory dir = newFSDirectory(oldIndxeDir);\n\n      assertEquals(\"Original index must be optimized\", 1, getNumberOfSegments(dir));\n\n      // create a bunch of dummy segments\n      int id = 40;\n      RAMDirectory ramDir = new RAMDirectory();\n      for (int i = 0; i < 3; i++) {\n        // only use Log- or TieredMergePolicy, to make document addition predictable and not suddenly merge:\n        MergePolicy mp = random.nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();\n        IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n          .setMergePolicy(mp);\n        IndexWriter w = new IndexWriter(ramDir, iwc);\n        // add few more docs:\n        for(int j = 0; j < RANDOM_MULTIPLIER * random.nextInt(30); j++) {\n          addDoc(w, id++);\n        }\n        w.close(false);\n      }\n      \n      // add dummy segments (which are all in current version) to optimized index\n      MergePolicy mp = random.nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();\n      IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, null)\n        .setMergePolicy(mp);\n      IndexWriter w = new IndexWriter(dir, iwc);\n      w.addIndexes(ramDir);\n      w.close(false);\n      \n      // determine count of segments in modified index\n      final int origSegCount = getNumberOfSegments(dir);\n      \n      new IndexUpgrader(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, null), false)\n        .upgrade();\n\n      final int segCount = checkAllSegmentsUpgraded(dir);\n      assertEquals(\"Index must still contain the same number of segments, as only one segment was upgraded and nothing else merged\",\n        origSegCount, segCount);\n      \n      dir.close();\n      _TestUtil.rmDir(oldIndxeDir);\n    }\n  }\n\n","sourceOld":"  public void testUpgradeOldOptimizedIndexWithAdditions() throws Exception {\n    for (String name : oldOptimizedNames) {\n      if (VERBOSE) {\n        System.out.println(\"testUpgradeOldOptimizedIndexWithAdditions: index=\" +name);\n      }\n      File oldIndxeDir = _TestUtil.getTempDir(name);\n      _TestUtil.unzip(getDataFile(\"index.\" + name + \".zip\"), oldIndxeDir);\n      Directory dir = newFSDirectory(oldIndxeDir);\n\n      assertEquals(\"Original index must be optimized\", 1, getNumberOfSegments(dir));\n\n      // create a bunch of dummy segments\n      int id = 40;\n      RAMDirectory ramDir = new RAMDirectory();\n      for (int i = 0; i < 3; i++) {\n        // only use Log- or TieredMergePolicy, to make document addition predictable and not suddenly merge:\n        MergePolicy mp = random.nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();\n        IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n          .setMergePolicy(mp);\n        IndexWriter w = new IndexWriter(ramDir, iwc);\n        // add few more docs:\n        for(int j = 0; j < RANDOM_MULTIPLIER * random.nextInt(30); j++) {\n          addDoc(w, id++);\n        }\n        w.close(false);\n      }\n      \n      // add dummy segments (which are all in current version) to optimized index\n      MergePolicy mp = random.nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();\n      IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, null)\n        .setMergePolicy(mp);\n      IndexWriter w = new IndexWriter(dir, iwc);\n      w.setInfoStream(VERBOSE ? System.out : null);\n      w.addIndexes(ramDir);\n      w.close(false);\n      \n      // determine count of segments in modified index\n      final int origSegCount = getNumberOfSegments(dir);\n      \n      new IndexUpgrader(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, null), VERBOSE ? System.out : null, false)\n        .upgrade();\n\n      final int segCount = checkAllSegmentsUpgraded(dir);\n      assertEquals(\"Index must still contain the same number of segments, as only one segment was upgraded and nothing else merged\",\n        origSegCount, segCount);\n      \n      dir.close();\n      _TestUtil.rmDir(oldIndxeDir);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d14e8d18c0e3970c20354dbeeb49da11bd587fbd","date":1321041051,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#testUpgradeOldSingleSegmentIndexWithAdditions().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#testUpgradeOldOptimizedIndexWithAdditions().mjava","sourceNew":"  public void testUpgradeOldSingleSegmentIndexWithAdditions() throws Exception {\n    for (String name : oldSingleSegmentNames) {\n      if (VERBOSE) {\n        System.out.println(\"testUpgradeOldSingleSegmentIndexWithAdditions: index=\" +name);\n      }\n      File oldIndxeDir = _TestUtil.getTempDir(name);\n      _TestUtil.unzip(getDataFile(\"index.\" + name + \".zip\"), oldIndxeDir);\n      Directory dir = newFSDirectory(oldIndxeDir);\n\n      assertEquals(\"Original index must be single segment\", 1, getNumberOfSegments(dir));\n\n      // create a bunch of dummy segments\n      int id = 40;\n      RAMDirectory ramDir = new RAMDirectory();\n      for (int i = 0; i < 3; i++) {\n        // only use Log- or TieredMergePolicy, to make document addition predictable and not suddenly merge:\n        MergePolicy mp = random.nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();\n        IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n          .setMergePolicy(mp);\n        IndexWriter w = new IndexWriter(ramDir, iwc);\n        // add few more docs:\n        for(int j = 0; j < RANDOM_MULTIPLIER * random.nextInt(30); j++) {\n          addDoc(w, id++);\n        }\n        w.close(false);\n      }\n      \n      // add dummy segments (which are all in current\n      // version) to single segment index\n      MergePolicy mp = random.nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();\n      IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, null)\n        .setMergePolicy(mp);\n      IndexWriter w = new IndexWriter(dir, iwc);\n      w.addIndexes(ramDir);\n      w.close(false);\n      \n      // determine count of segments in modified index\n      final int origSegCount = getNumberOfSegments(dir);\n      \n      new IndexUpgrader(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, null), false)\n        .upgrade();\n\n      final int segCount = checkAllSegmentsUpgraded(dir);\n      assertEquals(\"Index must still contain the same number of segments, as only one segment was upgraded and nothing else merged\",\n        origSegCount, segCount);\n      \n      dir.close();\n      _TestUtil.rmDir(oldIndxeDir);\n    }\n  }\n\n","sourceOld":"  public void testUpgradeOldOptimizedIndexWithAdditions() throws Exception {\n    for (String name : oldOptimizedNames) {\n      if (VERBOSE) {\n        System.out.println(\"testUpgradeOldOptimizedIndexWithAdditions: index=\" +name);\n      }\n      File oldIndxeDir = _TestUtil.getTempDir(name);\n      _TestUtil.unzip(getDataFile(\"index.\" + name + \".zip\"), oldIndxeDir);\n      Directory dir = newFSDirectory(oldIndxeDir);\n\n      assertEquals(\"Original index must be optimized\", 1, getNumberOfSegments(dir));\n\n      // create a bunch of dummy segments\n      int id = 40;\n      RAMDirectory ramDir = new RAMDirectory();\n      for (int i = 0; i < 3; i++) {\n        // only use Log- or TieredMergePolicy, to make document addition predictable and not suddenly merge:\n        MergePolicy mp = random.nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();\n        IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n          .setMergePolicy(mp);\n        IndexWriter w = new IndexWriter(ramDir, iwc);\n        // add few more docs:\n        for(int j = 0; j < RANDOM_MULTIPLIER * random.nextInt(30); j++) {\n          addDoc(w, id++);\n        }\n        w.close(false);\n      }\n      \n      // add dummy segments (which are all in current version) to optimized index\n      MergePolicy mp = random.nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();\n      IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, null)\n        .setMergePolicy(mp);\n      IndexWriter w = new IndexWriter(dir, iwc);\n      w.addIndexes(ramDir);\n      w.close(false);\n      \n      // determine count of segments in modified index\n      final int origSegCount = getNumberOfSegments(dir);\n      \n      new IndexUpgrader(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, null), false)\n        .upgrade();\n\n      final int segCount = checkAllSegmentsUpgraded(dir);\n      assertEquals(\"Index must still contain the same number of segments, as only one segment was upgraded and nothing else merged\",\n        origSegCount, segCount);\n      \n      dir.close();\n      _TestUtil.rmDir(oldIndxeDir);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"06584e6e98d592b34e1329b384182f368d2025e8":["536f6911665ce844b868182b8d0ca107ea0eceab"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","536f6911665ce844b868182b8d0ca107ea0eceab"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","536f6911665ce844b868182b8d0ca107ea0eceab"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["06584e6e98d592b34e1329b384182f368d2025e8"],"536f6911665ce844b868182b8d0ca107ea0eceab":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"]},"commit2Childs":{"06584e6e98d592b34e1329b384182f368d2025e8":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"a3776dccca01c11e7046323cfad46a3b4a471233":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a3776dccca01c11e7046323cfad46a3b4a471233","c700f8d0842d3e52bb2bdfbfdc046a137e836edb","536f6911665ce844b868182b8d0ca107ea0eceab"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":[],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"536f6911665ce844b868182b8d0ca107ea0eceab":["06584e6e98d592b34e1329b384182f368d2025e8","a3776dccca01c11e7046323cfad46a3b4a471233","c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a3776dccca01c11e7046323cfad46a3b4a471233","c700f8d0842d3e52bb2bdfbfdc046a137e836edb","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}