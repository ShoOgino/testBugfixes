{"path":"solr/contrib/analytics/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","commits":[{"id":"0f719faa74f7213d4a395510dbc1f1b7cb178484","date":1410881394,"type":1,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/contrib/analytics/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","sourceNew":"  /**\n   * Calculates the analytics requested in the Parameters.\n   * \n   * @return List of results formated to mirror the input XML.\n   * @throws IOException if execution fails\n   */\n  public NamedList<?> execute() throws IOException {\n    statsCollector.startRequest();\n    NamedList<Object> res = new NamedList<>();\n    List<AnalyticsRequest> requests;\n    \n    requests = AnalyticsRequestFactory.parse(searcher.getSchema(), params);\n\n    if(requests == null || requests.size()==0){\n      return res;\n    }\n    statsCollector.addRequests(requests.size());\n    \n    // Get filter to all docs\n    Filter filter = docs.getTopFilter();\n    \n    // Computing each Analytics Request Seperately\n    for( AnalyticsRequest areq : requests ){\n      // The Accumulator which will control the statistics generation\n      // for the entire analytics request\n      ValueAccumulator accumulator; \n      \n      // The number of total facet requests\n      int facets = areq.getFieldFacets().size()+areq.getRangeFacets().size()+areq.getQueryFacets().size();\n      try {\n        if( facets== 0 ){\n          accumulator = BasicAccumulator.create(searcher, docs, areq);\n        } else {\n          accumulator = FacetingAccumulator.create(searcher, docs, areq, req);\n        }\n      } catch (IOException e) {\n        log.warn(\"Analytics request '\"+areq.getName()+\"' failed\", e);\n        continue;\n      }\n\n      statsCollector.addStatsCollected(((BasicAccumulator)accumulator).getNumStatsCollectors());\n      statsCollector.addStatsRequests(areq.getExpressions().size());\n      statsCollector.addFieldFacets(areq.getFieldFacets().size());\n      statsCollector.addRangeFacets(areq.getRangeFacets().size());\n      statsCollector.addQueryFacets(areq.getQueryFacets().size());\n      statsCollector.addQueries(((BasicAccumulator)accumulator).getNumQueries());\n      \n      // Loop through the documents returned by the query and add to accumulator\n      List<AtomicReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        AtomicReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n\n        if (disi != null) {\n          accumulator.getLeafCollector(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            accumulator.collect(doc);\n            doc = disi.nextDoc();\n          }\n        }\n      }\n      \n      // do some post-processing\n      accumulator.postProcess();\n     \n      // compute the stats\n      accumulator.compute();\n      \n      res.add(areq.getName(),accumulator.export());\n    }\n\n    statsCollector.endRequest();\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Calculates the analytics requested in the Parameters.\n   * \n   * @return List of results formated to mirror the input XML.\n   * @throws IOException if execution fails\n   */\n  public NamedList<?> execute() throws IOException {\n    statsCollector.startRequest();\n    NamedList<Object> res = new NamedList<>();\n    List<AnalyticsRequest> requests;\n    \n    requests = AnalyticsRequestFactory.parse(searcher.getSchema(), params);\n\n    if(requests == null || requests.size()==0){\n      return res;\n    }\n    statsCollector.addRequests(requests.size());\n    \n    // Get filter to all docs\n    Filter filter = docs.getTopFilter();\n    \n    // Computing each Analytics Request Seperately\n    for( AnalyticsRequest areq : requests ){\n      // The Accumulator which will control the statistics generation\n      // for the entire analytics request\n      ValueAccumulator accumulator; \n      \n      // The number of total facet requests\n      int facets = areq.getFieldFacets().size()+areq.getRangeFacets().size()+areq.getQueryFacets().size();\n      try {\n        if( facets== 0 ){\n          accumulator = BasicAccumulator.create(searcher, docs, areq);\n        } else {\n          accumulator = FacetingAccumulator.create(searcher, docs, areq, req);\n        }\n      } catch (IOException e) {\n        log.warn(\"Analytics request '\"+areq.getName()+\"' failed\", e);\n        continue;\n      }\n\n      statsCollector.addStatsCollected(((BasicAccumulator)accumulator).getNumStatsCollectors());\n      statsCollector.addStatsRequests(areq.getExpressions().size());\n      statsCollector.addFieldFacets(areq.getFieldFacets().size());\n      statsCollector.addRangeFacets(areq.getRangeFacets().size());\n      statsCollector.addQueryFacets(areq.getQueryFacets().size());\n      statsCollector.addQueries(((BasicAccumulator)accumulator).getNumQueries());\n      \n      // Loop through the documents returned by the query and add to accumulator\n      List<AtomicReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        AtomicReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n\n        if (disi != null) {\n          accumulator.getLeafCollector(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            accumulator.collect(doc);\n            doc = disi.nextDoc();\n          }\n        }\n      }\n      \n      // do some post-processing\n      accumulator.postProcess();\n     \n      // compute the stats\n      accumulator.compute();\n      \n      res.add(areq.getName(),accumulator.export());\n    }\n\n    statsCollector.endRequest();\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"solr/contrib/analytics/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","pathOld":"solr/contrib/analytics/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","sourceNew":"  /**\n   * Calculates the analytics requested in the Parameters.\n   * \n   * @return List of results formated to mirror the input XML.\n   * @throws IOException if execution fails\n   */\n  public NamedList<?> execute() throws IOException {\n    statsCollector.startRequest();\n    NamedList<Object> res = new NamedList<>();\n    List<AnalyticsRequest> requests;\n    \n    requests = AnalyticsRequestFactory.parse(searcher.getSchema(), params);\n\n    if(requests == null || requests.size()==0){\n      return res;\n    }\n    statsCollector.addRequests(requests.size());\n    \n    // Get filter to all docs\n    Filter filter = docs.getTopFilter();\n    \n    // Computing each Analytics Request Seperately\n    for( AnalyticsRequest areq : requests ){\n      // The Accumulator which will control the statistics generation\n      // for the entire analytics request\n      ValueAccumulator accumulator; \n      \n      // The number of total facet requests\n      int facets = areq.getFieldFacets().size()+areq.getRangeFacets().size()+areq.getQueryFacets().size();\n      try {\n        if( facets== 0 ){\n          accumulator = BasicAccumulator.create(searcher, docs, areq);\n        } else {\n          accumulator = FacetingAccumulator.create(searcher, docs, areq, req);\n        }\n      } catch (IOException e) {\n        log.warn(\"Analytics request '\"+areq.getName()+\"' failed\", e);\n        continue;\n      }\n\n      statsCollector.addStatsCollected(((BasicAccumulator)accumulator).getNumStatsCollectors());\n      statsCollector.addStatsRequests(areq.getExpressions().size());\n      statsCollector.addFieldFacets(areq.getFieldFacets().size());\n      statsCollector.addRangeFacets(areq.getRangeFacets().size());\n      statsCollector.addQueryFacets(areq.getQueryFacets().size());\n      statsCollector.addQueries(((BasicAccumulator)accumulator).getNumQueries());\n      \n      // Loop through the documents returned by the query and add to accumulator\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        LeafReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n\n        if (disi != null) {\n          accumulator.getLeafCollector(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            accumulator.collect(doc);\n            doc = disi.nextDoc();\n          }\n        }\n      }\n      \n      // do some post-processing\n      accumulator.postProcess();\n     \n      // compute the stats\n      accumulator.compute();\n      \n      res.add(areq.getName(),accumulator.export());\n    }\n\n    statsCollector.endRequest();\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Calculates the analytics requested in the Parameters.\n   * \n   * @return List of results formated to mirror the input XML.\n   * @throws IOException if execution fails\n   */\n  public NamedList<?> execute() throws IOException {\n    statsCollector.startRequest();\n    NamedList<Object> res = new NamedList<>();\n    List<AnalyticsRequest> requests;\n    \n    requests = AnalyticsRequestFactory.parse(searcher.getSchema(), params);\n\n    if(requests == null || requests.size()==0){\n      return res;\n    }\n    statsCollector.addRequests(requests.size());\n    \n    // Get filter to all docs\n    Filter filter = docs.getTopFilter();\n    \n    // Computing each Analytics Request Seperately\n    for( AnalyticsRequest areq : requests ){\n      // The Accumulator which will control the statistics generation\n      // for the entire analytics request\n      ValueAccumulator accumulator; \n      \n      // The number of total facet requests\n      int facets = areq.getFieldFacets().size()+areq.getRangeFacets().size()+areq.getQueryFacets().size();\n      try {\n        if( facets== 0 ){\n          accumulator = BasicAccumulator.create(searcher, docs, areq);\n        } else {\n          accumulator = FacetingAccumulator.create(searcher, docs, areq, req);\n        }\n      } catch (IOException e) {\n        log.warn(\"Analytics request '\"+areq.getName()+\"' failed\", e);\n        continue;\n      }\n\n      statsCollector.addStatsCollected(((BasicAccumulator)accumulator).getNumStatsCollectors());\n      statsCollector.addStatsRequests(areq.getExpressions().size());\n      statsCollector.addFieldFacets(areq.getFieldFacets().size());\n      statsCollector.addRangeFacets(areq.getRangeFacets().size());\n      statsCollector.addQueryFacets(areq.getQueryFacets().size());\n      statsCollector.addQueries(((BasicAccumulator)accumulator).getNumQueries());\n      \n      // Loop through the documents returned by the query and add to accumulator\n      List<AtomicReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        AtomicReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n\n        if (disi != null) {\n          accumulator.getLeafCollector(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            accumulator.collect(doc);\n            doc = disi.nextDoc();\n          }\n        }\n      }\n      \n      // do some post-processing\n      accumulator.postProcess();\n     \n      // compute the stats\n      accumulator.compute();\n      \n      res.add(areq.getName(),accumulator.export());\n    }\n\n    statsCollector.endRequest();\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"18e7cf5eab4be20c96aa36554daa39b53f43cf6e","date":1490632275,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/contrib/analytics/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","pathOld":"solr/contrib/analytics/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","sourceNew":"  /**\n   * Calculates the analytics requested in the Parameters.\n   * \n   * @return List of results formated to mirror the input XML.\n   * @throws IOException if execution fails\n   */\n  public NamedList<?> execute() throws IOException {\n    statsCollector.startRequest();\n    NamedList<Object> res = new NamedList<>();\n    List<AnalyticsRequest> requests;\n    \n    requests = AnalyticsRequestFactory.parse(searcher.getSchema(), params);\n\n    if(requests == null || requests.size()==0){\n      return res;\n    }\n    statsCollector.addRequests(requests.size());\n    \n    // Get filter to all docs\n    Filter filter = docs.getTopFilter();\n    \n    // Computing each Analytics Request Separately\n    for( AnalyticsRequest areq : requests ){\n      // The Accumulator which will control the statistics generation\n      // for the entire analytics request\n      ValueAccumulator accumulator; \n      \n      // The number of total facet requests\n      int facets = areq.getFieldFacets().size()+areq.getRangeFacets().size()+areq.getQueryFacets().size();\n      try {\n        if( facets== 0 ){\n          accumulator = BasicAccumulator.create(searcher, docs, areq);\n        } else {\n          accumulator = FacetingAccumulator.create(searcher, docs, areq, req);\n        }\n      } catch (IOException e) {\n        log.warn(\"Analytics request '\"+areq.getName()+\"' failed\", e);\n        continue;\n      }\n\n      statsCollector.addStatsCollected(((BasicAccumulator)accumulator).getNumStatsCollectors());\n      statsCollector.addStatsRequests(areq.getExpressions().size());\n      statsCollector.addFieldFacets(areq.getFieldFacets().size());\n      statsCollector.addRangeFacets(areq.getRangeFacets().size());\n      statsCollector.addQueryFacets(areq.getQueryFacets().size());\n      statsCollector.addQueries(((BasicAccumulator)accumulator).getNumQueries());\n      \n      // Loop through the documents returned by the query and add to accumulator\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        LeafReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n\n        if (disi != null) {\n          accumulator.getLeafCollector(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            accumulator.collect(doc);\n            doc = disi.nextDoc();\n          }\n        }\n      }\n      \n      // do some post-processing\n      accumulator.postProcess();\n     \n      // compute the stats\n      accumulator.compute();\n      \n      res.add(areq.getName(),accumulator.export());\n    }\n\n    statsCollector.endRequest();\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Calculates the analytics requested in the Parameters.\n   * \n   * @return List of results formated to mirror the input XML.\n   * @throws IOException if execution fails\n   */\n  public NamedList<?> execute() throws IOException {\n    statsCollector.startRequest();\n    NamedList<Object> res = new NamedList<>();\n    List<AnalyticsRequest> requests;\n    \n    requests = AnalyticsRequestFactory.parse(searcher.getSchema(), params);\n\n    if(requests == null || requests.size()==0){\n      return res;\n    }\n    statsCollector.addRequests(requests.size());\n    \n    // Get filter to all docs\n    Filter filter = docs.getTopFilter();\n    \n    // Computing each Analytics Request Seperately\n    for( AnalyticsRequest areq : requests ){\n      // The Accumulator which will control the statistics generation\n      // for the entire analytics request\n      ValueAccumulator accumulator; \n      \n      // The number of total facet requests\n      int facets = areq.getFieldFacets().size()+areq.getRangeFacets().size()+areq.getQueryFacets().size();\n      try {\n        if( facets== 0 ){\n          accumulator = BasicAccumulator.create(searcher, docs, areq);\n        } else {\n          accumulator = FacetingAccumulator.create(searcher, docs, areq, req);\n        }\n      } catch (IOException e) {\n        log.warn(\"Analytics request '\"+areq.getName()+\"' failed\", e);\n        continue;\n      }\n\n      statsCollector.addStatsCollected(((BasicAccumulator)accumulator).getNumStatsCollectors());\n      statsCollector.addStatsRequests(areq.getExpressions().size());\n      statsCollector.addFieldFacets(areq.getFieldFacets().size());\n      statsCollector.addRangeFacets(areq.getRangeFacets().size());\n      statsCollector.addQueryFacets(areq.getQueryFacets().size());\n      statsCollector.addQueries(((BasicAccumulator)accumulator).getNumQueries());\n      \n      // Loop through the documents returned by the query and add to accumulator\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        LeafReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n\n        if (disi != null) {\n          accumulator.getLeafCollector(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            accumulator.collect(doc);\n            doc = disi.nextDoc();\n          }\n        }\n      }\n      \n      // do some post-processing\n      accumulator.postProcess();\n     \n      // compute the stats\n      accumulator.compute();\n      \n      res.add(areq.getName(),accumulator.export());\n    }\n\n    statsCollector.endRequest();\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"febf4fa8feff6bbc932c1b388cbd758a3e6697f7","date":1490873944,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/contrib/analytics/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","pathOld":"solr/contrib/analytics/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","sourceNew":"  /**\n   * Calculates the analytics requested in the Parameters.\n   * \n   * @return List of results formated to mirror the input XML.\n   * @throws IOException if execution fails\n   */\n  public NamedList<?> execute() throws IOException {\n    statsCollector.startRequest();\n    NamedList<Object> res = new NamedList<>();\n    List<AnalyticsRequest> requests;\n    \n    requests = AnalyticsRequestFactory.parse(searcher.getSchema(), params);\n\n    if(requests == null || requests.size()==0){\n      return res;\n    }\n    statsCollector.addRequests(requests.size());\n    \n    // Get filter to all docs\n    Filter filter = docs.getTopFilter();\n    \n    // Computing each Analytics Request Separately\n    for( AnalyticsRequest areq : requests ){\n      // The Accumulator which will control the statistics generation\n      // for the entire analytics request\n      ValueAccumulator accumulator; \n      \n      // The number of total facet requests\n      int facets = areq.getFieldFacets().size()+areq.getRangeFacets().size()+areq.getQueryFacets().size();\n      try {\n        if( facets== 0 ){\n          accumulator = BasicAccumulator.create(searcher, docs, areq);\n        } else {\n          accumulator = FacetingAccumulator.create(searcher, docs, areq, req);\n        }\n      } catch (IOException e) {\n        log.warn(\"Analytics request '\"+areq.getName()+\"' failed\", e);\n        continue;\n      }\n\n      statsCollector.addStatsCollected(((BasicAccumulator)accumulator).getNumStatsCollectors());\n      statsCollector.addStatsRequests(areq.getExpressions().size());\n      statsCollector.addFieldFacets(areq.getFieldFacets().size());\n      statsCollector.addRangeFacets(areq.getRangeFacets().size());\n      statsCollector.addQueryFacets(areq.getQueryFacets().size());\n      statsCollector.addQueries(((BasicAccumulator)accumulator).getNumQueries());\n      \n      // Loop through the documents returned by the query and add to accumulator\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        LeafReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n\n        if (disi != null) {\n          accumulator.getLeafCollector(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            accumulator.collect(doc);\n            doc = disi.nextDoc();\n          }\n        }\n      }\n      \n      // do some post-processing\n      accumulator.postProcess();\n     \n      // compute the stats\n      accumulator.compute();\n      \n      res.add(areq.getName(),accumulator.export());\n    }\n\n    statsCollector.endRequest();\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Calculates the analytics requested in the Parameters.\n   * \n   * @return List of results formated to mirror the input XML.\n   * @throws IOException if execution fails\n   */\n  public NamedList<?> execute() throws IOException {\n    statsCollector.startRequest();\n    NamedList<Object> res = new NamedList<>();\n    List<AnalyticsRequest> requests;\n    \n    requests = AnalyticsRequestFactory.parse(searcher.getSchema(), params);\n\n    if(requests == null || requests.size()==0){\n      return res;\n    }\n    statsCollector.addRequests(requests.size());\n    \n    // Get filter to all docs\n    Filter filter = docs.getTopFilter();\n    \n    // Computing each Analytics Request Seperately\n    for( AnalyticsRequest areq : requests ){\n      // The Accumulator which will control the statistics generation\n      // for the entire analytics request\n      ValueAccumulator accumulator; \n      \n      // The number of total facet requests\n      int facets = areq.getFieldFacets().size()+areq.getRangeFacets().size()+areq.getQueryFacets().size();\n      try {\n        if( facets== 0 ){\n          accumulator = BasicAccumulator.create(searcher, docs, areq);\n        } else {\n          accumulator = FacetingAccumulator.create(searcher, docs, areq, req);\n        }\n      } catch (IOException e) {\n        log.warn(\"Analytics request '\"+areq.getName()+\"' failed\", e);\n        continue;\n      }\n\n      statsCollector.addStatsCollected(((BasicAccumulator)accumulator).getNumStatsCollectors());\n      statsCollector.addStatsRequests(areq.getExpressions().size());\n      statsCollector.addFieldFacets(areq.getFieldFacets().size());\n      statsCollector.addRangeFacets(areq.getRangeFacets().size());\n      statsCollector.addQueryFacets(areq.getQueryFacets().size());\n      statsCollector.addQueries(((BasicAccumulator)accumulator).getNumQueries());\n      \n      // Loop through the documents returned by the query and add to accumulator\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        LeafReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n\n        if (disi != null) {\n          accumulator.getLeafCollector(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            accumulator.collect(doc);\n            doc = disi.nextDoc();\n          }\n        }\n      }\n      \n      // do some post-processing\n      accumulator.postProcess();\n     \n      // compute the stats\n      accumulator.compute();\n      \n      res.add(areq.getName(),accumulator.export());\n    }\n\n    statsCollector.endRequest();\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c495edcca4d0bc51bf62d9be3527c87bf9b44ded","date":1498673617,"type":4,"author":"Dennis Gove","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/contrib/analytics/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","sourceNew":null,"sourceOld":"  /**\n   * Calculates the analytics requested in the Parameters.\n   * \n   * @return List of results formated to mirror the input XML.\n   * @throws IOException if execution fails\n   */\n  public NamedList<?> execute() throws IOException {\n    statsCollector.startRequest();\n    NamedList<Object> res = new NamedList<>();\n    List<AnalyticsRequest> requests;\n    \n    requests = AnalyticsRequestFactory.parse(searcher.getSchema(), params);\n\n    if(requests == null || requests.size()==0){\n      return res;\n    }\n    statsCollector.addRequests(requests.size());\n    \n    // Get filter to all docs\n    Filter filter = docs.getTopFilter();\n    \n    // Computing each Analytics Request Separately\n    for( AnalyticsRequest areq : requests ){\n      // The Accumulator which will control the statistics generation\n      // for the entire analytics request\n      ValueAccumulator accumulator; \n      \n      // The number of total facet requests\n      int facets = areq.getFieldFacets().size()+areq.getRangeFacets().size()+areq.getQueryFacets().size();\n      try {\n        if( facets== 0 ){\n          accumulator = BasicAccumulator.create(searcher, docs, areq);\n        } else {\n          accumulator = FacetingAccumulator.create(searcher, docs, areq, req);\n        }\n      } catch (IOException e) {\n        log.warn(\"Analytics request '\"+areq.getName()+\"' failed\", e);\n        continue;\n      }\n\n      statsCollector.addStatsCollected(((BasicAccumulator)accumulator).getNumStatsCollectors());\n      statsCollector.addStatsRequests(areq.getExpressions().size());\n      statsCollector.addFieldFacets(areq.getFieldFacets().size());\n      statsCollector.addRangeFacets(areq.getRangeFacets().size());\n      statsCollector.addQueryFacets(areq.getQueryFacets().size());\n      statsCollector.addQueries(((BasicAccumulator)accumulator).getNumQueries());\n      \n      // Loop through the documents returned by the query and add to accumulator\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        LeafReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n\n        if (disi != null) {\n          accumulator.getLeafCollector(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            accumulator.collect(doc);\n            doc = disi.nextDoc();\n          }\n        }\n      }\n      \n      // do some post-processing\n      accumulator.postProcess();\n     \n      // compute the stats\n      accumulator.compute();\n      \n      res.add(areq.getName(),accumulator.export());\n    }\n\n    statsCollector.endRequest();\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":4,"author":"Karl Wright","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/contrib/analytics/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","sourceNew":null,"sourceOld":"  /**\n   * Calculates the analytics requested in the Parameters.\n   * \n   * @return List of results formated to mirror the input XML.\n   * @throws IOException if execution fails\n   */\n  public NamedList<?> execute() throws IOException {\n    statsCollector.startRequest();\n    NamedList<Object> res = new NamedList<>();\n    List<AnalyticsRequest> requests;\n    \n    requests = AnalyticsRequestFactory.parse(searcher.getSchema(), params);\n\n    if(requests == null || requests.size()==0){\n      return res;\n    }\n    statsCollector.addRequests(requests.size());\n    \n    // Get filter to all docs\n    Filter filter = docs.getTopFilter();\n    \n    // Computing each Analytics Request Separately\n    for( AnalyticsRequest areq : requests ){\n      // The Accumulator which will control the statistics generation\n      // for the entire analytics request\n      ValueAccumulator accumulator; \n      \n      // The number of total facet requests\n      int facets = areq.getFieldFacets().size()+areq.getRangeFacets().size()+areq.getQueryFacets().size();\n      try {\n        if( facets== 0 ){\n          accumulator = BasicAccumulator.create(searcher, docs, areq);\n        } else {\n          accumulator = FacetingAccumulator.create(searcher, docs, areq, req);\n        }\n      } catch (IOException e) {\n        log.warn(\"Analytics request '\"+areq.getName()+\"' failed\", e);\n        continue;\n      }\n\n      statsCollector.addStatsCollected(((BasicAccumulator)accumulator).getNumStatsCollectors());\n      statsCollector.addStatsRequests(areq.getExpressions().size());\n      statsCollector.addFieldFacets(areq.getFieldFacets().size());\n      statsCollector.addRangeFacets(areq.getRangeFacets().size());\n      statsCollector.addQueryFacets(areq.getQueryFacets().size());\n      statsCollector.addQueries(((BasicAccumulator)accumulator).getNumQueries());\n      \n      // Loop through the documents returned by the query and add to accumulator\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        LeafReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n\n        if (disi != null) {\n          accumulator.getLeafCollector(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            accumulator.collect(doc);\n            doc = disi.nextDoc();\n          }\n        }\n      }\n      \n      // do some post-processing\n      accumulator.postProcess();\n     \n      // compute the stats\n      accumulator.compute();\n      \n      res.add(areq.getName(),accumulator.export());\n    }\n\n    statsCollector.endRequest();\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"30c8e5574b55d57947e989443dfde611646530ee","date":1499131153,"type":4,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/contrib/analytics/src/java/org/apache/solr/analytics/request/AnalyticsStats#execute().mjava","sourceNew":null,"sourceOld":"  /**\n   * Calculates the analytics requested in the Parameters.\n   * \n   * @return List of results formated to mirror the input XML.\n   * @throws IOException if execution fails\n   */\n  public NamedList<?> execute() throws IOException {\n    statsCollector.startRequest();\n    NamedList<Object> res = new NamedList<>();\n    List<AnalyticsRequest> requests;\n    \n    requests = AnalyticsRequestFactory.parse(searcher.getSchema(), params);\n\n    if(requests == null || requests.size()==0){\n      return res;\n    }\n    statsCollector.addRequests(requests.size());\n    \n    // Get filter to all docs\n    Filter filter = docs.getTopFilter();\n    \n    // Computing each Analytics Request Separately\n    for( AnalyticsRequest areq : requests ){\n      // The Accumulator which will control the statistics generation\n      // for the entire analytics request\n      ValueAccumulator accumulator; \n      \n      // The number of total facet requests\n      int facets = areq.getFieldFacets().size()+areq.getRangeFacets().size()+areq.getQueryFacets().size();\n      try {\n        if( facets== 0 ){\n          accumulator = BasicAccumulator.create(searcher, docs, areq);\n        } else {\n          accumulator = FacetingAccumulator.create(searcher, docs, areq, req);\n        }\n      } catch (IOException e) {\n        log.warn(\"Analytics request '\"+areq.getName()+\"' failed\", e);\n        continue;\n      }\n\n      statsCollector.addStatsCollected(((BasicAccumulator)accumulator).getNumStatsCollectors());\n      statsCollector.addStatsRequests(areq.getExpressions().size());\n      statsCollector.addFieldFacets(areq.getFieldFacets().size());\n      statsCollector.addRangeFacets(areq.getRangeFacets().size());\n      statsCollector.addQueryFacets(areq.getQueryFacets().size());\n      statsCollector.addQueries(((BasicAccumulator)accumulator).getNumQueries());\n      \n      // Loop through the documents returned by the query and add to accumulator\n      List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n      for (int leafNum = 0; leafNum < contexts.size(); leafNum++) {\n        LeafReaderContext context = contexts.get(leafNum);\n        DocIdSet dis = filter.getDocIdSet(context, null); // solr docsets already exclude any deleted docs\n        DocIdSetIterator disi = null;\n        if (dis != null) {\n          disi = dis.iterator();\n        }\n\n        if (disi != null) {\n          accumulator.getLeafCollector(context);\n          int doc = disi.nextDoc();\n          while( doc != DocIdSetIterator.NO_MORE_DOCS){\n            // Add a document to the statistics being generated\n            accumulator.collect(doc);\n            doc = disi.nextDoc();\n          }\n        }\n      }\n      \n      // do some post-processing\n      accumulator.postProcess();\n     \n      // compute the stats\n      accumulator.compute();\n      \n      res.add(areq.getName(),accumulator.export());\n    }\n\n    statsCollector.endRequest();\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"febf4fa8feff6bbc932c1b388cbd758a3e6697f7":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"18e7cf5eab4be20c96aa36554daa39b53f43cf6e":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"30c8e5574b55d57947e989443dfde611646530ee":["febf4fa8feff6bbc932c1b388cbd758a3e6697f7","28288370235ed02234a64753cdbf0c6ec096304a"],"c495edcca4d0bc51bf62d9be3527c87bf9b44ded":["18e7cf5eab4be20c96aa36554daa39b53f43cf6e"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["0f719faa74f7213d4a395510dbc1f1b7cb178484"],"0f719faa74f7213d4a395510dbc1f1b7cb178484":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"28288370235ed02234a64753cdbf0c6ec096304a":["18e7cf5eab4be20c96aa36554daa39b53f43cf6e","c495edcca4d0bc51bf62d9be3527c87bf9b44ded"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["28288370235ed02234a64753cdbf0c6ec096304a"]},"commit2Childs":{"febf4fa8feff6bbc932c1b388cbd758a3e6697f7":["30c8e5574b55d57947e989443dfde611646530ee"],"18e7cf5eab4be20c96aa36554daa39b53f43cf6e":["c495edcca4d0bc51bf62d9be3527c87bf9b44ded","28288370235ed02234a64753cdbf0c6ec096304a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0f719faa74f7213d4a395510dbc1f1b7cb178484"],"30c8e5574b55d57947e989443dfde611646530ee":[],"c495edcca4d0bc51bf62d9be3527c87bf9b44ded":["28288370235ed02234a64753cdbf0c6ec096304a"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["febf4fa8feff6bbc932c1b388cbd758a3e6697f7","18e7cf5eab4be20c96aa36554daa39b53f43cf6e"],"28288370235ed02234a64753cdbf0c6ec096304a":["30c8e5574b55d57947e989443dfde611646530ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"0f719faa74f7213d4a395510dbc1f1b7cb178484":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["30c8e5574b55d57947e989443dfde611646530ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}