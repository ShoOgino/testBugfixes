{"path":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","commits":[{"id":"29a93e7fb303505e4a719e87f378d9a45db981d0","date":1412167802,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","sourceNew":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                      state.segmentSuffix, \n                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFile, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      int version = CodecUtil.checkSegmentHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                                     OrdsBlockTreeTermsWriter.VERSION_START,\n                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                     state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                        state.segmentSuffix, \n                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFile, state.context);\n      int indexVersion = CodecUtil.checkSegmentHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                               state.segmentInfo.getId(), state.segmentSuffix);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n      seekDir(indexIn);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                                  PostingsReaderBase postingsReader, IOContext ioContext,\n                                  String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, OrdsBlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = CodecUtil.checkHeader(in,\n                                      OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                      OrdsBlockTreeTermsWriter.VERSION_START,\n                                      OrdsBlockTreeTermsWriter.VERSION_CURRENT);\n      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                              ioContext);\n      int indexVersion = CodecUtil.checkHeader(indexIn,\n                                               OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9bb9a29a5e71a90295f175df8919802993142c9a","date":1412517673,"type":1,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","sourceNew":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                      state.segmentSuffix, \n                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFile, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      int version = CodecUtil.checkSegmentHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                                     OrdsBlockTreeTermsWriter.VERSION_START,\n                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                     state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                        state.segmentSuffix, \n                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFile, state.context);\n      int indexVersion = CodecUtil.checkSegmentHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                               state.segmentInfo.getId(), state.segmentSuffix);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n      seekDir(indexIn);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                                  PostingsReaderBase postingsReader, IOContext ioContext,\n                                  String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, OrdsBlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = CodecUtil.checkHeader(in,\n                                      OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                      OrdsBlockTreeTermsWriter.VERSION_START,\n                                      OrdsBlockTreeTermsWriter.VERSION_CURRENT);\n      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                              ioContext);\n      int indexVersion = CodecUtil.checkHeader(indexIn,\n                                               OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"99eb4a732d1a908f4636ace52928876136bf1896","date":1413829552,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                      state.segmentSuffix, \n                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFile, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      int version = CodecUtil.checkSegmentHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                                     OrdsBlockTreeTermsWriter.VERSION_START,\n                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                     state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                        state.segmentSuffix, \n                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFile, state.context);\n      int indexVersion = CodecUtil.checkSegmentHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                               state.segmentInfo.getId(), state.segmentSuffix);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n      seekDir(indexIn);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                      state.segmentSuffix, \n                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFile, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      int version = CodecUtil.checkSegmentHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                                     OrdsBlockTreeTermsWriter.VERSION_START,\n                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                     state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                        state.segmentSuffix, \n                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFile, state.context);\n      int indexVersion = CodecUtil.checkSegmentHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                               state.segmentInfo.getId(), state.segmentSuffix);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n      seekDir(indexIn);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3384e6013a93e4d11b7d75388693f8d0388602bf","date":1413951663,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                      state.segmentSuffix, \n                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFile, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      int version = CodecUtil.checkIndexHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                                     OrdsBlockTreeTermsWriter.VERSION_START,\n                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                     state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                        state.segmentSuffix, \n                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFile, state.context);\n      int indexVersion = CodecUtil.checkIndexHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                               state.segmentInfo.getId(), state.segmentSuffix);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n      seekDir(indexIn);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                      state.segmentSuffix, \n                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFile, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      int version = CodecUtil.checkSegmentHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                                     OrdsBlockTreeTermsWriter.VERSION_START,\n                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                     state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                        state.segmentSuffix, \n                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFile, state.context);\n      int indexVersion = CodecUtil.checkSegmentHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                               state.segmentInfo.getId(), state.segmentSuffix);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n      seekDir(indexIn);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"db68c63cbfaa8698b9c4475f75ed2b9c9696d238","date":1414118621,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                      state.segmentSuffix, \n                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFile, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      int version = CodecUtil.checkIndexHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                                     OrdsBlockTreeTermsWriter.VERSION_START,\n                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                     state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                        state.segmentSuffix, \n                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFile, state.context);\n      int indexVersion = CodecUtil.checkIndexHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                               state.segmentInfo.getId(), state.segmentSuffix);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n      seekDir(indexIn);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                      state.segmentSuffix, \n                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFile, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      int version = CodecUtil.checkSegmentHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                                     OrdsBlockTreeTermsWriter.VERSION_START,\n                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                     state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                        state.segmentSuffix, \n                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFile, state.context);\n      int indexVersion = CodecUtil.checkSegmentHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                               state.segmentInfo.getId(), state.segmentSuffix);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n      seekDir(indexIn);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2bb2842e561df4e8e9ad89010605fc86ac265465","date":1414768208,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                      state.segmentSuffix, \n                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFile, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      int version = CodecUtil.checkIndexHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                                     OrdsBlockTreeTermsWriter.VERSION_START,\n                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                     state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                        state.segmentSuffix, \n                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFile, state.context);\n      int indexVersion = CodecUtil.checkIndexHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                               state.segmentInfo.getId(), state.segmentSuffix);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n      seekDir(indexIn);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                      state.segmentSuffix, \n                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFile, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      int version = CodecUtil.checkIndexHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                                     OrdsBlockTreeTermsWriter.VERSION_START,\n                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                     state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                        state.segmentSuffix, \n                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFile, state.context);\n      int indexVersion = CodecUtil.checkIndexHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                               state.segmentInfo.getId(), state.segmentSuffix);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n      seekDir(indexIn);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b0267c69e2456a3477a1ad785723f2135da3117e","date":1425317087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                      state.segmentSuffix, \n                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFile, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      int version = CodecUtil.checkIndexHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                                     OrdsBlockTreeTermsWriter.VERSION_START,\n                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                     state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                        state.segmentSuffix, \n                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFile, state.context);\n      int indexVersion = CodecUtil.checkIndexHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                               state.segmentInfo.getId(), state.segmentSuffix);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n      seekDir(indexIn);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                      state.segmentSuffix, \n                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFile, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      int version = CodecUtil.checkIndexHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                                     OrdsBlockTreeTermsWriter.VERSION_START,\n                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                     state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                        state.segmentSuffix, \n                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFile, state.context);\n      int indexVersion = CodecUtil.checkIndexHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                               state.segmentInfo.getId(), state.segmentSuffix);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n      seekDir(indexIn);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b06445ae1731e049327712db0454e5643ca9b7fe","date":1425329139,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                      state.segmentSuffix, \n                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFile, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      int version = CodecUtil.checkIndexHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                                     OrdsBlockTreeTermsWriter.VERSION_START,\n                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                     state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                        state.segmentSuffix, \n                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFile, state.context);\n      int indexVersion = CodecUtil.checkIndexHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                               state.segmentInfo.getId(), state.segmentSuffix);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n      seekDir(indexIn);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                      state.segmentSuffix, \n                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFile, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      int version = CodecUtil.checkIndexHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                                     OrdsBlockTreeTermsWriter.VERSION_START,\n                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                     state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                        state.segmentSuffix, \n                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFile, state.context);\n      int indexVersion = CodecUtil.checkIndexHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                               state.segmentInfo.getId(), state.segmentSuffix);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n      seekDir(indexIn);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                      state.segmentSuffix, \n                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFile, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      int version = CodecUtil.checkIndexHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                                     OrdsBlockTreeTermsWriter.VERSION_START,\n                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                     state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                        state.segmentSuffix, \n                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFile, state.context);\n      int indexVersion = CodecUtil.checkIndexHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                               state.segmentInfo.getId(), state.segmentSuffix);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n      seekDir(indexIn);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                      state.segmentSuffix, \n                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFile, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      int version = CodecUtil.checkIndexHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                                     OrdsBlockTreeTermsWriter.VERSION_START,\n                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                     state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                        state.segmentSuffix, \n                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFile, state.context);\n      int indexVersion = CodecUtil.checkIndexHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                               state.segmentInfo.getId(), state.segmentSuffix);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n      seekDir(indexIn);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"086ffe31d8fba0110227db122974163709ecc1b4","date":1509678141,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                      state.segmentSuffix, \n                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFile, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      int version = CodecUtil.checkIndexHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                                     OrdsBlockTreeTermsWriter.VERSION_START,\n                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                     state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                        state.segmentSuffix, \n                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFile, state.context);\n      int indexVersion = CodecUtil.checkIndexHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                               state.segmentInfo.getId(), state.segmentSuffix);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n      seekDir(indexIn);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = in.readVLong();\n        // when frequencies are omitted, sumDocFreq=totalTermFreq and we only write one value\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                      state.segmentSuffix, \n                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFile, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      int version = CodecUtil.checkIndexHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                                     OrdsBlockTreeTermsWriter.VERSION_START,\n                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                     state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                        state.segmentSuffix, \n                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFile, state.context);\n      int indexVersion = CodecUtil.checkIndexHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                               state.segmentInfo.getId(), state.segmentSuffix);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n      seekDir(indexIn);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d523b8189b211dd1630166aa77b8c88bb48b3fcc","date":1510144168,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                      state.segmentSuffix, \n                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFile, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      int version = CodecUtil.checkIndexHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                                     OrdsBlockTreeTermsWriter.VERSION_START,\n                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                     state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                        state.segmentSuffix, \n                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFile, state.context);\n      int indexVersion = CodecUtil.checkIndexHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                               state.segmentInfo.getId(), state.segmentSuffix);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n      seekDir(indexIn);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = in.readVLong();\n        // when frequencies are omitted, sumDocFreq=totalTermFreq and we only write one value\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                      state.segmentSuffix, \n                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFile, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      int version = CodecUtil.checkIndexHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                                     OrdsBlockTreeTermsWriter.VERSION_START,\n                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                     state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                        state.segmentSuffix, \n                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFile, state.context);\n      int indexVersion = CodecUtil.checkIndexHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                               state.segmentInfo.getId(), state.segmentSuffix);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n      seekDir(indexIn);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cb77022ef17ff655c519a3f6ecd393747ac88bcf","date":1578579386,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                      state.segmentSuffix, \n                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFile, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      int version = CodecUtil.checkIndexHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                                     OrdsBlockTreeTermsWriter.VERSION_START,\n                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                     state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                        state.segmentSuffix, \n                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFile, state.context);\n      int indexVersion = CodecUtil.checkIndexHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                               state.segmentInfo.getId(), state.segmentSuffix);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n      seekDir(indexIn);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = in.readVLong();\n        // when frequencies are omitted, sumDocFreq=totalTermFreq and we only write one value\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : in.readVLong();\n        final int docCount = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                      state.segmentSuffix, \n                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFile, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      int version = CodecUtil.checkIndexHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                                     OrdsBlockTreeTermsWriter.VERSION_START,\n                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                     state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                        state.segmentSuffix, \n                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFile, state.context);\n      int indexVersion = CodecUtil.checkIndexHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                               state.segmentInfo.getId(), state.segmentSuffix);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n      seekDir(indexIn);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = in.readVLong();\n        // when frequencies are omitted, sumDocFreq=totalTermFreq and we only write one value\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"06ab276a5660cb79daae8c5ede063531c700a03a","date":1578587874,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                      state.segmentSuffix, \n                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFile, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      int version = CodecUtil.checkIndexHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                                     OrdsBlockTreeTermsWriter.VERSION_START,\n                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                     state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                        state.segmentSuffix, \n                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFile, state.context);\n      int indexVersion = CodecUtil.checkIndexHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                               state.segmentInfo.getId(), state.segmentSuffix);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n      seekDir(indexIn);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = in.readVLong();\n        // when frequencies are omitted, sumDocFreq=totalTermFreq and we only write one value\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                      state.segmentSuffix, \n                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFile, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      int version = CodecUtil.checkIndexHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                                     OrdsBlockTreeTermsWriter.VERSION_START,\n                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                     state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                        state.segmentSuffix, \n                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFile, state.context);\n      int indexVersion = CodecUtil.checkIndexHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                               state.segmentInfo.getId(), state.segmentSuffix);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n      seekDir(indexIn);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = in.readVLong();\n        // when frequencies are omitted, sumDocFreq=totalTermFreq and we only write one value\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : in.readVLong();\n        final int docCount = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"08a5168e06e037794c0aba7f94f76ff3c09704d2","date":1579264785,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader#OrdsBlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                      state.segmentSuffix, \n                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFile, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      int version = CodecUtil.checkIndexHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                                     OrdsBlockTreeTermsWriter.VERSION_START,\n                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                     state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                        state.segmentSuffix, \n                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFile, state.context);\n      int indexVersion = CodecUtil.checkIndexHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                               state.segmentInfo.getId(), state.segmentSuffix);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n      seekDir(indexIn);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = in.readVLong();\n        // when frequencies are omitted, sumDocFreq=totalTermFreq and we only write one value\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : in.readVLong();\n        final int docCount = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                      state.segmentSuffix, \n                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFile, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      int version = CodecUtil.checkIndexHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,\n                                                     OrdsBlockTreeTermsWriter.VERSION_START,\n                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                     state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, \n                                                        state.segmentSuffix, \n                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFile, state.context);\n      int indexVersion = CodecUtil.checkIndexHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,\n                                                               OrdsBlockTreeTermsWriter.VERSION_START,\n                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,\n                                                               state.segmentInfo.getId(), state.segmentSuffix);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in);\n      seekDir(indexIn);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        assert numTerms >= 0;\n        // System.out.println(\"read field=\" + field + \" numTerms=\" + numTerms + \" i=\" + i);\n        final int numBytes = in.readVInt();\n        final BytesRef code = new BytesRef(new byte[numBytes]);\n        in.readBytes(code.bytes, 0, numBytes);\n        code.length = numBytes;\n        final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        assert fieldInfo != null: \"field=\" + field;\n        assert numTerms <= Integer.MAX_VALUE;\n        final long sumTotalTermFreq = in.readVLong();\n        // when frequencies are omitted, sumDocFreq=totalTermFreq and we only write one value\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        // System.out.println(\"  longsSize=\" + longsSize);\n\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        OrdsFieldReader previous = fields.put(fieldInfo.name,       \n                                              new OrdsFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                                  indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"99eb4a732d1a908f4636ace52928876136bf1896":["29a93e7fb303505e4a719e87f378d9a45db981d0"],"d523b8189b211dd1630166aa77b8c88bb48b3fcc":["b0267c69e2456a3477a1ad785723f2135da3117e","086ffe31d8fba0110227db122974163709ecc1b4"],"b0267c69e2456a3477a1ad785723f2135da3117e":["2bb2842e561df4e8e9ad89010605fc86ac265465"],"cb77022ef17ff655c519a3f6ecd393747ac88bcf":["d523b8189b211dd1630166aa77b8c88bb48b3fcc"],"3384e6013a93e4d11b7d75388693f8d0388602bf":["99eb4a732d1a908f4636ace52928876136bf1896"],"2bb2842e561df4e8e9ad89010605fc86ac265465":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["2bb2842e561df4e8e9ad89010605fc86ac265465","b0267c69e2456a3477a1ad785723f2135da3117e"],"086ffe31d8fba0110227db122974163709ecc1b4":["b0267c69e2456a3477a1ad785723f2135da3117e"],"b06445ae1731e049327712db0454e5643ca9b7fe":["2bb2842e561df4e8e9ad89010605fc86ac265465","b0267c69e2456a3477a1ad785723f2135da3117e"],"9bb9a29a5e71a90295f175df8919802993142c9a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","29a93e7fb303505e4a719e87f378d9a45db981d0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"29a93e7fb303505e4a719e87f378d9a45db981d0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"08a5168e06e037794c0aba7f94f76ff3c09704d2":["06ab276a5660cb79daae8c5ede063531c700a03a"],"db68c63cbfaa8698b9c4475f75ed2b9c9696d238":["9bb9a29a5e71a90295f175df8919802993142c9a","3384e6013a93e4d11b7d75388693f8d0388602bf"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["08a5168e06e037794c0aba7f94f76ff3c09704d2"],"06ab276a5660cb79daae8c5ede063531c700a03a":["cb77022ef17ff655c519a3f6ecd393747ac88bcf"]},"commit2Childs":{"99eb4a732d1a908f4636ace52928876136bf1896":["3384e6013a93e4d11b7d75388693f8d0388602bf"],"d523b8189b211dd1630166aa77b8c88bb48b3fcc":["cb77022ef17ff655c519a3f6ecd393747ac88bcf"],"b0267c69e2456a3477a1ad785723f2135da3117e":["d523b8189b211dd1630166aa77b8c88bb48b3fcc","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","086ffe31d8fba0110227db122974163709ecc1b4","b06445ae1731e049327712db0454e5643ca9b7fe"],"cb77022ef17ff655c519a3f6ecd393747ac88bcf":["06ab276a5660cb79daae8c5ede063531c700a03a"],"3384e6013a93e4d11b7d75388693f8d0388602bf":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"2bb2842e561df4e8e9ad89010605fc86ac265465":["b0267c69e2456a3477a1ad785723f2135da3117e","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b06445ae1731e049327712db0454e5643ca9b7fe"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"086ffe31d8fba0110227db122974163709ecc1b4":["d523b8189b211dd1630166aa77b8c88bb48b3fcc"],"b06445ae1731e049327712db0454e5643ca9b7fe":[],"9bb9a29a5e71a90295f175df8919802993142c9a":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"29a93e7fb303505e4a719e87f378d9a45db981d0":["99eb4a732d1a908f4636ace52928876136bf1896","9bb9a29a5e71a90295f175df8919802993142c9a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9bb9a29a5e71a90295f175df8919802993142c9a","29a93e7fb303505e4a719e87f378d9a45db981d0"],"db68c63cbfaa8698b9c4475f75ed2b9c9696d238":["2bb2842e561df4e8e9ad89010605fc86ac265465"],"08a5168e06e037794c0aba7f94f76ff3c09704d2":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"06ab276a5660cb79daae8c5ede063531c700a03a":["08a5168e06e037794c0aba7f94f76ff3c09704d2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b06445ae1731e049327712db0454e5643ca9b7fe","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}