{"path":"lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","commits":[{"id":"4cc45c615dbb82bf79d5f9550286098367874fbf","date":1409571423,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","sourceNew":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    TestUtil.rm(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir, conf.setMergePolicy(NoMergePolicy.INSTANCE));\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    TestUtil.rm(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir, conf.setMergePolicy(NoMergePolicy.INSTANCE));\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"402ad3ddc9da7b70da1b167667a60ece6a1381fb","date":1409656478,"type":1,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","sourceNew":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    TestUtil.rm(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir, conf.setMergePolicy(NoMergePolicy.INSTANCE));\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    TestUtil.rm(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir, conf.setMergePolicy(NoMergePolicy.INSTANCE));\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cbc3688252d4a8045d69a164236b2cf87b721f17","date":1409846185,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","pathOld":"lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","sourceNew":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    IOUtils.rm(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir, conf.setMergePolicy(NoMergePolicy.INSTANCE));\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    TestUtil.rm(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir, conf.setMergePolicy(NoMergePolicy.INSTANCE));\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":["5eb2511ababf862ea11e10761c70ee560cd84510"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"07ec6c90f3abb2b32c6feeb48b1e4edfaa2d2699","date":1409860733,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","pathOld":"lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","sourceNew":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    IOUtils.rm(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    String codecName = System.getProperty(\"tests.codec\");\n    if (codecName == null || codecName.trim().isEmpty() || codecName.equals(\"random\")) {\n      fail(\"Must provide 'tests.codec' property to create BWC index\");\n    }\n    Codec codec = Codec.forName(codecName);\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp).setCodec(codec);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp).setCodec(codec);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n          .setMaxBufferedDocs(10).setMergePolicy(NoMergePolicy.INSTANCE).setCodec(codec);\n      writer = new IndexWriter(dir, conf);\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    IOUtils.rm(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir, conf.setMergePolicy(NoMergePolicy.INSTANCE));\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f4abec28b874149a7223e32cc7a01704c27790de","date":1410644789,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","pathOld":"lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","sourceNew":"  public Path createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    Path indexDir = Paths.get(\"/tmp/idx\").resolve(dirName);\n    IOUtils.rm(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    String codecName = System.getProperty(\"tests.codec\");\n    if (codecName == null || codecName.trim().isEmpty() || codecName.equals(\"random\")) {\n      fail(\"Must provide 'tests.codec' property to create BWC index\");\n    }\n    Codec codec = Codec.forName(codecName);\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp).setCodec(codec);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp).setCodec(codec);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n          .setMaxBufferedDocs(10).setMergePolicy(NoMergePolicy.INSTANCE).setCodec(codec);\n      writer = new IndexWriter(dir, conf);\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    IOUtils.rm(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    String codecName = System.getProperty(\"tests.codec\");\n    if (codecName == null || codecName.trim().isEmpty() || codecName.equals(\"random\")) {\n      fail(\"Must provide 'tests.codec' property to create BWC index\");\n    }\n    Codec codec = Codec.forName(codecName);\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp).setCodec(codec);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp).setCodec(codec);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n          .setMaxBufferedDocs(10).setMergePolicy(NoMergePolicy.INSTANCE).setCodec(codec);\n      writer = new IndexWriter(dir, conf);\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ec8882143e40fbd4aaa2cc02fc4abb1217eb24c7","date":1411591737,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","pathOld":"lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","sourceNew":"  public void createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    Path indexDir = getIndexDir().resolve(dirName);\n    Files.deleteIfExists(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n          .setMaxBufferedDocs(10).setMergePolicy(NoMergePolicy.INSTANCE);\n      writer = new IndexWriter(dir, conf);\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n  }\n\n","sourceOld":"  public Path createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    Path indexDir = Paths.get(\"/tmp/idx\").resolve(dirName);\n    IOUtils.rm(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    String codecName = System.getProperty(\"tests.codec\");\n    if (codecName == null || codecName.trim().isEmpty() || codecName.equals(\"random\")) {\n      fail(\"Must provide 'tests.codec' property to create BWC index\");\n    }\n    Codec codec = Codec.forName(codecName);\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp).setCodec(codec);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp).setCodec(codec);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n          .setMaxBufferedDocs(10).setMergePolicy(NoMergePolicy.INSTANCE).setCodec(codec);\n      writer = new IndexWriter(dir, conf);\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"945146c7fd6d18148836dae96cbac42c5c809cdf","date":1430264780,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","pathOld":"lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","sourceNew":"  public void createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    Path indexDir = getIndexDir().resolve(dirName);\n    Files.deleteIfExists(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(NoMergePolicy.INSTANCE);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(NoMergePolicy.INSTANCE);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n          .setMaxBufferedDocs(10).setMergePolicy(NoMergePolicy.INSTANCE);\n      writer = new IndexWriter(dir, conf);\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n  }\n\n","sourceOld":"  public void createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    Path indexDir = getIndexDir().resolve(dirName);\n    Files.deleteIfExists(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n          .setMaxBufferedDocs(10).setMergePolicy(NoMergePolicy.INSTANCE);\n      writer = new IndexWriter(dir, conf);\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"feb4029567b43f074ed7b6eb8fb126d355075dfd","date":1544812585,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","pathOld":"lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","sourceNew":"  public void createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    Path indexDir = getIndexDir().resolve(dirName);\n    Files.deleteIfExists(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(NoMergePolicy.INSTANCE);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.getDocStats().maxDoc);\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(NoMergePolicy.INSTANCE);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n          .setMaxBufferedDocs(10).setMergePolicy(NoMergePolicy.INSTANCE);\n      writer = new IndexWriter(dir, conf);\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n  }\n\n","sourceOld":"  public void createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    Path indexDir = getIndexDir().resolve(dirName);\n    Files.deleteIfExists(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(NoMergePolicy.INSTANCE);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(NoMergePolicy.INSTANCE);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n          .setMaxBufferedDocs(10).setMergePolicy(NoMergePolicy.INSTANCE);\n      writer = new IndexWriter(dir, conf);\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"cbc3688252d4a8045d69a164236b2cf87b721f17":["402ad3ddc9da7b70da1b167667a60ece6a1381fb"],"feb4029567b43f074ed7b6eb8fb126d355075dfd":["945146c7fd6d18148836dae96cbac42c5c809cdf"],"945146c7fd6d18148836dae96cbac42c5c809cdf":["ec8882143e40fbd4aaa2cc02fc4abb1217eb24c7"],"ec8882143e40fbd4aaa2cc02fc4abb1217eb24c7":["f4abec28b874149a7223e32cc7a01704c27790de"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4cc45c615dbb82bf79d5f9550286098367874fbf":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"f4abec28b874149a7223e32cc7a01704c27790de":["07ec6c90f3abb2b32c6feeb48b1e4edfaa2d2699"],"07ec6c90f3abb2b32c6feeb48b1e4edfaa2d2699":["cbc3688252d4a8045d69a164236b2cf87b721f17"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["feb4029567b43f074ed7b6eb8fb126d355075dfd"],"402ad3ddc9da7b70da1b167667a60ece6a1381fb":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","4cc45c615dbb82bf79d5f9550286098367874fbf"]},"commit2Childs":{"cbc3688252d4a8045d69a164236b2cf87b721f17":["07ec6c90f3abb2b32c6feeb48b1e4edfaa2d2699"],"feb4029567b43f074ed7b6eb8fb126d355075dfd":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"945146c7fd6d18148836dae96cbac42c5c809cdf":["feb4029567b43f074ed7b6eb8fb126d355075dfd"],"ec8882143e40fbd4aaa2cc02fc4abb1217eb24c7":["945146c7fd6d18148836dae96cbac42c5c809cdf"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4cc45c615dbb82bf79d5f9550286098367874fbf","402ad3ddc9da7b70da1b167667a60ece6a1381fb"],"4cc45c615dbb82bf79d5f9550286098367874fbf":["402ad3ddc9da7b70da1b167667a60ece6a1381fb"],"f4abec28b874149a7223e32cc7a01704c27790de":["ec8882143e40fbd4aaa2cc02fc4abb1217eb24c7"],"07ec6c90f3abb2b32c6feeb48b1e4edfaa2d2699":["f4abec28b874149a7223e32cc7a01704c27790de"],"402ad3ddc9da7b70da1b167667a60ece6a1381fb":["cbc3688252d4a8045d69a164236b2cf87b721f17"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}