{"path":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals,ScoreTermQueue).mjava","commits":[{"id":"683697c94be5a13ed67b070f48f5c5499ee8f6ea","date":1441704970,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals,ScoreTermQueue).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals).mjava","sourceNew":"  private void addTerms(IndexReader reader, FieldVals f, ScoreTermQueue q) throws IOException {\n    if (f.queryString == null) return;\n    final Terms terms = MultiFields.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    try (TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString)) {\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n      int corpusNumDocs = reader.numDocs();\n      HashSet<String> processedTerms = new HashSet<>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        String term = termAtt.toString();\n        if (!processedTerms.contains(term)) {\n          processedTerms.add(term);\n          ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n          float minScore = 0;\n          Term startTerm = new Term(f.fieldName, term);\n          AttributeSource atts = new AttributeSource();\n          MaxNonCompetitiveBoostAttribute maxBoostAtt =\n            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n          SlowFuzzyTermsEnum fe = new SlowFuzzyTermsEnum(terms, atts, startTerm, f.minSimilarity, f.prefixLength);\n          //store the df so all variants use same idf\n          int df = reader.docFreq(startTerm);\n          int numVariants = 0;\n          int totalVariantDocFreqs = 0;\n          BytesRef possibleMatch;\n          BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n          while ((possibleMatch = fe.next()) != null) {\n            numVariants++;\n            totalVariantDocFreqs += fe.docFreq();\n            float score = boostAtt.getBoost();\n            if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n              ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n              variantsQ.insertWithOverflow(st);\n              minScore = variantsQ.top().score; // maintain minScore\n            }\n            maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n          }\n\n          if (numVariants > 0) {\n            int avgDf = totalVariantDocFreqs / numVariants;\n            if (df == 0)//no direct match we can use as df for all variants\n            {\n              df = avgDf; //use avg df of all variants\n            }\n\n            // take the top variants (scored by edit distance) and reset the score\n            // to include an IDF factor then add to the global queue for ranking\n            // overall top query terms\n            int size = variantsQ.size();\n            for (int i = 0; i < size; i++) {\n              ScoreTerm st = variantsQ.pop();\n              st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n              q.insertWithOverflow(st);\n            }\n          }\n        }\n      }\n      ts.end();\n    }\n  }\n\n","sourceOld":"  private void addTerms(IndexReader reader, FieldVals f) throws IOException {\n    if (f.queryString == null) return;\n    final Terms terms = MultiFields.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    try (TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString)) {\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n      int corpusNumDocs = reader.numDocs();\n      HashSet<String> processedTerms = new HashSet<>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        String term = termAtt.toString();\n        if (!processedTerms.contains(term)) {\n          processedTerms.add(term);\n          ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n          float minScore = 0;\n          Term startTerm = new Term(f.fieldName, term);\n          AttributeSource atts = new AttributeSource();\n          MaxNonCompetitiveBoostAttribute maxBoostAtt =\n            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n          SlowFuzzyTermsEnum fe = new SlowFuzzyTermsEnum(terms, atts, startTerm, f.minSimilarity, f.prefixLength);\n          //store the df so all variants use same idf\n          int df = reader.docFreq(startTerm);\n          int numVariants = 0;\n          int totalVariantDocFreqs = 0;\n          BytesRef possibleMatch;\n          BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n          while ((possibleMatch = fe.next()) != null) {\n            numVariants++;\n            totalVariantDocFreqs += fe.docFreq();\n            float score = boostAtt.getBoost();\n            if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n              ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n              variantsQ.insertWithOverflow(st);\n              minScore = variantsQ.top().score; // maintain minScore\n            }\n            maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n          }\n\n          if (numVariants > 0) {\n            int avgDf = totalVariantDocFreqs / numVariants;\n            if (df == 0)//no direct match we can use as df for all variants\n            {\n              df = avgDf; //use avg df of all variants\n            }\n\n            // take the top variants (scored by edit distance) and reset the score\n            // to include an IDF factor then add to the global queue for ranking\n            // overall top query terms\n            int size = variantsQ.size();\n            for (int i = 0; i < size; i++) {\n              ScoreTerm st = variantsQ.pop();\n              st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n              q.insertWithOverflow(st);\n            }\n          }\n        }\n      }\n      ts.end();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b9c5c0e8e826d8fd169840564bcf8606cf81d15b","date":1473715404,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals,ScoreTermQueue).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals,ScoreTermQueue).mjava","sourceNew":"  private void addTerms(IndexReader reader, FieldVals f, ScoreTermQueue q) throws IOException {\n    if (f.queryString == null) return;\n    final Terms terms = MultiFields.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    try (TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString)) {\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n      int corpusNumDocs = reader.numDocs();\n      HashSet<String> processedTerms = new HashSet<>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        String term = termAtt.toString();\n        if (!processedTerms.contains(term)) {\n          processedTerms.add(term);\n          ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n          float minScore = 0;\n          Term startTerm = new Term(f.fieldName, term);\n          AttributeSource atts = new AttributeSource();\n          MaxNonCompetitiveBoostAttribute maxBoostAtt =\n            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n          FuzzyTermsEnum fe = new FuzzyTermsEnum(terms, atts, startTerm, f.maxEdits, f.prefixLength, true);\n          //store the df so all variants use same idf\n          int df = reader.docFreq(startTerm);\n          int numVariants = 0;\n          int totalVariantDocFreqs = 0;\n          BytesRef possibleMatch;\n          BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n          while ((possibleMatch = fe.next()) != null) {\n            numVariants++;\n            totalVariantDocFreqs += fe.docFreq();\n            float score = boostAtt.getBoost();\n            if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n              ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n              variantsQ.insertWithOverflow(st);\n              minScore = variantsQ.top().score; // maintain minScore\n            }\n            maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n          }\n\n          if (numVariants > 0) {\n            int avgDf = totalVariantDocFreqs / numVariants;\n            if (df == 0)//no direct match we can use as df for all variants\n              {\n                df = avgDf; //use avg df of all variants\n              }\n\n            // take the top variants (scored by edit distance) and reset the score\n            // to include an IDF factor then add to the global queue for ranking\n            // overall top query terms\n            int size = variantsQ.size();\n            for (int i = 0; i < size; i++) {\n              ScoreTerm st = variantsQ.pop();\n              st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n              q.insertWithOverflow(st);\n            }\n          }\n        }\n      }\n      ts.end();\n    }\n  }\n\n","sourceOld":"  private void addTerms(IndexReader reader, FieldVals f, ScoreTermQueue q) throws IOException {\n    if (f.queryString == null) return;\n    final Terms terms = MultiFields.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    try (TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString)) {\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n      int corpusNumDocs = reader.numDocs();\n      HashSet<String> processedTerms = new HashSet<>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        String term = termAtt.toString();\n        if (!processedTerms.contains(term)) {\n          processedTerms.add(term);\n          ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n          float minScore = 0;\n          Term startTerm = new Term(f.fieldName, term);\n          AttributeSource atts = new AttributeSource();\n          MaxNonCompetitiveBoostAttribute maxBoostAtt =\n            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n          SlowFuzzyTermsEnum fe = new SlowFuzzyTermsEnum(terms, atts, startTerm, f.minSimilarity, f.prefixLength);\n          //store the df so all variants use same idf\n          int df = reader.docFreq(startTerm);\n          int numVariants = 0;\n          int totalVariantDocFreqs = 0;\n          BytesRef possibleMatch;\n          BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n          while ((possibleMatch = fe.next()) != null) {\n            numVariants++;\n            totalVariantDocFreqs += fe.docFreq();\n            float score = boostAtt.getBoost();\n            if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n              ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n              variantsQ.insertWithOverflow(st);\n              minScore = variantsQ.top().score; // maintain minScore\n            }\n            maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n          }\n\n          if (numVariants > 0) {\n            int avgDf = totalVariantDocFreqs / numVariants;\n            if (df == 0)//no direct match we can use as df for all variants\n            {\n              df = avgDf; //use avg df of all variants\n            }\n\n            // take the top variants (scored by edit distance) and reset the score\n            // to include an IDF factor then add to the global queue for ranking\n            // overall top query terms\n            int size = variantsQ.size();\n            for (int i = 0; i < size; i++) {\n              ScoreTerm st = variantsQ.pop();\n              st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n              q.insertWithOverflow(st);\n            }\n          }\n        }\n      }\n      ts.end();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"89424def13674ea17829b41c5883c54ecc31a132","date":1473767373,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals,ScoreTermQueue).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals,ScoreTermQueue).mjava","sourceNew":"  private void addTerms(IndexReader reader, FieldVals f, ScoreTermQueue q) throws IOException {\n    if (f.queryString == null) return;\n    final Terms terms = MultiFields.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    try (TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString)) {\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n      int corpusNumDocs = reader.numDocs();\n      HashSet<String> processedTerms = new HashSet<>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        String term = termAtt.toString();\n        if (!processedTerms.contains(term)) {\n          processedTerms.add(term);\n          ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n          float minScore = 0;\n          Term startTerm = new Term(f.fieldName, term);\n          AttributeSource atts = new AttributeSource();\n          MaxNonCompetitiveBoostAttribute maxBoostAtt =\n            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n          FuzzyTermsEnum fe = new FuzzyTermsEnum(terms, atts, startTerm, f.maxEdits, f.prefixLength, true);\n          //store the df so all variants use same idf\n          int df = reader.docFreq(startTerm);\n          int numVariants = 0;\n          int totalVariantDocFreqs = 0;\n          BytesRef possibleMatch;\n          BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n          while ((possibleMatch = fe.next()) != null) {\n            numVariants++;\n            totalVariantDocFreqs += fe.docFreq();\n            float score = boostAtt.getBoost();\n            if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n              ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n              variantsQ.insertWithOverflow(st);\n              minScore = variantsQ.top().score; // maintain minScore\n            }\n            maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n          }\n\n          if (numVariants > 0) {\n            int avgDf = totalVariantDocFreqs / numVariants;\n            if (df == 0)//no direct match we can use as df for all variants\n              {\n                df = avgDf; //use avg df of all variants\n              }\n\n            // take the top variants (scored by edit distance) and reset the score\n            // to include an IDF factor then add to the global queue for ranking\n            // overall top query terms\n            int size = variantsQ.size();\n            for (int i = 0; i < size; i++) {\n              ScoreTerm st = variantsQ.pop();\n              st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n              q.insertWithOverflow(st);\n            }\n          }\n        }\n      }\n      ts.end();\n    }\n  }\n\n","sourceOld":"  private void addTerms(IndexReader reader, FieldVals f, ScoreTermQueue q) throws IOException {\n    if (f.queryString == null) return;\n    final Terms terms = MultiFields.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    try (TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString)) {\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n      int corpusNumDocs = reader.numDocs();\n      HashSet<String> processedTerms = new HashSet<>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        String term = termAtt.toString();\n        if (!processedTerms.contains(term)) {\n          processedTerms.add(term);\n          ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n          float minScore = 0;\n          Term startTerm = new Term(f.fieldName, term);\n          AttributeSource atts = new AttributeSource();\n          MaxNonCompetitiveBoostAttribute maxBoostAtt =\n            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n          SlowFuzzyTermsEnum fe = new SlowFuzzyTermsEnum(terms, atts, startTerm, f.minSimilarity, f.prefixLength);\n          //store the df so all variants use same idf\n          int df = reader.docFreq(startTerm);\n          int numVariants = 0;\n          int totalVariantDocFreqs = 0;\n          BytesRef possibleMatch;\n          BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n          while ((possibleMatch = fe.next()) != null) {\n            numVariants++;\n            totalVariantDocFreqs += fe.docFreq();\n            float score = boostAtt.getBoost();\n            if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n              ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n              variantsQ.insertWithOverflow(st);\n              minScore = variantsQ.top().score; // maintain minScore\n            }\n            maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n          }\n\n          if (numVariants > 0) {\n            int avgDf = totalVariantDocFreqs / numVariants;\n            if (df == 0)//no direct match we can use as df for all variants\n            {\n              df = avgDf; //use avg df of all variants\n            }\n\n            // take the top variants (scored by edit distance) and reset the score\n            // to include an IDF factor then add to the global queue for ranking\n            // overall top query terms\n            int size = variantsQ.size();\n            for (int i = 0; i < size; i++) {\n              ScoreTerm st = variantsQ.pop();\n              st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n              q.insertWithOverflow(st);\n            }\n          }\n        }\n      }\n      ts.end();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals,ScoreTermQueue).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals,ScoreTermQueue).mjava","sourceNew":"  private void addTerms(IndexReader reader, FieldVals f, ScoreTermQueue q) throws IOException {\n    if (f.queryString == null) return;\n    final Terms terms = MultiFields.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    try (TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString)) {\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n      int corpusNumDocs = reader.numDocs();\n      HashSet<String> processedTerms = new HashSet<>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        String term = termAtt.toString();\n        if (!processedTerms.contains(term)) {\n          processedTerms.add(term);\n          ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n          float minScore = 0;\n          Term startTerm = new Term(f.fieldName, term);\n          AttributeSource atts = new AttributeSource();\n          MaxNonCompetitiveBoostAttribute maxBoostAtt =\n            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n          FuzzyTermsEnum fe = new FuzzyTermsEnum(terms, atts, startTerm, f.maxEdits, f.prefixLength, true);\n          //store the df so all variants use same idf\n          int df = reader.docFreq(startTerm);\n          int numVariants = 0;\n          int totalVariantDocFreqs = 0;\n          BytesRef possibleMatch;\n          BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n          while ((possibleMatch = fe.next()) != null) {\n            numVariants++;\n            totalVariantDocFreqs += fe.docFreq();\n            float score = boostAtt.getBoost();\n            if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n              ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n              variantsQ.insertWithOverflow(st);\n              minScore = variantsQ.top().score; // maintain minScore\n            }\n            maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n          }\n\n          if (numVariants > 0) {\n            int avgDf = totalVariantDocFreqs / numVariants;\n            if (df == 0)//no direct match we can use as df for all variants\n              {\n                df = avgDf; //use avg df of all variants\n              }\n\n            // take the top variants (scored by edit distance) and reset the score\n            // to include an IDF factor then add to the global queue for ranking\n            // overall top query terms\n            int size = variantsQ.size();\n            for (int i = 0; i < size; i++) {\n              ScoreTerm st = variantsQ.pop();\n              st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n              q.insertWithOverflow(st);\n            }\n          }\n        }\n      }\n      ts.end();\n    }\n  }\n\n","sourceOld":"  private void addTerms(IndexReader reader, FieldVals f, ScoreTermQueue q) throws IOException {\n    if (f.queryString == null) return;\n    final Terms terms = MultiFields.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    try (TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString)) {\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n      int corpusNumDocs = reader.numDocs();\n      HashSet<String> processedTerms = new HashSet<>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        String term = termAtt.toString();\n        if (!processedTerms.contains(term)) {\n          processedTerms.add(term);\n          ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n          float minScore = 0;\n          Term startTerm = new Term(f.fieldName, term);\n          AttributeSource atts = new AttributeSource();\n          MaxNonCompetitiveBoostAttribute maxBoostAtt =\n            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n          SlowFuzzyTermsEnum fe = new SlowFuzzyTermsEnum(terms, atts, startTerm, f.minSimilarity, f.prefixLength);\n          //store the df so all variants use same idf\n          int df = reader.docFreq(startTerm);\n          int numVariants = 0;\n          int totalVariantDocFreqs = 0;\n          BytesRef possibleMatch;\n          BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n          while ((possibleMatch = fe.next()) != null) {\n            numVariants++;\n            totalVariantDocFreqs += fe.docFreq();\n            float score = boostAtt.getBoost();\n            if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n              ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n              variantsQ.insertWithOverflow(st);\n              minScore = variantsQ.top().score; // maintain minScore\n            }\n            maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n          }\n\n          if (numVariants > 0) {\n            int avgDf = totalVariantDocFreqs / numVariants;\n            if (df == 0)//no direct match we can use as df for all variants\n            {\n              df = avgDf; //use avg df of all variants\n            }\n\n            // take the top variants (scored by edit distance) and reset the score\n            // to include an IDF factor then add to the global queue for ranking\n            // overall top query terms\n            int size = variantsQ.size();\n            for (int i = 0; i < size; i++) {\n              ScoreTerm st = variantsQ.pop();\n              st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n              q.insertWithOverflow(st);\n            }\n          }\n        }\n      }\n      ts.end();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals,ScoreTermQueue).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals,ScoreTermQueue).mjava","sourceNew":"  private void addTerms(IndexReader reader, FieldVals f, ScoreTermQueue q) throws IOException {\n    if (f.queryString == null) return;\n    final Terms terms = MultiFields.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    try (TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString)) {\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n      int corpusNumDocs = reader.numDocs();\n      HashSet<String> processedTerms = new HashSet<>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        String term = termAtt.toString();\n        if (!processedTerms.contains(term)) {\n          processedTerms.add(term);\n          ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n          float minScore = 0;\n          Term startTerm = new Term(f.fieldName, term);\n          AttributeSource atts = new AttributeSource();\n          MaxNonCompetitiveBoostAttribute maxBoostAtt =\n            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n          FuzzyTermsEnum fe = new FuzzyTermsEnum(terms, atts, startTerm, f.maxEdits, f.prefixLength, true);\n          //store the df so all variants use same idf\n          int df = reader.docFreq(startTerm);\n          int numVariants = 0;\n          int totalVariantDocFreqs = 0;\n          BytesRef possibleMatch;\n          BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n          while ((possibleMatch = fe.next()) != null) {\n            numVariants++;\n            totalVariantDocFreqs += fe.docFreq();\n            float score = boostAtt.getBoost();\n            if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n              ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n              variantsQ.insertWithOverflow(st);\n              minScore = variantsQ.top().score; // maintain minScore\n            }\n            maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n          }\n\n          if (numVariants > 0) {\n            int avgDf = totalVariantDocFreqs / numVariants;\n            if (df == 0)//no direct match we can use as df for all variants\n              {\n                df = avgDf; //use avg df of all variants\n              }\n\n            // take the top variants (scored by edit distance) and reset the score\n            // to include an IDF factor then add to the global queue for ranking\n            // overall top query terms\n            int size = variantsQ.size();\n            for (int i = 0; i < size; i++) {\n              ScoreTerm st = variantsQ.pop();\n              st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n              q.insertWithOverflow(st);\n            }\n          }\n        }\n      }\n      ts.end();\n    }\n  }\n\n","sourceOld":"  private void addTerms(IndexReader reader, FieldVals f, ScoreTermQueue q) throws IOException {\n    if (f.queryString == null) return;\n    final Terms terms = MultiFields.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    try (TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString)) {\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n      int corpusNumDocs = reader.numDocs();\n      HashSet<String> processedTerms = new HashSet<>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        String term = termAtt.toString();\n        if (!processedTerms.contains(term)) {\n          processedTerms.add(term);\n          ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n          float minScore = 0;\n          Term startTerm = new Term(f.fieldName, term);\n          AttributeSource atts = new AttributeSource();\n          MaxNonCompetitiveBoostAttribute maxBoostAtt =\n            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n          SlowFuzzyTermsEnum fe = new SlowFuzzyTermsEnum(terms, atts, startTerm, f.minSimilarity, f.prefixLength);\n          //store the df so all variants use same idf\n          int df = reader.docFreq(startTerm);\n          int numVariants = 0;\n          int totalVariantDocFreqs = 0;\n          BytesRef possibleMatch;\n          BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n          while ((possibleMatch = fe.next()) != null) {\n            numVariants++;\n            totalVariantDocFreqs += fe.docFreq();\n            float score = boostAtt.getBoost();\n            if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n              ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n              variantsQ.insertWithOverflow(st);\n              minScore = variantsQ.top().score; // maintain minScore\n            }\n            maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n          }\n\n          if (numVariants > 0) {\n            int avgDf = totalVariantDocFreqs / numVariants;\n            if (df == 0)//no direct match we can use as df for all variants\n            {\n              df = avgDf; //use avg df of all variants\n            }\n\n            // take the top variants (scored by edit distance) and reset the score\n            // to include an IDF factor then add to the global queue for ranking\n            // overall top query terms\n            int size = variantsQ.size();\n            for (int i = 0; i < size; i++) {\n              ScoreTerm st = variantsQ.pop();\n              st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n              q.insertWithOverflow(st);\n            }\n          }\n        }\n      }\n      ts.end();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"04e775de416dd2d8067b10db1c8af975a1d5017e","date":1539906554,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals,ScoreTermQueue).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals,ScoreTermQueue).mjava","sourceNew":"  private void addTerms(IndexReader reader, FieldVals f, ScoreTermQueue q) throws IOException {\n    if (f.queryString == null) return;\n    final Terms terms = MultiTerms.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    try (TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString)) {\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n      int corpusNumDocs = reader.numDocs();\n      HashSet<String> processedTerms = new HashSet<>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        String term = termAtt.toString();\n        if (!processedTerms.contains(term)) {\n          processedTerms.add(term);\n          ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n          float minScore = 0;\n          Term startTerm = new Term(f.fieldName, term);\n          AttributeSource atts = new AttributeSource();\n          MaxNonCompetitiveBoostAttribute maxBoostAtt =\n            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n          FuzzyTermsEnum fe = new FuzzyTermsEnum(terms, atts, startTerm, f.maxEdits, f.prefixLength, true);\n          //store the df so all variants use same idf\n          int df = reader.docFreq(startTerm);\n          int numVariants = 0;\n          int totalVariantDocFreqs = 0;\n          BytesRef possibleMatch;\n          BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n          while ((possibleMatch = fe.next()) != null) {\n            numVariants++;\n            totalVariantDocFreqs += fe.docFreq();\n            float score = boostAtt.getBoost();\n            if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n              ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n              variantsQ.insertWithOverflow(st);\n              minScore = variantsQ.top().score; // maintain minScore\n            }\n            maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n          }\n\n          if (numVariants > 0) {\n            int avgDf = totalVariantDocFreqs / numVariants;\n            if (df == 0)//no direct match we can use as df for all variants\n              {\n                df = avgDf; //use avg df of all variants\n              }\n\n            // take the top variants (scored by edit distance) and reset the score\n            // to include an IDF factor then add to the global queue for ranking\n            // overall top query terms\n            int size = variantsQ.size();\n            for (int i = 0; i < size; i++) {\n              ScoreTerm st = variantsQ.pop();\n              st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n              q.insertWithOverflow(st);\n            }\n          }\n        }\n      }\n      ts.end();\n    }\n  }\n\n","sourceOld":"  private void addTerms(IndexReader reader, FieldVals f, ScoreTermQueue q) throws IOException {\n    if (f.queryString == null) return;\n    final Terms terms = MultiFields.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    try (TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString)) {\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n      int corpusNumDocs = reader.numDocs();\n      HashSet<String> processedTerms = new HashSet<>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        String term = termAtt.toString();\n        if (!processedTerms.contains(term)) {\n          processedTerms.add(term);\n          ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n          float minScore = 0;\n          Term startTerm = new Term(f.fieldName, term);\n          AttributeSource atts = new AttributeSource();\n          MaxNonCompetitiveBoostAttribute maxBoostAtt =\n            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n          FuzzyTermsEnum fe = new FuzzyTermsEnum(terms, atts, startTerm, f.maxEdits, f.prefixLength, true);\n          //store the df so all variants use same idf\n          int df = reader.docFreq(startTerm);\n          int numVariants = 0;\n          int totalVariantDocFreqs = 0;\n          BytesRef possibleMatch;\n          BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n          while ((possibleMatch = fe.next()) != null) {\n            numVariants++;\n            totalVariantDocFreqs += fe.docFreq();\n            float score = boostAtt.getBoost();\n            if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n              ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n              variantsQ.insertWithOverflow(st);\n              minScore = variantsQ.top().score; // maintain minScore\n            }\n            maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n          }\n\n          if (numVariants > 0) {\n            int avgDf = totalVariantDocFreqs / numVariants;\n            if (df == 0)//no direct match we can use as df for all variants\n              {\n                df = avgDf; //use avg df of all variants\n              }\n\n            // take the top variants (scored by edit distance) and reset the score\n            // to include an IDF factor then add to the global queue for ranking\n            // overall top query terms\n            int size = variantsQ.size();\n            for (int i = 0; i < size; i++) {\n              ScoreTerm st = variantsQ.pop();\n              st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n              q.insertWithOverflow(st);\n            }\n          }\n        }\n      }\n      ts.end();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3e99d92de6748e3bbd2dd7b72695cdb952b2d835","date":1579100291,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals,ScoreTermQueue).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery#addTerms(IndexReader,FieldVals,ScoreTermQueue).mjava","sourceNew":"  private void addTerms(IndexReader reader, FieldVals f, ScoreTermQueue q) throws IOException {\n    if (f.queryString == null) return;\n    final Terms terms = MultiTerms.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    try (TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString)) {\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n      int corpusNumDocs = reader.numDocs();\n      HashSet<String> processedTerms = new HashSet<>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        String term = termAtt.toString();\n        if (!processedTerms.contains(term)) {\n          processedTerms.add(term);\n          ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n          float minScore = 0;\n          Term startTerm = new Term(f.fieldName, term);\n          FuzzyTermsEnum fe = new FuzzyTermsEnum(terms, startTerm, f.maxEdits, f.prefixLength, true);\n          //store the df so all variants use same idf\n          int df = reader.docFreq(startTerm);\n          int numVariants = 0;\n          int totalVariantDocFreqs = 0;\n          BytesRef possibleMatch;\n          BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n          while ((possibleMatch = fe.next()) != null) {\n            numVariants++;\n            totalVariantDocFreqs += fe.docFreq();\n            float score = boostAtt.getBoost();\n            if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n              ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n              variantsQ.insertWithOverflow(st);\n              minScore = variantsQ.top().score; // maintain minScore\n            }\n            fe.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n          }\n\n          if (numVariants > 0) {\n            int avgDf = totalVariantDocFreqs / numVariants;\n            if (df == 0)//no direct match we can use as df for all variants\n              {\n                df = avgDf; //use avg df of all variants\n              }\n\n            // take the top variants (scored by edit distance) and reset the score\n            // to include an IDF factor then add to the global queue for ranking\n            // overall top query terms\n            int size = variantsQ.size();\n            for (int i = 0; i < size; i++) {\n              ScoreTerm st = variantsQ.pop();\n              st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n              q.insertWithOverflow(st);\n            }\n          }\n        }\n      }\n      ts.end();\n    }\n  }\n\n","sourceOld":"  private void addTerms(IndexReader reader, FieldVals f, ScoreTermQueue q) throws IOException {\n    if (f.queryString == null) return;\n    final Terms terms = MultiTerms.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    try (TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString)) {\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n      int corpusNumDocs = reader.numDocs();\n      HashSet<String> processedTerms = new HashSet<>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        String term = termAtt.toString();\n        if (!processedTerms.contains(term)) {\n          processedTerms.add(term);\n          ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n          float minScore = 0;\n          Term startTerm = new Term(f.fieldName, term);\n          AttributeSource atts = new AttributeSource();\n          MaxNonCompetitiveBoostAttribute maxBoostAtt =\n            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n          FuzzyTermsEnum fe = new FuzzyTermsEnum(terms, atts, startTerm, f.maxEdits, f.prefixLength, true);\n          //store the df so all variants use same idf\n          int df = reader.docFreq(startTerm);\n          int numVariants = 0;\n          int totalVariantDocFreqs = 0;\n          BytesRef possibleMatch;\n          BoostAttribute boostAtt =\n            fe.attributes().addAttribute(BoostAttribute.class);\n          while ((possibleMatch = fe.next()) != null) {\n            numVariants++;\n            totalVariantDocFreqs += fe.docFreq();\n            float score = boostAtt.getBoost();\n            if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n              ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n              variantsQ.insertWithOverflow(st);\n              minScore = variantsQ.top().score; // maintain minScore\n            }\n            maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n          }\n\n          if (numVariants > 0) {\n            int avgDf = totalVariantDocFreqs / numVariants;\n            if (df == 0)//no direct match we can use as df for all variants\n              {\n                df = avgDf; //use avg df of all variants\n              }\n\n            // take the top variants (scored by edit distance) and reset the score\n            // to include an IDF factor then add to the global queue for ranking\n            // overall top query terms\n            int size = variantsQ.size();\n            for (int i = 0; i < size; i++) {\n              ScoreTerm st = variantsQ.pop();\n              st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);\n              q.insertWithOverflow(st);\n            }\n          }\n        }\n      }\n      ts.end();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3e99d92de6748e3bbd2dd7b72695cdb952b2d835":["04e775de416dd2d8067b10db1c8af975a1d5017e"],"683697c94be5a13ed67b070f48f5c5499ee8f6ea":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["683697c94be5a13ed67b070f48f5c5499ee8f6ea","89424def13674ea17829b41c5883c54ecc31a132"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"b9c5c0e8e826d8fd169840564bcf8606cf81d15b":["683697c94be5a13ed67b070f48f5c5499ee8f6ea"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["683697c94be5a13ed67b070f48f5c5499ee8f6ea","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3e99d92de6748e3bbd2dd7b72695cdb952b2d835"],"89424def13674ea17829b41c5883c54ecc31a132":["683697c94be5a13ed67b070f48f5c5499ee8f6ea","b9c5c0e8e826d8fd169840564bcf8606cf81d15b"]},"commit2Childs":{"3e99d92de6748e3bbd2dd7b72695cdb952b2d835":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"683697c94be5a13ed67b070f48f5c5499ee8f6ea":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","b9c5c0e8e826d8fd169840564bcf8606cf81d15b","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","89424def13674ea17829b41c5883c54ecc31a132"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["683697c94be5a13ed67b070f48f5c5499ee8f6ea"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["04e775de416dd2d8067b10db1c8af975a1d5017e","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["3e99d92de6748e3bbd2dd7b72695cdb952b2d835"],"b9c5c0e8e826d8fd169840564bcf8606cf81d15b":["89424def13674ea17829b41c5883c54ecc31a132"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"89424def13674ea17829b41c5883c54ecc31a132":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}