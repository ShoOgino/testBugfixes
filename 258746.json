{"path":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","commits":[{"id":"61c45e99cf6676da48f19d7511c73712ad39402b","date":1495508331,"type":0,"author":"Tomas Fernandez Lobbe","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","pathOld":"/dev/null","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n    \n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(),\n            cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f","344b0840364d990b29b97467bfcc766ff8325d11","a26124f3206f25b02a763a16b9ef8bfc3d81bc27"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e9017cf144952056066919f1ebc7897ff9bd71b1","date":1496757600,"type":0,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","pathOld":"/dev/null","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n    \n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(),\n            cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"344b0840364d990b29b97467bfcc766ff8325d11","date":1501574100,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n    \n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n    \n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(),\n            cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":["61c45e99cf6676da48f19d7511c73712ad39402b"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7a23cf16c8fa265dc0a564adcabb55e3f054e0ac","date":1502192746,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n    \n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n    \n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(),\n            cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c7ff8a6fbdc9c2d84bc6e9e71e1c738e49c441ac","date":1503580177,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n    \n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n    \n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b869898f50ca80263bac2e3ae0949f7700e5c977","date":1503580229,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n    \n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n    \n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"85212dad4ed576c7f7e6c165ee19e597b7b4efc8","date":1507997740,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n    \n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n    \n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":null,"bugIntro":["420b2cd54774495de0bb67f068f5231f3da5e494"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"84f20f331d8001864545c7021812d8c6509c7593","date":1517216128,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, core.getCoreDescriptor(), true);\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n\n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leader.getCoreUrl(),\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(), recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leader.getCoreUrl()), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n    \n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":null,"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1455c941cc4ce652efc776fc23471b0e499246f6","date":1528086751,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if (ulog.existOldBufferLog()) {\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, core.getCoreDescriptor(), true);\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n\n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        // recalling buffer updates will drop the old buffer tlog\n        ulog.bufferUpdates();\n\n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leader.getCoreUrl(),\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(), recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leader.getCoreUrl()), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n\n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, core.getCoreDescriptor(), true);\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n\n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leader.getCoreUrl(),\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(), recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leader.getCoreUrl()), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f592209545c71895260367152601e9200399776d","date":1528238935,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if (ulog.existOldBufferLog()) {\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, core.getCoreDescriptor(), true);\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n\n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        // recalling buffer updates will drop the old buffer tlog\n        ulog.bufferUpdates();\n\n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leader.getCoreUrl(),\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(), recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leader.getCoreUrl()), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n\n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, core.getCoreDescriptor(), true);\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n\n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leader.getCoreUrl(),\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(), recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leader.getCoreUrl()), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b70042a8a492f7054d480ccdd2be9796510d4327","date":1528386658,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if (ulog.existOldBufferLog()) {\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, core.getCoreDescriptor(), true);\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n\n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        // recalling buffer updates will drop the old buffer tlog\n        ulog.bufferUpdates();\n\n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leader.getCoreUrl(),\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(), recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leader.getCoreUrl()), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n\n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, core.getCoreDescriptor(), true);\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n\n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leader.getCoreUrl(),\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(), recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leader.getCoreUrl()), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ba0e7b86ac6002d5286b4589d87b3c80bbcabdc7","date":1529486762,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if (ulog.existOldBufferLog()) {\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, core.getCoreDescriptor(), true);\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n\n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        // recalling buffer updates will drop the old buffer tlog\n        ulog.bufferUpdates();\n\n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leader.getCoreUrl(),\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        //TODO since SOLR-11216, we probably won't need this\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(), recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSyncWithLeader peerSyncWithLeader = new PeerSyncWithLeader(core,\n              leader.getCoreUrl(), ulog.getNumRecordsToKeep());\n          boolean syncSuccess = peerSyncWithLeader.sync(recentVersions).isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            req.close();\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n\n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if (ulog.existOldBufferLog()) {\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, core.getCoreDescriptor(), true);\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n\n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        // recalling buffer updates will drop the old buffer tlog\n        ulog.bufferUpdates();\n\n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leader.getCoreUrl(),\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(), recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leader.getCoreUrl()), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n\n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":null,"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26","date":1531589977,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if (ulog.existOldBufferLog()) {\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, core.getCoreDescriptor(), true);\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n\n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        // recalling buffer updates will drop the old buffer tlog\n        ulog.bufferUpdates();\n\n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leader.getCoreUrl(),\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        //TODO since SOLR-11216, we probably won't need this\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(), recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSyncWithLeader peerSyncWithLeader = new PeerSyncWithLeader(core,\n              leader.getCoreUrl(), ulog.getNumRecordsToKeep());\n          boolean syncSuccess = peerSyncWithLeader.sync(recentVersions).isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            req.close();\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n\n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if (ulog.existOldBufferLog()) {\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, core.getCoreDescriptor(), true);\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n\n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        // recalling buffer updates will drop the old buffer tlog\n        ulog.bufferUpdates();\n\n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leader.getCoreUrl(),\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(), recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leader.getCoreUrl()), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n\n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","date":1531905561,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if (ulog.existOldBufferLog()) {\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, core.getCoreDescriptor(), true);\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n\n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        // recalling buffer updates will drop the old buffer tlog\n        ulog.bufferUpdates();\n\n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leader.getCoreUrl(),\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        //TODO since SOLR-11216, we probably won't need this\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(), recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSyncWithLeader peerSyncWithLeader = new PeerSyncWithLeader(core,\n              leader.getCoreUrl(), ulog.getNumRecordsToKeep());\n          boolean syncSuccess = peerSyncWithLeader.sync(recentVersions).isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            req.close();\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n\n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if (ulog.existOldBufferLog()) {\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, core.getCoreDescriptor(), true);\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n\n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        // recalling buffer updates will drop the old buffer tlog\n        ulog.bufferUpdates();\n\n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leader.getCoreUrl(),\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(), recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leader.getCoreUrl()), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n\n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e9c81f7e703d7ccca5bc78beb61253f0a8a22afd","date":1534976797,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          log.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if (ulog.existOldBufferLog()) {\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, core.getCoreDescriptor(), true);\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader!\");\n          log.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n\n        log.info(\"Begin buffering updates. core=[{}]\", coreName);\n        // recalling buffer updates will drop the old buffer tlog\n        ulog.bufferUpdates();\n\n        log.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leader.getCoreUrl(),\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n        \n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        //TODO since SOLR-11216, we probably won't need this\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(), recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSyncWithLeader peerSyncWithLeader = new PeerSyncWithLeader(core,\n              leader.getCoreUrl(), ulog.getNumRecordsToKeep());\n          boolean syncSuccess = peerSyncWithLeader.sync(recentVersions).isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            req.close();\n            log.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            log.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n\n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (successfulRecovery) {\n          log.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            log.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          log.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              log.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      log.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    log.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if (ulog.existOldBufferLog()) {\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, core.getCoreDescriptor(), true);\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n\n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        // recalling buffer updates will drop the old buffer tlog\n        ulog.bufferUpdates();\n\n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leader.getCoreUrl(),\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        //TODO since SOLR-11216, we probably won't need this\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(), recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSyncWithLeader peerSyncWithLeader = new PeerSyncWithLeader(core,\n              leader.getCoreUrl(), ulog.getNumRecordsToKeep());\n          boolean syncSuccess = peerSyncWithLeader.sync(recentVersions).isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            req.close();\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n\n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":null,"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"425c2986e128f9e4aadd629cdf3b04e7aacb7c80","date":1536202585,"type":3,"author":"Varun Thacker","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"Found new versions added after startup: num=[{}]\", oldIdx);\n          log.info(\"currentVersions size={} range=[{} to {}]\", recentVersions.size(), recentVersions.get(0), recentVersions.get(recentVersions.size()-1));\n        }\n\n        log.info(\"startupVersions size={} range=[{} to {}]\", startingVersions.size(), startingVersions.get(0), startingVersions.get(startingVersions.size()-1));\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if (ulog.existOldBufferLog()) {\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, core.getCoreDescriptor(), true);\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader!\");\n          log.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n\n        log.info(\"Begin buffering updates. core=[{}]\", coreName);\n        // recalling buffer updates will drop the old buffer tlog\n        ulog.bufferUpdates();\n\n        log.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leader.getCoreUrl(),\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n        \n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        //TODO since SOLR-11216, we probably won't need this\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(), recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSyncWithLeader peerSyncWithLeader = new PeerSyncWithLeader(core,\n              leader.getCoreUrl(), ulog.getNumRecordsToKeep());\n          boolean syncSuccess = peerSyncWithLeader.sync(recentVersions).isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            req.close();\n            log.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            log.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n\n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (successfulRecovery) {\n          log.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            log.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          log.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              log.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      log.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    log.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          log.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if (ulog.existOldBufferLog()) {\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, core.getCoreDescriptor(), true);\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader!\");\n          log.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n\n        log.info(\"Begin buffering updates. core=[{}]\", coreName);\n        // recalling buffer updates will drop the old buffer tlog\n        ulog.bufferUpdates();\n\n        log.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leader.getCoreUrl(),\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n        \n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        //TODO since SOLR-11216, we probably won't need this\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(), recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSyncWithLeader peerSyncWithLeader = new PeerSyncWithLeader(core,\n              leader.getCoreUrl(), ulog.getNumRecordsToKeep());\n          boolean syncSuccess = peerSyncWithLeader.sync(recentVersions).isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            req.close();\n            log.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            log.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n\n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (successfulRecovery) {\n          log.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            log.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          log.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              log.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      log.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    log.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":null,"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f","420b2cd54774495de0bb67f068f5231f3da5e494"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"420b2cd54774495de0bb67f068f5231f3da5e494","date":1538382330,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public final void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"Found new versions added after startup: num=[{}]\", oldIdx);\n          log.info(\"currentVersions size={} range=[{} to {}]\", recentVersions.size(), recentVersions.get(0), recentVersions.get(recentVersions.size()-1));\n        }\n\n        if (startingVersions.isEmpty()) {\n          log.info(\"startupVersions is empty\");\n        } else {\n          log.info(\"startupVersions size={} range=[{} to {}]\", startingVersions.size(), startingVersions.get(0), startingVersions.get(startingVersions.size()-1));\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if (ulog.existOldBufferLog()) {\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, core.getCoreDescriptor(), true);\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader!\");\n          log.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n\n        log.info(\"Begin buffering updates. core=[{}]\", coreName);\n        // recalling buffer updates will drop the old buffer tlog\n        ulog.bufferUpdates();\n\n        log.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leader.getCoreUrl(),\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n        \n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        //TODO since SOLR-11216, we probably won't need this\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(), recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSyncWithLeader peerSyncWithLeader = new PeerSyncWithLeader(core,\n              leader.getCoreUrl(), ulog.getNumRecordsToKeep());\n          boolean syncSuccess = peerSyncWithLeader.sync(recentVersions).isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            req.close();\n            log.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            log.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n\n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (successfulRecovery) {\n          log.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            log.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          log.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              log.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      log.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    log.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"Found new versions added after startup: num=[{}]\", oldIdx);\n          log.info(\"currentVersions size={} range=[{} to {}]\", recentVersions.size(), recentVersions.get(0), recentVersions.get(recentVersions.size()-1));\n        }\n\n        log.info(\"startupVersions size={} range=[{} to {}]\", startingVersions.size(), startingVersions.get(0), startingVersions.get(startingVersions.size()-1));\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if (ulog.existOldBufferLog()) {\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, core.getCoreDescriptor(), true);\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader!\");\n          log.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n\n        log.info(\"Begin buffering updates. core=[{}]\", coreName);\n        // recalling buffer updates will drop the old buffer tlog\n        ulog.bufferUpdates();\n\n        log.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leader.getCoreUrl(),\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n        \n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        //TODO since SOLR-11216, we probably won't need this\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(), recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSyncWithLeader peerSyncWithLeader = new PeerSyncWithLeader(core,\n              leader.getCoreUrl(), ulog.getNumRecordsToKeep());\n          boolean syncSuccess = peerSyncWithLeader.sync(recentVersions).isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            req.close();\n            log.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            log.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n\n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (successfulRecovery) {\n          log.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            log.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          log.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              log.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      log.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    log.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":["85212dad4ed576c7f7e6c165ee19e597b7b4efc8","425c2986e128f9e4aadd629cdf3b04e7aacb7c80"],"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a26124f3206f25b02a763a16b9ef8bfc3d81bc27","date":1542726079,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public final void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"Found new versions added after startup: num=[{}]\", oldIdx);\n          log.info(\"currentVersions size={} range=[{} to {}]\", recentVersions.size(), recentVersions.get(0), recentVersions.get(recentVersions.size()-1));\n        }\n\n        if (startingVersions.isEmpty()) {\n          log.info(\"startupVersions is empty\");\n        } else {\n          log.info(\"startupVersions size={} range=[{} to {}]\", startingVersions.size(), startingVersions.get(0), startingVersions.get(startingVersions.size()-1));\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if (ulog.existOldBufferLog()) {\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, core.getCoreDescriptor(), true);\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader!\");\n          log.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n\n        log.info(\"Begin buffering updates. core=[{}]\", coreName);\n        // recalling buffer updates will drop the old buffer tlog\n        ulog.bufferUpdates();\n\n        log.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leader.getCoreUrl(),\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n        \n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        //TODO since SOLR-11216, we probably won't need this\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(), recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSyncWithLeader peerSyncWithLeader = new PeerSyncWithLeader(core,\n              leader.getCoreUrl(), ulog.getNumRecordsToKeep());\n          boolean syncSuccess = peerSyncWithLeader.sync(recentVersions).isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            req.close();\n            log.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            log.info(\"Replaying updates buffered during PeerSync.\");\n            replayFuture = replay(core);\n\n            // sync success\n            successfulRecovery = true;\n            break;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (successfulRecovery) {\n          log.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            log.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          log.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              log.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      log.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    log.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public final void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"Found new versions added after startup: num=[{}]\", oldIdx);\n          log.info(\"currentVersions size={} range=[{} to {}]\", recentVersions.size(), recentVersions.get(0), recentVersions.get(recentVersions.size()-1));\n        }\n\n        if (startingVersions.isEmpty()) {\n          log.info(\"startupVersions is empty\");\n        } else {\n          log.info(\"startupVersions size={} range=[{} to {}]\", startingVersions.size(), startingVersions.get(0), startingVersions.get(startingVersions.size()-1));\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if (ulog.existOldBufferLog()) {\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, core.getCoreDescriptor(), true);\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader!\");\n          log.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n\n        log.info(\"Begin buffering updates. core=[{}]\", coreName);\n        // recalling buffer updates will drop the old buffer tlog\n        ulog.bufferUpdates();\n\n        log.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leader.getCoreUrl(),\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n        \n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        //TODO since SOLR-11216, we probably won't need this\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(), recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSyncWithLeader peerSyncWithLeader = new PeerSyncWithLeader(core,\n              leader.getCoreUrl(), ulog.getNumRecordsToKeep());\n          boolean syncSuccess = peerSyncWithLeader.sync(recentVersions).isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            req.close();\n            log.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            log.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n\n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (successfulRecovery) {\n          log.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            log.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          log.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              log.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      log.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    log.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":["61c45e99cf6676da48f19d7511c73712ad39402b"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bb222a3f9d9421d5c95afce73013fbd8de07ea1f","date":1543514331,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public final void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          this.coreDescriptor);\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n\n        if (oldIdx > 0) {\n          log.info(\"Found new versions added after startup: num=[{}]\", oldIdx);\n          log.info(\"currentVersions size={} range=[{} to {}]\", recentVersions.size(), recentVersions.get(0),\n              recentVersions.get(recentVersions.size() - 1));\n        }\n\n        if (startingVersions.isEmpty()) {\n          log.info(\"startupVersions is empty\");\n        } else {\n          log.info(\"startupVersions size={} range=[{} to {}]\", startingVersions.size(), startingVersions.get(0),\n              startingVersions.get(startingVersions.size() - 1));\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down. We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if (ulog.existOldBufferLog()) {\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or\n                                                                                            // it will close channels\n                                                                                            // though\n      try {\n        CloudDescriptor cloudDesc = this.coreDescriptor.getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, this.coreDescriptor, true);\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader!\");\n          log.info(\"Finished recovery process.\");\n          zkController.publish(this.coreDescriptor, Replica.State.ACTIVE);\n          return;\n        }\n\n        log.info(\"Begin buffering updates. core=[{}]\", coreName);\n        // recalling buffer updates will drop the old buffer tlog\n        ulog.bufferUpdates();\n\n        log.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(),\n            leader.getCoreUrl(),\n            ourUrl);\n        zkController.publish(this.coreDescriptor, Replica.State.RECOVERING);\n\n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state\n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        // TODO since SOLR-11216, we probably won't need this\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(),\n              recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSyncWithLeader peerSyncWithLeader = new PeerSyncWithLeader(core,\n              leader.getCoreUrl(), ulog.getNumRecordsToKeep());\n          boolean syncSuccess = peerSyncWithLeader.sync(recentVersions).isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            req.close();\n            log.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n\n            log.info(\"Replaying updates buffered during PeerSync.\");\n            replayFuture = replay(core);\n\n            // sync success\n            successfulRecovery = true;\n            break;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        log.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (successfulRecovery) {\n          log.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(this.coreDescriptor, Replica.State.ACTIVE);\n          } catch (Exception e) {\n            log.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n\n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \")\");\n\n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, this.coreDescriptor);\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 2 seconds and work up to a minute.\n          // Since we sleep at 2 seconds sub-intervals in\n          // order to check if we were closed, 30 is chosen as the maximum loopCount (2s * 30 = 1m).\n          double loopCount = Math.min(Math.pow(2, retries - 1), 30);\n          log.info(\"Wait [{}] seconds before trying to recover again (attempt={})\",\n              loopCount * startingRecoveryDelayMilliSeconds, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              log.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      log.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    log.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public final void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"Found new versions added after startup: num=[{}]\", oldIdx);\n          log.info(\"currentVersions size={} range=[{} to {}]\", recentVersions.size(), recentVersions.get(0), recentVersions.get(recentVersions.size()-1));\n        }\n\n        if (startingVersions.isEmpty()) {\n          log.info(\"startupVersions is empty\");\n        } else {\n          log.info(\"startupVersions size={} range=[{} to {}]\", startingVersions.size(), startingVersions.get(0), startingVersions.get(startingVersions.size()-1));\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if (ulog.existOldBufferLog()) {\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, core.getCoreDescriptor(), true);\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader!\");\n          log.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n\n        log.info(\"Begin buffering updates. core=[{}]\", coreName);\n        // recalling buffer updates will drop the old buffer tlog\n        ulog.bufferUpdates();\n\n        log.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leader.getCoreUrl(),\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n        \n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        //TODO since SOLR-11216, we probably won't need this\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(), recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSyncWithLeader peerSyncWithLeader = new PeerSyncWithLeader(core,\n              leader.getCoreUrl(), ulog.getNumRecordsToKeep());\n          boolean syncSuccess = peerSyncWithLeader.sync(recentVersions).isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            req.close();\n            log.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            log.info(\"Replaying updates buffered during PeerSync.\");\n            replayFuture = replay(core);\n\n            // sync success\n            successfulRecovery = true;\n            break;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (successfulRecovery) {\n          log.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            log.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          log.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              log.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      log.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    log.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":["e9c81f7e703d7ccca5bc78beb61253f0a8a22afd","ba0e7b86ac6002d5286b4589d87b3c80bbcabdc7","84f20f331d8001864545c7021812d8c6509c7593","420b2cd54774495de0bb67f068f5231f3da5e494","61c45e99cf6676da48f19d7511c73712ad39402b","425c2986e128f9e4aadd629cdf3b04e7aacb7c80"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad4957cde742defe6db19689abdc267c5d948066","date":1587990850,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public final void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          this.coreDescriptor);\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n\n        if (oldIdx > 0) {\n          log.info(\"Found new versions added after startup: num=[{}]\", oldIdx);\n          if (log.isInfoEnabled()) {\n            log.info(\"currentVersions size={} range=[{} to {}]\", recentVersions.size(), recentVersions.get(0),\n                recentVersions.get(recentVersions.size() - 1));\n          }\n        }\n\n        if (startingVersions.isEmpty()) {\n          log.info(\"startupVersions is empty\");\n        } else {\n          if (log.isInfoEnabled()) {\n            log.info(\"startupVersions size={} range=[{} to {}]\", startingVersions.size(), startingVersions.get(0),\n                startingVersions.get(startingVersions.size() - 1));\n          }\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down. We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if (ulog.existOldBufferLog()) {\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or\n                                                                                            // it will close channels\n                                                                                            // though\n      try {\n        CloudDescriptor cloudDesc = this.coreDescriptor.getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, this.coreDescriptor, true);\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader!\");\n          log.info(\"Finished recovery process.\");\n          zkController.publish(this.coreDescriptor, Replica.State.ACTIVE);\n          return;\n        }\n\n        log.info(\"Begin buffering updates. core=[{}]\", coreName);\n        // recalling buffer updates will drop the old buffer tlog\n        ulog.bufferUpdates();\n\n        if (log.isInfoEnabled()) {\n          log.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(),\n              leader.getCoreUrl(),\n              ourUrl);\n        }\n        zkController.publish(this.coreDescriptor, Replica.State.RECOVERING);\n\n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state\n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        // TODO since SOLR-11216, we probably won't need this\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          if (log.isInfoEnabled()) {\n            log.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(),\n                recoveringAfterStartup);\n          }\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSyncWithLeader peerSyncWithLeader = new PeerSyncWithLeader(core,\n              leader.getCoreUrl(), ulog.getNumRecordsToKeep());\n          boolean syncSuccess = peerSyncWithLeader.sync(recentVersions).isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            req.close();\n            log.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n\n            log.info(\"Replaying updates buffered during PeerSync.\");\n            replayFuture = replay(core);\n\n            // sync success\n            successfulRecovery = true;\n            break;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        log.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (successfulRecovery) {\n          log.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(this.coreDescriptor, Replica.State.ACTIVE);\n          } catch (Exception e) {\n            log.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n\n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          log.error(\"Recovery failed - trying again... ({})\", retries);\n\n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, this.coreDescriptor);\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 2 seconds and work up to a minute.\n          // Since we sleep at 2 seconds sub-intervals in\n          // order to check if we were closed, 30 is chosen as the maximum loopCount (2s * 30 = 1m).\n          double loopCount = Math.min(Math.pow(2, retries - 1), 30);\n          log.info(\"Wait [{}] seconds before trying to recover again (attempt={})\",\n              loopCount * startingRecoveryDelayMilliSeconds, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              log.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      log.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    log.info(\"Finished recovery process, successful=[{}]\", successfulRecovery);\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public final void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          this.coreDescriptor);\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n\n        if (oldIdx > 0) {\n          log.info(\"Found new versions added after startup: num=[{}]\", oldIdx);\n          log.info(\"currentVersions size={} range=[{} to {}]\", recentVersions.size(), recentVersions.get(0),\n              recentVersions.get(recentVersions.size() - 1));\n        }\n\n        if (startingVersions.isEmpty()) {\n          log.info(\"startupVersions is empty\");\n        } else {\n          log.info(\"startupVersions size={} range=[{} to {}]\", startingVersions.size(), startingVersions.get(0),\n              startingVersions.get(startingVersions.size() - 1));\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down. We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if (ulog.existOldBufferLog()) {\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or\n                                                                                            // it will close channels\n                                                                                            // though\n      try {\n        CloudDescriptor cloudDesc = this.coreDescriptor.getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, this.coreDescriptor, true);\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader!\");\n          log.info(\"Finished recovery process.\");\n          zkController.publish(this.coreDescriptor, Replica.State.ACTIVE);\n          return;\n        }\n\n        log.info(\"Begin buffering updates. core=[{}]\", coreName);\n        // recalling buffer updates will drop the old buffer tlog\n        ulog.bufferUpdates();\n\n        log.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(),\n            leader.getCoreUrl(),\n            ourUrl);\n        zkController.publish(this.coreDescriptor, Replica.State.RECOVERING);\n\n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state\n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        // TODO since SOLR-11216, we probably won't need this\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(),\n              recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSyncWithLeader peerSyncWithLeader = new PeerSyncWithLeader(core,\n              leader.getCoreUrl(), ulog.getNumRecordsToKeep());\n          boolean syncSuccess = peerSyncWithLeader.sync(recentVersions).isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            req.close();\n            log.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n\n            log.info(\"Replaying updates buffered during PeerSync.\");\n            replayFuture = replay(core);\n\n            // sync success\n            successfulRecovery = true;\n            break;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        log.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (successfulRecovery) {\n          log.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(this.coreDescriptor, Replica.State.ACTIVE);\n          } catch (Exception e) {\n            log.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n\n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \")\");\n\n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, this.coreDescriptor);\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 2 seconds and work up to a minute.\n          // Since we sleep at 2 seconds sub-intervals in\n          // order to check if we were closed, 30 is chosen as the maximum loopCount (2s * 30 = 1m).\n          double loopCount = Math.min(Math.pow(2, retries - 1), 30);\n          log.info(\"Wait [{}] seconds before trying to recover again (attempt={})\",\n              loopCount * startingRecoveryDelayMilliSeconds, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              log.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      log.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    log.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e6e01fcbda8c04bde654f99836a2f8813f9444f3","date":1589564391,"type":3,"author":"erick","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doSyncOrReplicateRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public final void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          this.coreDescriptor);\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n\n        if (oldIdx > 0) {\n          log.info(\"Found new versions added after startup: num=[{}]\", oldIdx);\n          if (log.isInfoEnabled()) {\n            log.info(\"currentVersions size={} range=[{} to {}]\", recentVersions.size(), recentVersions.get(0),\n                recentVersions.get(recentVersions.size() - 1));\n          }\n        }\n\n        if (startingVersions.isEmpty()) {\n          log.info(\"startupVersions is empty\");\n        } else {\n          if (log.isInfoEnabled()) {\n            log.info(\"startupVersions size={} range=[{} to {}]\", startingVersions.size(), startingVersions.get(0),\n                startingVersions.get(startingVersions.size() - 1));\n          }\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down. We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if (ulog.existOldBufferLog()) {\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or\n                                                                                            // it will close channels\n                                                                                            // though\n      try {\n        CloudDescriptor cloudDesc = this.coreDescriptor.getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, this.coreDescriptor, true);\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader!\");\n          log.info(\"Finished recovery process.\");\n          zkController.publish(this.coreDescriptor, Replica.State.ACTIVE);\n          return;\n        }\n\n        log.info(\"Begin buffering updates. core=[{}]\", coreName);\n        // recalling buffer updates will drop the old buffer tlog\n        ulog.bufferUpdates();\n\n        if (log.isInfoEnabled()) {\n          log.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(),\n              leader.getCoreUrl(),\n              ourUrl);\n        }\n        zkController.publish(this.coreDescriptor, Replica.State.RECOVERING);\n\n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state\n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        // TODO since SOLR-11216, we probably won't need this\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          if (log.isInfoEnabled()) {\n            log.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(),\n                recoveringAfterStartup);\n          }\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          boolean syncSuccess;\n          try (PeerSyncWithLeader peerSyncWithLeader = new PeerSyncWithLeader(core,\n              leader.getCoreUrl(), ulog.getNumRecordsToKeep())) {\n            syncSuccess = peerSyncWithLeader.sync(recentVersions).isSuccess();\n          }\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            req.close();\n            log.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n\n            log.info(\"Replaying updates buffered during PeerSync.\");\n            replayFuture = replay(core);\n\n            // sync success\n            successfulRecovery = true;\n            break;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        log.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (successfulRecovery) {\n          log.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(this.coreDescriptor, Replica.State.ACTIVE);\n          } catch (Exception e) {\n            log.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n\n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          log.error(\"Recovery failed - trying again... ({})\", retries);\n\n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, this.coreDescriptor);\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 2 seconds and work up to a minute.\n          // Since we sleep at 2 seconds sub-intervals in\n          // order to check if we were closed, 30 is chosen as the maximum loopCount (2s * 30 = 1m).\n          double loopCount = Math.min(Math.pow(2, retries - 1), 30);\n          log.info(\"Wait [{}] seconds before trying to recover again (attempt={})\",\n              loopCount * startingRecoveryDelayMilliSeconds, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              log.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      log.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    log.info(\"Finished recovery process, successful=[{}]\", successfulRecovery);\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public final void doSyncOrReplicateRecovery(SolrCore core) throws Exception {\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          this.coreDescriptor);\n      return;\n    }\n\n    // we temporary ignore peersync for tlog replicas\n    boolean firstTime = replicaType != Replica.Type.TLOG;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n\n        if (oldIdx > 0) {\n          log.info(\"Found new versions added after startup: num=[{}]\", oldIdx);\n          if (log.isInfoEnabled()) {\n            log.info(\"currentVersions size={} range=[{} to {}]\", recentVersions.size(), recentVersions.get(0),\n                recentVersions.get(recentVersions.size() - 1));\n          }\n        }\n\n        if (startingVersions.isEmpty()) {\n          log.info(\"startupVersions is empty\");\n        } else {\n          if (log.isInfoEnabled()) {\n            log.info(\"startupVersions size={} range=[{} to {}]\", startingVersions.size(), startingVersions.get(0),\n                startingVersions.get(startingVersions.size() - 1));\n          }\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down. We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if (ulog.existOldBufferLog()) {\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (replicaType == Replica.Type.TLOG) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    final String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or\n                                                                                            // it will close channels\n                                                                                            // though\n      try {\n        CloudDescriptor cloudDesc = this.coreDescriptor.getCloudDescriptor();\n        final Replica leader = pingLeader(ourUrl, this.coreDescriptor, true);\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        boolean isLeader = leader.getCoreUrl().equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader!\");\n          log.info(\"Finished recovery process.\");\n          zkController.publish(this.coreDescriptor, Replica.State.ACTIVE);\n          return;\n        }\n\n        log.info(\"Begin buffering updates. core=[{}]\", coreName);\n        // recalling buffer updates will drop the old buffer tlog\n        ulog.bufferUpdates();\n\n        if (log.isInfoEnabled()) {\n          log.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(),\n              leader.getCoreUrl(),\n              ourUrl);\n        }\n        zkController.publish(this.coreDescriptor, Replica.State.RECOVERING);\n\n        final Slice slice = zkStateReader.getClusterState().getCollection(cloudDesc.getCollectionName())\n            .getSlice(cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leader.getBaseUrl(), leader.getCoreName(), slice);\n\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state\n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        // TODO since SOLR-11216, we probably won't need this\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          if (log.isInfoEnabled()) {\n            log.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leader.getCoreUrl(),\n                recoveringAfterStartup);\n          }\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSyncWithLeader peerSyncWithLeader = new PeerSyncWithLeader(core,\n              leader.getCoreUrl(), ulog.getNumRecordsToKeep());\n          boolean syncSuccess = peerSyncWithLeader.sync(recentVersions).isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            req.close();\n            log.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n\n            log.info(\"Replaying updates buffered during PeerSync.\");\n            replayFuture = replay(core);\n\n            // sync success\n            successfulRecovery = true;\n            break;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        log.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leader);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (successfulRecovery) {\n          log.info(\"Registering as Active after recovery.\");\n          try {\n            if (replicaType == Replica.Type.TLOG) {\n              zkController.startReplicationFromLeader(coreName, true);\n            }\n            zkController.publish(this.coreDescriptor, Replica.State.ACTIVE);\n          } catch (Exception e) {\n            log.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n\n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          log.error(\"Recovery failed - trying again... ({})\", retries);\n\n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, this.coreDescriptor);\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 2 seconds and work up to a minute.\n          // Since we sleep at 2 seconds sub-intervals in\n          // order to check if we were closed, 30 is chosen as the maximum loopCount (2s * 30 = 1m).\n          double loopCount = Math.min(Math.pow(2, retries - 1), 30);\n          log.info(\"Wait [{}] seconds before trying to recover again (attempt={})\",\n              loopCount * startingRecoveryDelayMilliSeconds, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              log.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      log.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    log.info(\"Finished recovery process, successful=[{}]\", successfulRecovery);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"61c45e99cf6676da48f19d7511c73712ad39402b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c7ff8a6fbdc9c2d84bc6e9e71e1c738e49c441ac":["7a23cf16c8fa265dc0a564adcabb55e3f054e0ac"],"e6e01fcbda8c04bde654f99836a2f8813f9444f3":["ad4957cde742defe6db19689abdc267c5d948066"],"bb222a3f9d9421d5c95afce73013fbd8de07ea1f":["a26124f3206f25b02a763a16b9ef8bfc3d81bc27"],"344b0840364d990b29b97467bfcc766ff8325d11":["61c45e99cf6676da48f19d7511c73712ad39402b"],"85212dad4ed576c7f7e6c165ee19e597b7b4efc8":["344b0840364d990b29b97467bfcc766ff8325d11"],"ad4957cde742defe6db19689abdc267c5d948066":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"84f20f331d8001864545c7021812d8c6509c7593":["85212dad4ed576c7f7e6c165ee19e597b7b4efc8"],"b869898f50ca80263bac2e3ae0949f7700e5c977":["c7ff8a6fbdc9c2d84bc6e9e71e1c738e49c441ac"],"1455c941cc4ce652efc776fc23471b0e499246f6":["84f20f331d8001864545c7021812d8c6509c7593"],"e9017cf144952056066919f1ebc7897ff9bd71b1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","61c45e99cf6676da48f19d7511c73712ad39402b"],"425c2986e128f9e4aadd629cdf3b04e7aacb7c80":["e9c81f7e703d7ccca5bc78beb61253f0a8a22afd"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":["b70042a8a492f7054d480ccdd2be9796510d4327","ba0e7b86ac6002d5286b4589d87b3c80bbcabdc7"],"7a23cf16c8fa265dc0a564adcabb55e3f054e0ac":["e9017cf144952056066919f1ebc7897ff9bd71b1","344b0840364d990b29b97467bfcc766ff8325d11"],"420b2cd54774495de0bb67f068f5231f3da5e494":["425c2986e128f9e4aadd629cdf3b04e7aacb7c80"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b70042a8a492f7054d480ccdd2be9796510d4327":["84f20f331d8001864545c7021812d8c6509c7593","1455c941cc4ce652efc776fc23471b0e499246f6"],"a26124f3206f25b02a763a16b9ef8bfc3d81bc27":["420b2cd54774495de0bb67f068f5231f3da5e494"],"f592209545c71895260367152601e9200399776d":["84f20f331d8001864545c7021812d8c6509c7593","1455c941cc4ce652efc776fc23471b0e499246f6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e6e01fcbda8c04bde654f99836a2f8813f9444f3"],"e9c81f7e703d7ccca5bc78beb61253f0a8a22afd":["ba0e7b86ac6002d5286b4589d87b3c80bbcabdc7"],"ba0e7b86ac6002d5286b4589d87b3c80bbcabdc7":["1455c941cc4ce652efc776fc23471b0e499246f6"],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":["f592209545c71895260367152601e9200399776d","ba0e7b86ac6002d5286b4589d87b3c80bbcabdc7"]},"commit2Childs":{"61c45e99cf6676da48f19d7511c73712ad39402b":["344b0840364d990b29b97467bfcc766ff8325d11","e9017cf144952056066919f1ebc7897ff9bd71b1"],"c7ff8a6fbdc9c2d84bc6e9e71e1c738e49c441ac":["b869898f50ca80263bac2e3ae0949f7700e5c977"],"e6e01fcbda8c04bde654f99836a2f8813f9444f3":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"bb222a3f9d9421d5c95afce73013fbd8de07ea1f":["ad4957cde742defe6db19689abdc267c5d948066"],"344b0840364d990b29b97467bfcc766ff8325d11":["85212dad4ed576c7f7e6c165ee19e597b7b4efc8","7a23cf16c8fa265dc0a564adcabb55e3f054e0ac"],"85212dad4ed576c7f7e6c165ee19e597b7b4efc8":["84f20f331d8001864545c7021812d8c6509c7593"],"ad4957cde742defe6db19689abdc267c5d948066":["e6e01fcbda8c04bde654f99836a2f8813f9444f3"],"84f20f331d8001864545c7021812d8c6509c7593":["1455c941cc4ce652efc776fc23471b0e499246f6","b70042a8a492f7054d480ccdd2be9796510d4327","f592209545c71895260367152601e9200399776d"],"b869898f50ca80263bac2e3ae0949f7700e5c977":[],"1455c941cc4ce652efc776fc23471b0e499246f6":["b70042a8a492f7054d480ccdd2be9796510d4327","f592209545c71895260367152601e9200399776d","ba0e7b86ac6002d5286b4589d87b3c80bbcabdc7"],"e9017cf144952056066919f1ebc7897ff9bd71b1":["7a23cf16c8fa265dc0a564adcabb55e3f054e0ac"],"7a23cf16c8fa265dc0a564adcabb55e3f054e0ac":["c7ff8a6fbdc9c2d84bc6e9e71e1c738e49c441ac"],"425c2986e128f9e4aadd629cdf3b04e7aacb7c80":["420b2cd54774495de0bb67f068f5231f3da5e494"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":[],"420b2cd54774495de0bb67f068f5231f3da5e494":["a26124f3206f25b02a763a16b9ef8bfc3d81bc27"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["61c45e99cf6676da48f19d7511c73712ad39402b","e9017cf144952056066919f1ebc7897ff9bd71b1"],"b70042a8a492f7054d480ccdd2be9796510d4327":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5"],"a26124f3206f25b02a763a16b9ef8bfc3d81bc27":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"f592209545c71895260367152601e9200399776d":["7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"e9c81f7e703d7ccca5bc78beb61253f0a8a22afd":["425c2986e128f9e4aadd629cdf3b04e7aacb7c80"],"ba0e7b86ac6002d5286b4589d87b3c80bbcabdc7":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","e9c81f7e703d7ccca5bc78beb61253f0a8a22afd","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":[]},"heads":["b869898f50ca80263bac2e3ae0949f7700e5c977","0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","cd5edd1f2b162a5cfa08efd17851a07373a96817","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}