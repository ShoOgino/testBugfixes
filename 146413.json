{"path":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","commits":[{"id":"9d7606b7ab7992bc238d10c3bddfe82a639d212a","date":1295971955,"type":0,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(\n            MockTokenizer.WHITESPACE, true, usePayload)));\n    int numDocs = 499;\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(\" \");\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.YES,\n          Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    for (int i = 0; i < 39 * RANDOM_MULTIPLIER; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c19f985e36a65cc969e8e564fe337a0d41512075","date":1296330536,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","sourceNew":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(\n                                                                    MockTokenizer.WHITESPACE, true, usePayload)).setMergePolicy(newInOrderLogMergePolicy()));\n    int numDocs = 499;\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(\" \");\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.YES,\n          Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    for (int i = 0; i < 39 * RANDOM_MULTIPLIER; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(\n            MockTokenizer.WHITESPACE, true, usePayload)));\n    int numDocs = 499;\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(\" \");\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.YES,\n          Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    for (int i = 0; i < 39 * RANDOM_MULTIPLIER; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(\n                                                                    MockTokenizer.WHITESPACE, true, usePayload)).setMergePolicy(newInOrderLogMergePolicy()));\n    int numDocs = 499;\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(\" \");\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.YES,\n          Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    for (int i = 0; i < 39 * RANDOM_MULTIPLIER; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":0,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(\n                                                                    MockTokenizer.WHITESPACE, true, usePayload)).setMergePolicy(newInOrderLogMergePolicy()));\n    int numDocs = 499;\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(\" \");\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.YES,\n          Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    for (int i = 0; i < 39 * RANDOM_MULTIPLIER; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"01e5948db9a07144112d2f08f28ca2e3cd880348","date":1301759232,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","sourceNew":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(\n                                                                    MockTokenizer.WHITESPACE, true, usePayload)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = 499;\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(\" \");\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.YES,\n          Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    for (int i = 0; i < 39 * RANDOM_MULTIPLIER; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(\n                                                                    MockTokenizer.WHITESPACE, true, usePayload)).setMergePolicy(newInOrderLogMergePolicy()));\n    int numDocs = 499;\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(\" \");\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.YES,\n          Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    for (int i = 0; i < 39 * RANDOM_MULTIPLIER; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"45669a651c970812a680841b97a77cce06af559f","date":1301922222,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","sourceNew":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(\n                                                                    MockTokenizer.WHITESPACE, true, usePayload)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = 499;\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(\" \");\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.YES,\n          Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    for (int i = 0; i < 39 * RANDOM_MULTIPLIER; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(\n                                                                    MockTokenizer.WHITESPACE, true, usePayload)).setMergePolicy(newInOrderLogMergePolicy()));\n    int numDocs = 499;\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(\" \");\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.YES,\n          Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    for (int i = 0; i < 39 * RANDOM_MULTIPLIER; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","sourceNew":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = 499;\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(\" \");\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.YES,\n          Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    for (int i = 0; i < 39 * RANDOM_MULTIPLIER; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(\n                                                                    MockTokenizer.WHITESPACE, true, usePayload)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = 499;\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(\" \");\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.YES,\n          Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    for (int i = 0; i < 39 * RANDOM_MULTIPLIER; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","sourceNew":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = 499;\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(\" \");\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.YES,\n          Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    for (int i = 0; i < 39 * RANDOM_MULTIPLIER; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(\n                                                                    MockTokenizer.WHITESPACE, true, usePayload)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = 499;\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(\" \");\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.YES,\n          Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    for (int i = 0; i < 39 * RANDOM_MULTIPLIER; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","sourceNew":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = 499;\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(\" \");\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.YES,\n          Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    for (int i = 0; i < 39 * RANDOM_MULTIPLIER; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(\n                                                                    MockTokenizer.WHITESPACE, true, usePayload)).setMergePolicy(newInOrderLogMergePolicy()));\n    int numDocs = 499;\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(\" \");\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.YES,\n          Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    for (int i = 0; i < 39 * RANDOM_MULTIPLIER; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","sourceNew":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = 499;\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(\" \");\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.YES,\n          Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    for (int i = 0; i < 39 * RANDOM_MULTIPLIER; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(\n                                                                    MockTokenizer.WHITESPACE, true, usePayload)).setMergePolicy(newInOrderLogMergePolicy()));\n    int numDocs = 499;\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(\" \");\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.YES,\n          Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    for (int i = 0; i < 39 * RANDOM_MULTIPLIER; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ff6fd241dc6610f7f81b62e3ba4cedf105939623","date":1307331653,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","sourceNew":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = TEST_NIGHTLY ? 499 : 131;\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(\" \");\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.NO,\n          Field.Index.ANALYZED_NO_NORMS));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    for (int i = 0; i < (TEST_NIGHTLY ? 39 : 13) * RANDOM_MULTIPLIER; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = 499;\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(\" \");\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.YES,\n          Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    for (int i = 0; i < 39 * RANDOM_MULTIPLIER; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"79c2cb24929f2649a8875fb629086171f914d5ce","date":1307332717,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","sourceNew":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = TEST_NIGHTLY ? 499 : 131;\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(\" \");\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.NO,\n          Field.Index.ANALYZED_NO_NORMS));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    for (int i = 0; i < (TEST_NIGHTLY ? 39 : 13) * RANDOM_MULTIPLIER; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = 499;\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(\" \");\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.YES,\n          Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    for (int i = 0; i < 39 * RANDOM_MULTIPLIER; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cca56866c19997e28ef073622656669c15210540","date":1307449014,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","sourceNew":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = TEST_NIGHTLY ? 499 : 49;\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(' ');\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.NO,\n          Field.Index.ANALYZED_NO_NORMS));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    for (int i = 0; i < (TEST_NIGHTLY ? 39 : 13) * RANDOM_MULTIPLIER; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = TEST_NIGHTLY ? 499 : 131;\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(\" \");\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.NO,\n          Field.Index.ANALYZED_NO_NORMS));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    for (int i = 0; i < (TEST_NIGHTLY ? 39 : 13) * RANDOM_MULTIPLIER; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0762b640e0d0d12b6edb96db68986e13145c3484","date":1307575932,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","sourceNew":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = atLeast(49);\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(' ');\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.NO,\n          Field.Index.ANALYZED_NO_NORMS));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    int num = atLeast(13);\n    for (int i = 0; i < num; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = TEST_NIGHTLY ? 499 : 49;\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(' ');\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.NO,\n          Field.Index.ANALYZED_NO_NORMS));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    for (int i = 0; i < (TEST_NIGHTLY ? 39 : 13) * RANDOM_MULTIPLIER; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"77cf4379b2824f6ea34b091c495d6e95c38ff9e2","date":1307610475,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","sourceNew":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = atLeast(49);\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(' ');\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.NO,\n          Field.Index.ANALYZED_NO_NORMS));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    int num = atLeast(13);\n    for (int i = 0; i < num; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = 499;\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(\" \");\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.YES,\n          Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    for (int i = 0; i < 39 * RANDOM_MULTIPLIER; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","date":1307729864,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","sourceNew":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = atLeast(49);\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(' ');\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.NO,\n          Field.Index.ANALYZED_NO_NORMS));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    int num = atLeast(13);\n    for (int i = 0; i < num; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = TEST_NIGHTLY ? 499 : 131;\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(\" \");\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.NO,\n          Field.Index.ANALYZED_NO_NORMS));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    for (int i = 0; i < (TEST_NIGHTLY ? 39 : 13) * RANDOM_MULTIPLIER; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"60ba444201d2570214b6fcf1d15600dc1a01f548","date":1313868045,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","sourceNew":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n                                                     newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    writer.w.setInfoStream(VERBOSE ? System.out : null);\n    int numDocs = atLeast(49);\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(' ');\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.NO,\n          Field.Index.ANALYZED_NO_NORMS));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    int num = atLeast(13);\n    for (int i = 0; i < num; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = atLeast(49);\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(' ');\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.NO,\n          Field.Index.ANALYZED_NO_NORMS));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    int num = atLeast(13);\n    for (int i = 0; i < num; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","sourceNew":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n                                                     newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    writer.w.setInfoStream(VERBOSE ? System.out : null);\n    int numDocs = atLeast(49);\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setOmitNorms(true);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(' ');\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), customType));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    int num = atLeast(13);\n    for (int i = 0; i < num; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n                                                     newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    writer.w.setInfoStream(VERBOSE ? System.out : null);\n    int numDocs = atLeast(49);\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(' ');\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), Field.Store.NO,\n          Field.Index.ANALYZED_NO_NORMS));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    int num = atLeast(13);\n    for (int i = 0; i < num; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"06584e6e98d592b34e1329b384182f368d2025e8","date":1320850353,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","sourceNew":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n                                                     newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = atLeast(49);\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setOmitNorms(true);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(' ');\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), customType));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    int num = atLeast(13);\n    for (int i = 0; i < num; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n                                                     newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    writer.w.setInfoStream(VERBOSE ? System.out : null);\n    int numDocs = atLeast(49);\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setOmitNorms(true);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(' ');\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), customType));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    int num = atLeast(13);\n    for (int i = 0; i < num; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"872cff1d3a554e0cd64014cd97f88d3002b0f491","date":1323024658,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","sourceNew":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n                                                     newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = atLeast(49);\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setOmitNorms(true);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(' ');\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), customType));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    int num = atLeast(13);\n    for (int i = 0; i < num; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsEnum = _TestUtil.docs(random, context.reader, fieldName, bytes, null, null, true);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsEnum);\n          continue;\n        }\n        assertNotNull(docsEnum);\n        docsEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsEnum.docID());\n            assertEquals(docsEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n                                                     newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = atLeast(49);\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setOmitNorms(true);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(' ');\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), customType));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    int num = atLeast(13);\n    for (int i = 0; i < num; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b65b350ca9588f9fc76ce7d6804160d06c45ff42","date":1323026297,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","sourceNew":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n                                                     newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = atLeast(49);\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setOmitNorms(true);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(' ');\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), customType));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    int num = atLeast(13);\n    for (int i = 0; i < num; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsEnum = _TestUtil.docs(random, context.reader, fieldName, bytes, null, null, true);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsEnum);\n          continue;\n        }\n        assertNotNull(docsEnum);\n        docsEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsEnum.docID());\n            assertEquals(docsEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n                                                     newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = atLeast(49);\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setOmitNorms(true);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(' ');\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), customType));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    int num = atLeast(13);\n    for (int i = 0; i < num; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsAndPosEnum = getDocsEnum(context.reader, bytes, true, null);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsAndPosEnum);\n          continue;\n        }\n        assertNotNull(docsAndPosEnum);\n        docsAndPosEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsAndPosEnum.docID());\n            assertEquals(docsAndPosEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsAndPosEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsAndPosEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsAndPosEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsAndPosEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6fff8f4b218bd0626afcdce82027bafeb84a50a4","date":1327229950,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","sourceNew":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n                                                     newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = atLeast(49);\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setOmitNorms(true);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(' ');\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), customType));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    int num = atLeast(13);\n    for (int i = 0; i < num; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader().maxDoc();\n        DocsEnum docsEnum = _TestUtil.docs(random, context.reader(), fieldName, bytes, null, null, true);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsEnum);\n          continue;\n        }\n        assertNotNull(docsEnum);\n        docsEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsEnum.docID());\n            assertEquals(docsEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n                                                     newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = atLeast(49);\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setOmitNorms(true);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(' ');\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), customType));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    int num = atLeast(13);\n    for (int i = 0; i < num; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsEnum = _TestUtil.docs(random, context.reader, fieldName, bytes, null, null, true);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsEnum);\n          continue;\n        }\n        assertNotNull(docsEnum);\n        docsEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsEnum.docID());\n            assertEquals(docsEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8493985e6883b3fa8231d172694d2aa3a85cb182","date":1327920390,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","sourceNew":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n                                                     newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = atLeast(49);\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setOmitNorms(true);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(' ');\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), customType));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    int num = atLeast(13);\n    for (int i = 0; i < num; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      IndexReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader().maxDoc();\n        DocsEnum docsEnum = _TestUtil.docs(random, context.reader(), fieldName, bytes, null, null, true);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsEnum);\n          continue;\n        }\n        assertNotNull(docsEnum);\n        docsEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsEnum.docID());\n            assertEquals(docsEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n                                                     newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = atLeast(49);\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setOmitNorms(true);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(' ');\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), customType));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    int num = atLeast(13);\n    for (int i = 0; i < num; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader().maxDoc();\n        DocsEnum docsEnum = _TestUtil.docs(random, context.reader(), fieldName, bytes, null, null, true);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsEnum);\n          continue;\n        }\n        assertNotNull(docsEnum);\n        docsEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsEnum.docID());\n            assertEquals(docsEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5cab9a86bd67202d20b6adc463008c8e982b070a","date":1327966443,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","sourceNew":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n                                                     newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = atLeast(49);\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setOmitNorms(true);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(' ');\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), customType));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    int num = atLeast(13);\n    for (int i = 0; i < num; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      IndexReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader().maxDoc();\n        DocsEnum docsEnum = _TestUtil.docs(random, context.reader(), fieldName, bytes, null, null, true);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsEnum);\n          continue;\n        }\n        assertNotNull(docsEnum);\n        docsEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsEnum.docID());\n            assertEquals(docsEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n                                                     newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = atLeast(49);\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setOmitNorms(true);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(' ');\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), customType));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    int num = atLeast(13);\n    for (int i = 0; i < num; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      ReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader.maxDoc();\n        DocsEnum docsEnum = _TestUtil.docs(random, context.reader, fieldName, bytes, null, null, true);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsEnum);\n          continue;\n        }\n        assertNotNull(docsEnum);\n        docsEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsEnum.docID());\n            assertEquals(docsEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocsAndPositions#testRandomDocs().mjava","sourceNew":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n                                                     newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = atLeast(49);\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setOmitNorms(true);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(' ');\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), customType));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    int num = atLeast(13);\n    for (int i = 0; i < num; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      IndexReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader().maxDoc();\n        DocsEnum docsEnum = _TestUtil.docs(random, context.reader(), fieldName, bytes, null, null, true);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsEnum);\n          continue;\n        }\n        assertNotNull(docsEnum);\n        docsEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsEnum.docID());\n            assertEquals(docsEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomDocs() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n                                                     newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    int numDocs = atLeast(49);\n    int max = 15678;\n    int term = random.nextInt(max);\n    int[] freqInDoc = new int[numDocs];\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setOmitNorms(true);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      StringBuilder builder = new StringBuilder();\n      for (int j = 0; j < 199; j++) {\n        int nextInt = random.nextInt(max);\n        builder.append(nextInt).append(' ');\n        if (nextInt == term) {\n          freqInDoc[i]++;\n        }\n      }\n      doc.add(newField(fieldName, builder.toString(), customType));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    int num = atLeast(13);\n    for (int i = 0; i < num; i++) {\n      BytesRef bytes = new BytesRef(\"\" + term);\n      IndexReaderContext topReaderContext = reader.getTopReaderContext();\n      AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);\n      for (AtomicReaderContext context : leaves) {\n        int maxDoc = context.reader().maxDoc();\n        DocsEnum docsEnum = _TestUtil.docs(random, context.reader(), fieldName, bytes, null, null, true);\n        if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {\n          assertNull(docsEnum);\n          continue;\n        }\n        assertNotNull(docsEnum);\n        docsEnum.nextDoc();\n        for (int j = 0; j < maxDoc; j++) {\n          if (freqInDoc[context.docBase + j] != 0) {\n            assertEquals(j, docsEnum.docID());\n            assertEquals(docsEnum.freq(), freqInDoc[context.docBase +j]);\n            if (i % 2 == 0 && random.nextInt(10) == 0) {\n              int next = findNext(freqInDoc, context.docBase+j+1, context.docBase + maxDoc) - context.docBase;\n              int advancedTo = docsEnum.advance(next);\n              if (next >= maxDoc) {\n                assertEquals(DocsEnum.NO_MORE_DOCS, advancedTo);\n              } else {\n                assertTrue(\"advanced to: \" +advancedTo + \" but should be <= \" + next, next >= advancedTo);  \n              }\n            } else {\n              docsEnum.nextDoc();\n            }\n          } \n        }\n        assertEquals(\"docBase: \" + context.docBase + \" maxDoc: \" + maxDoc + \" \" + docsEnum.getClass(), DocsEnum.NO_MORE_DOCS, docsEnum.docID());\n      }\n      \n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"0762b640e0d0d12b6edb96db68986e13145c3484":["cca56866c19997e28ef073622656669c15210540"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"c19f985e36a65cc969e8e564fe337a0d41512075":["9d7606b7ab7992bc238d10c3bddfe82a639d212a"],"01e5948db9a07144112d2f08f28ca2e3cd880348":["c19f985e36a65cc969e8e564fe337a0d41512075"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["01e5948db9a07144112d2f08f28ca2e3cd880348"],"79c2cb24929f2649a8875fb629086171f914d5ce":["a3776dccca01c11e7046323cfad46a3b4a471233","ff6fd241dc6610f7f81b62e3ba4cedf105939623"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c19f985e36a65cc969e8e564fe337a0d41512075"],"9d7606b7ab7992bc238d10c3bddfe82a639d212a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["60ba444201d2570214b6fcf1d15600dc1a01f548"],"cca56866c19997e28ef073622656669c15210540":["ff6fd241dc6610f7f81b62e3ba4cedf105939623"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c19f985e36a65cc969e8e564fe337a0d41512075"],"8493985e6883b3fa8231d172694d2aa3a85cb182":["6fff8f4b218bd0626afcdce82027bafeb84a50a4"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["79c2cb24929f2649a8875fb629086171f914d5ce","0762b640e0d0d12b6edb96db68986e13145c3484"],"ff6fd241dc6610f7f81b62e3ba4cedf105939623":["f2c5f0cb44df114db4228c8f77861714b5cabaea"],"06584e6e98d592b34e1329b384182f368d2025e8":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"6fff8f4b218bd0626afcdce82027bafeb84a50a4":["872cff1d3a554e0cd64014cd97f88d3002b0f491"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["29ef99d61cda9641b6250bf9567329a6e65f901d","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"872cff1d3a554e0cd64014cd97f88d3002b0f491":["06584e6e98d592b34e1329b384182f368d2025e8"],"962d04139994fce5193143ef35615499a9a96d78":["45669a651c970812a680841b97a77cce06af559f","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["872cff1d3a554e0cd64014cd97f88d3002b0f491","8493985e6883b3fa8231d172694d2aa3a85cb182"],"60ba444201d2570214b6fcf1d15600dc1a01f548":["0762b640e0d0d12b6edb96db68986e13145c3484"],"b65b350ca9588f9fc76ce7d6804160d06c45ff42":["06584e6e98d592b34e1329b384182f368d2025e8","872cff1d3a554e0cd64014cd97f88d3002b0f491"],"a3776dccca01c11e7046323cfad46a3b4a471233":["c19f985e36a65cc969e8e564fe337a0d41512075","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":["135621f3a0670a9394eb563224a3b76cc4dddc0f","0762b640e0d0d12b6edb96db68986e13145c3484"],"45669a651c970812a680841b97a77cce06af559f":["bde51b089eb7f86171eb3406e38a274743f9b7ac","01e5948db9a07144112d2f08f28ca2e3cd880348"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"0762b640e0d0d12b6edb96db68986e13145c3484":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","60ba444201d2570214b6fcf1d15600dc1a01f548","77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c19f985e36a65cc969e8e564fe337a0d41512075":["01e5948db9a07144112d2f08f28ca2e3cd880348","29ef99d61cda9641b6250bf9567329a6e65f901d","bde51b089eb7f86171eb3406e38a274743f9b7ac","a3776dccca01c11e7046323cfad46a3b4a471233"],"01e5948db9a07144112d2f08f28ca2e3cd880348":["f2c5f0cb44df114db4228c8f77861714b5cabaea","45669a651c970812a680841b97a77cce06af559f"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["ff6fd241dc6610f7f81b62e3ba4cedf105939623","135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","a3776dccca01c11e7046323cfad46a3b4a471233"],"79c2cb24929f2649a8875fb629086171f914d5ce":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"9d7606b7ab7992bc238d10c3bddfe82a639d212a":["c19f985e36a65cc969e8e564fe337a0d41512075"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["29ef99d61cda9641b6250bf9567329a6e65f901d","9d7606b7ab7992bc238d10c3bddfe82a639d212a","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["06584e6e98d592b34e1329b384182f368d2025e8"],"cca56866c19997e28ef073622656669c15210540":["0762b640e0d0d12b6edb96db68986e13145c3484"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["45669a651c970812a680841b97a77cce06af559f"],"8493985e6883b3fa8231d172694d2aa3a85cb182":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":[],"ff6fd241dc6610f7f81b62e3ba4cedf105939623":["79c2cb24929f2649a8875fb629086171f914d5ce","cca56866c19997e28ef073622656669c15210540"],"06584e6e98d592b34e1329b384182f368d2025e8":["872cff1d3a554e0cd64014cd97f88d3002b0f491","b65b350ca9588f9fc76ce7d6804160d06c45ff42"],"6fff8f4b218bd0626afcdce82027bafeb84a50a4":["8493985e6883b3fa8231d172694d2aa3a85cb182"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"872cff1d3a554e0cd64014cd97f88d3002b0f491":["6fff8f4b218bd0626afcdce82027bafeb84a50a4","5cab9a86bd67202d20b6adc463008c8e982b070a","b65b350ca9588f9fc76ce7d6804160d06c45ff42"],"962d04139994fce5193143ef35615499a9a96d78":[],"5cab9a86bd67202d20b6adc463008c8e982b070a":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"60ba444201d2570214b6fcf1d15600dc1a01f548":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"b65b350ca9588f9fc76ce7d6804160d06c45ff42":[],"a3776dccca01c11e7046323cfad46a3b4a471233":["79c2cb24929f2649a8875fb629086171f914d5ce"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":[],"45669a651c970812a680841b97a77cce06af559f":["962d04139994fce5193143ef35615499a9a96d78"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","962d04139994fce5193143ef35615499a9a96d78","b65b350ca9588f9fc76ce7d6804160d06c45ff42","77cf4379b2824f6ea34b091c495d6e95c38ff9e2","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}