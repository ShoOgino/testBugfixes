{"path":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","commits":[{"id":"fe19cbe25754c715a0232f453039383119fc122c","date":1306110991,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","pathOld":"/dev/null","sourceNew":"  synchronized void openDocStores(SegmentInfo si) throws IOException {\n    \n    assert si.name.equals(segment);\n    \n    if (fieldsReaderOrig == null) {\n      final Directory storeDir;\n      if (si.getDocStoreOffset() != -1) {\n        if (si.getDocStoreIsCompoundFile()) {\n          assert storeCFSReader == null;\n          storeCFSReader = new CompoundFileReader(dir,\n              IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n              readBufferSize);\n          storeDir = storeCFSReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n      } else if (si.getUseCompoundFile()) {\n        // In some cases, we were originally opened when CFS\n        // was not used, but then we are asked to open doc\n        // stores after the segment has switched to CFS\n        if (cfsReader == null) {\n          cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n        }\n        storeDir = cfsReader;\n        assert storeDir != null;\n      } else {\n        storeDir = dir;\n        assert storeDir != null;\n      }\n      \n      final String storesSegment = si.getDocStoreSegment();\n      fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n          si.getDocStoreOffset(), si.docCount);\n      \n      // Verify two sources of \"maxDoc\" agree:\n      if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n      \n      if (si.getHasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae132b768aece5bf21cda14e2f17fba66eb6f7d6","date":1306128032,"type":0,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","pathOld":"/dev/null","sourceNew":"  synchronized void openDocStores(SegmentInfo si) throws IOException {\n    \n    assert si.name.equals(segment);\n    \n    if (fieldsReaderOrig == null) {\n      final Directory storeDir;\n      if (si.getDocStoreOffset() != -1) {\n        if (si.getDocStoreIsCompoundFile()) {\n          assert storeCFSReader == null;\n          storeCFSReader = new CompoundFileReader(dir,\n              IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n              readBufferSize);\n          storeDir = storeCFSReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n      } else if (si.getUseCompoundFile()) {\n        // In some cases, we were originally opened when CFS\n        // was not used, but then we are asked to open doc\n        // stores after the segment has switched to CFS\n        if (cfsReader == null) {\n          cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n        }\n        storeDir = cfsReader;\n        assert storeDir != null;\n      } else {\n        storeDir = dir;\n        assert storeDir != null;\n      }\n      \n      final String storesSegment = si.getDocStoreSegment();\n      fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n          si.getDocStoreOffset(), si.docCount);\n      \n      // Verify two sources of \"maxDoc\" agree:\n      if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n      \n      if (si.getHasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a","date":1306150983,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","pathOld":"/dev/null","sourceNew":"  synchronized void openDocStores(SegmentInfo si) throws IOException {\n    \n    assert si.name.equals(segment);\n    \n    if (fieldsReaderOrig == null) {\n      final Directory storeDir;\n      if (si.getDocStoreOffset() != -1) {\n        if (si.getDocStoreIsCompoundFile()) {\n          assert storeCFSReader == null;\n          storeCFSReader = new CompoundFileReader(dir,\n              IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n              readBufferSize);\n          storeDir = storeCFSReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n      } else if (si.getUseCompoundFile()) {\n        // In some cases, we were originally opened when CFS\n        // was not used, but then we are asked to open doc\n        // stores after the segment has switched to CFS\n        if (cfsReader == null) {\n          cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n        }\n        storeDir = cfsReader;\n        assert storeDir != null;\n      } else {\n        storeDir = dir;\n        assert storeDir != null;\n      }\n      \n      final String storesSegment = si.getDocStoreSegment();\n      fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n          si.getDocStoreOffset(), si.docCount);\n      \n      // Verify two sources of \"maxDoc\" agree:\n      if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n      \n      if (si.getHasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"639c36565ce03aed5b0fce7c9e4448e53a1f7efd","date":1308580104,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","sourceNew":"  synchronized void openDocStores(SegmentInfo si) throws IOException {\n    \n    assert si.name.equals(segment);\n    \n    if (fieldsReaderOrig == null) {\n      final Directory storeDir;\n      if (si.getDocStoreOffset() != -1) {\n        if (si.getDocStoreIsCompoundFile()) {\n          assert storeCFSReader == null;\n          storeCFSReader = new CompoundFileReader(dir,\n              IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n              context);\n          storeDir = storeCFSReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n      } else if (si.getUseCompoundFile()) {\n        // In some cases, we were originally opened when CFS\n        // was not used, but then we are asked to open doc\n        // stores after the segment has switched to CFS\n        if (cfsReader == null) {\n          cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), context);\n        }\n        storeDir = cfsReader;\n        assert storeDir != null;\n      } else {\n        storeDir = dir;\n        assert storeDir != null;\n      }\n      \n      final String storesSegment = si.getDocStoreSegment();\n      fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, context,\n          si.getDocStoreOffset(), si.docCount);\n      \n      // Verify two sources of \"maxDoc\" agree:\n      if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n      \n      if (si.getHasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, context, si.getDocStoreOffset(), si.docCount);\n      }\n    }\n  }\n\n","sourceOld":"  synchronized void openDocStores(SegmentInfo si) throws IOException {\n    \n    assert si.name.equals(segment);\n    \n    if (fieldsReaderOrig == null) {\n      final Directory storeDir;\n      if (si.getDocStoreOffset() != -1) {\n        if (si.getDocStoreIsCompoundFile()) {\n          assert storeCFSReader == null;\n          storeCFSReader = new CompoundFileReader(dir,\n              IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n              readBufferSize);\n          storeDir = storeCFSReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n      } else if (si.getUseCompoundFile()) {\n        // In some cases, we were originally opened when CFS\n        // was not used, but then we are asked to open doc\n        // stores after the segment has switched to CFS\n        if (cfsReader == null) {\n          cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n        }\n        storeDir = cfsReader;\n        assert storeDir != null;\n      } else {\n        storeDir = dir;\n        assert storeDir != null;\n      }\n      \n      final String storesSegment = si.getDocStoreSegment();\n      fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n          si.getDocStoreOffset(), si.docCount);\n      \n      // Verify two sources of \"maxDoc\" agree:\n      if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n      \n      if (si.getHasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0aab6e810b4b0d3743d6a048be0602801f4b3920","date":1308671625,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","sourceNew":"  synchronized void openDocStores(SegmentInfo si) throws IOException {\n    \n    assert si.name.equals(segment);\n    \n    if (fieldsReaderOrig == null) {\n      final Directory storeDir;\n      if (si.getDocStoreOffset() != -1) {\n        if (si.getDocStoreIsCompoundFile()) {\n          assert storeCFSReader == null;\n          storeCFSReader = dir.openCompoundInput(\n              IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n              readBufferSize);\n          storeDir = storeCFSReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n      } else if (si.getUseCompoundFile()) {\n        // In some cases, we were originally opened when CFS\n        // was not used, but then we are asked to open doc\n        // stores after the segment has switched to CFS\n        if (cfsReader == null) {\n          cfsReader = dir.openCompoundInput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n        }\n        storeDir = cfsReader;\n        assert storeDir != null;\n      } else {\n        storeDir = dir;\n        assert storeDir != null;\n      }\n      \n      final String storesSegment = si.getDocStoreSegment();\n      fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n          si.getDocStoreOffset(), si.docCount);\n      \n      // Verify two sources of \"maxDoc\" agree:\n      if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n      \n      if (si.getHasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n      }\n    }\n  }\n\n","sourceOld":"  synchronized void openDocStores(SegmentInfo si) throws IOException {\n    \n    assert si.name.equals(segment);\n    \n    if (fieldsReaderOrig == null) {\n      final Directory storeDir;\n      if (si.getDocStoreOffset() != -1) {\n        if (si.getDocStoreIsCompoundFile()) {\n          assert storeCFSReader == null;\n          storeCFSReader = new CompoundFileReader(dir,\n              IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n              readBufferSize);\n          storeDir = storeCFSReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n      } else if (si.getUseCompoundFile()) {\n        // In some cases, we were originally opened when CFS\n        // was not used, but then we are asked to open doc\n        // stores after the segment has switched to CFS\n        if (cfsReader == null) {\n          cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n        }\n        storeDir = cfsReader;\n        assert storeDir != null;\n      } else {\n        storeDir = dir;\n        assert storeDir != null;\n      }\n      \n      final String storesSegment = si.getDocStoreSegment();\n      fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n          si.getDocStoreOffset(), si.docCount);\n      \n      // Verify two sources of \"maxDoc\" agree:\n      if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n      \n      if (si.getHasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2553b00f699380c64959ccb27991289aae87be2e","date":1309290151,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","sourceNew":"  synchronized void openDocStores(SegmentInfo si) throws IOException {\n    \n    assert si.name.equals(segment);\n    \n    if (fieldsReaderOrig == null) {\n      final Directory storeDir;\n      if (si.getDocStoreOffset() != -1) {\n        if (si.getDocStoreIsCompoundFile()) {\n          assert storeCFSReader == null;\n          storeCFSReader = dir.openCompoundInput(\n              IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n              readBufferSize);\n          storeDir = storeCFSReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n      } else if (si.getUseCompoundFile()) {\n        // In some cases, we were originally opened when CFS\n        // was not used, but then we are asked to open doc\n        // stores after the segment has switched to CFS\n        if (cfsReader == null) {\n          cfsReader = dir.openCompoundInput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n        }\n        storeDir = cfsReader;\n        assert storeDir != null;\n      } else {\n        storeDir = dir;\n        assert storeDir != null;\n      }\n      \n      final String storesSegment = si.getDocStoreSegment();\n      fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n          si.getDocStoreOffset(), si.docCount);\n      \n      // Verify two sources of \"maxDoc\" agree:\n      if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n      \n      if (si.getHasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n      }\n    }\n  }\n\n","sourceOld":"  synchronized void openDocStores(SegmentInfo si) throws IOException {\n    \n    assert si.name.equals(segment);\n    \n    if (fieldsReaderOrig == null) {\n      final Directory storeDir;\n      if (si.getDocStoreOffset() != -1) {\n        if (si.getDocStoreIsCompoundFile()) {\n          assert storeCFSReader == null;\n          storeCFSReader = new CompoundFileReader(dir,\n              IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n              readBufferSize);\n          storeDir = storeCFSReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n      } else if (si.getUseCompoundFile()) {\n        // In some cases, we were originally opened when CFS\n        // was not used, but then we are asked to open doc\n        // stores after the segment has switched to CFS\n        if (cfsReader == null) {\n          cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n        }\n        storeDir = cfsReader;\n        assert storeDir != null;\n      } else {\n        storeDir = dir;\n        assert storeDir != null;\n      }\n      \n      final String storesSegment = si.getDocStoreSegment();\n      fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n          si.getDocStoreOffset(), si.docCount);\n      \n      // Verify two sources of \"maxDoc\" agree:\n      if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n      \n      if (si.getHasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","sourceNew":"  synchronized void openDocStores(SegmentInfo si) throws IOException {\n    \n    assert si.name.equals(segment);\n    \n    if (fieldsReaderOrig == null) {\n      final Directory storeDir;\n      if (si.getDocStoreOffset() != -1) {\n        if (si.getDocStoreIsCompoundFile()) {\n          assert storeCFSReader == null;\n          storeCFSReader = dir.openCompoundInput(\n              IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n              context);\n          storeDir = storeCFSReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n      } else if (si.getUseCompoundFile()) {\n        // In some cases, we were originally opened when CFS\n        // was not used, but then we are asked to open doc\n        // stores after the segment has switched to CFS\n        if (cfsReader == null) {\n          cfsReader = dir.openCompoundInput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), context);\n        }\n        storeDir = cfsReader;\n        assert storeDir != null;\n      } else {\n        storeDir = dir;\n        assert storeDir != null;\n      }\n      \n      final String storesSegment = si.getDocStoreSegment();\n      fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, context,\n          si.getDocStoreOffset(), si.docCount);\n      \n      // Verify two sources of \"maxDoc\" agree:\n      if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n      \n      if (si.getHasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, context, si.getDocStoreOffset(), si.docCount);\n      }\n    }\n  }\n\n","sourceOld":"  synchronized void openDocStores(SegmentInfo si) throws IOException {\n    \n    assert si.name.equals(segment);\n    \n    if (fieldsReaderOrig == null) {\n      final Directory storeDir;\n      if (si.getDocStoreOffset() != -1) {\n        if (si.getDocStoreIsCompoundFile()) {\n          assert storeCFSReader == null;\n          storeCFSReader = new CompoundFileReader(dir,\n              IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n              context);\n          storeDir = storeCFSReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n      } else if (si.getUseCompoundFile()) {\n        // In some cases, we were originally opened when CFS\n        // was not used, but then we are asked to open doc\n        // stores after the segment has switched to CFS\n        if (cfsReader == null) {\n          cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), context);\n        }\n        storeDir = cfsReader;\n        assert storeDir != null;\n      } else {\n        storeDir = dir;\n        assert storeDir != null;\n      }\n      \n      final String storesSegment = si.getDocStoreSegment();\n      fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, context,\n          si.getDocStoreOffset(), si.docCount);\n      \n      // Verify two sources of \"maxDoc\" agree:\n      if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n      \n      if (si.getHasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, context, si.getDocStoreOffset(), si.docCount);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ddc4c914be86e34b54f70023f45a60fa7f04e929","date":1310115160,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","sourceNew":"  synchronized void openDocStores(SegmentInfo si) throws IOException {\n    \n    assert si.name.equals(segment);\n    \n    if (fieldsReaderOrig == null) {\n      final Directory storeDir;\n      if (si.getDocStoreOffset() != -1) {\n        if (si.getDocStoreIsCompoundFile()) {\n          assert storeCFSReader == null;\n          storeCFSReader = dir.openCompoundInput(\n              IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n              context);\n          storeDir = storeCFSReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n      } else if (si.getUseCompoundFile()) {\n        // In some cases, we were originally opened when CFS\n        // was not used, but then we are asked to open doc\n        // stores after the segment has switched to CFS\n        if (cfsReader == null) {\n          cfsReader = dir.openCompoundInput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), context);\n        }\n        storeDir = cfsReader;\n        assert storeDir != null;\n      } else {\n        storeDir = dir;\n        assert storeDir != null;\n      }\n      \n      final String storesSegment = si.getDocStoreSegment();\n      fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, context,\n          si.getDocStoreOffset(), si.docCount);\n      \n      // Verify two sources of \"maxDoc\" agree:\n      if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n      \n      if (si.getHasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, context, si.getDocStoreOffset(), si.docCount);\n      }\n    }\n  }\n\n","sourceOld":"  synchronized void openDocStores(SegmentInfo si) throws IOException {\n    \n    assert si.name.equals(segment);\n    \n    if (fieldsReaderOrig == null) {\n      final Directory storeDir;\n      if (si.getDocStoreOffset() != -1) {\n        if (si.getDocStoreIsCompoundFile()) {\n          assert storeCFSReader == null;\n          storeCFSReader = dir.openCompoundInput(\n              IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n              readBufferSize);\n          storeDir = storeCFSReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n      } else if (si.getUseCompoundFile()) {\n        // In some cases, we were originally opened when CFS\n        // was not used, but then we are asked to open doc\n        // stores after the segment has switched to CFS\n        if (cfsReader == null) {\n          cfsReader = dir.openCompoundInput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n        }\n        storeDir = cfsReader;\n        assert storeDir != null;\n      } else {\n        storeDir = dir;\n        assert storeDir != null;\n      }\n      \n      final String storesSegment = si.getDocStoreSegment();\n      fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n          si.getDocStoreOffset(), si.docCount);\n      \n      // Verify two sources of \"maxDoc\" agree:\n      if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n      \n      if (si.getHasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5d004d0e0b3f65bb40da76d476d659d7888270e8","date":1310158940,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","sourceNew":"  synchronized void openDocStores(SegmentInfo si) throws IOException {\n    \n    assert si.name.equals(segment);\n    \n    if (fieldsReaderOrig == null) {\n      final Directory storeDir;\n      if (si.getDocStoreOffset() != -1) {\n        if (si.getDocStoreIsCompoundFile()) {\n          assert storeCFSReader == null;\n          storeCFSReader = dir.openCompoundInput(\n              IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n              context);\n          storeDir = storeCFSReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n      } else if (si.getUseCompoundFile()) {\n        // In some cases, we were originally opened when CFS\n        // was not used, but then we are asked to open doc\n        // stores after the segment has switched to CFS\n        if (cfsReader == null) {\n          cfsReader = dir.openCompoundInput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), context);\n        }\n        storeDir = cfsReader;\n        assert storeDir != null;\n      } else {\n        storeDir = dir;\n        assert storeDir != null;\n      }\n      \n      final String storesSegment = si.getDocStoreSegment();\n      fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, context,\n          si.getDocStoreOffset(), si.docCount);\n      \n      // Verify two sources of \"maxDoc\" agree:\n      if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n      \n      if (si.getHasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, context, si.getDocStoreOffset(), si.docCount);\n      }\n    }\n  }\n\n","sourceOld":"  synchronized void openDocStores(SegmentInfo si) throws IOException {\n    \n    assert si.name.equals(segment);\n    \n    if (fieldsReaderOrig == null) {\n      final Directory storeDir;\n      if (si.getDocStoreOffset() != -1) {\n        if (si.getDocStoreIsCompoundFile()) {\n          assert storeCFSReader == null;\n          storeCFSReader = dir.openCompoundInput(\n              IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n              readBufferSize);\n          storeDir = storeCFSReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n      } else if (si.getUseCompoundFile()) {\n        // In some cases, we were originally opened when CFS\n        // was not used, but then we are asked to open doc\n        // stores after the segment has switched to CFS\n        if (cfsReader == null) {\n          cfsReader = dir.openCompoundInput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);\n        }\n        storeDir = cfsReader;\n        assert storeDir != null;\n      } else {\n        storeDir = dir;\n        assert storeDir != null;\n      }\n      \n      final String storesSegment = si.getDocStoreSegment();\n      fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,\n          si.getDocStoreOffset(), si.docCount);\n      \n      // Verify two sources of \"maxDoc\" agree:\n      if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n      \n      if (si.getHasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"67aadace85f701c87a4e0721eedcda25d8415a70","date":1314201925,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","sourceNew":"  synchronized void openDocStores(SegmentInfo si) throws IOException {\n    \n    assert si.name.equals(segment);\n    \n    if (fieldsReaderOrig == null) {\n      final Directory storeDir;\n      if (si.getDocStoreOffset() != -1) {\n        if (si.getDocStoreIsCompoundFile()) {\n          assert storeCFSReader == null;\n          storeCFSReader = new CompoundFileDirectory(dir,\n              IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n              context, false);\n          storeDir = storeCFSReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n      } else if (si.getUseCompoundFile()) {\n        // In some cases, we were originally opened when CFS\n        // was not used, but then we are asked to open doc\n        // stores after the segment has switched to CFS\n        if (cfsReader == null) {\n          cfsReader = new CompoundFileDirectory(dir,IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), context, false);\n        }\n        storeDir = cfsReader;\n        assert storeDir != null;\n      } else {\n        storeDir = dir;\n        assert storeDir != null;\n      }\n      \n      final String storesSegment = si.getDocStoreSegment();\n      fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, context,\n          si.getDocStoreOffset(), si.docCount);\n      \n      // Verify two sources of \"maxDoc\" agree:\n      if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n      \n      if (si.getHasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, context, si.getDocStoreOffset(), si.docCount);\n      }\n    }\n  }\n\n","sourceOld":"  synchronized void openDocStores(SegmentInfo si) throws IOException {\n    \n    assert si.name.equals(segment);\n    \n    if (fieldsReaderOrig == null) {\n      final Directory storeDir;\n      if (si.getDocStoreOffset() != -1) {\n        if (si.getDocStoreIsCompoundFile()) {\n          assert storeCFSReader == null;\n          storeCFSReader = dir.openCompoundInput(\n              IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n              context);\n          storeDir = storeCFSReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n      } else if (si.getUseCompoundFile()) {\n        // In some cases, we were originally opened when CFS\n        // was not used, but then we are asked to open doc\n        // stores after the segment has switched to CFS\n        if (cfsReader == null) {\n          cfsReader = dir.openCompoundInput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), context);\n        }\n        storeDir = cfsReader;\n        assert storeDir != null;\n      } else {\n        storeDir = dir;\n        assert storeDir != null;\n      }\n      \n      final String storesSegment = si.getDocStoreSegment();\n      fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, context,\n          si.getDocStoreOffset(), si.docCount);\n      \n      // Verify two sources of \"maxDoc\" agree:\n      if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n      \n      if (si.getHasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, context, si.getDocStoreOffset(), si.docCount);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4b1110660886afcc62f57e9af901cd3f5dd294bc","date":1317830374,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","sourceNew":"  synchronized void openDocStores(SegmentInfo si) throws IOException {\n    \n    assert si.name.equals(segment);\n    \n    if (fieldsReaderOrig == null) {\n      final Directory storeDir;\n      if (si.getDocStoreOffset() != -1) {\n        if (si.getDocStoreIsCompoundFile()) {\n          assert storeCFSReader == null;\n          storeCFSReader = new CompoundFileDirectory(dir,\n              IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n              context, false);\n          storeDir = storeCFSReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n      } else if (si.getUseCompoundFile()) {\n        // In some cases, we were originally opened when CFS\n        // was not used, but then we are asked to open doc\n        // stores after the segment has switched to CFS\n        if (cfsReader == null) {\n          cfsReader = new CompoundFileDirectory(dir,IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), context, false);\n        }\n        storeDir = cfsReader;\n        assert storeDir != null;\n      } else {\n        storeDir = dir;\n        assert storeDir != null;\n      }\n      \n      final String storesSegment = si.getDocStoreSegment();\n      fieldsReaderOrig = si.getSegmentCodecs().provider.fieldsReader(storeDir, storesSegment, fieldInfos, context,\n          si.getDocStoreOffset(), si.docCount);\n      \n      // Verify two sources of \"maxDoc\" agree:\n      if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n      \n      if (si.getHasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, context, si.getDocStoreOffset(), si.docCount);\n      }\n    }\n  }\n\n","sourceOld":"  synchronized void openDocStores(SegmentInfo si) throws IOException {\n    \n    assert si.name.equals(segment);\n    \n    if (fieldsReaderOrig == null) {\n      final Directory storeDir;\n      if (si.getDocStoreOffset() != -1) {\n        if (si.getDocStoreIsCompoundFile()) {\n          assert storeCFSReader == null;\n          storeCFSReader = new CompoundFileDirectory(dir,\n              IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n              context, false);\n          storeDir = storeCFSReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n      } else if (si.getUseCompoundFile()) {\n        // In some cases, we were originally opened when CFS\n        // was not used, but then we are asked to open doc\n        // stores after the segment has switched to CFS\n        if (cfsReader == null) {\n          cfsReader = new CompoundFileDirectory(dir,IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), context, false);\n        }\n        storeDir = cfsReader;\n        assert storeDir != null;\n      } else {\n        storeDir = dir;\n        assert storeDir != null;\n      }\n      \n      final String storesSegment = si.getDocStoreSegment();\n      fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, context,\n          si.getDocStoreOffset(), si.docCount);\n      \n      // Verify two sources of \"maxDoc\" agree:\n      if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n      \n      if (si.getHasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, context, si.getDocStoreOffset(), si.docCount);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7b91922b55d15444d554721b352861d028eb8278","date":1320421415,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","sourceNew":"  synchronized void openDocStores(SegmentInfo si) throws IOException {\n    \n    assert si.name.equals(segment);\n    \n    if (fieldsReaderOrig == null) {\n      final Directory storeDir;\n      if (si.getDocStoreOffset() != -1) {\n        if (si.getDocStoreIsCompoundFile()) {\n          assert storeCFSReader == null;\n          storeCFSReader = new CompoundFileDirectory(dir,\n              IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n              context, false);\n          storeDir = storeCFSReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n      } else if (si.getUseCompoundFile()) {\n        // In some cases, we were originally opened when CFS\n        // was not used, but then we are asked to open doc\n        // stores after the segment has switched to CFS\n        if (cfsReader == null) {\n          cfsReader = new CompoundFileDirectory(dir,IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), context, false);\n        }\n        storeDir = cfsReader;\n        assert storeDir != null;\n      } else {\n        storeDir = dir;\n        assert storeDir != null;\n      }\n      \n      final String storesSegment = si.getDocStoreSegment();\n      fieldsReaderOrig = si.getCodec().fieldsFormat().fieldsReader(storeDir, storesSegment, fieldInfos, context,\n          si.getDocStoreOffset(), si.docCount);\n      \n      // Verify two sources of \"maxDoc\" agree:\n      if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n      \n      if (si.getHasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, context, si.getDocStoreOffset(), si.docCount);\n      }\n    }\n  }\n\n","sourceOld":"  synchronized void openDocStores(SegmentInfo si) throws IOException {\n    \n    assert si.name.equals(segment);\n    \n    if (fieldsReaderOrig == null) {\n      final Directory storeDir;\n      if (si.getDocStoreOffset() != -1) {\n        if (si.getDocStoreIsCompoundFile()) {\n          assert storeCFSReader == null;\n          storeCFSReader = new CompoundFileDirectory(dir,\n              IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n              context, false);\n          storeDir = storeCFSReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n      } else if (si.getUseCompoundFile()) {\n        // In some cases, we were originally opened when CFS\n        // was not used, but then we are asked to open doc\n        // stores after the segment has switched to CFS\n        if (cfsReader == null) {\n          cfsReader = new CompoundFileDirectory(dir,IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), context, false);\n        }\n        storeDir = cfsReader;\n        assert storeDir != null;\n      } else {\n        storeDir = dir;\n        assert storeDir != null;\n      }\n      \n      final String storesSegment = si.getDocStoreSegment();\n      fieldsReaderOrig = si.getSegmentCodecs().provider.fieldsReader(storeDir, storesSegment, fieldInfos, context,\n          si.getDocStoreOffset(), si.docCount);\n      \n      // Verify two sources of \"maxDoc\" agree:\n      if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n      \n      if (si.getHasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, context, si.getDocStoreOffset(), si.docCount);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"06584e6e98d592b34e1329b384182f368d2025e8","date":1320850353,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","sourceNew":"  synchronized void openDocStores(SegmentInfo si) throws IOException {\n    \n    assert si.name.equals(segment);\n    \n    if (fieldsReaderOrig == null) {\n      final Directory storeDir;\n      if (si.getDocStoreOffset() != -1) {\n        if (si.getDocStoreIsCompoundFile()) {\n          assert storeCFSReader == null;\n          storeCFSReader = new CompoundFileDirectory(dir,\n              IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n              context, false);\n          storeDir = storeCFSReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n      } else if (si.getUseCompoundFile()) {\n        // In some cases, we were originally opened when CFS\n        // was not used, but then we are asked to open doc\n        // stores after the segment has switched to CFS\n        if (cfsReader == null) {\n          cfsReader = new CompoundFileDirectory(dir,IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), context, false);\n        }\n        storeDir = cfsReader;\n        assert storeDir != null;\n      } else {\n        storeDir = dir;\n        assert storeDir != null;\n      }\n      \n      final String storesSegment = si.getDocStoreSegment();\n      fieldsReaderOrig = si.getCodec().storedFieldsFormat().fieldsReader(storeDir, si, fieldInfos, context);\n \n      if (si.getHasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, context, si.getDocStoreOffset(), si.docCount);\n      }\n    }\n  }\n\n","sourceOld":"  synchronized void openDocStores(SegmentInfo si) throws IOException {\n    \n    assert si.name.equals(segment);\n    \n    if (fieldsReaderOrig == null) {\n      final Directory storeDir;\n      if (si.getDocStoreOffset() != -1) {\n        if (si.getDocStoreIsCompoundFile()) {\n          assert storeCFSReader == null;\n          storeCFSReader = new CompoundFileDirectory(dir,\n              IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n              context, false);\n          storeDir = storeCFSReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n      } else if (si.getUseCompoundFile()) {\n        // In some cases, we were originally opened when CFS\n        // was not used, but then we are asked to open doc\n        // stores after the segment has switched to CFS\n        if (cfsReader == null) {\n          cfsReader = new CompoundFileDirectory(dir,IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), context, false);\n        }\n        storeDir = cfsReader;\n        assert storeDir != null;\n      } else {\n        storeDir = dir;\n        assert storeDir != null;\n      }\n      \n      final String storesSegment = si.getDocStoreSegment();\n      fieldsReaderOrig = si.getCodec().fieldsFormat().fieldsReader(storeDir, storesSegment, fieldInfos, context,\n          si.getDocStoreOffset(), si.docCount);\n      \n      // Verify two sources of \"maxDoc\" agree:\n      if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + segment + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n      \n      if (si.getHasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, context, si.getDocStoreOffset(), si.docCount);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cc749c053615f5871f3b95715fe292f34e70a53","date":1321470575,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","sourceNew":"  synchronized void openDocStores(SegmentInfo si) throws IOException {\n    \n    assert si.name.equals(segment);\n    \n    if (fieldsReaderOrig == null) {\n      final Directory storeDir;\n      if (si.getDocStoreOffset() != -1) {\n        if (si.getDocStoreIsCompoundFile()) {\n          assert storeCFSReader == null;\n          storeCFSReader = new CompoundFileDirectory(dir,\n              IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n              context, false);\n          storeDir = storeCFSReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n      } else if (si.getUseCompoundFile()) {\n        // In some cases, we were originally opened when CFS\n        // was not used, but then we are asked to open doc\n        // stores after the segment has switched to CFS\n        if (cfsReader == null) {\n          cfsReader = new CompoundFileDirectory(dir,IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), context, false);\n        }\n        storeDir = cfsReader;\n        assert storeDir != null;\n      } else {\n        storeDir = dir;\n        assert storeDir != null;\n      }\n      \n      fieldsReaderOrig = si.getCodec().storedFieldsFormat().fieldsReader(storeDir, si, fieldInfos, context);\n \n      if (si.getHasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = si.getCodec().termVectorsFormat().vectorsReader(storeDir, si, fieldInfos, context);\n      }\n    }\n  }\n\n","sourceOld":"  synchronized void openDocStores(SegmentInfo si) throws IOException {\n    \n    assert si.name.equals(segment);\n    \n    if (fieldsReaderOrig == null) {\n      final Directory storeDir;\n      if (si.getDocStoreOffset() != -1) {\n        if (si.getDocStoreIsCompoundFile()) {\n          assert storeCFSReader == null;\n          storeCFSReader = new CompoundFileDirectory(dir,\n              IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n              context, false);\n          storeDir = storeCFSReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n      } else if (si.getUseCompoundFile()) {\n        // In some cases, we were originally opened when CFS\n        // was not used, but then we are asked to open doc\n        // stores after the segment has switched to CFS\n        if (cfsReader == null) {\n          cfsReader = new CompoundFileDirectory(dir,IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), context, false);\n        }\n        storeDir = cfsReader;\n        assert storeDir != null;\n      } else {\n        storeDir = dir;\n        assert storeDir != null;\n      }\n      \n      final String storesSegment = si.getDocStoreSegment();\n      fieldsReaderOrig = si.getCodec().storedFieldsFormat().fieldsReader(storeDir, si, fieldInfos, context);\n \n      if (si.getHasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, context, si.getDocStoreOffset(), si.docCount);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9ce667c6d3400b22523701c549c0d35e26da8b46","date":1324405053,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentCoreReaders#openDocStores(SegmentInfo).mjava","sourceNew":null,"sourceOld":"  synchronized void openDocStores(SegmentInfo si) throws IOException {\n    \n    assert si.name.equals(segment);\n    \n    if (fieldsReaderOrig == null) {\n      final Directory storeDir;\n      if (si.getDocStoreOffset() != -1) {\n        if (si.getDocStoreIsCompoundFile()) {\n          assert storeCFSReader == null;\n          storeCFSReader = new CompoundFileDirectory(dir,\n              IndexFileNames.segmentFileName(si.getDocStoreSegment(), \"\", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),\n              context, false);\n          storeDir = storeCFSReader;\n          assert storeDir != null;\n        } else {\n          storeDir = dir;\n          assert storeDir != null;\n        }\n      } else if (si.getUseCompoundFile()) {\n        // In some cases, we were originally opened when CFS\n        // was not used, but then we are asked to open doc\n        // stores after the segment has switched to CFS\n        if (cfsReader == null) {\n          cfsReader = new CompoundFileDirectory(dir,IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), context, false);\n        }\n        storeDir = cfsReader;\n        assert storeDir != null;\n      } else {\n        storeDir = dir;\n        assert storeDir != null;\n      }\n      \n      fieldsReaderOrig = si.getCodec().storedFieldsFormat().fieldsReader(storeDir, si, fieldInfos, context);\n \n      if (si.getHasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = si.getCodec().termVectorsFormat().vectorsReader(storeDir, si, fieldInfos, context);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"ae132b768aece5bf21cda14e2f17fba66eb6f7d6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","fe19cbe25754c715a0232f453039383119fc122c"],"67aadace85f701c87a4e0721eedcda25d8415a70":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","fe19cbe25754c715a0232f453039383119fc122c"],"06584e6e98d592b34e1329b384182f368d2025e8":["7b91922b55d15444d554721b352861d028eb8278"],"0aab6e810b4b0d3743d6a048be0602801f4b3920":["fe19cbe25754c715a0232f453039383119fc122c"],"3cc749c053615f5871f3b95715fe292f34e70a53":["06584e6e98d592b34e1329b384182f368d2025e8"],"4b1110660886afcc62f57e9af901cd3f5dd294bc":["67aadace85f701c87a4e0721eedcda25d8415a70"],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["fe19cbe25754c715a0232f453039383119fc122c"],"2553b00f699380c64959ccb27991289aae87be2e":["ae132b768aece5bf21cda14e2f17fba66eb6f7d6","0aab6e810b4b0d3743d6a048be0602801f4b3920"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["639c36565ce03aed5b0fce7c9e4448e53a1f7efd","0aab6e810b4b0d3743d6a048be0602801f4b3920"],"7b91922b55d15444d554721b352861d028eb8278":["4b1110660886afcc62f57e9af901cd3f5dd294bc"],"fe19cbe25754c715a0232f453039383119fc122c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5d004d0e0b3f65bb40da76d476d659d7888270e8":["2553b00f699380c64959ccb27991289aae87be2e","ddc4c914be86e34b54f70023f45a60fa7f04e929"],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["0aab6e810b4b0d3743d6a048be0602801f4b3920","d083e83f225b11e5fdd900e83d26ddb385b6955c"],"9ce667c6d3400b22523701c549c0d35e26da8b46":["3cc749c053615f5871f3b95715fe292f34e70a53"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9ce667c6d3400b22523701c549c0d35e26da8b46"]},"commit2Childs":{"ae132b768aece5bf21cda14e2f17fba66eb6f7d6":["2553b00f699380c64959ccb27991289aae87be2e"],"67aadace85f701c87a4e0721eedcda25d8415a70":["4b1110660886afcc62f57e9af901cd3f5dd294bc"],"5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a":[],"06584e6e98d592b34e1329b384182f368d2025e8":["3cc749c053615f5871f3b95715fe292f34e70a53"],"0aab6e810b4b0d3743d6a048be0602801f4b3920":["2553b00f699380c64959ccb27991289aae87be2e","d083e83f225b11e5fdd900e83d26ddb385b6955c","ddc4c914be86e34b54f70023f45a60fa7f04e929"],"3cc749c053615f5871f3b95715fe292f34e70a53":["9ce667c6d3400b22523701c549c0d35e26da8b46"],"4b1110660886afcc62f57e9af901cd3f5dd294bc":["7b91922b55d15444d554721b352861d028eb8278"],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["d083e83f225b11e5fdd900e83d26ddb385b6955c"],"2553b00f699380c64959ccb27991289aae87be2e":["5d004d0e0b3f65bb40da76d476d659d7888270e8"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"7b91922b55d15444d554721b352861d028eb8278":["06584e6e98d592b34e1329b384182f368d2025e8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["ae132b768aece5bf21cda14e2f17fba66eb6f7d6","5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a","fe19cbe25754c715a0232f453039383119fc122c"],"fe19cbe25754c715a0232f453039383119fc122c":["ae132b768aece5bf21cda14e2f17fba66eb6f7d6","5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a","0aab6e810b4b0d3743d6a048be0602801f4b3920","639c36565ce03aed5b0fce7c9e4448e53a1f7efd"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":[],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["67aadace85f701c87a4e0721eedcda25d8415a70","5d004d0e0b3f65bb40da76d476d659d7888270e8"],"9ce667c6d3400b22523701c549c0d35e26da8b46":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5c698c0cb88bac4bcd36a1b1001a0c6a2163ea2a","5d004d0e0b3f65bb40da76d476d659d7888270e8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}