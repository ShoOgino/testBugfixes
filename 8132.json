{"path":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","sourceNew":"  void finishDocument() throws IOException {\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = termsHashPerField.bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    if (numPostings > maxNumPostings)\n      maxNumPostings = numPostings;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    assert termsWriter.vectorFieldsInOrder(fieldInfo);\n\n    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = termsHashPerField.sortPostings(tv.getComparator());\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;\n\n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          termsHashPerField.initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          termsHashPerField.initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n    }\n\n    termsHashPerField.reset();\n\n    // commit the termVectors once successful success - FI will otherwise reset them\n    fieldInfo.setStoreTermVectors();\n  }\n\n","sourceOld":"  void finishDocument() throws IOException {\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = termsHashPerField.bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    if (numPostings > maxNumPostings)\n      maxNumPostings = numPostings;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    assert termsWriter.vectorFieldsInOrder(fieldInfo);\n\n    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = termsHashPerField.sortPostings(tv.getComparator());\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;\n\n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          termsHashPerField.initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          termsHashPerField.initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n    }\n\n    termsHashPerField.reset();\n\n    // commit the termVectors once successful success - FI will otherwise reset them\n    fieldInfo.setStoreTermVectors();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4356000e349e38c9fb48034695b7c309abd54557","date":1337460341,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","sourceNew":"  void finishDocument() throws IOException {\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = termsHashPerField.bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    if (numPostings > maxNumPostings)\n      maxNumPostings = numPostings;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    assert termsWriter.vectorFieldsInOrder(fieldInfo);\n\n    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = termsHashPerField.sortPostings(tv.getComparator());\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;\n\n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          termsHashPerField.initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          termsHashPerField.initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n    }\n\n    termsHashPerField.reset();\n\n    // commit the termVectors once successful - FI will otherwise reset them\n    fieldInfo.setStoreTermVectors();\n  }\n\n","sourceOld":"  void finishDocument() throws IOException {\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = termsHashPerField.bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    if (numPostings > maxNumPostings)\n      maxNumPostings = numPostings;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    assert termsWriter.vectorFieldsInOrder(fieldInfo);\n\n    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = termsHashPerField.sortPostings(tv.getComparator());\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;\n\n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          termsHashPerField.initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          termsHashPerField.initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n    }\n\n    termsHashPerField.reset();\n\n    // commit the termVectors once successful success - FI will otherwise reset them\n    fieldInfo.setStoreTermVectors();\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"615ddbd81799980d0fdd95e0238e1c498b6f47b0","date":1338233290,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","sourceNew":"  void finishDocument() throws IOException {\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = termsHashPerField.bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    if (numPostings > maxNumPostings)\n      maxNumPostings = numPostings;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    assert termsWriter.vectorFieldsInOrder(fieldInfo);\n\n    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = termsHashPerField.sortPostings(tv.getComparator());\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;\n\n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          termsHashPerField.initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          termsHashPerField.initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n    }\n\n    termsHashPerField.reset();\n\n    // commit the termVectors once successful - FI will otherwise reset them\n    fieldInfo.setStoreTermVectors();\n  }\n\n","sourceOld":"  void finishDocument() throws IOException {\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = termsHashPerField.bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    if (numPostings > maxNumPostings)\n      maxNumPostings = numPostings;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    assert termsWriter.vectorFieldsInOrder(fieldInfo);\n\n    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = termsHashPerField.sortPostings(tv.getComparator());\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;\n\n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          termsHashPerField.initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          termsHashPerField.initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n    }\n\n    termsHashPerField.reset();\n\n    // commit the termVectors once successful success - FI will otherwise reset them\n    fieldInfo.setStoreTermVectors();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0935c850ea562932997b72c69d93e345f21d7f45","date":1344711506,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","sourceNew":"  void finishDocument() throws IOException {\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = termsHashPerField.bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    if (numPostings > maxNumPostings)\n      maxNumPostings = numPostings;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    assert termsWriter.vectorFieldsInOrder(fieldInfo);\n\n    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = termsHashPerField.sortPostings(tv.getComparator());\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets, hasPayloads);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;\n\n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          termsHashPerField.initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          termsHashPerField.initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n    }\n\n    termsHashPerField.reset();\n\n    // commit the termVectors once successful - FI will otherwise reset them\n    fieldInfo.setStoreTermVectors();\n  }\n\n","sourceOld":"  void finishDocument() throws IOException {\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = termsHashPerField.bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    if (numPostings > maxNumPostings)\n      maxNumPostings = numPostings;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    assert termsWriter.vectorFieldsInOrder(fieldInfo);\n\n    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = termsHashPerField.sortPostings(tv.getComparator());\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;\n\n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          termsHashPerField.initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          termsHashPerField.initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n    }\n\n    termsHashPerField.reset();\n\n    // commit the termVectors once successful - FI will otherwise reset them\n    fieldInfo.setStoreTermVectors();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c7869f64c874ebf7f317d22c00baf2b6857797a6","date":1344856617,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","sourceNew":"  void finishDocument() throws IOException {\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = termsHashPerField.bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    if (numPostings > maxNumPostings)\n      maxNumPostings = numPostings;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    assert termsWriter.vectorFieldsInOrder(fieldInfo);\n\n    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = termsHashPerField.sortPostings(tv.getComparator());\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets, hasPayloads);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;\n\n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          termsHashPerField.initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          termsHashPerField.initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n    }\n\n    termsHashPerField.reset();\n\n    // commit the termVectors once successful - FI will otherwise reset them\n    fieldInfo.setStoreTermVectors();\n  }\n\n","sourceOld":"  void finishDocument() throws IOException {\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = termsHashPerField.bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    if (numPostings > maxNumPostings)\n      maxNumPostings = numPostings;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    assert termsWriter.vectorFieldsInOrder(fieldInfo);\n\n    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = termsHashPerField.sortPostings(tv.getComparator());\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;\n\n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          termsHashPerField.initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          termsHashPerField.initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n    }\n\n    termsHashPerField.reset();\n\n    // commit the termVectors once successful - FI will otherwise reset them\n    fieldInfo.setStoreTermVectors();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9","date":1344867506,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","sourceNew":"  void finishDocument() throws IOException {\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = termsHashPerField.bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    if (numPostings > maxNumPostings)\n      maxNumPostings = numPostings;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    assert termsWriter.vectorFieldsInOrder(fieldInfo);\n\n    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = termsHashPerField.sortPostings(tv.getComparator());\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets, hasPayloads);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;\n\n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          termsHashPerField.initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          termsHashPerField.initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n    }\n\n    termsHashPerField.reset();\n\n    // commit the termVectors once successful - FI will otherwise reset them\n    fieldInfo.setStoreTermVectors();\n  }\n\n","sourceOld":"  void finishDocument() throws IOException {\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = termsHashPerField.bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    if (numPostings > maxNumPostings)\n      maxNumPostings = numPostings;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    assert termsWriter.vectorFieldsInOrder(fieldInfo);\n\n    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = termsHashPerField.sortPostings(tv.getComparator());\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;\n\n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          termsHashPerField.initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          termsHashPerField.initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n    }\n\n    termsHashPerField.reset();\n\n    // commit the termVectors once successful - FI will otherwise reset them\n    fieldInfo.setStoreTermVectors();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fc7a7bb1aa79cf53564793bb5ffa270250c679da","date":1357817084,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","sourceNew":"  void finishDocument() throws IOException {\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = termsHashPerField.bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    if (numPostings > maxNumPostings)\n      maxNumPostings = numPostings;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    assert termsWriter.vectorFieldsInOrder(fieldInfo);\n\n    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = termsHashPerField.sortPostings(tv.getComparator());\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets, hasPayloads);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;\n\n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          termsHashPerField.initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          termsHashPerField.initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n      tv.finishTerm();\n    }\n    tv.finishField();\n\n    termsHashPerField.reset();\n\n    // commit the termVectors once successful - FI will otherwise reset them\n    fieldInfo.setStoreTermVectors();\n  }\n\n","sourceOld":"  void finishDocument() throws IOException {\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = termsHashPerField.bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    if (numPostings > maxNumPostings)\n      maxNumPostings = numPostings;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    assert termsWriter.vectorFieldsInOrder(fieldInfo);\n\n    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = termsHashPerField.sortPostings(tv.getComparator());\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets, hasPayloads);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;\n\n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          termsHashPerField.initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          termsHashPerField.initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n    }\n\n    termsHashPerField.reset();\n\n    // commit the termVectors once successful - FI will otherwise reset them\n    fieldInfo.setStoreTermVectors();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4e6354dd7c71fe122926fc53d7d29f715b1283db","date":1357915185,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","sourceNew":"  void finishDocument() throws IOException {\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = termsHashPerField.bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    if (numPostings > maxNumPostings)\n      maxNumPostings = numPostings;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    assert termsWriter.vectorFieldsInOrder(fieldInfo);\n\n    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = termsHashPerField.sortPostings(tv.getComparator());\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets, hasPayloads);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;\n\n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          termsHashPerField.initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          termsHashPerField.initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n      tv.finishTerm();\n    }\n    tv.finishField();\n\n    termsHashPerField.reset();\n\n    // commit the termVectors once successful - FI will otherwise reset them\n    fieldInfo.setStoreTermVectors();\n  }\n\n","sourceOld":"  void finishDocument() throws IOException {\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = termsHashPerField.bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    if (numPostings > maxNumPostings)\n      maxNumPostings = numPostings;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    assert termsWriter.vectorFieldsInOrder(fieldInfo);\n\n    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = termsHashPerField.sortPostings(tv.getComparator());\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets, hasPayloads);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;\n\n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          termsHashPerField.initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          termsHashPerField.initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n    }\n\n    termsHashPerField.reset();\n\n    // commit the termVectors once successful - FI will otherwise reset them\n    fieldInfo.setStoreTermVectors();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a1552916e7512a3bbf5d2364c3a97a677ce055ea","date":1358290892,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","sourceNew":"  void finishDocument() throws IOException {\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = termsHashPerField.bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    if (numPostings > maxNumPostings)\n      maxNumPostings = numPostings;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    assert termsWriter.vectorFieldsInOrder(fieldInfo);\n\n    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = termsHashPerField.sortPostings(tv.getComparator());\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets, hasPayloads);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;\n\n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          termsHashPerField.initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          termsHashPerField.initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n      tv.finishTerm();\n    }\n    tv.finishField();\n\n    termsHashPerField.reset();\n\n    fieldInfo.setStoreTermVectors();\n  }\n\n","sourceOld":"  void finishDocument() throws IOException {\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = termsHashPerField.bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    if (numPostings > maxNumPostings)\n      maxNumPostings = numPostings;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    assert termsWriter.vectorFieldsInOrder(fieldInfo);\n\n    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = termsHashPerField.sortPostings(tv.getComparator());\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets, hasPayloads);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;\n\n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          termsHashPerField.initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          termsHashPerField.initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n      tv.finishTerm();\n    }\n    tv.finishField();\n\n    termsHashPerField.reset();\n\n    // commit the termVectors once successful - FI will otherwise reset them\n    fieldInfo.setStoreTermVectors();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d4d69c535930b5cce125cff868d40f6373dc27d4","date":1360270101,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","sourceNew":"  void finishDocument() throws IOException {\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = termsHashPerField.bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    if (numPostings > maxNumPostings)\n      maxNumPostings = numPostings;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    assert termsWriter.vectorFieldsInOrder(fieldInfo);\n\n    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = termsHashPerField.sortPostings(tv.getComparator());\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets, hasPayloads);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;\n\n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          termsHashPerField.initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          termsHashPerField.initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n      tv.finishTerm();\n    }\n    tv.finishField();\n\n    termsHashPerField.reset();\n\n    fieldInfo.setStoreTermVectors();\n  }\n\n","sourceOld":"  void finishDocument() throws IOException {\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = termsHashPerField.bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    if (numPostings > maxNumPostings)\n      maxNumPostings = numPostings;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    assert termsWriter.vectorFieldsInOrder(fieldInfo);\n\n    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = termsHashPerField.sortPostings(tv.getComparator());\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets, hasPayloads);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;\n\n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          termsHashPerField.initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          termsHashPerField.initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n      tv.finishTerm();\n    }\n    tv.finishField();\n\n    termsHashPerField.reset();\n\n    // commit the termVectors once successful - FI will otherwise reset them\n    fieldInfo.setStoreTermVectors();\n  }\n\n","bugFix":["4356000e349e38c9fb48034695b7c309abd54557"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5","date":1379624229,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","sourceNew":"  void finishDocument() throws IOException {\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = termsHashPerField.bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    if (numPostings > maxNumPostings)\n      maxNumPostings = numPostings;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    assert termsWriter.vectorFieldsInOrder(fieldInfo);\n\n    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = termsHashPerField.sortPostings();\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets, hasPayloads);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;\n\n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          termsHashPerField.initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          termsHashPerField.initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n      tv.finishTerm();\n    }\n    tv.finishField();\n\n    termsHashPerField.reset();\n\n    fieldInfo.setStoreTermVectors();\n  }\n\n","sourceOld":"  void finishDocument() throws IOException {\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = termsHashPerField.bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    if (numPostings > maxNumPostings)\n      maxNumPostings = numPostings;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    assert termsWriter.vectorFieldsInOrder(fieldInfo);\n\n    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = termsHashPerField.sortPostings(tv.getComparator());\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets, hasPayloads);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;\n\n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          termsHashPerField.initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          termsHashPerField.initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n      tv.finishTerm();\n    }\n    tv.finishField();\n\n    termsHashPerField.reset();\n\n    fieldInfo.setStoreTermVectors();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"52c7e49be259508735752fba88085255014a6ecf","date":1398706273,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","sourceNew":"  void finishDocument() throws IOException {\n    if (doVectors == false) {\n      return;\n    }\n\n    doVectors = false;\n\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    TermVectorsPostingsArray postings = termVectorsPostingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = sortPostings();\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets, hasPayloads);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n      tv.finishTerm();\n    }\n    tv.finishField();\n\n    reset();\n\n    fieldInfo.setStoreTermVectors();\n  }\n\n","sourceOld":"  void finishDocument() throws IOException {\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = termsHashPerField.bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    if (numPostings > maxNumPostings)\n      maxNumPostings = numPostings;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    assert termsWriter.vectorFieldsInOrder(fieldInfo);\n\n    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = termsHashPerField.sortPostings();\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets, hasPayloads);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;\n\n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          termsHashPerField.initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          termsHashPerField.initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n      tv.finishTerm();\n    }\n    tv.finishField();\n\n    termsHashPerField.reset();\n\n    fieldInfo.setStoreTermVectors();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3394716f52b34ab259ad5247e7595d9f9db6e935","date":1398791921,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","sourceNew":"  void finishDocument() throws IOException {\n    if (doVectors == false) {\n      return;\n    }\n\n    doVectors = false;\n\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    TermVectorsPostingsArray postings = termVectorsPostingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = sortPostings();\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets, hasPayloads);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n      tv.finishTerm();\n    }\n    tv.finishField();\n\n    reset();\n\n    fieldInfo.setStoreTermVectors();\n  }\n\n","sourceOld":"  void finishDocument() throws IOException {\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = termsHashPerField.bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    if (numPostings > maxNumPostings)\n      maxNumPostings = numPostings;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    assert termsWriter.vectorFieldsInOrder(fieldInfo);\n\n    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = termsHashPerField.sortPostings();\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets, hasPayloads);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;\n\n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          termsHashPerField.initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          termsHashPerField.initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n      tv.finishTerm();\n    }\n    tv.finishField();\n\n    termsHashPerField.reset();\n\n    fieldInfo.setStoreTermVectors();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","date":1398844771,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","sourceNew":"  void finishDocument() throws IOException {\n    if (doVectors == false) {\n      return;\n    }\n\n    doVectors = false;\n\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    TermVectorsPostingsArray postings = termVectorsPostingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = sortPostings();\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets, hasPayloads);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n      tv.finishTerm();\n    }\n    tv.finishField();\n\n    reset();\n\n    fieldInfo.setStoreTermVectors();\n  }\n\n","sourceOld":"  void finishDocument() throws IOException {\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = termsHashPerField.bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    if (numPostings > maxNumPostings)\n      maxNumPostings = numPostings;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    assert termsWriter.vectorFieldsInOrder(fieldInfo);\n\n    TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = termsHashPerField.sortPostings();\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets, hasPayloads);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    final ByteBlockPool termBytePool = termsHashPerField.termBytePool;\n\n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          termsHashPerField.initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          termsHashPerField.initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n      tv.finishTerm();\n    }\n    tv.finishField();\n\n    termsHashPerField.reset();\n\n    fieldInfo.setStoreTermVectors();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9cdbc2cadeaf282528fe4d1c06e9f8bee38ccec4","date":1414017220,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","sourceNew":"  void finishDocument() throws IOException {\n    if (doVectors == false) {\n      return;\n    }\n\n    doVectors = false;\n\n    final int numPostings = bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    TermVectorsPostingsArray postings = termVectorsPostingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = sortPostings();\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets, hasPayloads);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n      tv.finishTerm();\n    }\n    tv.finishField();\n\n    reset();\n\n    fieldInfo.setStoreTermVectors();\n  }\n\n","sourceOld":"  void finishDocument() throws IOException {\n    if (doVectors == false) {\n      return;\n    }\n\n    doVectors = false;\n\n    assert docState.testPoint(\"TermVectorsTermsWriterPerField.finish start\");\n\n    final int numPostings = bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    TermVectorsPostingsArray postings = termVectorsPostingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = sortPostings();\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets, hasPayloads);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n      tv.finishTerm();\n    }\n    tv.finishField();\n\n    reset();\n\n    fieldInfo.setStoreTermVectors();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3cc3fa1ecad75b99ec55169e44628808f9866ad","date":1592311545,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField#finishDocument().mjava","sourceNew":"  void finishDocument() throws IOException {\n    if (doVectors == false) {\n      return;\n    }\n\n    doVectors = false;\n\n    final int numPostings = getNumTerms();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    TermVectorsPostingsArray postings = termVectorsPostingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    sortTerms();\n    final int[] termIDs = getSortedTermIDs();\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets, hasPayloads);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n      tv.finishTerm();\n    }\n    tv.finishField();\n\n    reset();\n\n    fieldInfo.setStoreTermVectors();\n  }\n\n","sourceOld":"  void finishDocument() throws IOException {\n    if (doVectors == false) {\n      return;\n    }\n\n    doVectors = false;\n\n    final int numPostings = bytesHash.size();\n\n    final BytesRef flushTerm = termsWriter.flushTerm;\n\n    assert numPostings >= 0;\n\n    // This is called once, after inverting all occurrences\n    // of a given field in the doc.  At this point we flush\n    // our hash into the DocWriter.\n\n    TermVectorsPostingsArray postings = termVectorsPostingsArray;\n    final TermVectorsWriter tv = termsWriter.writer;\n\n    final int[] termIDs = sortPostings();\n\n    tv.startField(fieldInfo, numPostings, doVectorPositions, doVectorOffsets, hasPayloads);\n    \n    final ByteSliceReader posReader = doVectorPositions ? termsWriter.vectorSliceReaderPos : null;\n    final ByteSliceReader offReader = doVectorOffsets ? termsWriter.vectorSliceReaderOff : null;\n    \n    for(int j=0;j<numPostings;j++) {\n      final int termID = termIDs[j];\n      final int freq = postings.freqs[termID];\n\n      // Get BytesRef\n      termBytePool.setBytesRef(flushTerm, postings.textStarts[termID]);\n      tv.startTerm(flushTerm, freq);\n      \n      if (doVectorPositions || doVectorOffsets) {\n        if (posReader != null) {\n          initReader(posReader, termID, 0);\n        }\n        if (offReader != null) {\n          initReader(offReader, termID, 1);\n        }\n        tv.addProx(freq, posReader, offReader);\n      }\n      tv.finishTerm();\n    }\n    tv.finishField();\n\n    reset();\n\n    fieldInfo.setStoreTermVectors();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"9cdbc2cadeaf282528fe4d1c06e9f8bee38ccec4":["3394716f52b34ab259ad5247e7595d9f9db6e935"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":["519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5","3394716f52b34ab259ad5247e7595d9f9db6e935"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3394716f52b34ab259ad5247e7595d9f9db6e935":["519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5","52c7e49be259508735752fba88085255014a6ecf"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","4356000e349e38c9fb48034695b7c309abd54557"],"4356000e349e38c9fb48034695b7c309abd54557":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":["615ddbd81799980d0fdd95e0238e1c498b6f47b0","0935c850ea562932997b72c69d93e345f21d7f45"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["fc7a7bb1aa79cf53564793bb5ffa270250c679da","a1552916e7512a3bbf5d2364c3a97a677ce055ea"],"a1552916e7512a3bbf5d2364c3a97a677ce055ea":["4e6354dd7c71fe122926fc53d7d29f715b1283db"],"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"fc7a7bb1aa79cf53564793bb5ffa270250c679da":["0935c850ea562932997b72c69d93e345f21d7f45"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0935c850ea562932997b72c69d93e345f21d7f45":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"52c7e49be259508735752fba88085255014a6ecf":["519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5"],"d3cc3fa1ecad75b99ec55169e44628808f9866ad":["9cdbc2cadeaf282528fe4d1c06e9f8bee38ccec4"],"4e6354dd7c71fe122926fc53d7d29f715b1283db":["0935c850ea562932997b72c69d93e345f21d7f45","fc7a7bb1aa79cf53564793bb5ffa270250c679da"],"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9":["615ddbd81799980d0fdd95e0238e1c498b6f47b0","0935c850ea562932997b72c69d93e345f21d7f45"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d3cc3fa1ecad75b99ec55169e44628808f9866ad"]},"commit2Childs":{"9cdbc2cadeaf282528fe4d1c06e9f8bee38ccec4":["d3cc3fa1ecad75b99ec55169e44628808f9866ad"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["615ddbd81799980d0fdd95e0238e1c498b6f47b0","4356000e349e38c9fb48034695b7c309abd54557"],"3394716f52b34ab259ad5247e7595d9f9db6e935":["9cdbc2cadeaf282528fe4d1c06e9f8bee38ccec4","c0cd85fde84cb318b4dc97710dcf15e2959a1bbe"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["c7869f64c874ebf7f317d22c00baf2b6857797a6","0935c850ea562932997b72c69d93e345f21d7f45","d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9"],"4356000e349e38c9fb48034695b7c309abd54557":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":[],"d4d69c535930b5cce125cff868d40f6373dc27d4":["519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5"],"a1552916e7512a3bbf5d2364c3a97a677ce055ea":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","3394716f52b34ab259ad5247e7595d9f9db6e935","52c7e49be259508735752fba88085255014a6ecf"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"fc7a7bb1aa79cf53564793bb5ffa270250c679da":["d4d69c535930b5cce125cff868d40f6373dc27d4","4e6354dd7c71fe122926fc53d7d29f715b1283db"],"0935c850ea562932997b72c69d93e345f21d7f45":["c7869f64c874ebf7f317d22c00baf2b6857797a6","fc7a7bb1aa79cf53564793bb5ffa270250c679da","4e6354dd7c71fe122926fc53d7d29f715b1283db","d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9"],"52c7e49be259508735752fba88085255014a6ecf":["3394716f52b34ab259ad5247e7595d9f9db6e935"],"4e6354dd7c71fe122926fc53d7d29f715b1283db":["a1552916e7512a3bbf5d2364c3a97a677ce055ea"],"d3cc3fa1ecad75b99ec55169e44628808f9866ad":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","c7869f64c874ebf7f317d22c00baf2b6857797a6","d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}