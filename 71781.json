{"path":"lucene/highlighter/src/test/org/apache/lucene/search/uhighlight/TestUnifiedHighlighterRanking#testRanking().mjava","commits":[{"id":"1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2","date":1475611903,"type":0,"author":"David Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/uhighlight/TestUnifiedHighlighterRanking#testRanking().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * indexes a bunch of gibberish, and then highlights top(n).\n   * asserts that top(n) highlights is a subset of top(n+1) up to some max N\n   */\n  // TODO: this only tests single-valued fields. we should also index multiple values per field!\n  public void testRanking() throws Exception {\n    // number of documents: we will check each one\n    final int numDocs = atLeast(100);\n    // number of top-N snippets, we will check 1 .. N\n    final int maxTopN = 5;\n    // maximum number of elements to put in a sentence.\n    final int maxSentenceLength = 10;\n    // maximum number of sentences in a document\n    final int maxNumSentences = 20;\n\n    Directory dir = newDirectory();\n    indexAnalyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, indexAnalyzer);\n    Document document = new Document();\n    Field id = new StringField(\"id\", \"\", Field.Store.NO);\n    Field body = new Field(\"body\", \"\", fieldType);\n    document.add(id);\n    document.add(body);\n\n    for (int i = 0; i < numDocs; i++) {\n      StringBuilder bodyText = new StringBuilder();\n      int numSentences = TestUtil.nextInt(random(), 1, maxNumSentences);\n      for (int j = 0; j < numSentences; j++) {\n        bodyText.append(newSentence(random(), maxSentenceLength));\n      }\n      body.setStringValue(bodyText.toString());\n      id.setStringValue(Integer.toString(i));\n      iw.addDocument(document);\n    }\n\n    IndexReader ir = iw.getReader();\n    IndexSearcher searcher = newSearcher(ir);\n    for (int i = 0; i < numDocs; i++) {\n      checkDocument(searcher, i, maxTopN);\n    }\n    iw.close();\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/uhighlight/TestUnifiedHighlighterRanking#testRanking().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * indexes a bunch of gibberish, and then highlights top(n).\n   * asserts that top(n) highlights is a subset of top(n+1) up to some max N\n   */\n  // TODO: this only tests single-valued fields. we should also index multiple values per field!\n  public void testRanking() throws Exception {\n    // number of documents: we will check each one\n    final int numDocs = atLeast(100);\n    // number of top-N snippets, we will check 1 .. N\n    final int maxTopN = 5;\n    // maximum number of elements to put in a sentence.\n    final int maxSentenceLength = 10;\n    // maximum number of sentences in a document\n    final int maxNumSentences = 20;\n\n    Directory dir = newDirectory();\n    indexAnalyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, indexAnalyzer);\n    Document document = new Document();\n    Field id = new StringField(\"id\", \"\", Field.Store.NO);\n    Field body = new Field(\"body\", \"\", fieldType);\n    document.add(id);\n    document.add(body);\n\n    for (int i = 0; i < numDocs; i++) {\n      StringBuilder bodyText = new StringBuilder();\n      int numSentences = TestUtil.nextInt(random(), 1, maxNumSentences);\n      for (int j = 0; j < numSentences; j++) {\n        bodyText.append(newSentence(random(), maxSentenceLength));\n      }\n      body.setStringValue(bodyText.toString());\n      id.setStringValue(Integer.toString(i));\n      iw.addDocument(document);\n    }\n\n    IndexReader ir = iw.getReader();\n    IndexSearcher searcher = newSearcher(ir);\n    for (int i = 0; i < numDocs; i++) {\n      checkDocument(searcher, i, maxTopN);\n    }\n    iw.close();\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f","date":1579652839,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/uhighlight/TestUnifiedHighlighterRanking#testRanking().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/uhighlight/TestUnifiedHighlighterRanking#testRanking().mjava","sourceNew":"  /**\n   * indexes a bunch of gibberish, and then highlights top(n).\n   * asserts that top(n) highlights is a subset of top(n+1) up to some max N\n   */\n  // TODO: this only tests single-valued fields. we should also index multiple values per field!\n  public void testRanking() throws Exception {\n    // number of documents: we will check each one\n    final int numDocs = atLeast(20);\n    // number of top-N snippets, we will check 1 .. N\n    final int maxTopN = 3;\n    // maximum number of elements to put in a sentence.\n    final int maxSentenceLength = 10;\n    // maximum number of sentences in a document\n    final int maxNumSentences = 20;\n\n    Directory dir = newDirectory();\n    indexAnalyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, indexAnalyzer);\n    Document document = new Document();\n    Field id = new StringField(\"id\", \"\", Field.Store.NO);\n    Field body = new Field(\"body\", \"\", fieldType);\n    document.add(id);\n    document.add(body);\n\n    for (int i = 0; i < numDocs; i++) {\n      StringBuilder bodyText = new StringBuilder();\n      int numSentences = TestUtil.nextInt(random(), 1, maxNumSentences);\n      for (int j = 0; j < numSentences; j++) {\n        bodyText.append(newSentence(random(), maxSentenceLength));\n      }\n      body.setStringValue(bodyText.toString());\n      id.setStringValue(Integer.toString(i));\n      iw.addDocument(document);\n    }\n\n    IndexReader ir = iw.getReader();\n    IndexSearcher searcher = newSearcher(ir);\n    for (int i = 0; i < numDocs; i++) {\n      checkDocument(searcher, i, maxTopN);\n    }\n    iw.close();\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * indexes a bunch of gibberish, and then highlights top(n).\n   * asserts that top(n) highlights is a subset of top(n+1) up to some max N\n   */\n  // TODO: this only tests single-valued fields. we should also index multiple values per field!\n  public void testRanking() throws Exception {\n    // number of documents: we will check each one\n    final int numDocs = atLeast(100);\n    // number of top-N snippets, we will check 1 .. N\n    final int maxTopN = 5;\n    // maximum number of elements to put in a sentence.\n    final int maxSentenceLength = 10;\n    // maximum number of sentences in a document\n    final int maxNumSentences = 20;\n\n    Directory dir = newDirectory();\n    indexAnalyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, indexAnalyzer);\n    Document document = new Document();\n    Field id = new StringField(\"id\", \"\", Field.Store.NO);\n    Field body = new Field(\"body\", \"\", fieldType);\n    document.add(id);\n    document.add(body);\n\n    for (int i = 0; i < numDocs; i++) {\n      StringBuilder bodyText = new StringBuilder();\n      int numSentences = TestUtil.nextInt(random(), 1, maxNumSentences);\n      for (int j = 0; j < numSentences; j++) {\n        bodyText.append(newSentence(random(), maxSentenceLength));\n      }\n      body.setStringValue(bodyText.toString());\n      id.setStringValue(Integer.toString(i));\n      iw.addDocument(document);\n    }\n\n    IndexReader ir = iw.getReader();\n    IndexSearcher searcher = newSearcher(ir);\n    for (int i = 0; i < numDocs; i++) {\n      checkDocument(searcher, i, maxTopN);\n    }\n    iw.close();\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f":["1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f"]},"commit2Childs":{"fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2":["fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}