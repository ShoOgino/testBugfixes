{"path":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertTokenStreamContents(TokenStream,String[],int[],int[],String[],int[],int[],Integer,Integer,boolean[],boolean).mjava","commits":[{"id":"f3f8a0f8bebc057ea4bdf65150b3fdc539db3d03","date":1377018786,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertTokenStreamContents(TokenStream,String[],int[],int[],String[],int[],int[],Integer,Integer,boolean[],boolean).mjava","pathOld":"/dev/null","sourceNew":"  // offsetsAreCorrect also validates:\n  //   - graph offsets are correct (all tokens leaving from\n  //     pos X have the same startOffset; all tokens\n  //     arriving to pos Y have the same endOffset)\n  //   - offsets only move forwards (startOffset >=\n  //     lastStartOffset)\n  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[],\n                                               int posLengths[], Integer finalOffset, Integer finalPosInc, boolean[] keywordAtts,\n                                               boolean offsetsAreCorrect) throws IOException {\n    assertNotNull(output);\n    CheckClearAttributesAttribute checkClearAtt = ts.addAttribute(CheckClearAttributesAttribute.class);\n    \n    CharTermAttribute termAtt = null;\n    if (output.length > 0) {\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      termAtt = ts.getAttribute(CharTermAttribute.class);\n    }\n    \n    OffsetAttribute offsetAtt = null;\n    if (startOffsets != null || endOffsets != null || finalOffset != null) {\n      assertTrue(\"has no OffsetAttribute\", ts.hasAttribute(OffsetAttribute.class));\n      offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    }\n    \n    TypeAttribute typeAtt = null;\n    if (types != null) {\n      assertTrue(\"has no TypeAttribute\", ts.hasAttribute(TypeAttribute.class));\n      typeAtt = ts.getAttribute(TypeAttribute.class);\n    }\n    \n    PositionIncrementAttribute posIncrAtt = null;\n    if (posIncrements != null || finalPosInc != null) {\n      assertTrue(\"has no PositionIncrementAttribute\", ts.hasAttribute(PositionIncrementAttribute.class));\n      posIncrAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    }\n\n    PositionLengthAttribute posLengthAtt = null;\n    if (posLengths != null) {\n      assertTrue(\"has no PositionLengthAttribute\", ts.hasAttribute(PositionLengthAttribute.class));\n      posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    }\n\n    KeywordAttribute keywordAtt = null;\n    if (keywordAtts != null) {\n      assertTrue(\"has no KeywordAttribute\", ts.hasAttribute(KeywordAttribute.class));\n      keywordAtt = ts.getAttribute(KeywordAttribute.class);\n    }\n    \n    // Maps position to the start/end offset:\n    final Map<Integer,Integer> posToStartOffset = new HashMap<Integer,Integer>();\n    final Map<Integer,Integer> posToEndOffset = new HashMap<Integer,Integer>();\n\n    ts.reset();\n    int pos = -1;\n    int lastStartOffset = 0;\n    for (int i = 0; i < output.length; i++) {\n      // extra safety to enforce, that the state is not preserved and also assign bogus values\n      ts.clearAttributes();\n      termAtt.setEmpty().append(\"bogusTerm\");\n      if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n      if (typeAtt != null) typeAtt.setType(\"bogusType\");\n      if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n      if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n      if (keywordAtt != null) keywordAtt.setKeyword((i&1) == 0);\n      \n      checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n      assertTrue(\"token \"+i+\" does not exist\", ts.incrementToken());\n      assertTrue(\"clearAttributes() was not called correctly in TokenStream chain\", checkClearAtt.getAndResetClearCalled());\n      \n      assertEquals(\"term \"+i, output[i], termAtt.toString());\n      if (startOffsets != null) {\n        assertEquals(\"startOffset \"+i, startOffsets[i], offsetAtt.startOffset());\n      }\n      if (endOffsets != null) {\n        assertEquals(\"endOffset \"+i, endOffsets[i], offsetAtt.endOffset());\n      }\n      if (types != null) {\n        assertEquals(\"type \"+i, types[i], typeAtt.type());\n      }\n      if (posIncrements != null) {\n        assertEquals(\"posIncrement \"+i, posIncrements[i], posIncrAtt.getPositionIncrement());\n      }\n      if (posLengths != null) {\n        assertEquals(\"posLength \"+i, posLengths[i], posLengthAtt.getPositionLength());\n      }\n      if (keywordAtts != null) {\n        assertEquals(\"keywordAtt \" + i, keywordAtts[i], keywordAtt.isKeyword());\n      }\n      \n      // we can enforce some basic things about a few attributes even if the caller doesn't check:\n      if (offsetAtt != null) {\n        final int startOffset = offsetAtt.startOffset();\n        final int endOffset = offsetAtt.endOffset();\n        if (finalOffset != null) {\n          assertTrue(\"startOffset must be <= finalOffset\", startOffset <= finalOffset.intValue());\n          assertTrue(\"endOffset must be <= finalOffset: got endOffset=\" + endOffset + \" vs finalOffset=\" + finalOffset.intValue(),\n                     endOffset <= finalOffset.intValue());\n        }\n\n        if (offsetsAreCorrect) {\n          assertTrue(\"offsets must not go backwards startOffset=\" + startOffset + \" is < lastStartOffset=\" + lastStartOffset, offsetAtt.startOffset() >= lastStartOffset);\n          lastStartOffset = offsetAtt.startOffset();\n        }\n\n        if (offsetsAreCorrect && posLengthAtt != null && posIncrAtt != null) {\n          // Validate offset consistency in the graph, ie\n          // all tokens leaving from a certain pos have the\n          // same startOffset, and all tokens arriving to a\n          // certain pos have the same endOffset:\n          final int posInc = posIncrAtt.getPositionIncrement();\n          pos += posInc;\n\n          final int posLength = posLengthAtt.getPositionLength();\n\n          if (!posToStartOffset.containsKey(pos)) {\n            // First time we've seen a token leaving from this position:\n            posToStartOffset.put(pos, startOffset);\n            //System.out.println(\"  + s \" + pos + \" -> \" + startOffset);\n          } else {\n            // We've seen a token leaving from this position\n            // before; verify the startOffset is the same:\n            //System.out.println(\"  + vs \" + pos + \" -> \" + startOffset);\n            assertEquals(\"pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToStartOffset.get(pos).intValue(), startOffset);\n          }\n\n          final int endPos = pos + posLength;\n\n          if (!posToEndOffset.containsKey(endPos)) {\n            // First time we've seen a token arriving to this position:\n            posToEndOffset.put(endPos, endOffset);\n            //System.out.println(\"  + e \" + endPos + \" -> \" + endOffset);\n          } else {\n            // We've seen a token arriving to this position\n            // before; verify the endOffset is the same:\n            //System.out.println(\"  + ve \" + endPos + \" -> \" + endOffset);\n            assertEquals(\"pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToEndOffset.get(endPos).intValue(), endOffset);\n          }\n        }\n      }\n      if (posIncrAtt != null) {\n        if (i == 0) {\n          assertTrue(\"first posIncrement must be >= 1\", posIncrAtt.getPositionIncrement() >= 1);\n        } else {\n          assertTrue(\"posIncrement must be >= 0\", posIncrAtt.getPositionIncrement() >= 0);\n        }\n      }\n      if (posLengthAtt != null) {\n        assertTrue(\"posLength must be >= 1\", posLengthAtt.getPositionLength() >= 1);\n      }\n    }\n\n    if (ts.incrementToken()) {\n      fail(\"TokenStream has more tokens than expected (expected count=\" + output.length + \"); extra token=\" + termAtt.toString());\n    }\n\n    // repeat our extra safety checks for end()\n    ts.clearAttributes();\n    if (termAtt != null) termAtt.setEmpty().append(\"bogusTerm\");\n    if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n    if (typeAtt != null) typeAtt.setType(\"bogusType\");\n    if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n    if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n    \n    checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n\n    ts.end();\n    assertTrue(\"super.end()/clearAttributes() was not called correctly in end()\", checkClearAtt.getAndResetClearCalled());\n    \n    if (finalOffset != null) {\n      assertEquals(\"finalOffset\", finalOffset.intValue(), offsetAtt.endOffset());\n    }\n    if (offsetAtt != null) {\n      assertTrue(\"finalOffset must be >= 0\", offsetAtt.endOffset() >= 0);\n    }\n    if (finalPosInc != null) {\n      assertEquals(\"finalPosInc\", finalPosInc.intValue(), posIncrAtt.getPositionIncrement());\n    }\n\n    ts.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["995993f24c9f6feb42b49b71e1982cda8fa0b37c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":0,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertTokenStreamContents(TokenStream,String[],int[],int[],String[],int[],int[],Integer,Integer,boolean[],boolean).mjava","pathOld":"/dev/null","sourceNew":"  // offsetsAreCorrect also validates:\n  //   - graph offsets are correct (all tokens leaving from\n  //     pos X have the same startOffset; all tokens\n  //     arriving to pos Y have the same endOffset)\n  //   - offsets only move forwards (startOffset >=\n  //     lastStartOffset)\n  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[],\n                                               int posLengths[], Integer finalOffset, Integer finalPosInc, boolean[] keywordAtts,\n                                               boolean offsetsAreCorrect) throws IOException {\n    assertNotNull(output);\n    CheckClearAttributesAttribute checkClearAtt = ts.addAttribute(CheckClearAttributesAttribute.class);\n    \n    CharTermAttribute termAtt = null;\n    if (output.length > 0) {\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      termAtt = ts.getAttribute(CharTermAttribute.class);\n    }\n    \n    OffsetAttribute offsetAtt = null;\n    if (startOffsets != null || endOffsets != null || finalOffset != null) {\n      assertTrue(\"has no OffsetAttribute\", ts.hasAttribute(OffsetAttribute.class));\n      offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    }\n    \n    TypeAttribute typeAtt = null;\n    if (types != null) {\n      assertTrue(\"has no TypeAttribute\", ts.hasAttribute(TypeAttribute.class));\n      typeAtt = ts.getAttribute(TypeAttribute.class);\n    }\n    \n    PositionIncrementAttribute posIncrAtt = null;\n    if (posIncrements != null || finalPosInc != null) {\n      assertTrue(\"has no PositionIncrementAttribute\", ts.hasAttribute(PositionIncrementAttribute.class));\n      posIncrAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    }\n\n    PositionLengthAttribute posLengthAtt = null;\n    if (posLengths != null) {\n      assertTrue(\"has no PositionLengthAttribute\", ts.hasAttribute(PositionLengthAttribute.class));\n      posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    }\n\n    KeywordAttribute keywordAtt = null;\n    if (keywordAtts != null) {\n      assertTrue(\"has no KeywordAttribute\", ts.hasAttribute(KeywordAttribute.class));\n      keywordAtt = ts.getAttribute(KeywordAttribute.class);\n    }\n    \n    // Maps position to the start/end offset:\n    final Map<Integer,Integer> posToStartOffset = new HashMap<Integer,Integer>();\n    final Map<Integer,Integer> posToEndOffset = new HashMap<Integer,Integer>();\n\n    ts.reset();\n    int pos = -1;\n    int lastStartOffset = 0;\n    for (int i = 0; i < output.length; i++) {\n      // extra safety to enforce, that the state is not preserved and also assign bogus values\n      ts.clearAttributes();\n      termAtt.setEmpty().append(\"bogusTerm\");\n      if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n      if (typeAtt != null) typeAtt.setType(\"bogusType\");\n      if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n      if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n      if (keywordAtt != null) keywordAtt.setKeyword((i&1) == 0);\n      \n      checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n      assertTrue(\"token \"+i+\" does not exist\", ts.incrementToken());\n      assertTrue(\"clearAttributes() was not called correctly in TokenStream chain\", checkClearAtt.getAndResetClearCalled());\n      \n      assertEquals(\"term \"+i, output[i], termAtt.toString());\n      if (startOffsets != null) {\n        assertEquals(\"startOffset \"+i, startOffsets[i], offsetAtt.startOffset());\n      }\n      if (endOffsets != null) {\n        assertEquals(\"endOffset \"+i, endOffsets[i], offsetAtt.endOffset());\n      }\n      if (types != null) {\n        assertEquals(\"type \"+i, types[i], typeAtt.type());\n      }\n      if (posIncrements != null) {\n        assertEquals(\"posIncrement \"+i, posIncrements[i], posIncrAtt.getPositionIncrement());\n      }\n      if (posLengths != null) {\n        assertEquals(\"posLength \"+i, posLengths[i], posLengthAtt.getPositionLength());\n      }\n      if (keywordAtts != null) {\n        assertEquals(\"keywordAtt \" + i, keywordAtts[i], keywordAtt.isKeyword());\n      }\n      \n      // we can enforce some basic things about a few attributes even if the caller doesn't check:\n      if (offsetAtt != null) {\n        final int startOffset = offsetAtt.startOffset();\n        final int endOffset = offsetAtt.endOffset();\n        if (finalOffset != null) {\n          assertTrue(\"startOffset must be <= finalOffset\", startOffset <= finalOffset.intValue());\n          assertTrue(\"endOffset must be <= finalOffset: got endOffset=\" + endOffset + \" vs finalOffset=\" + finalOffset.intValue(),\n                     endOffset <= finalOffset.intValue());\n        }\n\n        if (offsetsAreCorrect) {\n          assertTrue(\"offsets must not go backwards startOffset=\" + startOffset + \" is < lastStartOffset=\" + lastStartOffset, offsetAtt.startOffset() >= lastStartOffset);\n          lastStartOffset = offsetAtt.startOffset();\n        }\n\n        if (offsetsAreCorrect && posLengthAtt != null && posIncrAtt != null) {\n          // Validate offset consistency in the graph, ie\n          // all tokens leaving from a certain pos have the\n          // same startOffset, and all tokens arriving to a\n          // certain pos have the same endOffset:\n          final int posInc = posIncrAtt.getPositionIncrement();\n          pos += posInc;\n\n          final int posLength = posLengthAtt.getPositionLength();\n\n          if (!posToStartOffset.containsKey(pos)) {\n            // First time we've seen a token leaving from this position:\n            posToStartOffset.put(pos, startOffset);\n            //System.out.println(\"  + s \" + pos + \" -> \" + startOffset);\n          } else {\n            // We've seen a token leaving from this position\n            // before; verify the startOffset is the same:\n            //System.out.println(\"  + vs \" + pos + \" -> \" + startOffset);\n            assertEquals(\"pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToStartOffset.get(pos).intValue(), startOffset);\n          }\n\n          final int endPos = pos + posLength;\n\n          if (!posToEndOffset.containsKey(endPos)) {\n            // First time we've seen a token arriving to this position:\n            posToEndOffset.put(endPos, endOffset);\n            //System.out.println(\"  + e \" + endPos + \" -> \" + endOffset);\n          } else {\n            // We've seen a token arriving to this position\n            // before; verify the endOffset is the same:\n            //System.out.println(\"  + ve \" + endPos + \" -> \" + endOffset);\n            assertEquals(\"pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToEndOffset.get(endPos).intValue(), endOffset);\n          }\n        }\n      }\n      if (posIncrAtt != null) {\n        if (i == 0) {\n          assertTrue(\"first posIncrement must be >= 1\", posIncrAtt.getPositionIncrement() >= 1);\n        } else {\n          assertTrue(\"posIncrement must be >= 0\", posIncrAtt.getPositionIncrement() >= 0);\n        }\n      }\n      if (posLengthAtt != null) {\n        assertTrue(\"posLength must be >= 1\", posLengthAtt.getPositionLength() >= 1);\n      }\n    }\n\n    if (ts.incrementToken()) {\n      fail(\"TokenStream has more tokens than expected (expected count=\" + output.length + \"); extra token=\" + termAtt.toString());\n    }\n\n    // repeat our extra safety checks for end()\n    ts.clearAttributes();\n    if (termAtt != null) termAtt.setEmpty().append(\"bogusTerm\");\n    if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n    if (typeAtt != null) typeAtt.setType(\"bogusType\");\n    if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n    if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n    \n    checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n\n    ts.end();\n    assertTrue(\"super.end()/clearAttributes() was not called correctly in end()\", checkClearAtt.getAndResetClearCalled());\n    \n    if (finalOffset != null) {\n      assertEquals(\"finalOffset\", finalOffset.intValue(), offsetAtt.endOffset());\n    }\n    if (offsetAtt != null) {\n      assertTrue(\"finalOffset must be >= 0\", offsetAtt.endOffset() >= 0);\n    }\n    if (finalPosInc != null) {\n      assertEquals(\"finalPosInc\", finalPosInc.intValue(), posIncrAtt.getPositionIncrement());\n    }\n\n    ts.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertTokenStreamContents(TokenStream,String[],int[],int[],String[],int[],int[],Integer,Integer,boolean[],boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertTokenStreamContents(TokenStream,String[],int[],int[],String[],int[],int[],Integer,Integer,boolean[],boolean).mjava","sourceNew":"  // offsetsAreCorrect also validates:\n  //   - graph offsets are correct (all tokens leaving from\n  //     pos X have the same startOffset; all tokens\n  //     arriving to pos Y have the same endOffset)\n  //   - offsets only move forwards (startOffset >=\n  //     lastStartOffset)\n  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[],\n                                               int posLengths[], Integer finalOffset, Integer finalPosInc, boolean[] keywordAtts,\n                                               boolean offsetsAreCorrect) throws IOException {\n    assertNotNull(output);\n    CheckClearAttributesAttribute checkClearAtt = ts.addAttribute(CheckClearAttributesAttribute.class);\n    \n    CharTermAttribute termAtt = null;\n    if (output.length > 0) {\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      termAtt = ts.getAttribute(CharTermAttribute.class);\n    }\n    \n    OffsetAttribute offsetAtt = null;\n    if (startOffsets != null || endOffsets != null || finalOffset != null) {\n      assertTrue(\"has no OffsetAttribute\", ts.hasAttribute(OffsetAttribute.class));\n      offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    }\n    \n    TypeAttribute typeAtt = null;\n    if (types != null) {\n      assertTrue(\"has no TypeAttribute\", ts.hasAttribute(TypeAttribute.class));\n      typeAtt = ts.getAttribute(TypeAttribute.class);\n    }\n    \n    PositionIncrementAttribute posIncrAtt = null;\n    if (posIncrements != null || finalPosInc != null) {\n      assertTrue(\"has no PositionIncrementAttribute\", ts.hasAttribute(PositionIncrementAttribute.class));\n      posIncrAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    }\n\n    PositionLengthAttribute posLengthAtt = null;\n    if (posLengths != null) {\n      assertTrue(\"has no PositionLengthAttribute\", ts.hasAttribute(PositionLengthAttribute.class));\n      posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    }\n\n    KeywordAttribute keywordAtt = null;\n    if (keywordAtts != null) {\n      assertTrue(\"has no KeywordAttribute\", ts.hasAttribute(KeywordAttribute.class));\n      keywordAtt = ts.getAttribute(KeywordAttribute.class);\n    }\n    \n    // Maps position to the start/end offset:\n    final Map<Integer,Integer> posToStartOffset = new HashMap<>();\n    final Map<Integer,Integer> posToEndOffset = new HashMap<>();\n\n    ts.reset();\n    int pos = -1;\n    int lastStartOffset = 0;\n    for (int i = 0; i < output.length; i++) {\n      // extra safety to enforce, that the state is not preserved and also assign bogus values\n      ts.clearAttributes();\n      termAtt.setEmpty().append(\"bogusTerm\");\n      if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n      if (typeAtt != null) typeAtt.setType(\"bogusType\");\n      if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n      if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n      if (keywordAtt != null) keywordAtt.setKeyword((i&1) == 0);\n      \n      checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n      assertTrue(\"token \"+i+\" does not exist\", ts.incrementToken());\n      assertTrue(\"clearAttributes() was not called correctly in TokenStream chain\", checkClearAtt.getAndResetClearCalled());\n      \n      assertEquals(\"term \"+i, output[i], termAtt.toString());\n      if (startOffsets != null) {\n        assertEquals(\"startOffset \"+i, startOffsets[i], offsetAtt.startOffset());\n      }\n      if (endOffsets != null) {\n        assertEquals(\"endOffset \"+i, endOffsets[i], offsetAtt.endOffset());\n      }\n      if (types != null) {\n        assertEquals(\"type \"+i, types[i], typeAtt.type());\n      }\n      if (posIncrements != null) {\n        assertEquals(\"posIncrement \"+i, posIncrements[i], posIncrAtt.getPositionIncrement());\n      }\n      if (posLengths != null) {\n        assertEquals(\"posLength \"+i, posLengths[i], posLengthAtt.getPositionLength());\n      }\n      if (keywordAtts != null) {\n        assertEquals(\"keywordAtt \" + i, keywordAtts[i], keywordAtt.isKeyword());\n      }\n      \n      // we can enforce some basic things about a few attributes even if the caller doesn't check:\n      if (offsetAtt != null) {\n        final int startOffset = offsetAtt.startOffset();\n        final int endOffset = offsetAtt.endOffset();\n        if (finalOffset != null) {\n          assertTrue(\"startOffset must be <= finalOffset\", startOffset <= finalOffset.intValue());\n          assertTrue(\"endOffset must be <= finalOffset: got endOffset=\" + endOffset + \" vs finalOffset=\" + finalOffset.intValue(),\n                     endOffset <= finalOffset.intValue());\n        }\n\n        if (offsetsAreCorrect) {\n          assertTrue(\"offsets must not go backwards startOffset=\" + startOffset + \" is < lastStartOffset=\" + lastStartOffset, offsetAtt.startOffset() >= lastStartOffset);\n          lastStartOffset = offsetAtt.startOffset();\n        }\n\n        if (offsetsAreCorrect && posLengthAtt != null && posIncrAtt != null) {\n          // Validate offset consistency in the graph, ie\n          // all tokens leaving from a certain pos have the\n          // same startOffset, and all tokens arriving to a\n          // certain pos have the same endOffset:\n          final int posInc = posIncrAtt.getPositionIncrement();\n          pos += posInc;\n\n          final int posLength = posLengthAtt.getPositionLength();\n\n          if (!posToStartOffset.containsKey(pos)) {\n            // First time we've seen a token leaving from this position:\n            posToStartOffset.put(pos, startOffset);\n            //System.out.println(\"  + s \" + pos + \" -> \" + startOffset);\n          } else {\n            // We've seen a token leaving from this position\n            // before; verify the startOffset is the same:\n            //System.out.println(\"  + vs \" + pos + \" -> \" + startOffset);\n            assertEquals(\"pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToStartOffset.get(pos).intValue(), startOffset);\n          }\n\n          final int endPos = pos + posLength;\n\n          if (!posToEndOffset.containsKey(endPos)) {\n            // First time we've seen a token arriving to this position:\n            posToEndOffset.put(endPos, endOffset);\n            //System.out.println(\"  + e \" + endPos + \" -> \" + endOffset);\n          } else {\n            // We've seen a token arriving to this position\n            // before; verify the endOffset is the same:\n            //System.out.println(\"  + ve \" + endPos + \" -> \" + endOffset);\n            assertEquals(\"pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToEndOffset.get(endPos).intValue(), endOffset);\n          }\n        }\n      }\n      if (posIncrAtt != null) {\n        if (i == 0) {\n          assertTrue(\"first posIncrement must be >= 1\", posIncrAtt.getPositionIncrement() >= 1);\n        } else {\n          assertTrue(\"posIncrement must be >= 0\", posIncrAtt.getPositionIncrement() >= 0);\n        }\n      }\n      if (posLengthAtt != null) {\n        assertTrue(\"posLength must be >= 1\", posLengthAtt.getPositionLength() >= 1);\n      }\n    }\n\n    if (ts.incrementToken()) {\n      fail(\"TokenStream has more tokens than expected (expected count=\" + output.length + \"); extra token=\" + termAtt.toString());\n    }\n\n    // repeat our extra safety checks for end()\n    ts.clearAttributes();\n    if (termAtt != null) termAtt.setEmpty().append(\"bogusTerm\");\n    if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n    if (typeAtt != null) typeAtt.setType(\"bogusType\");\n    if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n    if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n    \n    checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n\n    ts.end();\n    assertTrue(\"super.end()/clearAttributes() was not called correctly in end()\", checkClearAtt.getAndResetClearCalled());\n    \n    if (finalOffset != null) {\n      assertEquals(\"finalOffset\", finalOffset.intValue(), offsetAtt.endOffset());\n    }\n    if (offsetAtt != null) {\n      assertTrue(\"finalOffset must be >= 0\", offsetAtt.endOffset() >= 0);\n    }\n    if (finalPosInc != null) {\n      assertEquals(\"finalPosInc\", finalPosInc.intValue(), posIncrAtt.getPositionIncrement());\n    }\n\n    ts.close();\n  }\n\n","sourceOld":"  // offsetsAreCorrect also validates:\n  //   - graph offsets are correct (all tokens leaving from\n  //     pos X have the same startOffset; all tokens\n  //     arriving to pos Y have the same endOffset)\n  //   - offsets only move forwards (startOffset >=\n  //     lastStartOffset)\n  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[],\n                                               int posLengths[], Integer finalOffset, Integer finalPosInc, boolean[] keywordAtts,\n                                               boolean offsetsAreCorrect) throws IOException {\n    assertNotNull(output);\n    CheckClearAttributesAttribute checkClearAtt = ts.addAttribute(CheckClearAttributesAttribute.class);\n    \n    CharTermAttribute termAtt = null;\n    if (output.length > 0) {\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      termAtt = ts.getAttribute(CharTermAttribute.class);\n    }\n    \n    OffsetAttribute offsetAtt = null;\n    if (startOffsets != null || endOffsets != null || finalOffset != null) {\n      assertTrue(\"has no OffsetAttribute\", ts.hasAttribute(OffsetAttribute.class));\n      offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    }\n    \n    TypeAttribute typeAtt = null;\n    if (types != null) {\n      assertTrue(\"has no TypeAttribute\", ts.hasAttribute(TypeAttribute.class));\n      typeAtt = ts.getAttribute(TypeAttribute.class);\n    }\n    \n    PositionIncrementAttribute posIncrAtt = null;\n    if (posIncrements != null || finalPosInc != null) {\n      assertTrue(\"has no PositionIncrementAttribute\", ts.hasAttribute(PositionIncrementAttribute.class));\n      posIncrAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    }\n\n    PositionLengthAttribute posLengthAtt = null;\n    if (posLengths != null) {\n      assertTrue(\"has no PositionLengthAttribute\", ts.hasAttribute(PositionLengthAttribute.class));\n      posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    }\n\n    KeywordAttribute keywordAtt = null;\n    if (keywordAtts != null) {\n      assertTrue(\"has no KeywordAttribute\", ts.hasAttribute(KeywordAttribute.class));\n      keywordAtt = ts.getAttribute(KeywordAttribute.class);\n    }\n    \n    // Maps position to the start/end offset:\n    final Map<Integer,Integer> posToStartOffset = new HashMap<Integer,Integer>();\n    final Map<Integer,Integer> posToEndOffset = new HashMap<Integer,Integer>();\n\n    ts.reset();\n    int pos = -1;\n    int lastStartOffset = 0;\n    for (int i = 0; i < output.length; i++) {\n      // extra safety to enforce, that the state is not preserved and also assign bogus values\n      ts.clearAttributes();\n      termAtt.setEmpty().append(\"bogusTerm\");\n      if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n      if (typeAtt != null) typeAtt.setType(\"bogusType\");\n      if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n      if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n      if (keywordAtt != null) keywordAtt.setKeyword((i&1) == 0);\n      \n      checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n      assertTrue(\"token \"+i+\" does not exist\", ts.incrementToken());\n      assertTrue(\"clearAttributes() was not called correctly in TokenStream chain\", checkClearAtt.getAndResetClearCalled());\n      \n      assertEquals(\"term \"+i, output[i], termAtt.toString());\n      if (startOffsets != null) {\n        assertEquals(\"startOffset \"+i, startOffsets[i], offsetAtt.startOffset());\n      }\n      if (endOffsets != null) {\n        assertEquals(\"endOffset \"+i, endOffsets[i], offsetAtt.endOffset());\n      }\n      if (types != null) {\n        assertEquals(\"type \"+i, types[i], typeAtt.type());\n      }\n      if (posIncrements != null) {\n        assertEquals(\"posIncrement \"+i, posIncrements[i], posIncrAtt.getPositionIncrement());\n      }\n      if (posLengths != null) {\n        assertEquals(\"posLength \"+i, posLengths[i], posLengthAtt.getPositionLength());\n      }\n      if (keywordAtts != null) {\n        assertEquals(\"keywordAtt \" + i, keywordAtts[i], keywordAtt.isKeyword());\n      }\n      \n      // we can enforce some basic things about a few attributes even if the caller doesn't check:\n      if (offsetAtt != null) {\n        final int startOffset = offsetAtt.startOffset();\n        final int endOffset = offsetAtt.endOffset();\n        if (finalOffset != null) {\n          assertTrue(\"startOffset must be <= finalOffset\", startOffset <= finalOffset.intValue());\n          assertTrue(\"endOffset must be <= finalOffset: got endOffset=\" + endOffset + \" vs finalOffset=\" + finalOffset.intValue(),\n                     endOffset <= finalOffset.intValue());\n        }\n\n        if (offsetsAreCorrect) {\n          assertTrue(\"offsets must not go backwards startOffset=\" + startOffset + \" is < lastStartOffset=\" + lastStartOffset, offsetAtt.startOffset() >= lastStartOffset);\n          lastStartOffset = offsetAtt.startOffset();\n        }\n\n        if (offsetsAreCorrect && posLengthAtt != null && posIncrAtt != null) {\n          // Validate offset consistency in the graph, ie\n          // all tokens leaving from a certain pos have the\n          // same startOffset, and all tokens arriving to a\n          // certain pos have the same endOffset:\n          final int posInc = posIncrAtt.getPositionIncrement();\n          pos += posInc;\n\n          final int posLength = posLengthAtt.getPositionLength();\n\n          if (!posToStartOffset.containsKey(pos)) {\n            // First time we've seen a token leaving from this position:\n            posToStartOffset.put(pos, startOffset);\n            //System.out.println(\"  + s \" + pos + \" -> \" + startOffset);\n          } else {\n            // We've seen a token leaving from this position\n            // before; verify the startOffset is the same:\n            //System.out.println(\"  + vs \" + pos + \" -> \" + startOffset);\n            assertEquals(\"pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToStartOffset.get(pos).intValue(), startOffset);\n          }\n\n          final int endPos = pos + posLength;\n\n          if (!posToEndOffset.containsKey(endPos)) {\n            // First time we've seen a token arriving to this position:\n            posToEndOffset.put(endPos, endOffset);\n            //System.out.println(\"  + e \" + endPos + \" -> \" + endOffset);\n          } else {\n            // We've seen a token arriving to this position\n            // before; verify the endOffset is the same:\n            //System.out.println(\"  + ve \" + endPos + \" -> \" + endOffset);\n            assertEquals(\"pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToEndOffset.get(endPos).intValue(), endOffset);\n          }\n        }\n      }\n      if (posIncrAtt != null) {\n        if (i == 0) {\n          assertTrue(\"first posIncrement must be >= 1\", posIncrAtt.getPositionIncrement() >= 1);\n        } else {\n          assertTrue(\"posIncrement must be >= 0\", posIncrAtt.getPositionIncrement() >= 0);\n        }\n      }\n      if (posLengthAtt != null) {\n        assertTrue(\"posLength must be >= 1\", posLengthAtt.getPositionLength() >= 1);\n      }\n    }\n\n    if (ts.incrementToken()) {\n      fail(\"TokenStream has more tokens than expected (expected count=\" + output.length + \"); extra token=\" + termAtt.toString());\n    }\n\n    // repeat our extra safety checks for end()\n    ts.clearAttributes();\n    if (termAtt != null) termAtt.setEmpty().append(\"bogusTerm\");\n    if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n    if (typeAtt != null) typeAtt.setType(\"bogusType\");\n    if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n    if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n    \n    checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n\n    ts.end();\n    assertTrue(\"super.end()/clearAttributes() was not called correctly in end()\", checkClearAtt.getAndResetClearCalled());\n    \n    if (finalOffset != null) {\n      assertEquals(\"finalOffset\", finalOffset.intValue(), offsetAtt.endOffset());\n    }\n    if (offsetAtt != null) {\n      assertTrue(\"finalOffset must be >= 0\", offsetAtt.endOffset() >= 0);\n    }\n    if (finalPosInc != null) {\n      assertEquals(\"finalPosInc\", finalPosInc.intValue(), posIncrAtt.getPositionIncrement());\n    }\n\n    ts.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"82dada84151ca949c188f7d68251edbda0271d8d","date":1421160470,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertTokenStreamContents(TokenStream,String[],int[],int[],String[],int[],int[],Integer,Integer,boolean[],boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertTokenStreamContents(TokenStream,String[],int[],int[],String[],int[],int[],Integer,Integer,boolean[],boolean).mjava","sourceNew":"  // offsetsAreCorrect also validates:\n  //   - graph offsets are correct (all tokens leaving from\n  //     pos X have the same startOffset; all tokens\n  //     arriving to pos Y have the same endOffset)\n  //   - offsets only move forwards (startOffset >=\n  //     lastStartOffset)\n  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[],\n                                               int posLengths[], Integer finalOffset, Integer finalPosInc, boolean[] keywordAtts,\n                                               boolean offsetsAreCorrect) throws IOException {\n    assertNotNull(output);\n    CheckClearAttributesAttribute checkClearAtt = ts.addAttribute(CheckClearAttributesAttribute.class);\n    \n    CharTermAttribute termAtt = null;\n    if (output.length > 0) {\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      termAtt = ts.getAttribute(CharTermAttribute.class);\n    }\n    \n    OffsetAttribute offsetAtt = null;\n    if (startOffsets != null || endOffsets != null || finalOffset != null) {\n      assertTrue(\"has no OffsetAttribute\", ts.hasAttribute(OffsetAttribute.class));\n      offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    }\n    \n    TypeAttribute typeAtt = null;\n    if (types != null) {\n      assertTrue(\"has no TypeAttribute\", ts.hasAttribute(TypeAttribute.class));\n      typeAtt = ts.getAttribute(TypeAttribute.class);\n    }\n    \n    PositionIncrementAttribute posIncrAtt = null;\n    if (posIncrements != null || finalPosInc != null) {\n      assertTrue(\"has no PositionIncrementAttribute\", ts.hasAttribute(PositionIncrementAttribute.class));\n      posIncrAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    }\n\n    PositionLengthAttribute posLengthAtt = null;\n    if (posLengths != null) {\n      assertTrue(\"has no PositionLengthAttribute\", ts.hasAttribute(PositionLengthAttribute.class));\n      posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    }\n\n    KeywordAttribute keywordAtt = null;\n    if (keywordAtts != null) {\n      assertTrue(\"has no KeywordAttribute\", ts.hasAttribute(KeywordAttribute.class));\n      keywordAtt = ts.getAttribute(KeywordAttribute.class);\n    }\n    \n    // Maps position to the start/end offset:\n    final Map<Integer,Integer> posToStartOffset = new HashMap<>();\n    final Map<Integer,Integer> posToEndOffset = new HashMap<>();\n\n    ts.reset();\n    int pos = -1;\n    int lastStartOffset = 0;\n    for (int i = 0; i < output.length; i++) {\n      // extra safety to enforce, that the state is not preserved and also assign bogus values\n      ts.clearAttributes();\n      termAtt.setEmpty().append(\"bogusTerm\");\n      if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n      if (typeAtt != null) typeAtt.setType(\"bogusType\");\n      if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n      if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n      if (keywordAtt != null) keywordAtt.setKeyword((i&1) == 0);\n      \n      checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n      assertTrue(\"token \"+i+\" does not exist\", ts.incrementToken());\n      assertTrue(\"clearAttributes() was not called correctly in TokenStream chain\", checkClearAtt.getAndResetClearCalled());\n      \n      assertEquals(\"term \"+i, output[i], termAtt.toString());\n      if (startOffsets != null) {\n        assertEquals(\"startOffset \"+i, startOffsets[i], offsetAtt.startOffset());\n      }\n      if (endOffsets != null) {\n        assertEquals(\"endOffset \"+i, endOffsets[i], offsetAtt.endOffset());\n      }\n      if (types != null) {\n        assertEquals(\"type \"+i, types[i], typeAtt.type());\n      }\n      if (posIncrements != null) {\n        assertEquals(\"posIncrement \"+i, posIncrements[i], posIncrAtt.getPositionIncrement());\n      }\n      if (posLengths != null) {\n        assertEquals(\"posLength \"+i, posLengths[i], posLengthAtt.getPositionLength());\n      }\n      if (keywordAtts != null) {\n        assertEquals(\"keywordAtt \" + i, keywordAtts[i], keywordAtt.isKeyword());\n      }\n      \n      // we can enforce some basic things about a few attributes even if the caller doesn't check:\n      if (offsetAtt != null) {\n        final int startOffset = offsetAtt.startOffset();\n        final int endOffset = offsetAtt.endOffset();\n        if (finalOffset != null) {\n          assertTrue(\"startOffset must be <= finalOffset\", startOffset <= finalOffset.intValue());\n          assertTrue(\"endOffset must be <= finalOffset: got endOffset=\" + endOffset + \" vs finalOffset=\" + finalOffset.intValue(),\n                     endOffset <= finalOffset.intValue());\n        }\n\n        if (offsetsAreCorrect) {\n          assertTrue(\"offsets must not go backwards startOffset=\" + startOffset + \" is < lastStartOffset=\" + lastStartOffset, offsetAtt.startOffset() >= lastStartOffset);\n          lastStartOffset = offsetAtt.startOffset();\n        }\n\n        if (offsetsAreCorrect && posLengthAtt != null && posIncrAtt != null) {\n          // Validate offset consistency in the graph, ie\n          // all tokens leaving from a certain pos have the\n          // same startOffset, and all tokens arriving to a\n          // certain pos have the same endOffset:\n          final int posInc = posIncrAtt.getPositionIncrement();\n          pos += posInc;\n\n          final int posLength = posLengthAtt.getPositionLength();\n\n          if (!posToStartOffset.containsKey(pos)) {\n            // First time we've seen a token leaving from this position:\n            posToStartOffset.put(pos, startOffset);\n            //System.out.println(\"  + s \" + pos + \" -> \" + startOffset);\n          } else {\n            // We've seen a token leaving from this position\n            // before; verify the startOffset is the same:\n            //System.out.println(\"  + vs \" + pos + \" -> \" + startOffset);\n            assertEquals(\"pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToStartOffset.get(pos).intValue(), startOffset);\n          }\n\n          final int endPos = pos + posLength;\n\n          if (!posToEndOffset.containsKey(endPos)) {\n            // First time we've seen a token arriving to this position:\n            posToEndOffset.put(endPos, endOffset);\n            //System.out.println(\"  + e \" + endPos + \" -> \" + endOffset);\n          } else {\n            // We've seen a token arriving to this position\n            // before; verify the endOffset is the same:\n            //System.out.println(\"  + ve \" + endPos + \" -> \" + endOffset);\n            assertEquals(\"pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToEndOffset.get(endPos).intValue(), endOffset);\n          }\n        }\n      }\n      if (posIncrAtt != null) {\n        if (i == 0) {\n          assertTrue(\"first posIncrement must be >= 1\", posIncrAtt.getPositionIncrement() >= 1);\n        } else {\n          assertTrue(\"posIncrement must be >= 0\", posIncrAtt.getPositionIncrement() >= 0);\n        }\n      }\n      if (posLengthAtt != null) {\n        assertTrue(\"posLength must be >= 1\", posLengthAtt.getPositionLength() >= 1);\n      }\n    }\n\n    if (ts.incrementToken()) {\n      fail(\"TokenStream has more tokens than expected (expected count=\" + output.length + \"); extra token=\" + termAtt);\n    }\n\n    // repeat our extra safety checks for end()\n    ts.clearAttributes();\n    if (termAtt != null) termAtt.setEmpty().append(\"bogusTerm\");\n    if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n    if (typeAtt != null) typeAtt.setType(\"bogusType\");\n    if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n    if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n    \n    checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n\n    ts.end();\n    assertTrue(\"super.end()/clearAttributes() was not called correctly in end()\", checkClearAtt.getAndResetClearCalled());\n    \n    if (finalOffset != null) {\n      assertEquals(\"finalOffset\", finalOffset.intValue(), offsetAtt.endOffset());\n    }\n    if (offsetAtt != null) {\n      assertTrue(\"finalOffset must be >= 0\", offsetAtt.endOffset() >= 0);\n    }\n    if (finalPosInc != null) {\n      assertEquals(\"finalPosInc\", finalPosInc.intValue(), posIncrAtt.getPositionIncrement());\n    }\n\n    ts.close();\n  }\n\n","sourceOld":"  // offsetsAreCorrect also validates:\n  //   - graph offsets are correct (all tokens leaving from\n  //     pos X have the same startOffset; all tokens\n  //     arriving to pos Y have the same endOffset)\n  //   - offsets only move forwards (startOffset >=\n  //     lastStartOffset)\n  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[],\n                                               int posLengths[], Integer finalOffset, Integer finalPosInc, boolean[] keywordAtts,\n                                               boolean offsetsAreCorrect) throws IOException {\n    assertNotNull(output);\n    CheckClearAttributesAttribute checkClearAtt = ts.addAttribute(CheckClearAttributesAttribute.class);\n    \n    CharTermAttribute termAtt = null;\n    if (output.length > 0) {\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      termAtt = ts.getAttribute(CharTermAttribute.class);\n    }\n    \n    OffsetAttribute offsetAtt = null;\n    if (startOffsets != null || endOffsets != null || finalOffset != null) {\n      assertTrue(\"has no OffsetAttribute\", ts.hasAttribute(OffsetAttribute.class));\n      offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    }\n    \n    TypeAttribute typeAtt = null;\n    if (types != null) {\n      assertTrue(\"has no TypeAttribute\", ts.hasAttribute(TypeAttribute.class));\n      typeAtt = ts.getAttribute(TypeAttribute.class);\n    }\n    \n    PositionIncrementAttribute posIncrAtt = null;\n    if (posIncrements != null || finalPosInc != null) {\n      assertTrue(\"has no PositionIncrementAttribute\", ts.hasAttribute(PositionIncrementAttribute.class));\n      posIncrAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    }\n\n    PositionLengthAttribute posLengthAtt = null;\n    if (posLengths != null) {\n      assertTrue(\"has no PositionLengthAttribute\", ts.hasAttribute(PositionLengthAttribute.class));\n      posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    }\n\n    KeywordAttribute keywordAtt = null;\n    if (keywordAtts != null) {\n      assertTrue(\"has no KeywordAttribute\", ts.hasAttribute(KeywordAttribute.class));\n      keywordAtt = ts.getAttribute(KeywordAttribute.class);\n    }\n    \n    // Maps position to the start/end offset:\n    final Map<Integer,Integer> posToStartOffset = new HashMap<>();\n    final Map<Integer,Integer> posToEndOffset = new HashMap<>();\n\n    ts.reset();\n    int pos = -1;\n    int lastStartOffset = 0;\n    for (int i = 0; i < output.length; i++) {\n      // extra safety to enforce, that the state is not preserved and also assign bogus values\n      ts.clearAttributes();\n      termAtt.setEmpty().append(\"bogusTerm\");\n      if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n      if (typeAtt != null) typeAtt.setType(\"bogusType\");\n      if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n      if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n      if (keywordAtt != null) keywordAtt.setKeyword((i&1) == 0);\n      \n      checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n      assertTrue(\"token \"+i+\" does not exist\", ts.incrementToken());\n      assertTrue(\"clearAttributes() was not called correctly in TokenStream chain\", checkClearAtt.getAndResetClearCalled());\n      \n      assertEquals(\"term \"+i, output[i], termAtt.toString());\n      if (startOffsets != null) {\n        assertEquals(\"startOffset \"+i, startOffsets[i], offsetAtt.startOffset());\n      }\n      if (endOffsets != null) {\n        assertEquals(\"endOffset \"+i, endOffsets[i], offsetAtt.endOffset());\n      }\n      if (types != null) {\n        assertEquals(\"type \"+i, types[i], typeAtt.type());\n      }\n      if (posIncrements != null) {\n        assertEquals(\"posIncrement \"+i, posIncrements[i], posIncrAtt.getPositionIncrement());\n      }\n      if (posLengths != null) {\n        assertEquals(\"posLength \"+i, posLengths[i], posLengthAtt.getPositionLength());\n      }\n      if (keywordAtts != null) {\n        assertEquals(\"keywordAtt \" + i, keywordAtts[i], keywordAtt.isKeyword());\n      }\n      \n      // we can enforce some basic things about a few attributes even if the caller doesn't check:\n      if (offsetAtt != null) {\n        final int startOffset = offsetAtt.startOffset();\n        final int endOffset = offsetAtt.endOffset();\n        if (finalOffset != null) {\n          assertTrue(\"startOffset must be <= finalOffset\", startOffset <= finalOffset.intValue());\n          assertTrue(\"endOffset must be <= finalOffset: got endOffset=\" + endOffset + \" vs finalOffset=\" + finalOffset.intValue(),\n                     endOffset <= finalOffset.intValue());\n        }\n\n        if (offsetsAreCorrect) {\n          assertTrue(\"offsets must not go backwards startOffset=\" + startOffset + \" is < lastStartOffset=\" + lastStartOffset, offsetAtt.startOffset() >= lastStartOffset);\n          lastStartOffset = offsetAtt.startOffset();\n        }\n\n        if (offsetsAreCorrect && posLengthAtt != null && posIncrAtt != null) {\n          // Validate offset consistency in the graph, ie\n          // all tokens leaving from a certain pos have the\n          // same startOffset, and all tokens arriving to a\n          // certain pos have the same endOffset:\n          final int posInc = posIncrAtt.getPositionIncrement();\n          pos += posInc;\n\n          final int posLength = posLengthAtt.getPositionLength();\n\n          if (!posToStartOffset.containsKey(pos)) {\n            // First time we've seen a token leaving from this position:\n            posToStartOffset.put(pos, startOffset);\n            //System.out.println(\"  + s \" + pos + \" -> \" + startOffset);\n          } else {\n            // We've seen a token leaving from this position\n            // before; verify the startOffset is the same:\n            //System.out.println(\"  + vs \" + pos + \" -> \" + startOffset);\n            assertEquals(\"pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToStartOffset.get(pos).intValue(), startOffset);\n          }\n\n          final int endPos = pos + posLength;\n\n          if (!posToEndOffset.containsKey(endPos)) {\n            // First time we've seen a token arriving to this position:\n            posToEndOffset.put(endPos, endOffset);\n            //System.out.println(\"  + e \" + endPos + \" -> \" + endOffset);\n          } else {\n            // We've seen a token arriving to this position\n            // before; verify the endOffset is the same:\n            //System.out.println(\"  + ve \" + endPos + \" -> \" + endOffset);\n            assertEquals(\"pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToEndOffset.get(endPos).intValue(), endOffset);\n          }\n        }\n      }\n      if (posIncrAtt != null) {\n        if (i == 0) {\n          assertTrue(\"first posIncrement must be >= 1\", posIncrAtt.getPositionIncrement() >= 1);\n        } else {\n          assertTrue(\"posIncrement must be >= 0\", posIncrAtt.getPositionIncrement() >= 0);\n        }\n      }\n      if (posLengthAtt != null) {\n        assertTrue(\"posLength must be >= 1\", posLengthAtt.getPositionLength() >= 1);\n      }\n    }\n\n    if (ts.incrementToken()) {\n      fail(\"TokenStream has more tokens than expected (expected count=\" + output.length + \"); extra token=\" + termAtt.toString());\n    }\n\n    // repeat our extra safety checks for end()\n    ts.clearAttributes();\n    if (termAtt != null) termAtt.setEmpty().append(\"bogusTerm\");\n    if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n    if (typeAtt != null) typeAtt.setType(\"bogusType\");\n    if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n    if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n    \n    checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n\n    ts.end();\n    assertTrue(\"super.end()/clearAttributes() was not called correctly in end()\", checkClearAtt.getAndResetClearCalled());\n    \n    if (finalOffset != null) {\n      assertEquals(\"finalOffset\", finalOffset.intValue(), offsetAtt.endOffset());\n    }\n    if (offsetAtt != null) {\n      assertTrue(\"finalOffset must be >= 0\", offsetAtt.endOffset() >= 0);\n    }\n    if (finalPosInc != null) {\n      assertEquals(\"finalPosInc\", finalPosInc.intValue(), posIncrAtt.getPositionIncrement());\n    }\n\n    ts.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"24a98f5fdd23e04f85819dbc63b47a12f7c44311","date":1482439157,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertTokenStreamContents(TokenStream,String[],int[],int[],String[],int[],int[],Integer,Integer,boolean[],boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertTokenStreamContents(TokenStream,String[],int[],int[],String[],int[],int[],Integer,Integer,boolean[],boolean).mjava","sourceNew":"  // offsetsAreCorrect also validates:\n  //   - graph offsets are correct (all tokens leaving from\n  //     pos X have the same startOffset; all tokens\n  //     arriving to pos Y have the same endOffset)\n  //   - offsets only move forwards (startOffset >=\n  //     lastStartOffset)\n  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[],\n                                               int posLengths[], Integer finalOffset, Integer finalPosInc, boolean[] keywordAtts,\n                                               boolean offsetsAreCorrect) throws IOException {\n    assertNotNull(output);\n    CheckClearAttributesAttribute checkClearAtt = ts.addAttribute(CheckClearAttributesAttribute.class);\n    \n    CharTermAttribute termAtt = null;\n    if (output.length > 0) {\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      termAtt = ts.getAttribute(CharTermAttribute.class);\n    }\n    \n    OffsetAttribute offsetAtt = null;\n    if (startOffsets != null || endOffsets != null || finalOffset != null) {\n      assertTrue(\"has no OffsetAttribute\", ts.hasAttribute(OffsetAttribute.class));\n      offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    }\n    \n    TypeAttribute typeAtt = null;\n    if (types != null) {\n      assertTrue(\"has no TypeAttribute\", ts.hasAttribute(TypeAttribute.class));\n      typeAtt = ts.getAttribute(TypeAttribute.class);\n    }\n    \n    PositionIncrementAttribute posIncrAtt = null;\n    if (posIncrements != null || finalPosInc != null) {\n      assertTrue(\"has no PositionIncrementAttribute\", ts.hasAttribute(PositionIncrementAttribute.class));\n      posIncrAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    }\n\n    PositionLengthAttribute posLengthAtt = null;\n    if (posLengths != null) {\n      assertTrue(\"has no PositionLengthAttribute\", ts.hasAttribute(PositionLengthAttribute.class));\n      posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    }\n\n    KeywordAttribute keywordAtt = null;\n    if (keywordAtts != null) {\n      assertTrue(\"has no KeywordAttribute\", ts.hasAttribute(KeywordAttribute.class));\n      keywordAtt = ts.getAttribute(KeywordAttribute.class);\n    }\n    \n    // Maps position to the start/end offset:\n    final Map<Integer,Integer> posToStartOffset = new HashMap<>();\n    final Map<Integer,Integer> posToEndOffset = new HashMap<>();\n\n    ts.reset();\n    int pos = -1;\n    int lastStartOffset = 0;\n    for (int i = 0; i < output.length; i++) {\n      // extra safety to enforce, that the state is not preserved and also assign bogus values\n      ts.clearAttributes();\n      termAtt.setEmpty().append(\"bogusTerm\");\n      if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n      if (typeAtt != null) typeAtt.setType(\"bogusType\");\n      if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n      if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n      if (keywordAtt != null) keywordAtt.setKeyword((i&1) == 0);\n      \n      checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n      assertTrue(\"token \"+i+\" does not exist\", ts.incrementToken());\n      assertTrue(\"clearAttributes() was not called correctly in TokenStream chain\", checkClearAtt.getAndResetClearCalled());\n      \n      assertEquals(\"term \"+i, output[i], termAtt.toString());\n      if (startOffsets != null) {\n        assertEquals(\"startOffset \" + i + \" term=\" + termAtt, startOffsets[i], offsetAtt.startOffset());\n      }\n      if (endOffsets != null) {\n        assertEquals(\"endOffset \" + i + \" term=\" + termAtt, endOffsets[i], offsetAtt.endOffset());\n      }\n      if (types != null) {\n        assertEquals(\"type \" + i + \" term=\" + termAtt, types[i], typeAtt.type());\n      }\n      if (posIncrements != null) {\n        assertEquals(\"posIncrement \" + i + \" term=\" + termAtt, posIncrements[i], posIncrAtt.getPositionIncrement());\n      }\n      if (posLengths != null) {\n        assertEquals(\"posLength \" + i + \" term=\" + termAtt, posLengths[i], posLengthAtt.getPositionLength());\n      }\n      if (keywordAtts != null) {\n        assertEquals(\"keywordAtt \" + i + \" term=\" + termAtt, keywordAtts[i], keywordAtt.isKeyword());\n      }\n      \n      // we can enforce some basic things about a few attributes even if the caller doesn't check:\n      if (offsetAtt != null) {\n        final int startOffset = offsetAtt.startOffset();\n        final int endOffset = offsetAtt.endOffset();\n        if (finalOffset != null) {\n          assertTrue(\"startOffset (= \" + startOffset + \") must be <= finalOffset (= \" + finalOffset + \") term=\" + termAtt, startOffset <= finalOffset.intValue());\n          assertTrue(\"endOffset must be <= finalOffset: got endOffset=\" + endOffset + \" vs finalOffset=\" + finalOffset.intValue() + \" term=\" + termAtt,\n                     endOffset <= finalOffset.intValue());\n        }\n\n        if (offsetsAreCorrect) {\n          assertTrue(\"offsets must not go backwards startOffset=\" + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" term=\" + termAtt, offsetAtt.startOffset() >= lastStartOffset);\n          lastStartOffset = offsetAtt.startOffset();\n        }\n\n        if (offsetsAreCorrect && posLengthAtt != null && posIncrAtt != null) {\n          // Validate offset consistency in the graph, ie\n          // all tokens leaving from a certain pos have the\n          // same startOffset, and all tokens arriving to a\n          // certain pos have the same endOffset:\n          final int posInc = posIncrAtt.getPositionIncrement();\n          pos += posInc;\n\n          final int posLength = posLengthAtt.getPositionLength();\n\n          if (!posToStartOffset.containsKey(pos)) {\n            // First time we've seen a token leaving from this position:\n            posToStartOffset.put(pos, startOffset);\n            //System.out.println(\"  + s \" + pos + \" -> \" + startOffset);\n          } else {\n            // We've seen a token leaving from this position\n            // before; verify the startOffset is the same:\n            //System.out.println(\"  + vs \" + pos + \" -> \" + startOffset);\n            assertEquals(i + \" inconsistent startOffset: pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToStartOffset.get(pos).intValue(), startOffset);\n          }\n\n          final int endPos = pos + posLength;\n\n          if (!posToEndOffset.containsKey(endPos)) {\n            // First time we've seen a token arriving to this position:\n            posToEndOffset.put(endPos, endOffset);\n            //System.out.println(\"  + e \" + endPos + \" -> \" + endOffset);\n          } else {\n            // We've seen a token arriving to this position\n            // before; verify the endOffset is the same:\n            //System.out.println(\"  + ve \" + endPos + \" -> \" + endOffset);\n            assertEquals(\"inconsistent endOffset \" + i + \" pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToEndOffset.get(endPos).intValue(), endOffset);\n          }\n        }\n      }\n      if (posIncrAtt != null) {\n        if (i == 0) {\n          assertTrue(\"first posIncrement must be >= 1\", posIncrAtt.getPositionIncrement() >= 1);\n        } else {\n          assertTrue(\"posIncrement must be >= 0\", posIncrAtt.getPositionIncrement() >= 0);\n        }\n      }\n      if (posLengthAtt != null) {\n        assertTrue(\"posLength must be >= 1\", posLengthAtt.getPositionLength() >= 1);\n      }\n    }\n\n    if (ts.incrementToken()) {\n      fail(\"TokenStream has more tokens than expected (expected count=\" + output.length + \"); extra token=\" + termAtt);\n    }\n\n    // repeat our extra safety checks for end()\n    ts.clearAttributes();\n    if (termAtt != null) termAtt.setEmpty().append(\"bogusTerm\");\n    if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n    if (typeAtt != null) typeAtt.setType(\"bogusType\");\n    if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n    if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n    \n    checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n\n    ts.end();\n    assertTrue(\"super.end()/clearAttributes() was not called correctly in end()\", checkClearAtt.getAndResetClearCalled());\n    \n    if (finalOffset != null) {\n      assertEquals(\"finalOffset\", finalOffset.intValue(), offsetAtt.endOffset());\n    }\n    if (offsetAtt != null) {\n      assertTrue(\"finalOffset must be >= 0\", offsetAtt.endOffset() >= 0);\n    }\n    if (finalPosInc != null) {\n      assertEquals(\"finalPosInc\", finalPosInc.intValue(), posIncrAtt.getPositionIncrement());\n    }\n\n    ts.close();\n  }\n\n","sourceOld":"  // offsetsAreCorrect also validates:\n  //   - graph offsets are correct (all tokens leaving from\n  //     pos X have the same startOffset; all tokens\n  //     arriving to pos Y have the same endOffset)\n  //   - offsets only move forwards (startOffset >=\n  //     lastStartOffset)\n  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[],\n                                               int posLengths[], Integer finalOffset, Integer finalPosInc, boolean[] keywordAtts,\n                                               boolean offsetsAreCorrect) throws IOException {\n    assertNotNull(output);\n    CheckClearAttributesAttribute checkClearAtt = ts.addAttribute(CheckClearAttributesAttribute.class);\n    \n    CharTermAttribute termAtt = null;\n    if (output.length > 0) {\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      termAtt = ts.getAttribute(CharTermAttribute.class);\n    }\n    \n    OffsetAttribute offsetAtt = null;\n    if (startOffsets != null || endOffsets != null || finalOffset != null) {\n      assertTrue(\"has no OffsetAttribute\", ts.hasAttribute(OffsetAttribute.class));\n      offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    }\n    \n    TypeAttribute typeAtt = null;\n    if (types != null) {\n      assertTrue(\"has no TypeAttribute\", ts.hasAttribute(TypeAttribute.class));\n      typeAtt = ts.getAttribute(TypeAttribute.class);\n    }\n    \n    PositionIncrementAttribute posIncrAtt = null;\n    if (posIncrements != null || finalPosInc != null) {\n      assertTrue(\"has no PositionIncrementAttribute\", ts.hasAttribute(PositionIncrementAttribute.class));\n      posIncrAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    }\n\n    PositionLengthAttribute posLengthAtt = null;\n    if (posLengths != null) {\n      assertTrue(\"has no PositionLengthAttribute\", ts.hasAttribute(PositionLengthAttribute.class));\n      posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    }\n\n    KeywordAttribute keywordAtt = null;\n    if (keywordAtts != null) {\n      assertTrue(\"has no KeywordAttribute\", ts.hasAttribute(KeywordAttribute.class));\n      keywordAtt = ts.getAttribute(KeywordAttribute.class);\n    }\n    \n    // Maps position to the start/end offset:\n    final Map<Integer,Integer> posToStartOffset = new HashMap<>();\n    final Map<Integer,Integer> posToEndOffset = new HashMap<>();\n\n    ts.reset();\n    int pos = -1;\n    int lastStartOffset = 0;\n    for (int i = 0; i < output.length; i++) {\n      // extra safety to enforce, that the state is not preserved and also assign bogus values\n      ts.clearAttributes();\n      termAtt.setEmpty().append(\"bogusTerm\");\n      if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n      if (typeAtt != null) typeAtt.setType(\"bogusType\");\n      if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n      if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n      if (keywordAtt != null) keywordAtt.setKeyword((i&1) == 0);\n      \n      checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n      assertTrue(\"token \"+i+\" does not exist\", ts.incrementToken());\n      assertTrue(\"clearAttributes() was not called correctly in TokenStream chain\", checkClearAtt.getAndResetClearCalled());\n      \n      assertEquals(\"term \"+i, output[i], termAtt.toString());\n      if (startOffsets != null) {\n        assertEquals(\"startOffset \"+i, startOffsets[i], offsetAtt.startOffset());\n      }\n      if (endOffsets != null) {\n        assertEquals(\"endOffset \"+i, endOffsets[i], offsetAtt.endOffset());\n      }\n      if (types != null) {\n        assertEquals(\"type \"+i, types[i], typeAtt.type());\n      }\n      if (posIncrements != null) {\n        assertEquals(\"posIncrement \"+i, posIncrements[i], posIncrAtt.getPositionIncrement());\n      }\n      if (posLengths != null) {\n        assertEquals(\"posLength \"+i, posLengths[i], posLengthAtt.getPositionLength());\n      }\n      if (keywordAtts != null) {\n        assertEquals(\"keywordAtt \" + i, keywordAtts[i], keywordAtt.isKeyword());\n      }\n      \n      // we can enforce some basic things about a few attributes even if the caller doesn't check:\n      if (offsetAtt != null) {\n        final int startOffset = offsetAtt.startOffset();\n        final int endOffset = offsetAtt.endOffset();\n        if (finalOffset != null) {\n          assertTrue(\"startOffset must be <= finalOffset\", startOffset <= finalOffset.intValue());\n          assertTrue(\"endOffset must be <= finalOffset: got endOffset=\" + endOffset + \" vs finalOffset=\" + finalOffset.intValue(),\n                     endOffset <= finalOffset.intValue());\n        }\n\n        if (offsetsAreCorrect) {\n          assertTrue(\"offsets must not go backwards startOffset=\" + startOffset + \" is < lastStartOffset=\" + lastStartOffset, offsetAtt.startOffset() >= lastStartOffset);\n          lastStartOffset = offsetAtt.startOffset();\n        }\n\n        if (offsetsAreCorrect && posLengthAtt != null && posIncrAtt != null) {\n          // Validate offset consistency in the graph, ie\n          // all tokens leaving from a certain pos have the\n          // same startOffset, and all tokens arriving to a\n          // certain pos have the same endOffset:\n          final int posInc = posIncrAtt.getPositionIncrement();\n          pos += posInc;\n\n          final int posLength = posLengthAtt.getPositionLength();\n\n          if (!posToStartOffset.containsKey(pos)) {\n            // First time we've seen a token leaving from this position:\n            posToStartOffset.put(pos, startOffset);\n            //System.out.println(\"  + s \" + pos + \" -> \" + startOffset);\n          } else {\n            // We've seen a token leaving from this position\n            // before; verify the startOffset is the same:\n            //System.out.println(\"  + vs \" + pos + \" -> \" + startOffset);\n            assertEquals(\"pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToStartOffset.get(pos).intValue(), startOffset);\n          }\n\n          final int endPos = pos + posLength;\n\n          if (!posToEndOffset.containsKey(endPos)) {\n            // First time we've seen a token arriving to this position:\n            posToEndOffset.put(endPos, endOffset);\n            //System.out.println(\"  + e \" + endPos + \" -> \" + endOffset);\n          } else {\n            // We've seen a token arriving to this position\n            // before; verify the endOffset is the same:\n            //System.out.println(\"  + ve \" + endPos + \" -> \" + endOffset);\n            assertEquals(\"pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToEndOffset.get(endPos).intValue(), endOffset);\n          }\n        }\n      }\n      if (posIncrAtt != null) {\n        if (i == 0) {\n          assertTrue(\"first posIncrement must be >= 1\", posIncrAtt.getPositionIncrement() >= 1);\n        } else {\n          assertTrue(\"posIncrement must be >= 0\", posIncrAtt.getPositionIncrement() >= 0);\n        }\n      }\n      if (posLengthAtt != null) {\n        assertTrue(\"posLength must be >= 1\", posLengthAtt.getPositionLength() >= 1);\n      }\n    }\n\n    if (ts.incrementToken()) {\n      fail(\"TokenStream has more tokens than expected (expected count=\" + output.length + \"); extra token=\" + termAtt);\n    }\n\n    // repeat our extra safety checks for end()\n    ts.clearAttributes();\n    if (termAtt != null) termAtt.setEmpty().append(\"bogusTerm\");\n    if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n    if (typeAtt != null) typeAtt.setType(\"bogusType\");\n    if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n    if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n    \n    checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n\n    ts.end();\n    assertTrue(\"super.end()/clearAttributes() was not called correctly in end()\", checkClearAtt.getAndResetClearCalled());\n    \n    if (finalOffset != null) {\n      assertEquals(\"finalOffset\", finalOffset.intValue(), offsetAtt.endOffset());\n    }\n    if (offsetAtt != null) {\n      assertTrue(\"finalOffset must be >= 0\", offsetAtt.endOffset() >= 0);\n    }\n    if (finalPosInc != null) {\n      assertEquals(\"finalPosInc\", finalPosInc.intValue(), posIncrAtt.getPositionIncrement());\n    }\n\n    ts.close();\n  }\n\n","bugFix":null,"bugIntro":["995993f24c9f6feb42b49b71e1982cda8fa0b37c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f03e4bed5023ec3ef93a771b8888cae991cf448d","date":1483469262,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertTokenStreamContents(TokenStream,String[],int[],int[],String[],int[],int[],Integer,Integer,boolean[],boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertTokenStreamContents(TokenStream,String[],int[],int[],String[],int[],int[],Integer,Integer,boolean[],boolean).mjava","sourceNew":"  // offsetsAreCorrect also validates:\n  //   - graph offsets are correct (all tokens leaving from\n  //     pos X have the same startOffset; all tokens\n  //     arriving to pos Y have the same endOffset)\n  //   - offsets only move forwards (startOffset >=\n  //     lastStartOffset)\n  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[],\n                                               int posLengths[], Integer finalOffset, Integer finalPosInc, boolean[] keywordAtts,\n                                               boolean offsetsAreCorrect) throws IOException {\n    assertNotNull(output);\n    CheckClearAttributesAttribute checkClearAtt = ts.addAttribute(CheckClearAttributesAttribute.class);\n    \n    CharTermAttribute termAtt = null;\n    if (output.length > 0) {\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      termAtt = ts.getAttribute(CharTermAttribute.class);\n    }\n    \n    OffsetAttribute offsetAtt = null;\n    if (startOffsets != null || endOffsets != null || finalOffset != null) {\n      assertTrue(\"has no OffsetAttribute\", ts.hasAttribute(OffsetAttribute.class));\n      offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    }\n    \n    TypeAttribute typeAtt = null;\n    if (types != null) {\n      assertTrue(\"has no TypeAttribute\", ts.hasAttribute(TypeAttribute.class));\n      typeAtt = ts.getAttribute(TypeAttribute.class);\n    }\n    \n    PositionIncrementAttribute posIncrAtt = null;\n    if (posIncrements != null || finalPosInc != null) {\n      assertTrue(\"has no PositionIncrementAttribute\", ts.hasAttribute(PositionIncrementAttribute.class));\n      posIncrAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    }\n\n    PositionLengthAttribute posLengthAtt = null;\n    if (posLengths != null) {\n      assertTrue(\"has no PositionLengthAttribute\", ts.hasAttribute(PositionLengthAttribute.class));\n      posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    }\n\n    KeywordAttribute keywordAtt = null;\n    if (keywordAtts != null) {\n      assertTrue(\"has no KeywordAttribute\", ts.hasAttribute(KeywordAttribute.class));\n      keywordAtt = ts.getAttribute(KeywordAttribute.class);\n    }\n    \n    // Maps position to the start/end offset:\n    final Map<Integer,Integer> posToStartOffset = new HashMap<>();\n    final Map<Integer,Integer> posToEndOffset = new HashMap<>();\n\n    ts.reset();\n    int pos = -1;\n    int lastStartOffset = 0;\n    for (int i = 0; i < output.length; i++) {\n      // extra safety to enforce, that the state is not preserved and also assign bogus values\n      ts.clearAttributes();\n      termAtt.setEmpty().append(\"bogusTerm\");\n      if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n      if (typeAtt != null) typeAtt.setType(\"bogusType\");\n      if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n      if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n      if (keywordAtt != null) keywordAtt.setKeyword((i&1) == 0);\n      \n      checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n      assertTrue(\"token \"+i+\" does not exist\", ts.incrementToken());\n      assertTrue(\"clearAttributes() was not called correctly in TokenStream chain\", checkClearAtt.getAndResetClearCalled());\n      \n      assertEquals(\"term \"+i, output[i], termAtt.toString());\n      if (startOffsets != null) {\n        assertEquals(\"startOffset \" + i + \" term=\" + termAtt, startOffsets[i], offsetAtt.startOffset());\n      }\n      if (endOffsets != null) {\n        assertEquals(\"endOffset \" + i + \" term=\" + termAtt, endOffsets[i], offsetAtt.endOffset());\n      }\n      if (types != null) {\n        assertEquals(\"type \" + i + \" term=\" + termAtt, types[i], typeAtt.type());\n      }\n      if (posIncrements != null) {\n        assertEquals(\"posIncrement \" + i + \" term=\" + termAtt, posIncrements[i], posIncrAtt.getPositionIncrement());\n      }\n      if (posLengths != null) {\n        assertEquals(\"posLength \" + i + \" term=\" + termAtt, posLengths[i], posLengthAtt.getPositionLength());\n      }\n      if (keywordAtts != null) {\n        assertEquals(\"keywordAtt \" + i + \" term=\" + termAtt, keywordAtts[i], keywordAtt.isKeyword());\n      }\n      \n      // we can enforce some basic things about a few attributes even if the caller doesn't check:\n      if (offsetAtt != null) {\n        final int startOffset = offsetAtt.startOffset();\n        final int endOffset = offsetAtt.endOffset();\n        if (finalOffset != null) {\n          assertTrue(\"startOffset (= \" + startOffset + \") must be <= finalOffset (= \" + finalOffset + \") term=\" + termAtt, startOffset <= finalOffset.intValue());\n          assertTrue(\"endOffset must be <= finalOffset: got endOffset=\" + endOffset + \" vs finalOffset=\" + finalOffset.intValue() + \" term=\" + termAtt,\n                     endOffset <= finalOffset.intValue());\n        }\n\n        if (offsetsAreCorrect) {\n          assertTrue(\"offsets must not go backwards startOffset=\" + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" term=\" + termAtt, offsetAtt.startOffset() >= lastStartOffset);\n          lastStartOffset = offsetAtt.startOffset();\n        }\n\n        if (offsetsAreCorrect && posLengthAtt != null && posIncrAtt != null) {\n          // Validate offset consistency in the graph, ie\n          // all tokens leaving from a certain pos have the\n          // same startOffset, and all tokens arriving to a\n          // certain pos have the same endOffset:\n          final int posInc = posIncrAtt.getPositionIncrement();\n          pos += posInc;\n\n          final int posLength = posLengthAtt.getPositionLength();\n\n          if (!posToStartOffset.containsKey(pos)) {\n            // First time we've seen a token leaving from this position:\n            posToStartOffset.put(pos, startOffset);\n            //System.out.println(\"  + s \" + pos + \" -> \" + startOffset);\n          } else {\n            // We've seen a token leaving from this position\n            // before; verify the startOffset is the same:\n            //System.out.println(\"  + vs \" + pos + \" -> \" + startOffset);\n            assertEquals(i + \" inconsistent startOffset: pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToStartOffset.get(pos).intValue(), startOffset);\n          }\n\n          final int endPos = pos + posLength;\n\n          if (!posToEndOffset.containsKey(endPos)) {\n            // First time we've seen a token arriving to this position:\n            posToEndOffset.put(endPos, endOffset);\n            //System.out.println(\"  + e \" + endPos + \" -> \" + endOffset);\n          } else {\n            // We've seen a token arriving to this position\n            // before; verify the endOffset is the same:\n            //System.out.println(\"  + ve \" + endPos + \" -> \" + endOffset);\n            assertEquals(\"inconsistent endOffset \" + i + \" pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToEndOffset.get(endPos).intValue(), endOffset);\n          }\n        }\n      }\n      if (posIncrAtt != null) {\n        if (i == 0) {\n          assertTrue(\"first posIncrement must be >= 1\", posIncrAtt.getPositionIncrement() >= 1);\n        } else {\n          assertTrue(\"posIncrement must be >= 0\", posIncrAtt.getPositionIncrement() >= 0);\n        }\n      }\n      if (posLengthAtt != null) {\n        assertTrue(\"posLength must be >= 1\", posLengthAtt.getPositionLength() >= 1);\n      }\n    }\n\n    if (ts.incrementToken()) {\n      fail(\"TokenStream has more tokens than expected (expected count=\" + output.length + \"); extra token=\" + termAtt);\n    }\n\n    // repeat our extra safety checks for end()\n    ts.clearAttributes();\n    if (termAtt != null) termAtt.setEmpty().append(\"bogusTerm\");\n    if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n    if (typeAtt != null) typeAtt.setType(\"bogusType\");\n    if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n    if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n    \n    checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n\n    ts.end();\n    assertTrue(\"super.end()/clearAttributes() was not called correctly in end()\", checkClearAtt.getAndResetClearCalled());\n    \n    if (finalOffset != null) {\n      assertEquals(\"finalOffset\", finalOffset.intValue(), offsetAtt.endOffset());\n    }\n    if (offsetAtt != null) {\n      assertTrue(\"finalOffset must be >= 0\", offsetAtt.endOffset() >= 0);\n    }\n    if (finalPosInc != null) {\n      assertEquals(\"finalPosInc\", finalPosInc.intValue(), posIncrAtt.getPositionIncrement());\n    }\n\n    ts.close();\n  }\n\n","sourceOld":"  // offsetsAreCorrect also validates:\n  //   - graph offsets are correct (all tokens leaving from\n  //     pos X have the same startOffset; all tokens\n  //     arriving to pos Y have the same endOffset)\n  //   - offsets only move forwards (startOffset >=\n  //     lastStartOffset)\n  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[],\n                                               int posLengths[], Integer finalOffset, Integer finalPosInc, boolean[] keywordAtts,\n                                               boolean offsetsAreCorrect) throws IOException {\n    assertNotNull(output);\n    CheckClearAttributesAttribute checkClearAtt = ts.addAttribute(CheckClearAttributesAttribute.class);\n    \n    CharTermAttribute termAtt = null;\n    if (output.length > 0) {\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      termAtt = ts.getAttribute(CharTermAttribute.class);\n    }\n    \n    OffsetAttribute offsetAtt = null;\n    if (startOffsets != null || endOffsets != null || finalOffset != null) {\n      assertTrue(\"has no OffsetAttribute\", ts.hasAttribute(OffsetAttribute.class));\n      offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    }\n    \n    TypeAttribute typeAtt = null;\n    if (types != null) {\n      assertTrue(\"has no TypeAttribute\", ts.hasAttribute(TypeAttribute.class));\n      typeAtt = ts.getAttribute(TypeAttribute.class);\n    }\n    \n    PositionIncrementAttribute posIncrAtt = null;\n    if (posIncrements != null || finalPosInc != null) {\n      assertTrue(\"has no PositionIncrementAttribute\", ts.hasAttribute(PositionIncrementAttribute.class));\n      posIncrAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    }\n\n    PositionLengthAttribute posLengthAtt = null;\n    if (posLengths != null) {\n      assertTrue(\"has no PositionLengthAttribute\", ts.hasAttribute(PositionLengthAttribute.class));\n      posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    }\n\n    KeywordAttribute keywordAtt = null;\n    if (keywordAtts != null) {\n      assertTrue(\"has no KeywordAttribute\", ts.hasAttribute(KeywordAttribute.class));\n      keywordAtt = ts.getAttribute(KeywordAttribute.class);\n    }\n    \n    // Maps position to the start/end offset:\n    final Map<Integer,Integer> posToStartOffset = new HashMap<>();\n    final Map<Integer,Integer> posToEndOffset = new HashMap<>();\n\n    ts.reset();\n    int pos = -1;\n    int lastStartOffset = 0;\n    for (int i = 0; i < output.length; i++) {\n      // extra safety to enforce, that the state is not preserved and also assign bogus values\n      ts.clearAttributes();\n      termAtt.setEmpty().append(\"bogusTerm\");\n      if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n      if (typeAtt != null) typeAtt.setType(\"bogusType\");\n      if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n      if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n      if (keywordAtt != null) keywordAtt.setKeyword((i&1) == 0);\n      \n      checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n      assertTrue(\"token \"+i+\" does not exist\", ts.incrementToken());\n      assertTrue(\"clearAttributes() was not called correctly in TokenStream chain\", checkClearAtt.getAndResetClearCalled());\n      \n      assertEquals(\"term \"+i, output[i], termAtt.toString());\n      if (startOffsets != null) {\n        assertEquals(\"startOffset \"+i, startOffsets[i], offsetAtt.startOffset());\n      }\n      if (endOffsets != null) {\n        assertEquals(\"endOffset \"+i, endOffsets[i], offsetAtt.endOffset());\n      }\n      if (types != null) {\n        assertEquals(\"type \"+i, types[i], typeAtt.type());\n      }\n      if (posIncrements != null) {\n        assertEquals(\"posIncrement \"+i, posIncrements[i], posIncrAtt.getPositionIncrement());\n      }\n      if (posLengths != null) {\n        assertEquals(\"posLength \"+i, posLengths[i], posLengthAtt.getPositionLength());\n      }\n      if (keywordAtts != null) {\n        assertEquals(\"keywordAtt \" + i, keywordAtts[i], keywordAtt.isKeyword());\n      }\n      \n      // we can enforce some basic things about a few attributes even if the caller doesn't check:\n      if (offsetAtt != null) {\n        final int startOffset = offsetAtt.startOffset();\n        final int endOffset = offsetAtt.endOffset();\n        if (finalOffset != null) {\n          assertTrue(\"startOffset must be <= finalOffset\", startOffset <= finalOffset.intValue());\n          assertTrue(\"endOffset must be <= finalOffset: got endOffset=\" + endOffset + \" vs finalOffset=\" + finalOffset.intValue(),\n                     endOffset <= finalOffset.intValue());\n        }\n\n        if (offsetsAreCorrect) {\n          assertTrue(\"offsets must not go backwards startOffset=\" + startOffset + \" is < lastStartOffset=\" + lastStartOffset, offsetAtt.startOffset() >= lastStartOffset);\n          lastStartOffset = offsetAtt.startOffset();\n        }\n\n        if (offsetsAreCorrect && posLengthAtt != null && posIncrAtt != null) {\n          // Validate offset consistency in the graph, ie\n          // all tokens leaving from a certain pos have the\n          // same startOffset, and all tokens arriving to a\n          // certain pos have the same endOffset:\n          final int posInc = posIncrAtt.getPositionIncrement();\n          pos += posInc;\n\n          final int posLength = posLengthAtt.getPositionLength();\n\n          if (!posToStartOffset.containsKey(pos)) {\n            // First time we've seen a token leaving from this position:\n            posToStartOffset.put(pos, startOffset);\n            //System.out.println(\"  + s \" + pos + \" -> \" + startOffset);\n          } else {\n            // We've seen a token leaving from this position\n            // before; verify the startOffset is the same:\n            //System.out.println(\"  + vs \" + pos + \" -> \" + startOffset);\n            assertEquals(\"pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToStartOffset.get(pos).intValue(), startOffset);\n          }\n\n          final int endPos = pos + posLength;\n\n          if (!posToEndOffset.containsKey(endPos)) {\n            // First time we've seen a token arriving to this position:\n            posToEndOffset.put(endPos, endOffset);\n            //System.out.println(\"  + e \" + endPos + \" -> \" + endOffset);\n          } else {\n            // We've seen a token arriving to this position\n            // before; verify the endOffset is the same:\n            //System.out.println(\"  + ve \" + endPos + \" -> \" + endOffset);\n            assertEquals(\"pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToEndOffset.get(endPos).intValue(), endOffset);\n          }\n        }\n      }\n      if (posIncrAtt != null) {\n        if (i == 0) {\n          assertTrue(\"first posIncrement must be >= 1\", posIncrAtt.getPositionIncrement() >= 1);\n        } else {\n          assertTrue(\"posIncrement must be >= 0\", posIncrAtt.getPositionIncrement() >= 0);\n        }\n      }\n      if (posLengthAtt != null) {\n        assertTrue(\"posLength must be >= 1\", posLengthAtt.getPositionLength() >= 1);\n      }\n    }\n\n    if (ts.incrementToken()) {\n      fail(\"TokenStream has more tokens than expected (expected count=\" + output.length + \"); extra token=\" + termAtt);\n    }\n\n    // repeat our extra safety checks for end()\n    ts.clearAttributes();\n    if (termAtt != null) termAtt.setEmpty().append(\"bogusTerm\");\n    if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n    if (typeAtt != null) typeAtt.setType(\"bogusType\");\n    if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n    if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n    \n    checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n\n    ts.end();\n    assertTrue(\"super.end()/clearAttributes() was not called correctly in end()\", checkClearAtt.getAndResetClearCalled());\n    \n    if (finalOffset != null) {\n      assertEquals(\"finalOffset\", finalOffset.intValue(), offsetAtt.endOffset());\n    }\n    if (offsetAtt != null) {\n      assertTrue(\"finalOffset must be >= 0\", offsetAtt.endOffset() >= 0);\n    }\n    if (finalPosInc != null) {\n      assertEquals(\"finalPosInc\", finalPosInc.intValue(), posIncrAtt.getPositionIncrement());\n    }\n\n    ts.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"098528909bb70948871fd7ed865fafb87ed73964","date":1484667487,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertTokenStreamContents(TokenStream,String[],int[],int[],String[],int[],int[],Integer,Integer,boolean[],boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertTokenStreamContents(TokenStream,String[],int[],int[],String[],int[],int[],Integer,Integer,boolean[],boolean).mjava","sourceNew":"  // offsetsAreCorrect also validates:\n  //   - graph offsets are correct (all tokens leaving from\n  //     pos X have the same startOffset; all tokens\n  //     arriving to pos Y have the same endOffset)\n  //   - offsets only move forwards (startOffset >=\n  //     lastStartOffset)\n  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[],\n                                               int posLengths[], Integer finalOffset, Integer finalPosInc, boolean[] keywordAtts,\n                                               boolean offsetsAreCorrect) throws IOException {\n    assertNotNull(output);\n    CheckClearAttributesAttribute checkClearAtt = ts.addAttribute(CheckClearAttributesAttribute.class);\n    \n    CharTermAttribute termAtt = null;\n    if (output.length > 0) {\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      termAtt = ts.getAttribute(CharTermAttribute.class);\n    }\n    \n    OffsetAttribute offsetAtt = null;\n    if (startOffsets != null || endOffsets != null || finalOffset != null) {\n      assertTrue(\"has no OffsetAttribute\", ts.hasAttribute(OffsetAttribute.class));\n      offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    }\n    \n    TypeAttribute typeAtt = null;\n    if (types != null) {\n      assertTrue(\"has no TypeAttribute\", ts.hasAttribute(TypeAttribute.class));\n      typeAtt = ts.getAttribute(TypeAttribute.class);\n    }\n    \n    PositionIncrementAttribute posIncrAtt = null;\n    if (posIncrements != null || finalPosInc != null) {\n      assertTrue(\"has no PositionIncrementAttribute\", ts.hasAttribute(PositionIncrementAttribute.class));\n      posIncrAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    }\n\n    PositionLengthAttribute posLengthAtt = null;\n    if (posLengths != null) {\n      assertTrue(\"has no PositionLengthAttribute\", ts.hasAttribute(PositionLengthAttribute.class));\n      posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    }\n\n    KeywordAttribute keywordAtt = null;\n    if (keywordAtts != null) {\n      assertTrue(\"has no KeywordAttribute\", ts.hasAttribute(KeywordAttribute.class));\n      keywordAtt = ts.getAttribute(KeywordAttribute.class);\n    }\n    \n    // Maps position to the start/end offset:\n    final Map<Integer,Integer> posToStartOffset = new HashMap<>();\n    final Map<Integer,Integer> posToEndOffset = new HashMap<>();\n\n    // TODO: would be nice to be able to assert silly duplicated tokens are not created, but a number of cases do this \"legitimately\": LUCENE-7622\n\n    ts.reset();\n    int pos = -1;\n    int lastStartOffset = 0;\n    for (int i = 0; i < output.length; i++) {\n      // extra safety to enforce, that the state is not preserved and also assign bogus values\n      ts.clearAttributes();\n      termAtt.setEmpty().append(\"bogusTerm\");\n      if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n      if (typeAtt != null) typeAtt.setType(\"bogusType\");\n      if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n      if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n      if (keywordAtt != null) keywordAtt.setKeyword((i&1) == 0);\n      \n      checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n      assertTrue(\"token \"+i+\" does not exist\", ts.incrementToken());\n      assertTrue(\"clearAttributes() was not called correctly in TokenStream chain\", checkClearAtt.getAndResetClearCalled());\n\n      assertEquals(\"term \"+i, output[i], termAtt.toString());\n      if (startOffsets != null) {\n        assertEquals(\"startOffset \" + i + \" term=\" + termAtt, startOffsets[i], offsetAtt.startOffset());\n      }\n      if (endOffsets != null) {\n        assertEquals(\"endOffset \" + i + \" term=\" + termAtt, endOffsets[i], offsetAtt.endOffset());\n      }\n      if (types != null) {\n        assertEquals(\"type \" + i + \" term=\" + termAtt, types[i], typeAtt.type());\n      }\n      if (posIncrements != null) {\n        assertEquals(\"posIncrement \" + i + \" term=\" + termAtt, posIncrements[i], posIncrAtt.getPositionIncrement());\n      }\n      if (posLengths != null) {\n        assertEquals(\"posLength \" + i + \" term=\" + termAtt, posLengths[i], posLengthAtt.getPositionLength());\n      }\n      if (keywordAtts != null) {\n        assertEquals(\"keywordAtt \" + i + \" term=\" + termAtt, keywordAtts[i], keywordAtt.isKeyword());\n      }\n      \n      // we can enforce some basic things about a few attributes even if the caller doesn't check:\n      if (offsetAtt != null) {\n        final int startOffset = offsetAtt.startOffset();\n        final int endOffset = offsetAtt.endOffset();\n        if (finalOffset != null) {\n          assertTrue(\"startOffset (= \" + startOffset + \") must be <= finalOffset (= \" + finalOffset + \") term=\" + termAtt, startOffset <= finalOffset.intValue());\n          assertTrue(\"endOffset must be <= finalOffset: got endOffset=\" + endOffset + \" vs finalOffset=\" + finalOffset.intValue() + \" term=\" + termAtt,\n                     endOffset <= finalOffset.intValue());\n        }\n\n        if (offsetsAreCorrect) {\n          assertTrue(\"offsets must not go backwards startOffset=\" + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" term=\" + termAtt, offsetAtt.startOffset() >= lastStartOffset);\n          lastStartOffset = offsetAtt.startOffset();\n        }\n\n        if (offsetsAreCorrect && posLengthAtt != null && posIncrAtt != null) {\n          // Validate offset consistency in the graph, ie\n          // all tokens leaving from a certain pos have the\n          // same startOffset, and all tokens arriving to a\n          // certain pos have the same endOffset:\n          final int posInc = posIncrAtt.getPositionIncrement();\n          pos += posInc;\n\n          final int posLength = posLengthAtt.getPositionLength();\n\n          if (!posToStartOffset.containsKey(pos)) {\n            // First time we've seen a token leaving from this position:\n            posToStartOffset.put(pos, startOffset);\n            //System.out.println(\"  + s \" + pos + \" -> \" + startOffset);\n          } else {\n            // We've seen a token leaving from this position\n            // before; verify the startOffset is the same:\n            //System.out.println(\"  + vs \" + pos + \" -> \" + startOffset);\n            assertEquals(i + \" inconsistent startOffset: pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToStartOffset.get(pos).intValue(), startOffset);\n          }\n\n          final int endPos = pos + posLength;\n\n          if (!posToEndOffset.containsKey(endPos)) {\n            // First time we've seen a token arriving to this position:\n            posToEndOffset.put(endPos, endOffset);\n            //System.out.println(\"  + e \" + endPos + \" -> \" + endOffset);\n          } else {\n            // We've seen a token arriving to this position\n            // before; verify the endOffset is the same:\n            //System.out.println(\"  + ve \" + endPos + \" -> \" + endOffset);\n            assertEquals(\"inconsistent endOffset \" + i + \" pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToEndOffset.get(endPos).intValue(), endOffset);\n          }\n        }\n      }\n      if (posIncrAtt != null) {\n        if (i == 0) {\n          assertTrue(\"first posIncrement must be >= 1\", posIncrAtt.getPositionIncrement() >= 1);\n        } else {\n          assertTrue(\"posIncrement must be >= 0\", posIncrAtt.getPositionIncrement() >= 0);\n        }\n      }\n      if (posLengthAtt != null) {\n        assertTrue(\"posLength must be >= 1; got: \" + posLengthAtt.getPositionLength(), posLengthAtt.getPositionLength() >= 1);\n      }\n    }\n\n    if (ts.incrementToken()) {\n      fail(\"TokenStream has more tokens than expected (expected count=\" + output.length + \"); extra token=\" + ts.getAttribute(CharTermAttribute.class));\n    }\n\n    // repeat our extra safety checks for end()\n    ts.clearAttributes();\n    if (termAtt != null) termAtt.setEmpty().append(\"bogusTerm\");\n    if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n    if (typeAtt != null) typeAtt.setType(\"bogusType\");\n    if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n    if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n    \n    checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n\n    ts.end();\n    assertTrue(\"super.end()/clearAttributes() was not called correctly in end()\", checkClearAtt.getAndResetClearCalled());\n    \n    if (finalOffset != null) {\n      assertEquals(\"finalOffset\", finalOffset.intValue(), offsetAtt.endOffset());\n    }\n    if (offsetAtt != null) {\n      assertTrue(\"finalOffset must be >= 0\", offsetAtt.endOffset() >= 0);\n    }\n    if (finalPosInc != null) {\n      assertEquals(\"finalPosInc\", finalPosInc.intValue(), posIncrAtt.getPositionIncrement());\n    }\n\n    ts.close();\n  }\n\n","sourceOld":"  // offsetsAreCorrect also validates:\n  //   - graph offsets are correct (all tokens leaving from\n  //     pos X have the same startOffset; all tokens\n  //     arriving to pos Y have the same endOffset)\n  //   - offsets only move forwards (startOffset >=\n  //     lastStartOffset)\n  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[],\n                                               int posLengths[], Integer finalOffset, Integer finalPosInc, boolean[] keywordAtts,\n                                               boolean offsetsAreCorrect) throws IOException {\n    assertNotNull(output);\n    CheckClearAttributesAttribute checkClearAtt = ts.addAttribute(CheckClearAttributesAttribute.class);\n    \n    CharTermAttribute termAtt = null;\n    if (output.length > 0) {\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      termAtt = ts.getAttribute(CharTermAttribute.class);\n    }\n    \n    OffsetAttribute offsetAtt = null;\n    if (startOffsets != null || endOffsets != null || finalOffset != null) {\n      assertTrue(\"has no OffsetAttribute\", ts.hasAttribute(OffsetAttribute.class));\n      offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    }\n    \n    TypeAttribute typeAtt = null;\n    if (types != null) {\n      assertTrue(\"has no TypeAttribute\", ts.hasAttribute(TypeAttribute.class));\n      typeAtt = ts.getAttribute(TypeAttribute.class);\n    }\n    \n    PositionIncrementAttribute posIncrAtt = null;\n    if (posIncrements != null || finalPosInc != null) {\n      assertTrue(\"has no PositionIncrementAttribute\", ts.hasAttribute(PositionIncrementAttribute.class));\n      posIncrAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    }\n\n    PositionLengthAttribute posLengthAtt = null;\n    if (posLengths != null) {\n      assertTrue(\"has no PositionLengthAttribute\", ts.hasAttribute(PositionLengthAttribute.class));\n      posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    }\n\n    KeywordAttribute keywordAtt = null;\n    if (keywordAtts != null) {\n      assertTrue(\"has no KeywordAttribute\", ts.hasAttribute(KeywordAttribute.class));\n      keywordAtt = ts.getAttribute(KeywordAttribute.class);\n    }\n    \n    // Maps position to the start/end offset:\n    final Map<Integer,Integer> posToStartOffset = new HashMap<>();\n    final Map<Integer,Integer> posToEndOffset = new HashMap<>();\n\n    ts.reset();\n    int pos = -1;\n    int lastStartOffset = 0;\n    for (int i = 0; i < output.length; i++) {\n      // extra safety to enforce, that the state is not preserved and also assign bogus values\n      ts.clearAttributes();\n      termAtt.setEmpty().append(\"bogusTerm\");\n      if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n      if (typeAtt != null) typeAtt.setType(\"bogusType\");\n      if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n      if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n      if (keywordAtt != null) keywordAtt.setKeyword((i&1) == 0);\n      \n      checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n      assertTrue(\"token \"+i+\" does not exist\", ts.incrementToken());\n      assertTrue(\"clearAttributes() was not called correctly in TokenStream chain\", checkClearAtt.getAndResetClearCalled());\n      \n      assertEquals(\"term \"+i, output[i], termAtt.toString());\n      if (startOffsets != null) {\n        assertEquals(\"startOffset \" + i + \" term=\" + termAtt, startOffsets[i], offsetAtt.startOffset());\n      }\n      if (endOffsets != null) {\n        assertEquals(\"endOffset \" + i + \" term=\" + termAtt, endOffsets[i], offsetAtt.endOffset());\n      }\n      if (types != null) {\n        assertEquals(\"type \" + i + \" term=\" + termAtt, types[i], typeAtt.type());\n      }\n      if (posIncrements != null) {\n        assertEquals(\"posIncrement \" + i + \" term=\" + termAtt, posIncrements[i], posIncrAtt.getPositionIncrement());\n      }\n      if (posLengths != null) {\n        assertEquals(\"posLength \" + i + \" term=\" + termAtt, posLengths[i], posLengthAtt.getPositionLength());\n      }\n      if (keywordAtts != null) {\n        assertEquals(\"keywordAtt \" + i + \" term=\" + termAtt, keywordAtts[i], keywordAtt.isKeyword());\n      }\n      \n      // we can enforce some basic things about a few attributes even if the caller doesn't check:\n      if (offsetAtt != null) {\n        final int startOffset = offsetAtt.startOffset();\n        final int endOffset = offsetAtt.endOffset();\n        if (finalOffset != null) {\n          assertTrue(\"startOffset (= \" + startOffset + \") must be <= finalOffset (= \" + finalOffset + \") term=\" + termAtt, startOffset <= finalOffset.intValue());\n          assertTrue(\"endOffset must be <= finalOffset: got endOffset=\" + endOffset + \" vs finalOffset=\" + finalOffset.intValue() + \" term=\" + termAtt,\n                     endOffset <= finalOffset.intValue());\n        }\n\n        if (offsetsAreCorrect) {\n          assertTrue(\"offsets must not go backwards startOffset=\" + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" term=\" + termAtt, offsetAtt.startOffset() >= lastStartOffset);\n          lastStartOffset = offsetAtt.startOffset();\n        }\n\n        if (offsetsAreCorrect && posLengthAtt != null && posIncrAtt != null) {\n          // Validate offset consistency in the graph, ie\n          // all tokens leaving from a certain pos have the\n          // same startOffset, and all tokens arriving to a\n          // certain pos have the same endOffset:\n          final int posInc = posIncrAtt.getPositionIncrement();\n          pos += posInc;\n\n          final int posLength = posLengthAtt.getPositionLength();\n\n          if (!posToStartOffset.containsKey(pos)) {\n            // First time we've seen a token leaving from this position:\n            posToStartOffset.put(pos, startOffset);\n            //System.out.println(\"  + s \" + pos + \" -> \" + startOffset);\n          } else {\n            // We've seen a token leaving from this position\n            // before; verify the startOffset is the same:\n            //System.out.println(\"  + vs \" + pos + \" -> \" + startOffset);\n            assertEquals(i + \" inconsistent startOffset: pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToStartOffset.get(pos).intValue(), startOffset);\n          }\n\n          final int endPos = pos + posLength;\n\n          if (!posToEndOffset.containsKey(endPos)) {\n            // First time we've seen a token arriving to this position:\n            posToEndOffset.put(endPos, endOffset);\n            //System.out.println(\"  + e \" + endPos + \" -> \" + endOffset);\n          } else {\n            // We've seen a token arriving to this position\n            // before; verify the endOffset is the same:\n            //System.out.println(\"  + ve \" + endPos + \" -> \" + endOffset);\n            assertEquals(\"inconsistent endOffset \" + i + \" pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToEndOffset.get(endPos).intValue(), endOffset);\n          }\n        }\n      }\n      if (posIncrAtt != null) {\n        if (i == 0) {\n          assertTrue(\"first posIncrement must be >= 1\", posIncrAtt.getPositionIncrement() >= 1);\n        } else {\n          assertTrue(\"posIncrement must be >= 0\", posIncrAtt.getPositionIncrement() >= 0);\n        }\n      }\n      if (posLengthAtt != null) {\n        assertTrue(\"posLength must be >= 1\", posLengthAtt.getPositionLength() >= 1);\n      }\n    }\n\n    if (ts.incrementToken()) {\n      fail(\"TokenStream has more tokens than expected (expected count=\" + output.length + \"); extra token=\" + termAtt);\n    }\n\n    // repeat our extra safety checks for end()\n    ts.clearAttributes();\n    if (termAtt != null) termAtt.setEmpty().append(\"bogusTerm\");\n    if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n    if (typeAtt != null) typeAtt.setType(\"bogusType\");\n    if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n    if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n    \n    checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n\n    ts.end();\n    assertTrue(\"super.end()/clearAttributes() was not called correctly in end()\", checkClearAtt.getAndResetClearCalled());\n    \n    if (finalOffset != null) {\n      assertEquals(\"finalOffset\", finalOffset.intValue(), offsetAtt.endOffset());\n    }\n    if (offsetAtt != null) {\n      assertTrue(\"finalOffset must be >= 0\", offsetAtt.endOffset() >= 0);\n    }\n    if (finalPosInc != null) {\n      assertEquals(\"finalPosInc\", finalPosInc.intValue(), posIncrAtt.getPositionIncrement());\n    }\n\n    ts.close();\n  }\n\n","bugFix":null,"bugIntro":["995993f24c9f6feb42b49b71e1982cda8fa0b37c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"302d34f2c66e8d489ee13078305c330cbf67b226","date":1484754357,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertTokenStreamContents(TokenStream,String[],int[],int[],String[],int[],int[],Integer,Integer,boolean[],boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertTokenStreamContents(TokenStream,String[],int[],int[],String[],int[],int[],Integer,Integer,boolean[],boolean).mjava","sourceNew":"  // offsetsAreCorrect also validates:\n  //   - graph offsets are correct (all tokens leaving from\n  //     pos X have the same startOffset; all tokens\n  //     arriving to pos Y have the same endOffset)\n  //   - offsets only move forwards (startOffset >=\n  //     lastStartOffset)\n  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[],\n                                               int posLengths[], Integer finalOffset, Integer finalPosInc, boolean[] keywordAtts,\n                                               boolean offsetsAreCorrect) throws IOException {\n    assertNotNull(output);\n    CheckClearAttributesAttribute checkClearAtt = ts.addAttribute(CheckClearAttributesAttribute.class);\n    \n    CharTermAttribute termAtt = null;\n    if (output.length > 0) {\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      termAtt = ts.getAttribute(CharTermAttribute.class);\n    }\n    \n    OffsetAttribute offsetAtt = null;\n    if (startOffsets != null || endOffsets != null || finalOffset != null) {\n      assertTrue(\"has no OffsetAttribute\", ts.hasAttribute(OffsetAttribute.class));\n      offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    }\n    \n    TypeAttribute typeAtt = null;\n    if (types != null) {\n      assertTrue(\"has no TypeAttribute\", ts.hasAttribute(TypeAttribute.class));\n      typeAtt = ts.getAttribute(TypeAttribute.class);\n    }\n    \n    PositionIncrementAttribute posIncrAtt = null;\n    if (posIncrements != null || finalPosInc != null) {\n      assertTrue(\"has no PositionIncrementAttribute\", ts.hasAttribute(PositionIncrementAttribute.class));\n      posIncrAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    }\n\n    PositionLengthAttribute posLengthAtt = null;\n    if (posLengths != null) {\n      assertTrue(\"has no PositionLengthAttribute\", ts.hasAttribute(PositionLengthAttribute.class));\n      posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    }\n\n    KeywordAttribute keywordAtt = null;\n    if (keywordAtts != null) {\n      assertTrue(\"has no KeywordAttribute\", ts.hasAttribute(KeywordAttribute.class));\n      keywordAtt = ts.getAttribute(KeywordAttribute.class);\n    }\n    \n    // Maps position to the start/end offset:\n    final Map<Integer,Integer> posToStartOffset = new HashMap<>();\n    final Map<Integer,Integer> posToEndOffset = new HashMap<>();\n\n    // TODO: would be nice to be able to assert silly duplicated tokens are not created, but a number of cases do this \"legitimately\": LUCENE-7622\n\n    ts.reset();\n    int pos = -1;\n    int lastStartOffset = 0;\n    for (int i = 0; i < output.length; i++) {\n      // extra safety to enforce, that the state is not preserved and also assign bogus values\n      ts.clearAttributes();\n      termAtt.setEmpty().append(\"bogusTerm\");\n      if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n      if (typeAtt != null) typeAtt.setType(\"bogusType\");\n      if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n      if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n      if (keywordAtt != null) keywordAtt.setKeyword((i&1) == 0);\n      \n      checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n      assertTrue(\"token \"+i+\" does not exist\", ts.incrementToken());\n      assertTrue(\"clearAttributes() was not called correctly in TokenStream chain\", checkClearAtt.getAndResetClearCalled());\n\n      assertEquals(\"term \"+i, output[i], termAtt.toString());\n      if (startOffsets != null) {\n        assertEquals(\"startOffset \" + i + \" term=\" + termAtt, startOffsets[i], offsetAtt.startOffset());\n      }\n      if (endOffsets != null) {\n        assertEquals(\"endOffset \" + i + \" term=\" + termAtt, endOffsets[i], offsetAtt.endOffset());\n      }\n      if (types != null) {\n        assertEquals(\"type \" + i + \" term=\" + termAtt, types[i], typeAtt.type());\n      }\n      if (posIncrements != null) {\n        assertEquals(\"posIncrement \" + i + \" term=\" + termAtt, posIncrements[i], posIncrAtt.getPositionIncrement());\n      }\n      if (posLengths != null) {\n        assertEquals(\"posLength \" + i + \" term=\" + termAtt, posLengths[i], posLengthAtt.getPositionLength());\n      }\n      if (keywordAtts != null) {\n        assertEquals(\"keywordAtt \" + i + \" term=\" + termAtt, keywordAtts[i], keywordAtt.isKeyword());\n      }\n      \n      // we can enforce some basic things about a few attributes even if the caller doesn't check:\n      if (offsetAtt != null) {\n        final int startOffset = offsetAtt.startOffset();\n        final int endOffset = offsetAtt.endOffset();\n        if (finalOffset != null) {\n          assertTrue(\"startOffset (= \" + startOffset + \") must be <= finalOffset (= \" + finalOffset + \") term=\" + termAtt, startOffset <= finalOffset.intValue());\n          assertTrue(\"endOffset must be <= finalOffset: got endOffset=\" + endOffset + \" vs finalOffset=\" + finalOffset.intValue() + \" term=\" + termAtt,\n                     endOffset <= finalOffset.intValue());\n        }\n\n        if (offsetsAreCorrect) {\n          assertTrue(\"offsets must not go backwards startOffset=\" + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" term=\" + termAtt, offsetAtt.startOffset() >= lastStartOffset);\n          lastStartOffset = offsetAtt.startOffset();\n        }\n\n        if (offsetsAreCorrect && posLengthAtt != null && posIncrAtt != null) {\n          // Validate offset consistency in the graph, ie\n          // all tokens leaving from a certain pos have the\n          // same startOffset, and all tokens arriving to a\n          // certain pos have the same endOffset:\n          final int posInc = posIncrAtt.getPositionIncrement();\n          pos += posInc;\n\n          final int posLength = posLengthAtt.getPositionLength();\n\n          if (!posToStartOffset.containsKey(pos)) {\n            // First time we've seen a token leaving from this position:\n            posToStartOffset.put(pos, startOffset);\n            //System.out.println(\"  + s \" + pos + \" -> \" + startOffset);\n          } else {\n            // We've seen a token leaving from this position\n            // before; verify the startOffset is the same:\n            //System.out.println(\"  + vs \" + pos + \" -> \" + startOffset);\n            assertEquals(i + \" inconsistent startOffset: pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToStartOffset.get(pos).intValue(), startOffset);\n          }\n\n          final int endPos = pos + posLength;\n\n          if (!posToEndOffset.containsKey(endPos)) {\n            // First time we've seen a token arriving to this position:\n            posToEndOffset.put(endPos, endOffset);\n            //System.out.println(\"  + e \" + endPos + \" -> \" + endOffset);\n          } else {\n            // We've seen a token arriving to this position\n            // before; verify the endOffset is the same:\n            //System.out.println(\"  + ve \" + endPos + \" -> \" + endOffset);\n            assertEquals(\"inconsistent endOffset \" + i + \" pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToEndOffset.get(endPos).intValue(), endOffset);\n          }\n        }\n      }\n      if (posIncrAtt != null) {\n        if (i == 0) {\n          assertTrue(\"first posIncrement must be >= 1\", posIncrAtt.getPositionIncrement() >= 1);\n        } else {\n          assertTrue(\"posIncrement must be >= 0\", posIncrAtt.getPositionIncrement() >= 0);\n        }\n      }\n      if (posLengthAtt != null) {\n        assertTrue(\"posLength must be >= 1; got: \" + posLengthAtt.getPositionLength(), posLengthAtt.getPositionLength() >= 1);\n      }\n    }\n\n    if (ts.incrementToken()) {\n      fail(\"TokenStream has more tokens than expected (expected count=\" + output.length + \"); extra token=\" + ts.getAttribute(CharTermAttribute.class));\n    }\n\n    // repeat our extra safety checks for end()\n    ts.clearAttributes();\n    if (termAtt != null) termAtt.setEmpty().append(\"bogusTerm\");\n    if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n    if (typeAtt != null) typeAtt.setType(\"bogusType\");\n    if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n    if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n    \n    checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n\n    ts.end();\n    assertTrue(\"super.end()/clearAttributes() was not called correctly in end()\", checkClearAtt.getAndResetClearCalled());\n    \n    if (finalOffset != null) {\n      assertEquals(\"finalOffset\", finalOffset.intValue(), offsetAtt.endOffset());\n    }\n    if (offsetAtt != null) {\n      assertTrue(\"finalOffset must be >= 0\", offsetAtt.endOffset() >= 0);\n    }\n    if (finalPosInc != null) {\n      assertEquals(\"finalPosInc\", finalPosInc.intValue(), posIncrAtt.getPositionIncrement());\n    }\n\n    ts.close();\n  }\n\n","sourceOld":"  // offsetsAreCorrect also validates:\n  //   - graph offsets are correct (all tokens leaving from\n  //     pos X have the same startOffset; all tokens\n  //     arriving to pos Y have the same endOffset)\n  //   - offsets only move forwards (startOffset >=\n  //     lastStartOffset)\n  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[],\n                                               int posLengths[], Integer finalOffset, Integer finalPosInc, boolean[] keywordAtts,\n                                               boolean offsetsAreCorrect) throws IOException {\n    assertNotNull(output);\n    CheckClearAttributesAttribute checkClearAtt = ts.addAttribute(CheckClearAttributesAttribute.class);\n    \n    CharTermAttribute termAtt = null;\n    if (output.length > 0) {\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      termAtt = ts.getAttribute(CharTermAttribute.class);\n    }\n    \n    OffsetAttribute offsetAtt = null;\n    if (startOffsets != null || endOffsets != null || finalOffset != null) {\n      assertTrue(\"has no OffsetAttribute\", ts.hasAttribute(OffsetAttribute.class));\n      offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    }\n    \n    TypeAttribute typeAtt = null;\n    if (types != null) {\n      assertTrue(\"has no TypeAttribute\", ts.hasAttribute(TypeAttribute.class));\n      typeAtt = ts.getAttribute(TypeAttribute.class);\n    }\n    \n    PositionIncrementAttribute posIncrAtt = null;\n    if (posIncrements != null || finalPosInc != null) {\n      assertTrue(\"has no PositionIncrementAttribute\", ts.hasAttribute(PositionIncrementAttribute.class));\n      posIncrAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    }\n\n    PositionLengthAttribute posLengthAtt = null;\n    if (posLengths != null) {\n      assertTrue(\"has no PositionLengthAttribute\", ts.hasAttribute(PositionLengthAttribute.class));\n      posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    }\n\n    KeywordAttribute keywordAtt = null;\n    if (keywordAtts != null) {\n      assertTrue(\"has no KeywordAttribute\", ts.hasAttribute(KeywordAttribute.class));\n      keywordAtt = ts.getAttribute(KeywordAttribute.class);\n    }\n    \n    // Maps position to the start/end offset:\n    final Map<Integer,Integer> posToStartOffset = new HashMap<>();\n    final Map<Integer,Integer> posToEndOffset = new HashMap<>();\n\n    ts.reset();\n    int pos = -1;\n    int lastStartOffset = 0;\n    for (int i = 0; i < output.length; i++) {\n      // extra safety to enforce, that the state is not preserved and also assign bogus values\n      ts.clearAttributes();\n      termAtt.setEmpty().append(\"bogusTerm\");\n      if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n      if (typeAtt != null) typeAtt.setType(\"bogusType\");\n      if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n      if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n      if (keywordAtt != null) keywordAtt.setKeyword((i&1) == 0);\n      \n      checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n      assertTrue(\"token \"+i+\" does not exist\", ts.incrementToken());\n      assertTrue(\"clearAttributes() was not called correctly in TokenStream chain\", checkClearAtt.getAndResetClearCalled());\n      \n      assertEquals(\"term \"+i, output[i], termAtt.toString());\n      if (startOffsets != null) {\n        assertEquals(\"startOffset \" + i + \" term=\" + termAtt, startOffsets[i], offsetAtt.startOffset());\n      }\n      if (endOffsets != null) {\n        assertEquals(\"endOffset \" + i + \" term=\" + termAtt, endOffsets[i], offsetAtt.endOffset());\n      }\n      if (types != null) {\n        assertEquals(\"type \" + i + \" term=\" + termAtt, types[i], typeAtt.type());\n      }\n      if (posIncrements != null) {\n        assertEquals(\"posIncrement \" + i + \" term=\" + termAtt, posIncrements[i], posIncrAtt.getPositionIncrement());\n      }\n      if (posLengths != null) {\n        assertEquals(\"posLength \" + i + \" term=\" + termAtt, posLengths[i], posLengthAtt.getPositionLength());\n      }\n      if (keywordAtts != null) {\n        assertEquals(\"keywordAtt \" + i + \" term=\" + termAtt, keywordAtts[i], keywordAtt.isKeyword());\n      }\n      \n      // we can enforce some basic things about a few attributes even if the caller doesn't check:\n      if (offsetAtt != null) {\n        final int startOffset = offsetAtt.startOffset();\n        final int endOffset = offsetAtt.endOffset();\n        if (finalOffset != null) {\n          assertTrue(\"startOffset (= \" + startOffset + \") must be <= finalOffset (= \" + finalOffset + \") term=\" + termAtt, startOffset <= finalOffset.intValue());\n          assertTrue(\"endOffset must be <= finalOffset: got endOffset=\" + endOffset + \" vs finalOffset=\" + finalOffset.intValue() + \" term=\" + termAtt,\n                     endOffset <= finalOffset.intValue());\n        }\n\n        if (offsetsAreCorrect) {\n          assertTrue(\"offsets must not go backwards startOffset=\" + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" term=\" + termAtt, offsetAtt.startOffset() >= lastStartOffset);\n          lastStartOffset = offsetAtt.startOffset();\n        }\n\n        if (offsetsAreCorrect && posLengthAtt != null && posIncrAtt != null) {\n          // Validate offset consistency in the graph, ie\n          // all tokens leaving from a certain pos have the\n          // same startOffset, and all tokens arriving to a\n          // certain pos have the same endOffset:\n          final int posInc = posIncrAtt.getPositionIncrement();\n          pos += posInc;\n\n          final int posLength = posLengthAtt.getPositionLength();\n\n          if (!posToStartOffset.containsKey(pos)) {\n            // First time we've seen a token leaving from this position:\n            posToStartOffset.put(pos, startOffset);\n            //System.out.println(\"  + s \" + pos + \" -> \" + startOffset);\n          } else {\n            // We've seen a token leaving from this position\n            // before; verify the startOffset is the same:\n            //System.out.println(\"  + vs \" + pos + \" -> \" + startOffset);\n            assertEquals(i + \" inconsistent startOffset: pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToStartOffset.get(pos).intValue(), startOffset);\n          }\n\n          final int endPos = pos + posLength;\n\n          if (!posToEndOffset.containsKey(endPos)) {\n            // First time we've seen a token arriving to this position:\n            posToEndOffset.put(endPos, endOffset);\n            //System.out.println(\"  + e \" + endPos + \" -> \" + endOffset);\n          } else {\n            // We've seen a token arriving to this position\n            // before; verify the endOffset is the same:\n            //System.out.println(\"  + ve \" + endPos + \" -> \" + endOffset);\n            assertEquals(\"inconsistent endOffset \" + i + \" pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToEndOffset.get(endPos).intValue(), endOffset);\n          }\n        }\n      }\n      if (posIncrAtt != null) {\n        if (i == 0) {\n          assertTrue(\"first posIncrement must be >= 1\", posIncrAtt.getPositionIncrement() >= 1);\n        } else {\n          assertTrue(\"posIncrement must be >= 0\", posIncrAtt.getPositionIncrement() >= 0);\n        }\n      }\n      if (posLengthAtt != null) {\n        assertTrue(\"posLength must be >= 1\", posLengthAtt.getPositionLength() >= 1);\n      }\n    }\n\n    if (ts.incrementToken()) {\n      fail(\"TokenStream has more tokens than expected (expected count=\" + output.length + \"); extra token=\" + termAtt);\n    }\n\n    // repeat our extra safety checks for end()\n    ts.clearAttributes();\n    if (termAtt != null) termAtt.setEmpty().append(\"bogusTerm\");\n    if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n    if (typeAtt != null) typeAtt.setType(\"bogusType\");\n    if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n    if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n    \n    checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n\n    ts.end();\n    assertTrue(\"super.end()/clearAttributes() was not called correctly in end()\", checkClearAtt.getAndResetClearCalled());\n    \n    if (finalOffset != null) {\n      assertEquals(\"finalOffset\", finalOffset.intValue(), offsetAtt.endOffset());\n    }\n    if (offsetAtt != null) {\n      assertTrue(\"finalOffset must be >= 0\", offsetAtt.endOffset() >= 0);\n    }\n    if (finalPosInc != null) {\n      assertEquals(\"finalPosInc\", finalPosInc.intValue(), posIncrAtt.getPositionIncrement());\n    }\n\n    ts.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"afc5b4b2446e392448f36ae4f5a164540f2ccb65","date":1513355058,"type":5,"author":"Steve Rowe","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertTokenStreamContents(TokenStream,String[],int[],int[],String[],int[],int[],Integer,Integer,boolean[],boolean,byte[][]).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertTokenStreamContents(TokenStream,String[],int[],int[],String[],int[],int[],Integer,Integer,boolean[],boolean).mjava","sourceNew":"  // offsetsAreCorrect also validates:\n  //   - graph offsets are correct (all tokens leaving from\n  //     pos X have the same startOffset; all tokens\n  //     arriving to pos Y have the same endOffset)\n  //   - offsets only move forwards (startOffset >=\n  //     lastStartOffset)\n  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[],\n                                               int posLengths[], Integer finalOffset, Integer finalPosInc, boolean[] keywordAtts,\n                                               boolean offsetsAreCorrect, byte[][] payloads) throws IOException {\n    assertNotNull(output);\n    CheckClearAttributesAttribute checkClearAtt = ts.addAttribute(CheckClearAttributesAttribute.class);\n    \n    CharTermAttribute termAtt = null;\n    if (output.length > 0) {\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      termAtt = ts.getAttribute(CharTermAttribute.class);\n    }\n    \n    OffsetAttribute offsetAtt = null;\n    if (startOffsets != null || endOffsets != null || finalOffset != null) {\n      assertTrue(\"has no OffsetAttribute\", ts.hasAttribute(OffsetAttribute.class));\n      offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    }\n    \n    TypeAttribute typeAtt = null;\n    if (types != null) {\n      assertTrue(\"has no TypeAttribute\", ts.hasAttribute(TypeAttribute.class));\n      typeAtt = ts.getAttribute(TypeAttribute.class);\n    }\n    \n    PositionIncrementAttribute posIncrAtt = null;\n    if (posIncrements != null || finalPosInc != null) {\n      assertTrue(\"has no PositionIncrementAttribute\", ts.hasAttribute(PositionIncrementAttribute.class));\n      posIncrAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    }\n\n    PositionLengthAttribute posLengthAtt = null;\n    if (posLengths != null) {\n      assertTrue(\"has no PositionLengthAttribute\", ts.hasAttribute(PositionLengthAttribute.class));\n      posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    }\n\n    KeywordAttribute keywordAtt = null;\n    if (keywordAtts != null) {\n      assertTrue(\"has no KeywordAttribute\", ts.hasAttribute(KeywordAttribute.class));\n      keywordAtt = ts.getAttribute(KeywordAttribute.class);\n    }\n\n    PayloadAttribute payloadAtt = null;\n    if (payloads != null) {\n      assertTrue(\"has no PayloadAttribute\", ts.hasAttribute(PayloadAttribute.class));\n      payloadAtt = ts.getAttribute(PayloadAttribute.class);\n    }\n    \n    // Maps position to the start/end offset:\n    final Map<Integer,Integer> posToStartOffset = new HashMap<>();\n    final Map<Integer,Integer> posToEndOffset = new HashMap<>();\n\n    // TODO: would be nice to be able to assert silly duplicated tokens are not created, but a number of cases do this \"legitimately\": LUCENE-7622\n\n    ts.reset();\n    int pos = -1;\n    int lastStartOffset = 0;\n    for (int i = 0; i < output.length; i++) {\n      // extra safety to enforce, that the state is not preserved and also assign bogus values\n      ts.clearAttributes();\n      termAtt.setEmpty().append(\"bogusTerm\");\n      if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n      if (typeAtt != null) typeAtt.setType(\"bogusType\");\n      if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n      if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n      if (keywordAtt != null) keywordAtt.setKeyword((i&1) == 0);\n      if (payloadAtt != null) payloadAtt.setPayload(new BytesRef(new byte[] { 0x00, -0x21, 0x12, -0x43, 0x24 }));\n      \n      checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n      assertTrue(\"token \"+i+\" does not exist\", ts.incrementToken());\n      assertTrue(\"clearAttributes() was not called correctly in TokenStream chain\", checkClearAtt.getAndResetClearCalled());\n\n      assertEquals(\"term \"+i, output[i], termAtt.toString());\n      if (startOffsets != null) {\n        assertEquals(\"startOffset \" + i + \" term=\" + termAtt, startOffsets[i], offsetAtt.startOffset());\n      }\n      if (endOffsets != null) {\n        assertEquals(\"endOffset \" + i + \" term=\" + termAtt, endOffsets[i], offsetAtt.endOffset());\n      }\n      if (types != null) {\n        assertEquals(\"type \" + i + \" term=\" + termAtt, types[i], typeAtt.type());\n      }\n      if (posIncrements != null) {\n        assertEquals(\"posIncrement \" + i + \" term=\" + termAtt, posIncrements[i], posIncrAtt.getPositionIncrement());\n      }\n      if (posLengths != null) {\n        assertEquals(\"posLength \" + i + \" term=\" + termAtt, posLengths[i], posLengthAtt.getPositionLength());\n      }\n      if (keywordAtts != null) {\n        assertEquals(\"keywordAtt \" + i + \" term=\" + termAtt, keywordAtts[i], keywordAtt.isKeyword());\n      }\n      if (payloads != null) {\n        if (payloads[i] != null) {\n          assertEquals(\"payloads \" + i, new BytesRef(payloads[i]), payloadAtt.getPayload());\n        } else {\n          assertNull(\"payloads \" + i, payloads[i]);\n        }\n      }\n\n      // we can enforce some basic things about a few attributes even if the caller doesn't check:\n      if (offsetAtt != null) {\n        final int startOffset = offsetAtt.startOffset();\n        final int endOffset = offsetAtt.endOffset();\n        if (finalOffset != null) {\n          assertTrue(\"startOffset (= \" + startOffset + \") must be <= finalOffset (= \" + finalOffset + \") term=\" + termAtt, startOffset <= finalOffset.intValue());\n          assertTrue(\"endOffset must be <= finalOffset: got endOffset=\" + endOffset + \" vs finalOffset=\" + finalOffset.intValue() + \" term=\" + termAtt,\n                     endOffset <= finalOffset.intValue());\n        }\n\n        if (offsetsAreCorrect) {\n          assertTrue(\"offsets must not go backwards startOffset=\" + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" term=\" + termAtt, offsetAtt.startOffset() >= lastStartOffset);\n          lastStartOffset = offsetAtt.startOffset();\n        }\n\n        if (offsetsAreCorrect && posLengthAtt != null && posIncrAtt != null) {\n          // Validate offset consistency in the graph, ie\n          // all tokens leaving from a certain pos have the\n          // same startOffset, and all tokens arriving to a\n          // certain pos have the same endOffset:\n          final int posInc = posIncrAtt.getPositionIncrement();\n          pos += posInc;\n\n          final int posLength = posLengthAtt.getPositionLength();\n\n          if (!posToStartOffset.containsKey(pos)) {\n            // First time we've seen a token leaving from this position:\n            posToStartOffset.put(pos, startOffset);\n            //System.out.println(\"  + s \" + pos + \" -> \" + startOffset);\n          } else {\n            // We've seen a token leaving from this position\n            // before; verify the startOffset is the same:\n            //System.out.println(\"  + vs \" + pos + \" -> \" + startOffset);\n            assertEquals(i + \" inconsistent startOffset: pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToStartOffset.get(pos).intValue(), startOffset);\n          }\n\n          final int endPos = pos + posLength;\n\n          if (!posToEndOffset.containsKey(endPos)) {\n            // First time we've seen a token arriving to this position:\n            posToEndOffset.put(endPos, endOffset);\n            //System.out.println(\"  + e \" + endPos + \" -> \" + endOffset);\n          } else {\n            // We've seen a token arriving to this position\n            // before; verify the endOffset is the same:\n            //System.out.println(\"  + ve \" + endPos + \" -> \" + endOffset);\n            assertEquals(\"inconsistent endOffset \" + i + \" pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToEndOffset.get(endPos).intValue(), endOffset);\n          }\n        }\n      }\n      if (posIncrAtt != null) {\n        if (i == 0) {\n          assertTrue(\"first posIncrement must be >= 1\", posIncrAtt.getPositionIncrement() >= 1);\n        } else {\n          assertTrue(\"posIncrement must be >= 0\", posIncrAtt.getPositionIncrement() >= 0);\n        }\n      }\n      if (posLengthAtt != null) {\n        assertTrue(\"posLength must be >= 1; got: \" + posLengthAtt.getPositionLength(), posLengthAtt.getPositionLength() >= 1);\n      }\n    }\n\n    if (ts.incrementToken()) {\n      fail(\"TokenStream has more tokens than expected (expected count=\" + output.length + \"); extra token=\" + ts.getAttribute(CharTermAttribute.class));\n    }\n\n    // repeat our extra safety checks for end()\n    ts.clearAttributes();\n    if (termAtt != null) termAtt.setEmpty().append(\"bogusTerm\");\n    if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n    if (typeAtt != null) typeAtt.setType(\"bogusType\");\n    if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n    if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n    if (keywordAtt != null) keywordAtt.setKeyword(true);\n    if (payloadAtt != null) payloadAtt.setPayload(new BytesRef(new byte[] { 0x00, -0x21, 0x12, -0x43, 0x24 }));\n\n    checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n\n    ts.end();\n    assertTrue(\"super.end()/clearAttributes() was not called correctly in end()\", checkClearAtt.getAndResetClearCalled());\n    \n    if (finalOffset != null) {\n      assertEquals(\"finalOffset\", finalOffset.intValue(), offsetAtt.endOffset());\n    }\n    if (offsetAtt != null) {\n      assertTrue(\"finalOffset must be >= 0\", offsetAtt.endOffset() >= 0);\n    }\n    if (finalPosInc != null) {\n      assertEquals(\"finalPosInc\", finalPosInc.intValue(), posIncrAtt.getPositionIncrement());\n    }\n\n    ts.close();\n  }\n\n","sourceOld":"  // offsetsAreCorrect also validates:\n  //   - graph offsets are correct (all tokens leaving from\n  //     pos X have the same startOffset; all tokens\n  //     arriving to pos Y have the same endOffset)\n  //   - offsets only move forwards (startOffset >=\n  //     lastStartOffset)\n  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[],\n                                               int posLengths[], Integer finalOffset, Integer finalPosInc, boolean[] keywordAtts,\n                                               boolean offsetsAreCorrect) throws IOException {\n    assertNotNull(output);\n    CheckClearAttributesAttribute checkClearAtt = ts.addAttribute(CheckClearAttributesAttribute.class);\n    \n    CharTermAttribute termAtt = null;\n    if (output.length > 0) {\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      termAtt = ts.getAttribute(CharTermAttribute.class);\n    }\n    \n    OffsetAttribute offsetAtt = null;\n    if (startOffsets != null || endOffsets != null || finalOffset != null) {\n      assertTrue(\"has no OffsetAttribute\", ts.hasAttribute(OffsetAttribute.class));\n      offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    }\n    \n    TypeAttribute typeAtt = null;\n    if (types != null) {\n      assertTrue(\"has no TypeAttribute\", ts.hasAttribute(TypeAttribute.class));\n      typeAtt = ts.getAttribute(TypeAttribute.class);\n    }\n    \n    PositionIncrementAttribute posIncrAtt = null;\n    if (posIncrements != null || finalPosInc != null) {\n      assertTrue(\"has no PositionIncrementAttribute\", ts.hasAttribute(PositionIncrementAttribute.class));\n      posIncrAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    }\n\n    PositionLengthAttribute posLengthAtt = null;\n    if (posLengths != null) {\n      assertTrue(\"has no PositionLengthAttribute\", ts.hasAttribute(PositionLengthAttribute.class));\n      posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    }\n\n    KeywordAttribute keywordAtt = null;\n    if (keywordAtts != null) {\n      assertTrue(\"has no KeywordAttribute\", ts.hasAttribute(KeywordAttribute.class));\n      keywordAtt = ts.getAttribute(KeywordAttribute.class);\n    }\n    \n    // Maps position to the start/end offset:\n    final Map<Integer,Integer> posToStartOffset = new HashMap<>();\n    final Map<Integer,Integer> posToEndOffset = new HashMap<>();\n\n    // TODO: would be nice to be able to assert silly duplicated tokens are not created, but a number of cases do this \"legitimately\": LUCENE-7622\n\n    ts.reset();\n    int pos = -1;\n    int lastStartOffset = 0;\n    for (int i = 0; i < output.length; i++) {\n      // extra safety to enforce, that the state is not preserved and also assign bogus values\n      ts.clearAttributes();\n      termAtt.setEmpty().append(\"bogusTerm\");\n      if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n      if (typeAtt != null) typeAtt.setType(\"bogusType\");\n      if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n      if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n      if (keywordAtt != null) keywordAtt.setKeyword((i&1) == 0);\n      \n      checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n      assertTrue(\"token \"+i+\" does not exist\", ts.incrementToken());\n      assertTrue(\"clearAttributes() was not called correctly in TokenStream chain\", checkClearAtt.getAndResetClearCalled());\n\n      assertEquals(\"term \"+i, output[i], termAtt.toString());\n      if (startOffsets != null) {\n        assertEquals(\"startOffset \" + i + \" term=\" + termAtt, startOffsets[i], offsetAtt.startOffset());\n      }\n      if (endOffsets != null) {\n        assertEquals(\"endOffset \" + i + \" term=\" + termAtt, endOffsets[i], offsetAtt.endOffset());\n      }\n      if (types != null) {\n        assertEquals(\"type \" + i + \" term=\" + termAtt, types[i], typeAtt.type());\n      }\n      if (posIncrements != null) {\n        assertEquals(\"posIncrement \" + i + \" term=\" + termAtt, posIncrements[i], posIncrAtt.getPositionIncrement());\n      }\n      if (posLengths != null) {\n        assertEquals(\"posLength \" + i + \" term=\" + termAtt, posLengths[i], posLengthAtt.getPositionLength());\n      }\n      if (keywordAtts != null) {\n        assertEquals(\"keywordAtt \" + i + \" term=\" + termAtt, keywordAtts[i], keywordAtt.isKeyword());\n      }\n      \n      // we can enforce some basic things about a few attributes even if the caller doesn't check:\n      if (offsetAtt != null) {\n        final int startOffset = offsetAtt.startOffset();\n        final int endOffset = offsetAtt.endOffset();\n        if (finalOffset != null) {\n          assertTrue(\"startOffset (= \" + startOffset + \") must be <= finalOffset (= \" + finalOffset + \") term=\" + termAtt, startOffset <= finalOffset.intValue());\n          assertTrue(\"endOffset must be <= finalOffset: got endOffset=\" + endOffset + \" vs finalOffset=\" + finalOffset.intValue() + \" term=\" + termAtt,\n                     endOffset <= finalOffset.intValue());\n        }\n\n        if (offsetsAreCorrect) {\n          assertTrue(\"offsets must not go backwards startOffset=\" + startOffset + \" is < lastStartOffset=\" + lastStartOffset + \" term=\" + termAtt, offsetAtt.startOffset() >= lastStartOffset);\n          lastStartOffset = offsetAtt.startOffset();\n        }\n\n        if (offsetsAreCorrect && posLengthAtt != null && posIncrAtt != null) {\n          // Validate offset consistency in the graph, ie\n          // all tokens leaving from a certain pos have the\n          // same startOffset, and all tokens arriving to a\n          // certain pos have the same endOffset:\n          final int posInc = posIncrAtt.getPositionIncrement();\n          pos += posInc;\n\n          final int posLength = posLengthAtt.getPositionLength();\n\n          if (!posToStartOffset.containsKey(pos)) {\n            // First time we've seen a token leaving from this position:\n            posToStartOffset.put(pos, startOffset);\n            //System.out.println(\"  + s \" + pos + \" -> \" + startOffset);\n          } else {\n            // We've seen a token leaving from this position\n            // before; verify the startOffset is the same:\n            //System.out.println(\"  + vs \" + pos + \" -> \" + startOffset);\n            assertEquals(i + \" inconsistent startOffset: pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToStartOffset.get(pos).intValue(), startOffset);\n          }\n\n          final int endPos = pos + posLength;\n\n          if (!posToEndOffset.containsKey(endPos)) {\n            // First time we've seen a token arriving to this position:\n            posToEndOffset.put(endPos, endOffset);\n            //System.out.println(\"  + e \" + endPos + \" -> \" + endOffset);\n          } else {\n            // We've seen a token arriving to this position\n            // before; verify the endOffset is the same:\n            //System.out.println(\"  + ve \" + endPos + \" -> \" + endOffset);\n            assertEquals(\"inconsistent endOffset \" + i + \" pos=\" + pos + \" posLen=\" + posLength + \" token=\" + termAtt, posToEndOffset.get(endPos).intValue(), endOffset);\n          }\n        }\n      }\n      if (posIncrAtt != null) {\n        if (i == 0) {\n          assertTrue(\"first posIncrement must be >= 1\", posIncrAtt.getPositionIncrement() >= 1);\n        } else {\n          assertTrue(\"posIncrement must be >= 0\", posIncrAtt.getPositionIncrement() >= 0);\n        }\n      }\n      if (posLengthAtt != null) {\n        assertTrue(\"posLength must be >= 1; got: \" + posLengthAtt.getPositionLength(), posLengthAtt.getPositionLength() >= 1);\n      }\n    }\n\n    if (ts.incrementToken()) {\n      fail(\"TokenStream has more tokens than expected (expected count=\" + output.length + \"); extra token=\" + ts.getAttribute(CharTermAttribute.class));\n    }\n\n    // repeat our extra safety checks for end()\n    ts.clearAttributes();\n    if (termAtt != null) termAtt.setEmpty().append(\"bogusTerm\");\n    if (offsetAtt != null) offsetAtt.setOffset(14584724,24683243);\n    if (typeAtt != null) typeAtt.setType(\"bogusType\");\n    if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);\n    if (posLengthAtt != null) posLengthAtt.setPositionLength(45987653);\n    \n    checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before\n\n    ts.end();\n    assertTrue(\"super.end()/clearAttributes() was not called correctly in end()\", checkClearAtt.getAndResetClearCalled());\n    \n    if (finalOffset != null) {\n      assertEquals(\"finalOffset\", finalOffset.intValue(), offsetAtt.endOffset());\n    }\n    if (offsetAtt != null) {\n      assertTrue(\"finalOffset must be >= 0\", offsetAtt.endOffset() >= 0);\n    }\n    if (finalPosInc != null) {\n      assertEquals(\"finalPosInc\", finalPosInc.intValue(), posIncrAtt.getPositionIncrement());\n    }\n\n    ts.close();\n  }\n\n","bugFix":null,"bugIntro":["995993f24c9f6feb42b49b71e1982cda8fa0b37c"],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["f3f8a0f8bebc057ea4bdf65150b3fdc539db3d03"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f3f8a0f8bebc057ea4bdf65150b3fdc539db3d03"],"098528909bb70948871fd7ed865fafb87ed73964":["24a98f5fdd23e04f85819dbc63b47a12f7c44311"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"24a98f5fdd23e04f85819dbc63b47a12f7c44311":["82dada84151ca949c188f7d68251edbda0271d8d"],"82dada84151ca949c188f7d68251edbda0271d8d":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"f3f8a0f8bebc057ea4bdf65150b3fdc539db3d03":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["afc5b4b2446e392448f36ae4f5a164540f2ccb65"],"afc5b4b2446e392448f36ae4f5a164540f2ccb65":["098528909bb70948871fd7ed865fafb87ed73964"],"f03e4bed5023ec3ef93a771b8888cae991cf448d":["82dada84151ca949c188f7d68251edbda0271d8d","24a98f5fdd23e04f85819dbc63b47a12f7c44311"],"302d34f2c66e8d489ee13078305c330cbf67b226":["f03e4bed5023ec3ef93a771b8888cae991cf448d","098528909bb70948871fd7ed865fafb87ed73964"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["82dada84151ca949c188f7d68251edbda0271d8d"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","f3f8a0f8bebc057ea4bdf65150b3fdc539db3d03"],"098528909bb70948871fd7ed865fafb87ed73964":["afc5b4b2446e392448f36ae4f5a164540f2ccb65","302d34f2c66e8d489ee13078305c330cbf67b226"],"24a98f5fdd23e04f85819dbc63b47a12f7c44311":["098528909bb70948871fd7ed865fafb87ed73964","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"f3f8a0f8bebc057ea4bdf65150b3fdc539db3d03":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","3dffec77fb8f7d0e9ca4869dddd6af94528b4576"],"82dada84151ca949c188f7d68251edbda0271d8d":["24a98f5fdd23e04f85819dbc63b47a12f7c44311","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"afc5b4b2446e392448f36ae4f5a164540f2ccb65":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"f03e4bed5023ec3ef93a771b8888cae991cf448d":["302d34f2c66e8d489ee13078305c330cbf67b226"],"302d34f2c66e8d489ee13078305c330cbf67b226":[]},"heads":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","cd5edd1f2b162a5cfa08efd17851a07373a96817","302d34f2c66e8d489ee13078305c330cbf67b226"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}