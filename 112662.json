{"path":"lucene/sandbox/src/java/org/apache/lucene/search/PhraseWildcardQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","commits":[{"id":"b5d819373a1ee4f8d8c858aecdb04c4b724b8e5e","date":1574848425,"type":0,"author":"Bruno Roustant","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/search/PhraseWildcardQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n    IndexReader reader = searcher.getIndexReader();\n\n    // Build a list of segments ordered by terms size (number of terms).\n    // The first segments to be searched are the smaller ones, which are by\n    // design containing the most recent documents. Any segment in this list\n    // may also be removed in the PhraseTerm.collectTermData() calls below\n    // if one of the phrase term does not match in the segment. This allows\n    // to early stop expanding multi-terms on removed segments.\n    // Additionally there is a global multi-term expansion limit across all multi-terms\n    // and all segments. So this is important to first start with the smallest\n    // segments to give back non-used expansion credits to the next multi-terms,\n    // as this is more probable with the small segments.\n    List<LeafReaderContext> sizeSortedSegments =\n        new SegmentTermsSizeComparator().createTermsSizeSortedCopyOf(reader.leaves());\n\n    // TermsData will contain the collected TermState and TermStatistics for all the terms\n    // of the phrase. It is filled during PhraseTerm.collectTermData() calls below.\n    TermsData termsData = createTermsData(sizeSortedSegments.size());\n\n    // Iterate the phrase terms, and collect the TermState for single-terms.\n    // - Early stop if a single term does not match.\n    int numMultiTerms = 0;\n    for (PhraseTerm phraseTerm : phraseTerms) {\n      if (phraseTerm.hasExpansions()) {\n        numMultiTerms++;\n      } else {\n        assert TestCounters.get().incSingleTermAnalysisCount();\n        int numMatches = phraseTerm.collectTermData(this, searcher, sizeSortedSegments, termsData);\n        if (numMatches == 0) {\n          // Early stop here because the single term does not match in any segment.\n          // So the whole phrase query cannot match.\n          return earlyStopWeight();\n        }\n      }\n    }\n\n    // Iterate the phrase terms and collect the TermState for multi-terms.\n    // - Early stop if a multi-term does not match.\n    // - Expand the multi-terms only when required.\n    int remainingExpansions = maxMultiTermExpansions;\n    int remainingMultiTerms = numMultiTerms;\n    for (PhraseTerm phraseTerm : phraseTerms) {\n      if (phraseTerm.hasExpansions()) {\n        assert TestCounters.get().incMultiTermAnalysisCount();\n        assert remainingExpansions >= 0 && remainingExpansions <= maxMultiTermExpansions;\n        assert remainingMultiTerms > 0;\n        // Consider the remaining expansions allowed for all remaining multi-terms.\n        // Divide it evenly to get the expansion limit for the current multi-term.\n        int maxExpansionsForTerm = remainingExpansions / remainingMultiTerms;\n        int numExpansions = phraseTerm.collectTermData(this, searcher, sizeSortedSegments, remainingMultiTerms, maxExpansionsForTerm, termsData);\n        assert numExpansions >= 0 && numExpansions <= maxExpansionsForTerm;\n        if (numExpansions == 0) {\n          // Early stop here because the multi-term does not match in any segment.\n          // So the whole phrase query cannot match.\n          return earlyStopWeight();\n        }\n        // Deduct the effectively used expansions. This may give more expansion\n        // credits to the next multi-terms.\n        remainingExpansions -= numExpansions;\n        remainingMultiTerms--;\n      }\n    }\n    assert remainingMultiTerms == 0;\n    assert remainingExpansions >= 0;\n\n//    TestCounters.get().printTestCounters(termsData);\n\n    return termsData.areAllTermsMatching() ?\n        createPhraseWeight(searcher, scoreMode, boost, termsData)\n        : noMatchWeight();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b5d819373a1ee4f8d8c858aecdb04c4b724b8e5e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b5d819373a1ee4f8d8c858aecdb04c4b724b8e5e"]},"commit2Childs":{"b5d819373a1ee4f8d8c858aecdb04c4b724b8e5e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b5d819373a1ee4f8d8c858aecdb04c4b724b8e5e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}