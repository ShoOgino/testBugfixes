{"path":"lucene/src/java/org/apache/lucene/analysis/Token#Token(String,int,int).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":2,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/analysis/Token#Token(String,int,int).mjava","pathOld":"backwards/src/java/org/apache/lucene/analysis/Token#Token(String,int,int).mjava","sourceNew":"  /** Constructs a Token with the given term text, and start\n   *  & end offsets.  The type defaults to \"word.\"\n   *  <b>NOTE:</b> for better indexing speed you should\n   *  instead use the char[] termBuffer methods to set the\n   *  term text.\n   *  @param text term text\n   *  @param start start offset\n   *  @param end end offset\n   */\n  public Token(String text, int start, int end) {\n    setTermBuffer(text);\n    startOffset = start;\n    endOffset = end;\n  }\n\n","sourceOld":"  /** Constructs a Token with the given term text, and start\n   *  & end offsets.  The type defaults to \"word.\"\n   *  <b>NOTE:</b> for better indexing speed you should\n   *  instead use the char[] termBuffer methods to set the\n   *  term text.\n   *  @param text term text\n   *  @param start start offset\n   *  @param end end offset\n   */\n  public Token(String text, int start, int end) {\n    setTermBuffer(text);\n    startOffset = start;\n    endOffset = end;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"955c32f886db6f6356c9fcdea6b1f1cb4effda24","date":1270581567,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/analysis/Token#Token(String,int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/analysis/Token#Token(String,int,int).mjava","sourceNew":"  /** Constructs a Token with the given term text, and start\n   *  & end offsets.  The type defaults to \"word.\"\n   *  <b>NOTE:</b> for better indexing speed you should\n   *  instead use the char[] termBuffer methods to set the\n   *  term text.\n   *  @param text term text\n   *  @param start start offset\n   *  @param end end offset\n   */\n  public Token(String text, int start, int end) {\n    append(text);\n    startOffset = start;\n    endOffset = end;\n  }\n\n","sourceOld":"  /** Constructs a Token with the given term text, and start\n   *  & end offsets.  The type defaults to \"word.\"\n   *  <b>NOTE:</b> for better indexing speed you should\n   *  instead use the char[] termBuffer methods to set the\n   *  term text.\n   *  @param text term text\n   *  @param start start offset\n   *  @param end end offset\n   */\n  public Token(String text, int start, int end) {\n    setTermBuffer(text);\n    startOffset = start;\n    endOffset = end;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/analysis/Token#Token(String,int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/analysis/Token#Token(String,int,int).mjava","sourceNew":"  /** Constructs a Token with the given term text, and start\n   *  & end offsets.  The type defaults to \"word.\"\n   *  <b>NOTE:</b> for better indexing speed you should\n   *  instead use the char[] termBuffer methods to set the\n   *  term text.\n   *  @param text term text\n   *  @param start start offset\n   *  @param end end offset\n   */\n  public Token(String text, int start, int end) {\n    append(text);\n    startOffset = start;\n    endOffset = end;\n  }\n\n","sourceOld":"  /** Constructs a Token with the given term text, and start\n   *  & end offsets.  The type defaults to \"word.\"\n   *  <b>NOTE:</b> for better indexing speed you should\n   *  instead use the char[] termBuffer methods to set the\n   *  term text.\n   *  @param text term text\n   *  @param start start offset\n   *  @param end end offset\n   */\n  public Token(String text, int start, int end) {\n    append(text);\n    startOffset = start;\n    endOffset = end;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}