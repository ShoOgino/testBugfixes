{"path":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","sourceNew":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      MockDirectoryWrapper dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(\n                random.nextBoolean() ? NoMergePolicy.COMPOUND_FILES\n                    : NoMergePolicy.NO_COMPOUND_FILES));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = IndexReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = IndexReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      MockDirectoryWrapper dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(\n                random.nextBoolean() ? NoMergePolicy.COMPOUND_FILES\n                    : NoMergePolicy.NO_COMPOUND_FILES));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = IndexReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = IndexReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","sourceNew":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      MockDirectoryWrapper dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(\n                random().nextBoolean() ? NoMergePolicy.COMPOUND_FILES\n                    : NoMergePolicy.NO_COMPOUND_FILES));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = IndexReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = IndexReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      MockDirectoryWrapper dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(\n                random.nextBoolean() ? NoMergePolicy.COMPOUND_FILES\n                    : NoMergePolicy.NO_COMPOUND_FILES));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = IndexReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = IndexReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","date":1338430031,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","sourceNew":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      MockDirectoryWrapper dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(\n                random().nextBoolean() ? NoMergePolicy.COMPOUND_FILES\n                    : NoMergePolicy.NO_COMPOUND_FILES));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      MockDirectoryWrapper dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(\n                random().nextBoolean() ? NoMergePolicy.COMPOUND_FILES\n                    : NoMergePolicy.NO_COMPOUND_FILES));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = IndexReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = IndexReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d19974432be9aed28ee7dca73bdf01d139e763a9","date":1342822166,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","sourceNew":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(\n                random().nextBoolean() ? NoMergePolicy.COMPOUND_FILES\n                    : NoMergePolicy.NO_COMPOUND_FILES));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      MockDirectoryWrapper dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(\n                random().nextBoolean() ? NoMergePolicy.COMPOUND_FILES\n                    : NoMergePolicy.NO_COMPOUND_FILES));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","bugFix":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","date":1343059585,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","sourceNew":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(\n                random().nextBoolean() ? NoMergePolicy.COMPOUND_FILES\n                    : NoMergePolicy.NO_COMPOUND_FILES));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      MockDirectoryWrapper dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(\n                random().nextBoolean() ? NoMergePolicy.COMPOUND_FILES\n                    : NoMergePolicy.NO_COMPOUND_FILES));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aba371508186796cc6151d8223a5b4e16d02e26e","date":1343474871,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","sourceNew":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(\n                random().nextBoolean() ? NoMergePolicy.COMPOUND_FILES\n                    : NoMergePolicy.NO_COMPOUND_FILES));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      MockDirectoryWrapper dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(\n                random().nextBoolean() ? NoMergePolicy.COMPOUND_FILES\n                    : NoMergePolicy.NO_COMPOUND_FILES));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"38061899d760e06a12fe186bc1f09ca9ff0e64a6","date":1376491296,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","sourceNew":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(\n                random().nextBoolean() ? NoMergePolicy.COMPOUND_FILES\n                    : NoMergePolicy.NO_COMPOUND_FILES));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(\n                random().nextBoolean() ? NoMergePolicy.COMPOUND_FILES\n                    : NoMergePolicy.NO_COMPOUND_FILES));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","sourceNew":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(\n                random().nextBoolean() ? NoMergePolicy.COMPOUND_FILES\n                    : NoMergePolicy.NO_COMPOUND_FILES));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(new Analyzer.PerFieldReuseStrategy()) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(\n                random().nextBoolean() ? NoMergePolicy.COMPOUND_FILES\n                    : NoMergePolicy.NO_COMPOUND_FILES));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","sourceNew":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(\n                random().nextBoolean() ? NoMergePolicy.COMPOUND_FILES\n                    : NoMergePolicy.NO_COMPOUND_FILES));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(\n                random().nextBoolean() ? NoMergePolicy.COMPOUND_FILES\n                    : NoMergePolicy.NO_COMPOUND_FILES));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","sourceNew":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(\n                random().nextBoolean() ? NoMergePolicy.COMPOUND_FILES\n                    : NoMergePolicy.NO_COMPOUND_FILES));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.shutdown();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.shutdown();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(\n                random().nextBoolean() ? NoMergePolicy.COMPOUND_FILES\n                    : NoMergePolicy.NO_COMPOUND_FILES));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7e2fb55c0777755badd3b46d8140f3d4301febed","date":1398881584,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","sourceNew":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(NoMergePolicy.INSTANCE));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.shutdown();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.shutdown();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(\n                random().nextBoolean() ? NoMergePolicy.COMPOUND_FILES\n                    : NoMergePolicy.NO_COMPOUND_FILES));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.shutdown();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.shutdown();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0a16b40feb4e6e0d55c1716733bde48296bedd20","date":1400540388,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","sourceNew":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(NoMergePolicy.INSTANCE));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.shutdown();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected - NUM_THREAD*NUM_ITER, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(\"i=\" + i, expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.shutdown();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(NoMergePolicy.INSTANCE));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.shutdown();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.shutdown();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"45a621dd071a902e1fd30367200d7bbbea037706","date":1400686915,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","sourceNew":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(NoMergePolicy.INSTANCE));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.shutdown();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.shutdown();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(NoMergePolicy.INSTANCE));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.shutdown();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected - NUM_THREAD*NUM_ITER, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(\"i=\" + i, expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.shutdown();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","sourceNew":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(-1)\n            .setMergePolicy(NoMergePolicy.INSTANCE));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.shutdown();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                                  .setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.shutdown();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(-1)\n            .setMergePolicy(NoMergePolicy.INSTANCE));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.shutdown();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.shutdown();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","sourceNew":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(-1)\n            .setMergePolicy(NoMergePolicy.INSTANCE));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                                  .setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(-1)\n            .setMergePolicy(NoMergePolicy.INSTANCE));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.shutdown();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                                  .setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.shutdown();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"05fe562aa248790944d43cdd478f512572835ba0","date":1455901667,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","sourceNew":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(-1)\n            .setMergePolicy(NoMergePolicy.INSTANCE));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    expectThrows(IOException.class, () -> {\n                      writer.addDocument(doc);\n                    });\n\n                    if (0 == finalI) {\n                      Document extraDoc = new Document();\n                      extraDoc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(extraDoc);\n                      writer.addDocument(extraDoc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                                  .setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(-1)\n            .setMergePolicy(NoMergePolicy.INSTANCE));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    try {\n                      writer.addDocument(doc);\n                      fail(\"did not hit expected exception\");\n                    } catch (IOException ioe) {\n                    }\n\n                    if (0 == finalI) {\n                      doc = new Document();\n                      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(doc);\n                      writer.addDocument(doc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                                  .setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","bugFix":["83bbb041887bbef07b8a98d08a0e1713ce137039","1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6be407dee495c09fdba67c96a73858848977cc20","date":1526477959,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","sourceNew":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(Integer.MAX_VALUE)\n            .setRAMBufferSizeMB(-1) // we don't want to flush automatically\n            .setMergePolicy(new FilterMergePolicy(NoMergePolicy.INSTANCE) {\n              // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n              // we also need to keep fully deleted segments since otherwise we clean up fully deleted ones and if we\n              // flush the one that has only the failed document the docFreq checks will be off below.\n              @Override\n              public boolean keepFullyDeletedSegment(IOSupplier<CodecReader> readerIOSupplier) {\n                return true;\n              }\n            }));\n\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    expectThrows(IOException.class, () -> {\n                      writer.addDocument(doc);\n                    });\n\n                    if (0 == finalI) {\n                      Document extraDoc = new Document();\n                      extraDoc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(extraDoc);\n                      writer.addDocument(extraDoc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                                  .setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(-1)\n            .setMergePolicy(NoMergePolicy.INSTANCE));\n        // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    expectThrows(IOException.class, () -> {\n                      writer.addDocument(doc);\n                    });\n\n                    if (0 == finalI) {\n                      Document extraDoc = new Document();\n                      extraDoc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(extraDoc);\n                      writer.addDocument(extraDoc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                                  .setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"04e775de416dd2d8067b10db1c8af975a1d5017e","date":1539906554,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","sourceNew":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(Integer.MAX_VALUE)\n            .setRAMBufferSizeMB(-1) // we don't want to flush automatically\n            .setMergePolicy(new FilterMergePolicy(NoMergePolicy.INSTANCE) {\n              // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n              // we also need to keep fully deleted segments since otherwise we clean up fully deleted ones and if we\n              // flush the one that has only the failed document the docFreq checks will be off below.\n              @Override\n              public boolean keepFullyDeletedSegment(IOSupplier<CodecReader> readerIOSupplier) {\n                return true;\n              }\n            }));\n\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    expectThrows(IOException.class, () -> {\n                      writer.addDocument(doc);\n                    });\n\n                    if (0 == finalI) {\n                      Document extraDoc = new Document();\n                      extraDoc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(extraDoc);\n                      writer.addDocument(extraDoc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiBits.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                                  .setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiBits.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(Integer.MAX_VALUE)\n            .setRAMBufferSizeMB(-1) // we don't want to flush automatically\n            .setMergePolicy(new FilterMergePolicy(NoMergePolicy.INSTANCE) {\n              // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n              // we also need to keep fully deleted segments since otherwise we clean up fully deleted ones and if we\n              // flush the one that has only the failed document the docFreq checks will be off below.\n              @Override\n              public boolean keepFullyDeletedSegment(IOSupplier<CodecReader> readerIOSupplier) {\n                return true;\n              }\n            }));\n\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    expectThrows(IOException.class, () -> {\n                      writer.addDocument(doc);\n                    });\n\n                    if (0 == finalI) {\n                      Document extraDoc = new Document();\n                      extraDoc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(extraDoc);\n                      writer.addDocument(extraDoc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                                  .setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiFields.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"57c6c784f777a2cc8fa014507ea129526822714d","date":1579733373,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testDocumentsWriterExceptionThreads().mjava","sourceNew":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = atLeast(10);\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(Integer.MAX_VALUE)\n            .setRAMBufferSizeMB(-1) // we don't want to flush automatically\n            .setMergePolicy(new FilterMergePolicy(NoMergePolicy.INSTANCE) {\n              // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n              // we also need to keep fully deleted segments since otherwise we clean up fully deleted ones and if we\n              // flush the one that has only the failed document the docFreq checks will be off below.\n              @Override\n              public boolean keepFullyDeletedSegment(IOSupplier<CodecReader> readerIOSupplier) {\n                return true;\n              }\n            }));\n\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    expectThrows(IOException.class, () -> {\n                      writer.addDocument(doc);\n                    });\n\n                    if (0 == finalI) {\n                      Document extraDoc = new Document();\n                      extraDoc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(extraDoc);\n                      writer.addDocument(extraDoc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiBits.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                                  .setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiBits.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testDocumentsWriterExceptionThreads() throws Exception {\n    Analyzer analyzer = new Analyzer(Analyzer.PER_FIELD_REUSE_STRATEGY) {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new CrashingFilter(fieldName, tokenizer));\n      }\n    };\n\n    final int NUM_THREAD = 3;\n    final int NUM_ITER = 100;\n\n    for(int i=0;i<2;i++) {\n      Directory dir = newDirectory();\n      {\n        final IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n            .setMaxBufferedDocs(Integer.MAX_VALUE)\n            .setRAMBufferSizeMB(-1) // we don't want to flush automatically\n            .setMergePolicy(new FilterMergePolicy(NoMergePolicy.INSTANCE) {\n              // don't use a merge policy here they depend on the DWPThreadPool and its max thread states etc.\n              // we also need to keep fully deleted segments since otherwise we clean up fully deleted ones and if we\n              // flush the one that has only the failed document the docFreq checks will be off below.\n              @Override\n              public boolean keepFullyDeletedSegment(IOSupplier<CodecReader> readerIOSupplier) {\n                return true;\n              }\n            }));\n\n        final int finalI = i;\n\n        Thread[] threads = new Thread[NUM_THREAD];\n        for(int t=0;t<NUM_THREAD;t++) {\n          threads[t] = new Thread() {\n              @Override\n              public void run() {\n                try {\n                  for(int iter=0;iter<NUM_ITER;iter++) {\n                    Document doc = new Document();\n                    doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                    writer.addDocument(doc);\n                    writer.addDocument(doc);\n                    doc.add(newField(\"crash\", \"this should crash after 4 terms\", DocCopyIterator.custom5));\n                    doc.add(newField(\"other\", \"this will not get indexed\", DocCopyIterator.custom5));\n                    expectThrows(IOException.class, () -> {\n                      writer.addDocument(doc);\n                    });\n\n                    if (0 == finalI) {\n                      Document extraDoc = new Document();\n                      extraDoc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n                      writer.addDocument(extraDoc);\n                      writer.addDocument(extraDoc);\n                    }\n                  }\n                } catch (Throwable t) {\n                  synchronized(this) {\n                    System.out.println(Thread.currentThread().getName() + \": ERROR: hit unexpected exception\");\n                    t.printStackTrace(System.out);\n                  }\n                  fail();\n                }\n              }\n            };\n          threads[t].start();\n        }\n\n        for(int t=0;t<NUM_THREAD;t++)\n          threads[t].join();\n\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(dir);\n      int expected = (3+(1-i)*2)*NUM_THREAD*NUM_ITER;\n      assertEquals(\"i=\" + i, expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      int numDel = 0;\n      final Bits liveDocs = MultiBits.getLiveDocs(reader);\n      assertNotNull(liveDocs);\n      for(int j=0;j<reader.maxDoc();j++) {\n        if (!liveDocs.get(j))\n          numDel++;\n        else {\n          reader.document(j);\n          reader.getTermVectors(j);\n        }\n      }\n      reader.close();\n\n      assertEquals(NUM_THREAD*NUM_ITER, numDel);\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                                  .setMaxBufferedDocs(10));\n      Document doc = new Document();\n      doc.add(newField(\"contents\", \"here are some contents\", DocCopyIterator.custom5));\n      for(int j=0;j<17;j++)\n        writer.addDocument(doc);\n      writer.forceMerge(1);\n      writer.close();\n\n      reader = DirectoryReader.open(dir);\n      expected += 17-NUM_THREAD*NUM_ITER;\n      assertEquals(expected, reader.docFreq(new Term(\"contents\", \"here\")));\n      assertEquals(expected, reader.maxDoc());\n      assertNull(MultiBits.getLiveDocs(reader));\n      for(int j=0;j<reader.maxDoc();j++) {\n        reader.document(j);\n        reader.getTermVectors(j);\n      }\n      reader.close();\n\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7e2fb55c0777755badd3b46d8140f3d4301febed":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"0a16b40feb4e6e0d55c1716733bde48296bedd20":["7e2fb55c0777755badd3b46d8140f3d4301febed"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["7e2fb55c0777755badd3b46d8140f3d4301febed"],"45a621dd071a902e1fd30367200d7bbbea037706":["0a16b40feb4e6e0d55c1716733bde48296bedd20"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["38061899d760e06a12fe186bc1f09ca9ff0e64a6"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["d19974432be9aed28ee7dca73bdf01d139e763a9","38061899d760e06a12fe186bc1f09ca9ff0e64a6"],"05fe562aa248790944d43cdd478f512572835ba0":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"57c6c784f777a2cc8fa014507ea129526822714d":["04e775de416dd2d8067b10db1c8af975a1d5017e"],"38061899d760e06a12fe186bc1f09ca9ff0e64a6":["d19974432be9aed28ee7dca73bdf01d139e763a9"],"aba371508186796cc6151d8223a5b4e16d02e26e":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","d19974432be9aed28ee7dca73bdf01d139e763a9"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"d19974432be9aed28ee7dca73bdf01d139e763a9":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["6be407dee495c09fdba67c96a73858848977cc20"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","d19974432be9aed28ee7dca73bdf01d139e763a9"],"6be407dee495c09fdba67c96a73858848977cc20":["05fe562aa248790944d43cdd478f512572835ba0"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["57c6c784f777a2cc8fa014507ea129526822714d"]},"commit2Childs":{"7e2fb55c0777755badd3b46d8140f3d4301febed":["0a16b40feb4e6e0d55c1716733bde48296bedd20","54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"0a16b40feb4e6e0d55c1716733bde48296bedd20":["45a621dd071a902e1fd30367200d7bbbea037706"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"45a621dd071a902e1fd30367200d7bbbea037706":[],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"05fe562aa248790944d43cdd478f512572835ba0":["6be407dee495c09fdba67c96a73858848977cc20"],"57c6c784f777a2cc8fa014507ea129526822714d":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"38061899d760e06a12fe186bc1f09ca9ff0e64a6":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","3dffec77fb8f7d0e9ca4869dddd6af94528b4576"],"aba371508186796cc6151d8223a5b4e16d02e26e":[],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["aba371508186796cc6151d8223a5b4e16d02e26e","d19974432be9aed28ee7dca73bdf01d139e763a9","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7"],"d19974432be9aed28ee7dca73bdf01d139e763a9":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","38061899d760e06a12fe186bc1f09ca9ff0e64a6","aba371508186796cc6151d8223a5b4e16d02e26e","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["05fe562aa248790944d43cdd478f512572835ba0"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["7e2fb55c0777755badd3b46d8140f3d4301febed"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["57c6c784f777a2cc8fa014507ea129526822714d"],"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7":[],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"6be407dee495c09fdba67c96a73858848977cc20":["04e775de416dd2d8067b10db1c8af975a1d5017e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["45a621dd071a902e1fd30367200d7bbbea037706","3dffec77fb8f7d0e9ca4869dddd6af94528b4576","aba371508186796cc6151d8223a5b4e16d02e26e","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}