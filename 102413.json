{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountFilterFactory#test().mjava","commits":[{"id":"cbe4cb0ba51d9578281533906fc02ee563b60520","date":1350514588,"type":0,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountFilterFactory#test().mjava","pathOld":"/dev/null","sourceNew":"  public void test() throws IOException {\n    LimitTokenCountFilterFactory factory = new LimitTokenCountFilterFactory();\n    Map<String, String> args = new HashMap<String, String>();\n    args.put(LimitTokenCountFilterFactory.MAX_TOKEN_COUNT_KEY, \"3\");\n    factory.init(args);\n    String test = \"A1 B2 C3 D4 E5 F6\";\n    MockTokenizer tok = new MockTokenizer(new StringReader(test), MockTokenizer.WHITESPACE, false);\n    // LimitTokenCountFilter doesn't consume the entire stream that it wraps\n    tok.setEnableChecks(false); \n    TokenStream stream = factory.create(tok);\n    assertTokenStreamContents(stream, new String[] { \"A1\", \"B2\", \"C3\" });\n\n    // param is required\n    factory = new LimitTokenCountFilterFactory();\n    args = new HashMap<String, String>();\n    IllegalArgumentException iae = null;\n    try {\n      factory.init(args);\n    } catch (IllegalArgumentException e) {\n      assertTrue(\"exception doesn't mention param: \" + e.getMessage(),\n                 0 < e.getMessage().indexOf(LimitTokenCountFilterFactory.MAX_TOKEN_COUNT_KEY));\n      iae = e;\n    }\n    assertNotNull(\"no exception thrown\", iae);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"db4fdbf3d262768eabc027cd8321edca0cd11fa8","date":1350574784,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountFilterFactory#test().mjava","pathOld":"/dev/null","sourceNew":"  public void test() throws IOException {\n    LimitTokenCountFilterFactory factory = new LimitTokenCountFilterFactory();\n    Map<String, String> args = new HashMap<String, String>();\n    args.put(LimitTokenCountFilterFactory.MAX_TOKEN_COUNT_KEY, \"3\");\n    factory.init(args);\n    String test = \"A1 B2 C3 D4 E5 F6\";\n    MockTokenizer tok = new MockTokenizer(new StringReader(test), MockTokenizer.WHITESPACE, false);\n    // LimitTokenCountFilter doesn't consume the entire stream that it wraps\n    tok.setEnableChecks(false); \n    TokenStream stream = factory.create(tok);\n    assertTokenStreamContents(stream, new String[] { \"A1\", \"B2\", \"C3\" });\n\n    // param is required\n    factory = new LimitTokenCountFilterFactory();\n    args = new HashMap<String, String>();\n    IllegalArgumentException iae = null;\n    try {\n      factory.init(args);\n    } catch (IllegalArgumentException e) {\n      assertTrue(\"exception doesn't mention param: \" + e.getMessage(),\n                 0 < e.getMessage().indexOf(LimitTokenCountFilterFactory.MAX_TOKEN_COUNT_KEY));\n      iae = e;\n    }\n    assertNotNull(\"no exception thrown\", iae);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"57da959ec15bb701bd1d1bf3c613b69009ff4bfd","date":1364833800,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountFilterFactory#test().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountFilterFactory#test().mjava","sourceNew":"  public void test() throws Exception {\n    Reader reader = new StringReader(\"A1 B2 C3 D4 E5 F6\");\n    MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n    // LimitTokenCountFilter doesn't consume the entire stream that it wraps\n    tokenizer.setEnableChecks(false);\n    TokenStream stream = tokenizer;\n    stream = tokenFilterFactory(\"LimitTokenCount\",\n        \"maxTokenCount\", \"3\").create(stream);\n    assertTokenStreamContents(stream, new String[] { \"A1\", \"B2\", \"C3\" });\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    LimitTokenCountFilterFactory factory = new LimitTokenCountFilterFactory();\n    Map<String, String> args = new HashMap<String, String>();\n    args.put(LimitTokenCountFilterFactory.MAX_TOKEN_COUNT_KEY, \"3\");\n    factory.init(args);\n    String test = \"A1 B2 C3 D4 E5 F6\";\n    MockTokenizer tok = new MockTokenizer(new StringReader(test), MockTokenizer.WHITESPACE, false);\n    // LimitTokenCountFilter doesn't consume the entire stream that it wraps\n    tok.setEnableChecks(false); \n    TokenStream stream = factory.create(tok);\n    assertTokenStreamContents(stream, new String[] { \"A1\", \"B2\", \"C3\" });\n\n    // param is required\n    factory = new LimitTokenCountFilterFactory();\n    args = new HashMap<String, String>();\n    IllegalArgumentException iae = null;\n    try {\n      factory.init(args);\n    } catch (IllegalArgumentException e) {\n      assertTrue(\"exception doesn't mention param: \" + e.getMessage(),\n                 0 < e.getMessage().indexOf(LimitTokenCountFilterFactory.MAX_TOKEN_COUNT_KEY));\n      iae = e;\n    }\n    assertNotNull(\"no exception thrown\", iae);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountFilterFactory#test().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountFilterFactory#test().mjava","sourceNew":"  public void test() throws Exception {\n    Reader reader = new StringReader(\"A1 B2 C3 D4 E5 F6\");\n    MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    tokenizer.setReader(reader);\n    // LimitTokenCountFilter doesn't consume the entire stream that it wraps\n    tokenizer.setEnableChecks(false);\n    TokenStream stream = tokenizer;\n    stream = tokenFilterFactory(\"LimitTokenCount\",\n        \"maxTokenCount\", \"3\").create(stream);\n    assertTokenStreamContents(stream, new String[] { \"A1\", \"B2\", \"C3\" });\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    Reader reader = new StringReader(\"A1 B2 C3 D4 E5 F6\");\n    MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n    // LimitTokenCountFilter doesn't consume the entire stream that it wraps\n    tokenizer.setEnableChecks(false);\n    TokenStream stream = tokenizer;\n    stream = tokenFilterFactory(\"LimitTokenCount\",\n        \"maxTokenCount\", \"3\").create(stream);\n    assertTokenStreamContents(stream, new String[] { \"A1\", \"B2\", \"C3\" });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"43acd3a99a12a5bec9c72097de0e294c80cb6daa","date":1396327381,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountFilterFactory#test().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountFilterFactory#test().mjava","sourceNew":"  public void test() throws Exception {\n    for (final boolean consumeAll : new boolean[]{true, false}) {\n      Reader reader = new StringReader(\"A1 B2 C3 D4 E5 F6\");\n      MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n      tokenizer.setReader(reader);\n      tokenizer.setEnableChecks(consumeAll);\n      TokenStream stream = tokenizer;\n      stream = tokenFilterFactory(\"LimitTokenCount\",\n          LimitTokenCountFilterFactory.MAX_TOKEN_COUNT_KEY, \"3\",\n          LimitTokenCountFilterFactory.CONSUME_ALL_TOKENS_KEY, Boolean.toString(consumeAll)\n      ).create(stream);\n      assertTokenStreamContents(stream, new String[]{\"A1\", \"B2\", \"C3\"});\n    }\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    Reader reader = new StringReader(\"A1 B2 C3 D4 E5 F6\");\n    MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    tokenizer.setReader(reader);\n    // LimitTokenCountFilter doesn't consume the entire stream that it wraps\n    tokenizer.setEnableChecks(false);\n    TokenStream stream = tokenizer;\n    stream = tokenFilterFactory(\"LimitTokenCount\",\n        \"maxTokenCount\", \"3\").create(stream);\n    assertTokenStreamContents(stream, new String[] { \"A1\", \"B2\", \"C3\" });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5eb2511ababf862ea11e10761c70ee560cd84510","date":1396607225,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountFilterFactory#test().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountFilterFactory#test().mjava","sourceNew":"  public void test() throws Exception {\n    for (final boolean consumeAll : new boolean[]{true, false}) {\n      Reader reader = new StringReader(\"A1 B2 C3 D4 E5 F6\");\n      MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n      tokenizer.setReader(reader);\n      tokenizer.setEnableChecks(consumeAll);\n      TokenStream stream = tokenizer;\n      stream = tokenFilterFactory(\"LimitTokenCount\",\n          LimitTokenCountFilterFactory.MAX_TOKEN_COUNT_KEY, \"3\",\n          LimitTokenCountFilterFactory.CONSUME_ALL_TOKENS_KEY, Boolean.toString(consumeAll)\n      ).create(stream);\n      assertTokenStreamContents(stream, new String[]{\"A1\", \"B2\", \"C3\"});\n    }\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    Reader reader = new StringReader(\"A1 B2 C3 D4 E5 F6\");\n    MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    tokenizer.setReader(reader);\n    // LimitTokenCountFilter doesn't consume the entire stream that it wraps\n    tokenizer.setEnableChecks(false);\n    TokenStream stream = tokenizer;\n    stream = tokenFilterFactory(\"LimitTokenCount\",\n        \"maxTokenCount\", \"3\").create(stream);\n    assertTokenStreamContents(stream, new String[] { \"A1\", \"B2\", \"C3\" });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"cbe4cb0ba51d9578281533906fc02ee563b60520":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"5eb2511ababf862ea11e10761c70ee560cd84510":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","43acd3a99a12a5bec9c72097de0e294c80cb6daa"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["57da959ec15bb701bd1d1bf3c613b69009ff4bfd"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"db4fdbf3d262768eabc027cd8321edca0cd11fa8":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","cbe4cb0ba51d9578281533906fc02ee563b60520"],"57da959ec15bb701bd1d1bf3c613b69009ff4bfd":["cbe4cb0ba51d9578281533906fc02ee563b60520"],"43acd3a99a12a5bec9c72097de0e294c80cb6daa":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["43acd3a99a12a5bec9c72097de0e294c80cb6daa"]},"commit2Childs":{"cbe4cb0ba51d9578281533906fc02ee563b60520":["db4fdbf3d262768eabc027cd8321edca0cd11fa8","57da959ec15bb701bd1d1bf3c613b69009ff4bfd"],"5eb2511ababf862ea11e10761c70ee560cd84510":[],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["5eb2511ababf862ea11e10761c70ee560cd84510","43acd3a99a12a5bec9c72097de0e294c80cb6daa"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cbe4cb0ba51d9578281533906fc02ee563b60520","db4fdbf3d262768eabc027cd8321edca0cd11fa8"],"db4fdbf3d262768eabc027cd8321edca0cd11fa8":[],"57da959ec15bb701bd1d1bf3c613b69009ff4bfd":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"43acd3a99a12a5bec9c72097de0e294c80cb6daa":["5eb2511ababf862ea11e10761c70ee560cd84510","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5eb2511ababf862ea11e10761c70ee560cd84510","db4fdbf3d262768eabc027cd8321edca0cd11fa8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}