{"path":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","commits":[{"id":"770281b8a8459cafcdd2354b6a06078fea2d83c9","date":1077308096,"type":0,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      for (int r = 0; r < readers.size(); r++) {\n        IndexReader reader = (IndexReader) readers.elementAt(r);\n        int maxDoc = reader.maxDoc();\n        for (int docNum = 0; docNum < maxDoc; docNum++) {\n          // skip deleted docs\n          if (reader.isDeleted(docNum)) {\n            continue;\n          }\n          termVectorsWriter.openDocument();\n\n          // get all term vectors\n          TermFreqVector[] sourceTermVector =\n            reader.getTermFreqVectors(docNum);\n\n          if (sourceTermVector != null) {\n            for (int f = 0; f < sourceTermVector.length; f++) {\n              // translate field numbers\n              TermFreqVector termVector = sourceTermVector[f];\n              termVectorsWriter.openField(termVector.getField());\n              String [] terms = termVector.getTerms();\n              int [] freqs = termVector.getTermFrequencies();\n              \n              for (int t = 0; t < terms.length; t++) {\n                termVectorsWriter.addTerm(terms[t], freqs[t]);\n              }\n            }\n            termVectorsWriter.closeDocument();\n          }\n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6177f0f28ace66d1538b1e6ac5f1773e5449a0b0","date":1096997448,"type":3,"author":"Christoph Goller","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","sourceNew":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      for (int r = 0; r < readers.size(); r++) {\n        IndexReader reader = (IndexReader) readers.elementAt(r);\n        int maxDoc = reader.maxDoc();\n        for (int docNum = 0; docNum < maxDoc; docNum++) {\n          // skip deleted docs\n          if (reader.isDeleted(docNum)) \n            continue;\n          termVectorsWriter.addAllDocVectors(reader.getTermFreqVectors(docNum));\n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      for (int r = 0; r < readers.size(); r++) {\n        IndexReader reader = (IndexReader) readers.elementAt(r);\n        int maxDoc = reader.maxDoc();\n        for (int docNum = 0; docNum < maxDoc; docNum++) {\n          // skip deleted docs\n          if (reader.isDeleted(docNum)) {\n            continue;\n          }\n          termVectorsWriter.openDocument();\n\n          // get all term vectors\n          TermFreqVector[] sourceTermVector =\n            reader.getTermFreqVectors(docNum);\n\n          if (sourceTermVector != null) {\n            for (int f = 0; f < sourceTermVector.length; f++) {\n              // translate field numbers\n              TermFreqVector termVector = sourceTermVector[f];\n              termVectorsWriter.openField(termVector.getField());\n              String [] terms = termVector.getTerms();\n              int [] freqs = termVector.getTermFrequencies();\n              \n              for (int t = 0; t < terms.length; t++) {\n                termVectorsWriter.addTerm(terms[t], freqs[t]);\n              }\n            }\n            termVectorsWriter.closeDocument();\n          }\n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"346d5897e4c4e77ed5dbd31f7730ff30973d5971","date":1198317988,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","sourceNew":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      for (int r = 0; r < readers.size(); r++) {\n        IndexReader reader = (IndexReader) readers.elementAt(r);\n        int maxDoc = reader.maxDoc();\n        for (int docNum = 0; docNum < maxDoc; docNum++) {\n          // skip deleted docs\n          if (reader.isDeleted(docNum)) \n            continue;\n          termVectorsWriter.addAllDocVectors(reader.getTermFreqVectors(docNum));\n          if (checkAbort != null)\n            checkAbort.work(300);\n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      for (int r = 0; r < readers.size(); r++) {\n        IndexReader reader = (IndexReader) readers.elementAt(r);\n        int maxDoc = reader.maxDoc();\n        for (int docNum = 0; docNum < maxDoc; docNum++) {\n          // skip deleted docs\n          if (reader.isDeleted(docNum)) \n            continue;\n          termVectorsWriter.addAllDocVectors(reader.getTermFreqVectors(docNum));\n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3d08461c77d39c25ea6ff0cd05b32f948fa2a33","date":1201260752,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","sourceNew":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      for (int r = 0; r < readers.size(); r++) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[r];\n        TermVectorsReader matchingVectorsReader;\n        if (matchingSegmentReader != null) {\n          matchingVectorsReader = matchingSegmentReader.termVectorsReaderOrig;\n\n          // If the TV* files are an older format then they\n          // cannot read raw docs:\n          if (matchingVectorsReader != null && !matchingVectorsReader.canReadRawDocs())\n            matchingVectorsReader = null;\n        } else\n          matchingVectorsReader = null;\n        IndexReader reader = (IndexReader) readers.elementAt(r);\n        int maxDoc = reader.maxDoc();\n        for (int docNum = 0; docNum < maxDoc;) {\n          // skip deleted docs\n          if (!reader.isDeleted(docNum)) {\n            if (matchingVectorsReader != null) {\n              // We can optimize this case (doing a bulk\n              // byte copy) since the field numbers are\n              // identical\n              int start = docNum;\n              int numDocs = 0;\n              do {\n                docNum++;\n                numDocs++;\n              } while(docNum < maxDoc && !matchingSegmentReader.isDeleted(docNum) && numDocs < MAX_RAW_MERGE_DOCS);\n\n              matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);\n              termVectorsWriter.addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);\n              if (checkAbort != null)\n                checkAbort.work(300*numDocs);\n            } else {\n              termVectorsWriter.addAllDocVectors(reader.getTermFreqVectors(docNum));\n              docNum++;\n              if (checkAbort != null)\n                checkAbort.work(300);\n            }\n          } else\n            docNum++;\n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      for (int r = 0; r < readers.size(); r++) {\n        IndexReader reader = (IndexReader) readers.elementAt(r);\n        int maxDoc = reader.maxDoc();\n        for (int docNum = 0; docNum < maxDoc; docNum++) {\n          // skip deleted docs\n          if (reader.isDeleted(docNum)) \n            continue;\n          termVectorsWriter.addAllDocVectors(reader.getTermFreqVectors(docNum));\n          if (checkAbort != null)\n            checkAbort.work(300);\n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"da3e8c2fef4ea558379c4c0879b3bcdecde41bcd","date":1206037293,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","sourceNew":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      for (int r = 0; r < readers.size(); r++) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[r];\n        TermVectorsReader matchingVectorsReader;\n        if (matchingSegmentReader != null) {\n          matchingVectorsReader = matchingSegmentReader.termVectorsReaderOrig;\n\n          // If the TV* files are an older format then they\n          // cannot read raw docs:\n          if (matchingVectorsReader != null && !matchingVectorsReader.canReadRawDocs())\n            matchingVectorsReader = null;\n        } else\n          matchingVectorsReader = null;\n        IndexReader reader = (IndexReader) readers.elementAt(r);\n        int maxDoc = reader.maxDoc();\n        for (int docNum = 0; docNum < maxDoc;) {\n          // skip deleted docs\n          if (!reader.isDeleted(docNum)) {\n            if (matchingVectorsReader != null) {\n              // We can optimize this case (doing a bulk\n              // byte copy) since the field numbers are\n              // identical\n              int start = docNum;\n              int numDocs = 0;\n              do {\n                docNum++;\n                numDocs++;\n              } while(docNum < maxDoc && !matchingSegmentReader.isDeleted(docNum) && numDocs < MAX_RAW_MERGE_DOCS);\n\n              matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);\n              termVectorsWriter.addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);\n              if (checkAbort != null)\n                checkAbort.work(300*numDocs);\n            } else {\n              termVectorsWriter.addAllDocVectors(reader.getTermFreqVectors(docNum));\n              docNum++;\n              if (checkAbort != null)\n                checkAbort.work(300);\n            }\n          } else\n            docNum++;\n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n\n    assert 4+mergedDocs*16 == directory.fileLength(segment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION) :\n      \"after mergeVectors: tvx size mismatch: \" + mergedDocs + \" docs vs \" + directory.fileLength(segment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION) + \" length in bytes of \" + segment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION;\n  }\n\n","sourceOld":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      for (int r = 0; r < readers.size(); r++) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[r];\n        TermVectorsReader matchingVectorsReader;\n        if (matchingSegmentReader != null) {\n          matchingVectorsReader = matchingSegmentReader.termVectorsReaderOrig;\n\n          // If the TV* files are an older format then they\n          // cannot read raw docs:\n          if (matchingVectorsReader != null && !matchingVectorsReader.canReadRawDocs())\n            matchingVectorsReader = null;\n        } else\n          matchingVectorsReader = null;\n        IndexReader reader = (IndexReader) readers.elementAt(r);\n        int maxDoc = reader.maxDoc();\n        for (int docNum = 0; docNum < maxDoc;) {\n          // skip deleted docs\n          if (!reader.isDeleted(docNum)) {\n            if (matchingVectorsReader != null) {\n              // We can optimize this case (doing a bulk\n              // byte copy) since the field numbers are\n              // identical\n              int start = docNum;\n              int numDocs = 0;\n              do {\n                docNum++;\n                numDocs++;\n              } while(docNum < maxDoc && !matchingSegmentReader.isDeleted(docNum) && numDocs < MAX_RAW_MERGE_DOCS);\n\n              matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);\n              termVectorsWriter.addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);\n              if (checkAbort != null)\n                checkAbort.work(300*numDocs);\n            } else {\n              termVectorsWriter.addAllDocVectors(reader.getTermFreqVectors(docNum));\n              docNum++;\n              if (checkAbort != null)\n                checkAbort.work(300);\n            }\n          } else\n            docNum++;\n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"84acdfa12c18361ff932244db20470fce117e52d","date":1206384355,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","sourceNew":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      for (int r = 0; r < readers.size(); r++) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[r];\n        TermVectorsReader matchingVectorsReader;\n        final boolean hasMatchingReader;\n        if (matchingSegmentReader != null) {\n          matchingVectorsReader = matchingSegmentReader.termVectorsReaderOrig;\n\n          // If the TV* files are an older format then they\n          // cannot read raw docs:\n          if (matchingVectorsReader != null && !matchingVectorsReader.canReadRawDocs()) {\n            matchingVectorsReader = null;\n            hasMatchingReader = false;\n          } else\n            hasMatchingReader = matchingVectorsReader != null;\n\n        } else {\n          hasMatchingReader = false;\n          matchingVectorsReader = null;\n        }\n        IndexReader reader = (IndexReader) readers.elementAt(r);\n        final boolean hasDeletions = reader.hasDeletions();\n        int maxDoc = reader.maxDoc();\n        for (int docNum = 0; docNum < maxDoc;) {\n          // skip deleted docs\n          if (!hasDeletions || !reader.isDeleted(docNum)) {\n            if (hasMatchingReader) {\n              // We can optimize this case (doing a bulk\n              // byte copy) since the field numbers are\n              // identical\n              int start = docNum;\n              int numDocs = 0;\n              do {\n                docNum++;\n                numDocs++;\n                if (docNum >= maxDoc)\n                  break;\n                if (hasDeletions && matchingSegmentReader.isDeleted(docNum)) {\n                  docNum++;\n                  break;\n                }\n              } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n              matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);\n              termVectorsWriter.addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);\n              if (checkAbort != null)\n                checkAbort.work(300*numDocs);\n            } else {\n              termVectorsWriter.addAllDocVectors(reader.getTermFreqVectors(docNum));\n              docNum++;\n              if (checkAbort != null)\n                checkAbort.work(300);\n            }\n          } else\n            docNum++;\n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n\n    assert 4+mergedDocs*16 == directory.fileLength(segment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION) :\n      \"after mergeVectors: tvx size mismatch: \" + mergedDocs + \" docs vs \" + directory.fileLength(segment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION) + \" length in bytes of \" + segment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION;\n  }\n\n","sourceOld":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      for (int r = 0; r < readers.size(); r++) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[r];\n        TermVectorsReader matchingVectorsReader;\n        if (matchingSegmentReader != null) {\n          matchingVectorsReader = matchingSegmentReader.termVectorsReaderOrig;\n\n          // If the TV* files are an older format then they\n          // cannot read raw docs:\n          if (matchingVectorsReader != null && !matchingVectorsReader.canReadRawDocs())\n            matchingVectorsReader = null;\n        } else\n          matchingVectorsReader = null;\n        IndexReader reader = (IndexReader) readers.elementAt(r);\n        int maxDoc = reader.maxDoc();\n        for (int docNum = 0; docNum < maxDoc;) {\n          // skip deleted docs\n          if (!reader.isDeleted(docNum)) {\n            if (matchingVectorsReader != null) {\n              // We can optimize this case (doing a bulk\n              // byte copy) since the field numbers are\n              // identical\n              int start = docNum;\n              int numDocs = 0;\n              do {\n                docNum++;\n                numDocs++;\n              } while(docNum < maxDoc && !matchingSegmentReader.isDeleted(docNum) && numDocs < MAX_RAW_MERGE_DOCS);\n\n              matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);\n              termVectorsWriter.addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);\n              if (checkAbort != null)\n                checkAbort.work(300*numDocs);\n            } else {\n              termVectorsWriter.addAllDocVectors(reader.getTermFreqVectors(docNum));\n              docNum++;\n              if (checkAbort != null)\n                checkAbort.work(300);\n            }\n          } else\n            docNum++;\n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n\n    assert 4+mergedDocs*16 == directory.fileLength(segment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION) :\n      \"after mergeVectors: tvx size mismatch: \" + mergedDocs + \" docs vs \" + directory.fileLength(segment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION) + \" length in bytes of \" + segment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4630870c8c0e172c76b902376619806cc2ec5e38","date":1211408056,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","sourceNew":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      for (int r = 0; r < readers.size(); r++) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[r];\n        TermVectorsReader matchingVectorsReader;\n        final boolean hasMatchingReader;\n        if (matchingSegmentReader != null) {\n          matchingVectorsReader = matchingSegmentReader.termVectorsReaderOrig;\n\n          // If the TV* files are an older format then they\n          // cannot read raw docs:\n          if (matchingVectorsReader != null && !matchingVectorsReader.canReadRawDocs()) {\n            matchingVectorsReader = null;\n            hasMatchingReader = false;\n          } else\n            hasMatchingReader = matchingVectorsReader != null;\n\n        } else {\n          hasMatchingReader = false;\n          matchingVectorsReader = null;\n        }\n        IndexReader reader = (IndexReader) readers.elementAt(r);\n        final boolean hasDeletions = reader.hasDeletions();\n        int maxDoc = reader.maxDoc();\n        for (int docNum = 0; docNum < maxDoc;) {\n          // skip deleted docs\n          if (!hasDeletions || !reader.isDeleted(docNum)) {\n            if (hasMatchingReader) {\n              // We can optimize this case (doing a bulk\n              // byte copy) since the field numbers are\n              // identical\n              int start = docNum;\n              int numDocs = 0;\n              do {\n                docNum++;\n                numDocs++;\n                if (docNum >= maxDoc)\n                  break;\n                if (hasDeletions && matchingSegmentReader.isDeleted(docNum)) {\n                  docNum++;\n                  break;\n                }\n              } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n              matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);\n              termVectorsWriter.addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);\n              if (checkAbort != null)\n                checkAbort.work(300*numDocs);\n            } else {\n              // NOTE: it's very important to first assign\n              // to vectors then pass it to\n              // termVectorsWriter.addAllDocVectors; see\n              // LUCENE-1282\n              TermFreqVector[] vectors = reader.getTermFreqVectors(docNum);\n              termVectorsWriter.addAllDocVectors(vectors);\n              docNum++;\n              if (checkAbort != null)\n                checkAbort.work(300);\n            }\n          } else\n            docNum++;\n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n\n    final long tvxSize = directory.fileLength(segment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n\n    if (4+mergedDocs*16 != tvxSize)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeVectors produced an invalid result: mergedDocs is \" + mergedDocs + \" but tvx size is \" + tvxSize + \"; now aborting this merge to prevent index corruption\");\n  }\n\n","sourceOld":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      for (int r = 0; r < readers.size(); r++) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[r];\n        TermVectorsReader matchingVectorsReader;\n        final boolean hasMatchingReader;\n        if (matchingSegmentReader != null) {\n          matchingVectorsReader = matchingSegmentReader.termVectorsReaderOrig;\n\n          // If the TV* files are an older format then they\n          // cannot read raw docs:\n          if (matchingVectorsReader != null && !matchingVectorsReader.canReadRawDocs()) {\n            matchingVectorsReader = null;\n            hasMatchingReader = false;\n          } else\n            hasMatchingReader = matchingVectorsReader != null;\n\n        } else {\n          hasMatchingReader = false;\n          matchingVectorsReader = null;\n        }\n        IndexReader reader = (IndexReader) readers.elementAt(r);\n        final boolean hasDeletions = reader.hasDeletions();\n        int maxDoc = reader.maxDoc();\n        for (int docNum = 0; docNum < maxDoc;) {\n          // skip deleted docs\n          if (!hasDeletions || !reader.isDeleted(docNum)) {\n            if (hasMatchingReader) {\n              // We can optimize this case (doing a bulk\n              // byte copy) since the field numbers are\n              // identical\n              int start = docNum;\n              int numDocs = 0;\n              do {\n                docNum++;\n                numDocs++;\n                if (docNum >= maxDoc)\n                  break;\n                if (hasDeletions && matchingSegmentReader.isDeleted(docNum)) {\n                  docNum++;\n                  break;\n                }\n              } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n              matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);\n              termVectorsWriter.addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);\n              if (checkAbort != null)\n                checkAbort.work(300*numDocs);\n            } else {\n              termVectorsWriter.addAllDocVectors(reader.getTermFreqVectors(docNum));\n              docNum++;\n              if (checkAbort != null)\n                checkAbort.work(300);\n            }\n          } else\n            docNum++;\n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n\n    assert 4+mergedDocs*16 == directory.fileLength(segment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION) :\n      \"after mergeVectors: tvx size mismatch: \" + mergedDocs + \" docs vs \" + directory.fileLength(segment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION) + \" length in bytes of \" + segment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION;\n  }\n\n","bugFix":null,"bugIntro":["a3a7f858bb80cdcedf1e39222961d53f11e1f6ed"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c7b6cdc70e097da94da79a655ed8f94477ff69f5","date":1220815360,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","sourceNew":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      for (int r = 0; r < readers.size(); r++) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[r];\n        TermVectorsReader matchingVectorsReader;\n        final boolean hasMatchingReader;\n        if (matchingSegmentReader != null) {\n          matchingVectorsReader = matchingSegmentReader.termVectorsReaderOrig;\n\n          // If the TV* files are an older format then they\n          // cannot read raw docs:\n          if (matchingVectorsReader != null && !matchingVectorsReader.canReadRawDocs()) {\n            matchingVectorsReader = null;\n            hasMatchingReader = false;\n          } else\n            hasMatchingReader = matchingVectorsReader != null;\n\n        } else {\n          hasMatchingReader = false;\n          matchingVectorsReader = null;\n        }\n        IndexReader reader = (IndexReader) readers.get(r);\n        final boolean hasDeletions = reader.hasDeletions();\n        int maxDoc = reader.maxDoc();\n        for (int docNum = 0; docNum < maxDoc;) {\n          // skip deleted docs\n          if (!hasDeletions || !reader.isDeleted(docNum)) {\n            if (hasMatchingReader) {\n              // We can optimize this case (doing a bulk\n              // byte copy) since the field numbers are\n              // identical\n              int start = docNum;\n              int numDocs = 0;\n              do {\n                docNum++;\n                numDocs++;\n                if (docNum >= maxDoc)\n                  break;\n                if (hasDeletions && matchingSegmentReader.isDeleted(docNum)) {\n                  docNum++;\n                  break;\n                }\n              } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n              matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);\n              termVectorsWriter.addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);\n              if (checkAbort != null)\n                checkAbort.work(300*numDocs);\n            } else {\n              // NOTE: it's very important to first assign\n              // to vectors then pass it to\n              // termVectorsWriter.addAllDocVectors; see\n              // LUCENE-1282\n              TermFreqVector[] vectors = reader.getTermFreqVectors(docNum);\n              termVectorsWriter.addAllDocVectors(vectors);\n              docNum++;\n              if (checkAbort != null)\n                checkAbort.work(300);\n            }\n          } else\n            docNum++;\n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n\n    final long tvxSize = directory.fileLength(segment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n\n    if (4+mergedDocs*16 != tvxSize)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeVectors produced an invalid result: mergedDocs is \" + mergedDocs + \" but tvx size is \" + tvxSize + \"; now aborting this merge to prevent index corruption\");\n  }\n\n","sourceOld":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      for (int r = 0; r < readers.size(); r++) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[r];\n        TermVectorsReader matchingVectorsReader;\n        final boolean hasMatchingReader;\n        if (matchingSegmentReader != null) {\n          matchingVectorsReader = matchingSegmentReader.termVectorsReaderOrig;\n\n          // If the TV* files are an older format then they\n          // cannot read raw docs:\n          if (matchingVectorsReader != null && !matchingVectorsReader.canReadRawDocs()) {\n            matchingVectorsReader = null;\n            hasMatchingReader = false;\n          } else\n            hasMatchingReader = matchingVectorsReader != null;\n\n        } else {\n          hasMatchingReader = false;\n          matchingVectorsReader = null;\n        }\n        IndexReader reader = (IndexReader) readers.elementAt(r);\n        final boolean hasDeletions = reader.hasDeletions();\n        int maxDoc = reader.maxDoc();\n        for (int docNum = 0; docNum < maxDoc;) {\n          // skip deleted docs\n          if (!hasDeletions || !reader.isDeleted(docNum)) {\n            if (hasMatchingReader) {\n              // We can optimize this case (doing a bulk\n              // byte copy) since the field numbers are\n              // identical\n              int start = docNum;\n              int numDocs = 0;\n              do {\n                docNum++;\n                numDocs++;\n                if (docNum >= maxDoc)\n                  break;\n                if (hasDeletions && matchingSegmentReader.isDeleted(docNum)) {\n                  docNum++;\n                  break;\n                }\n              } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n              matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);\n              termVectorsWriter.addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);\n              if (checkAbort != null)\n                checkAbort.work(300*numDocs);\n            } else {\n              // NOTE: it's very important to first assign\n              // to vectors then pass it to\n              // termVectorsWriter.addAllDocVectors; see\n              // LUCENE-1282\n              TermFreqVector[] vectors = reader.getTermFreqVectors(docNum);\n              termVectorsWriter.addAllDocVectors(vectors);\n              docNum++;\n              if (checkAbort != null)\n                checkAbort.work(300);\n            }\n          } else\n            docNum++;\n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n\n    final long tvxSize = directory.fileLength(segment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n\n    if (4+mergedDocs*16 != tvxSize)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeVectors produced an invalid result: mergedDocs is \" + mergedDocs + \" but tvx size is \" + tvxSize + \"; now aborting this merge to prevent index corruption\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a3a7f858bb80cdcedf1e39222961d53f11e1f6ed","date":1231945797,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","sourceNew":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      for (int r = 0; r < readers.size(); r++) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[r];\n        TermVectorsReader matchingVectorsReader;\n        final boolean hasMatchingReader;\n        if (matchingSegmentReader != null) {\n          matchingVectorsReader = matchingSegmentReader.termVectorsReaderOrig;\n\n          // If the TV* files are an older format then they\n          // cannot read raw docs:\n          if (matchingVectorsReader != null && !matchingVectorsReader.canReadRawDocs()) {\n            matchingVectorsReader = null;\n            hasMatchingReader = false;\n          } else\n            hasMatchingReader = matchingVectorsReader != null;\n\n        } else {\n          hasMatchingReader = false;\n          matchingVectorsReader = null;\n        }\n        IndexReader reader = (IndexReader) readers.get(r);\n        final boolean hasDeletions = reader.hasDeletions();\n        int maxDoc = reader.maxDoc();\n        for (int docNum = 0; docNum < maxDoc;) {\n          // skip deleted docs\n          if (!hasDeletions || !reader.isDeleted(docNum)) {\n            if (hasMatchingReader) {\n              // We can optimize this case (doing a bulk\n              // byte copy) since the field numbers are\n              // identical\n              int start = docNum;\n              int numDocs = 0;\n              do {\n                docNum++;\n                numDocs++;\n                if (docNum >= maxDoc)\n                  break;\n                if (hasDeletions && matchingSegmentReader.isDeleted(docNum)) {\n                  docNum++;\n                  break;\n                }\n              } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n              matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);\n              termVectorsWriter.addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);\n              if (checkAbort != null)\n                checkAbort.work(300*numDocs);\n            } else {\n              // NOTE: it's very important to first assign\n              // to vectors then pass it to\n              // termVectorsWriter.addAllDocVectors; see\n              // LUCENE-1282\n              TermFreqVector[] vectors = reader.getTermFreqVectors(docNum);\n              termVectorsWriter.addAllDocVectors(vectors);\n              docNum++;\n              if (checkAbort != null)\n                checkAbort.work(300);\n            }\n          } else\n            docNum++;\n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n\n    final long tvxSize = directory.fileLength(segment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n\n    if (4+((long) mergedDocs)*16 != tvxSize)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeVectors produced an invalid result: mergedDocs is \" + mergedDocs + \" but tvx size is \" + tvxSize + \"; now aborting this merge to prevent index corruption\");\n  }\n\n","sourceOld":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      for (int r = 0; r < readers.size(); r++) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[r];\n        TermVectorsReader matchingVectorsReader;\n        final boolean hasMatchingReader;\n        if (matchingSegmentReader != null) {\n          matchingVectorsReader = matchingSegmentReader.termVectorsReaderOrig;\n\n          // If the TV* files are an older format then they\n          // cannot read raw docs:\n          if (matchingVectorsReader != null && !matchingVectorsReader.canReadRawDocs()) {\n            matchingVectorsReader = null;\n            hasMatchingReader = false;\n          } else\n            hasMatchingReader = matchingVectorsReader != null;\n\n        } else {\n          hasMatchingReader = false;\n          matchingVectorsReader = null;\n        }\n        IndexReader reader = (IndexReader) readers.get(r);\n        final boolean hasDeletions = reader.hasDeletions();\n        int maxDoc = reader.maxDoc();\n        for (int docNum = 0; docNum < maxDoc;) {\n          // skip deleted docs\n          if (!hasDeletions || !reader.isDeleted(docNum)) {\n            if (hasMatchingReader) {\n              // We can optimize this case (doing a bulk\n              // byte copy) since the field numbers are\n              // identical\n              int start = docNum;\n              int numDocs = 0;\n              do {\n                docNum++;\n                numDocs++;\n                if (docNum >= maxDoc)\n                  break;\n                if (hasDeletions && matchingSegmentReader.isDeleted(docNum)) {\n                  docNum++;\n                  break;\n                }\n              } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n              matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);\n              termVectorsWriter.addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);\n              if (checkAbort != null)\n                checkAbort.work(300*numDocs);\n            } else {\n              // NOTE: it's very important to first assign\n              // to vectors then pass it to\n              // termVectorsWriter.addAllDocVectors; see\n              // LUCENE-1282\n              TermFreqVector[] vectors = reader.getTermFreqVectors(docNum);\n              termVectorsWriter.addAllDocVectors(vectors);\n              docNum++;\n              if (checkAbort != null)\n                checkAbort.work(300);\n            }\n          } else\n            docNum++;\n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n\n    final long tvxSize = directory.fileLength(segment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n\n    if (4+mergedDocs*16 != tvxSize)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeVectors produced an invalid result: mergedDocs is \" + mergedDocs + \" but tvx size is \" + tvxSize + \"; now aborting this merge to prevent index corruption\");\n  }\n\n","bugFix":["4630870c8c0e172c76b902376619806cc2ec5e38"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d736930237c54e1516a9e3bae803c92ff19ec4e5","date":1245789156,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","sourceNew":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (Iterator iter = readers.iterator(); iter.hasNext();) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        TermVectorsReader matchingVectorsReader = null;\n        if (matchingSegmentReader != null) {\n          TermVectorsReader vectorsReader = matchingSegmentReader.termVectorsReaderOrig;\n\n          // If the TV* files are an older format then they cannot read raw docs:\n          if (vectorsReader != null && vectorsReader.canReadRawDocs()) {\n            matchingVectorsReader = vectorsReader;\n          }\n        }\n        final IndexReader reader = (IndexReader) iter.next();\n        if (reader.hasDeletions()) {\n          copyVectorsWithDeletions(termVectorsWriter, matchingVectorsReader, reader);\n        } else {\n          copyVectorsNoDeletions(termVectorsWriter, matchingVectorsReader, reader);\n          \n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n\n    final long tvxSize = directory.fileLength(segment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n\n    if (4+((long) mergedDocs)*16 != tvxSize)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeVectors produced an invalid result: mergedDocs is \" + mergedDocs + \" but tvx size is \" + tvxSize + \"; now aborting this merge to prevent index corruption\");\n  }\n\n","sourceOld":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      for (int r = 0; r < readers.size(); r++) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[r];\n        TermVectorsReader matchingVectorsReader;\n        final boolean hasMatchingReader;\n        if (matchingSegmentReader != null) {\n          matchingVectorsReader = matchingSegmentReader.termVectorsReaderOrig;\n\n          // If the TV* files are an older format then they\n          // cannot read raw docs:\n          if (matchingVectorsReader != null && !matchingVectorsReader.canReadRawDocs()) {\n            matchingVectorsReader = null;\n            hasMatchingReader = false;\n          } else\n            hasMatchingReader = matchingVectorsReader != null;\n\n        } else {\n          hasMatchingReader = false;\n          matchingVectorsReader = null;\n        }\n        IndexReader reader = (IndexReader) readers.get(r);\n        final boolean hasDeletions = reader.hasDeletions();\n        int maxDoc = reader.maxDoc();\n        for (int docNum = 0; docNum < maxDoc;) {\n          // skip deleted docs\n          if (!hasDeletions || !reader.isDeleted(docNum)) {\n            if (hasMatchingReader) {\n              // We can optimize this case (doing a bulk\n              // byte copy) since the field numbers are\n              // identical\n              int start = docNum;\n              int numDocs = 0;\n              do {\n                docNum++;\n                numDocs++;\n                if (docNum >= maxDoc)\n                  break;\n                if (hasDeletions && matchingSegmentReader.isDeleted(docNum)) {\n                  docNum++;\n                  break;\n                }\n              } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n              matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);\n              termVectorsWriter.addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);\n              if (checkAbort != null)\n                checkAbort.work(300*numDocs);\n            } else {\n              // NOTE: it's very important to first assign\n              // to vectors then pass it to\n              // termVectorsWriter.addAllDocVectors; see\n              // LUCENE-1282\n              TermFreqVector[] vectors = reader.getTermFreqVectors(docNum);\n              termVectorsWriter.addAllDocVectors(vectors);\n              docNum++;\n              if (checkAbort != null)\n                checkAbort.work(300);\n            }\n          } else\n            docNum++;\n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n\n    final long tvxSize = directory.fileLength(segment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n\n    if (4+((long) mergedDocs)*16 != tvxSize)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeVectors produced an invalid result: mergedDocs is \" + mergedDocs + \" but tvx size is \" + tvxSize + \"; now aborting this merge to prevent index corruption\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"66f3dadb253a44f4cccc81c8a21b685b18b201fb","date":1247245699,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","sourceNew":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (Iterator iter = readers.iterator(); iter.hasNext();) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        TermVectorsReader matchingVectorsReader = null;\n        if (matchingSegmentReader != null) {\n          TermVectorsReader vectorsReader = matchingSegmentReader.getTermVectorsReaderOrig();\n\n          // If the TV* files are an older format then they cannot read raw docs:\n          if (vectorsReader != null && vectorsReader.canReadRawDocs()) {\n            matchingVectorsReader = vectorsReader;\n          }\n        }\n        final IndexReader reader = (IndexReader) iter.next();\n        if (reader.hasDeletions()) {\n          copyVectorsWithDeletions(termVectorsWriter, matchingVectorsReader, reader);\n        } else {\n          copyVectorsNoDeletions(termVectorsWriter, matchingVectorsReader, reader);\n          \n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n\n    final long tvxSize = directory.fileLength(segment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n\n    if (4+((long) mergedDocs)*16 != tvxSize)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeVectors produced an invalid result: mergedDocs is \" + mergedDocs + \" but tvx size is \" + tvxSize + \"; now aborting this merge to prevent index corruption\");\n  }\n\n","sourceOld":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (Iterator iter = readers.iterator(); iter.hasNext();) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        TermVectorsReader matchingVectorsReader = null;\n        if (matchingSegmentReader != null) {\n          TermVectorsReader vectorsReader = matchingSegmentReader.termVectorsReaderOrig;\n\n          // If the TV* files are an older format then they cannot read raw docs:\n          if (vectorsReader != null && vectorsReader.canReadRawDocs()) {\n            matchingVectorsReader = vectorsReader;\n          }\n        }\n        final IndexReader reader = (IndexReader) iter.next();\n        if (reader.hasDeletions()) {\n          copyVectorsWithDeletions(termVectorsWriter, matchingVectorsReader, reader);\n        } else {\n          copyVectorsNoDeletions(termVectorsWriter, matchingVectorsReader, reader);\n          \n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n\n    final long tvxSize = directory.fileLength(segment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n\n    if (4+((long) mergedDocs)*16 != tvxSize)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeVectors produced an invalid result: mergedDocs is \" + mergedDocs + \" but tvx size is \" + tvxSize + \"; now aborting this merge to prevent index corruption\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"55ddd0022d1d26b52ed711e90ab77de2b36130c6","date":1251466592,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","sourceNew":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (Iterator iter = readers.iterator(); iter.hasNext();) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        TermVectorsReader matchingVectorsReader = null;\n        if (matchingSegmentReader != null) {\n          TermVectorsReader vectorsReader = matchingSegmentReader.getTermVectorsReaderOrig();\n\n          // If the TV* files are an older format then they cannot read raw docs:\n          if (vectorsReader != null && vectorsReader.canReadRawDocs()) {\n            matchingVectorsReader = vectorsReader;\n          }\n        }\n        final IndexReader reader = (IndexReader) iter.next();\n        if (reader.hasDeletions()) {\n          copyVectorsWithDeletions(termVectorsWriter, matchingVectorsReader, reader);\n        } else {\n          copyVectorsNoDeletions(termVectorsWriter, matchingVectorsReader, reader);\n          \n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n\n    final String fileName = segment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION;\n    final long tvxSize = directory.fileLength(fileName);\n\n    if (4+((long) mergedDocs)*16 != tvxSize)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeVectors produced an invalid result: mergedDocs is \" + mergedDocs + \" but tvx size is \" + tvxSize + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n  }\n\n","sourceOld":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (Iterator iter = readers.iterator(); iter.hasNext();) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        TermVectorsReader matchingVectorsReader = null;\n        if (matchingSegmentReader != null) {\n          TermVectorsReader vectorsReader = matchingSegmentReader.getTermVectorsReaderOrig();\n\n          // If the TV* files are an older format then they cannot read raw docs:\n          if (vectorsReader != null && vectorsReader.canReadRawDocs()) {\n            matchingVectorsReader = vectorsReader;\n          }\n        }\n        final IndexReader reader = (IndexReader) iter.next();\n        if (reader.hasDeletions()) {\n          copyVectorsWithDeletions(termVectorsWriter, matchingVectorsReader, reader);\n        } else {\n          copyVectorsNoDeletions(termVectorsWriter, matchingVectorsReader, reader);\n          \n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n\n    final long tvxSize = directory.fileLength(segment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION);\n\n    if (4+((long) mergedDocs)*16 != tvxSize)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeVectors produced an invalid result: mergedDocs is \" + mergedDocs + \" but tvx size is \" + tvxSize + \"; now aborting this merge to prevent index corruption\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fa27b750ee9a51ec4bed93ef328aef9ca1e2153d","date":1255859449,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","sourceNew":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (final IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        TermVectorsReader matchingVectorsReader = null;\n        if (matchingSegmentReader != null) {\n          TermVectorsReader vectorsReader = matchingSegmentReader.getTermVectorsReaderOrig();\n\n          // If the TV* files are an older format then they cannot read raw docs:\n          if (vectorsReader != null && vectorsReader.canReadRawDocs()) {\n            matchingVectorsReader = vectorsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          copyVectorsWithDeletions(termVectorsWriter, matchingVectorsReader, reader);\n        } else {\n          copyVectorsNoDeletions(termVectorsWriter, matchingVectorsReader, reader);\n          \n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n\n    final String fileName = segment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION;\n    final long tvxSize = directory.fileLength(fileName);\n\n    if (4+((long) mergedDocs)*16 != tvxSize)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeVectors produced an invalid result: mergedDocs is \" + mergedDocs + \" but tvx size is \" + tvxSize + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n  }\n\n","sourceOld":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (Iterator iter = readers.iterator(); iter.hasNext();) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        TermVectorsReader matchingVectorsReader = null;\n        if (matchingSegmentReader != null) {\n          TermVectorsReader vectorsReader = matchingSegmentReader.getTermVectorsReaderOrig();\n\n          // If the TV* files are an older format then they cannot read raw docs:\n          if (vectorsReader != null && vectorsReader.canReadRawDocs()) {\n            matchingVectorsReader = vectorsReader;\n          }\n        }\n        final IndexReader reader = (IndexReader) iter.next();\n        if (reader.hasDeletions()) {\n          copyVectorsWithDeletions(termVectorsWriter, matchingVectorsReader, reader);\n        } else {\n          copyVectorsNoDeletions(termVectorsWriter, matchingVectorsReader, reader);\n          \n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n\n    final String fileName = segment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION;\n    final long tvxSize = directory.fileLength(fileName);\n\n    if (4+((long) mergedDocs)*16 != tvxSize)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeVectors produced an invalid result: mergedDocs is \" + mergedDocs + \" but tvx size is \" + tvxSize + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"775efee7f959e0dd3df7960b93767d9e00b78751","date":1267203159,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","sourceNew":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (final IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        TermVectorsReader matchingVectorsReader = null;\n        if (matchingSegmentReader != null) {\n          TermVectorsReader vectorsReader = matchingSegmentReader.getTermVectorsReaderOrig();\n\n          // If the TV* files are an older format then they cannot read raw docs:\n          if (vectorsReader != null && vectorsReader.canReadRawDocs()) {\n            matchingVectorsReader = vectorsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          copyVectorsWithDeletions(termVectorsWriter, matchingVectorsReader, reader);\n        } else {\n          copyVectorsNoDeletions(termVectorsWriter, matchingVectorsReader, reader);\n          \n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, IndexFileNames.VECTORS_INDEX_EXTENSION);\n    final long tvxSize = directory.fileLength(fileName);\n\n    if (4+((long) mergedDocs)*16 != tvxSize)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeVectors produced an invalid result: mergedDocs is \" + mergedDocs + \" but tvx size is \" + tvxSize + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n  }\n\n","sourceOld":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (final IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        TermVectorsReader matchingVectorsReader = null;\n        if (matchingSegmentReader != null) {\n          TermVectorsReader vectorsReader = matchingSegmentReader.getTermVectorsReaderOrig();\n\n          // If the TV* files are an older format then they cannot read raw docs:\n          if (vectorsReader != null && vectorsReader.canReadRawDocs()) {\n            matchingVectorsReader = vectorsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          copyVectorsWithDeletions(termVectorsWriter, matchingVectorsReader, reader);\n        } else {\n          copyVectorsNoDeletions(termVectorsWriter, matchingVectorsReader, reader);\n          \n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n\n    final String fileName = segment + \".\" + IndexFileNames.VECTORS_INDEX_EXTENSION;\n    final long tvxSize = directory.fileLength(fileName);\n\n    if (4+((long) mergedDocs)*16 != tvxSize)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeVectors produced an invalid result: mergedDocs is \" + mergedDocs + \" but tvx size is \" + tvxSize + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeVectors().mjava","sourceNew":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (final IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        TermVectorsReader matchingVectorsReader = null;\n        if (matchingSegmentReader != null) {\n          TermVectorsReader vectorsReader = matchingSegmentReader.getTermVectorsReaderOrig();\n\n          // If the TV* files are an older format then they cannot read raw docs:\n          if (vectorsReader != null && vectorsReader.canReadRawDocs()) {\n            matchingVectorsReader = vectorsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          copyVectorsWithDeletions(termVectorsWriter, matchingVectorsReader, reader);\n        } else {\n          copyVectorsNoDeletions(termVectorsWriter, matchingVectorsReader, reader);\n          \n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, IndexFileNames.VECTORS_INDEX_EXTENSION);\n    final long tvxSize = directory.fileLength(fileName);\n\n    if (4+((long) mergedDocs)*16 != tvxSize)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeVectors produced an invalid result: mergedDocs is \" + mergedDocs + \" but tvx size is \" + tvxSize + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n  }\n\n","sourceOld":"  /**\n   * Merge the TermVectors from each of the segments into the new one.\n   * @throws IOException\n   */\n  private final void mergeVectors() throws IOException {\n    TermVectorsWriter termVectorsWriter = \n      new TermVectorsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (final IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        TermVectorsReader matchingVectorsReader = null;\n        if (matchingSegmentReader != null) {\n          TermVectorsReader vectorsReader = matchingSegmentReader.getTermVectorsReaderOrig();\n\n          // If the TV* files are an older format then they cannot read raw docs:\n          if (vectorsReader != null && vectorsReader.canReadRawDocs()) {\n            matchingVectorsReader = vectorsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          copyVectorsWithDeletions(termVectorsWriter, matchingVectorsReader, reader);\n        } else {\n          copyVectorsNoDeletions(termVectorsWriter, matchingVectorsReader, reader);\n          \n        }\n      }\n    } finally {\n      termVectorsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, IndexFileNames.VECTORS_INDEX_EXTENSION);\n    final long tvxSize = directory.fileLength(fileName);\n\n    if (4+((long) mergedDocs)*16 != tvxSize)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeVectors produced an invalid result: mergedDocs is \" + mergedDocs + \" but tvx size is \" + tvxSize + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"346d5897e4c4e77ed5dbd31f7730ff30973d5971":["6177f0f28ace66d1538b1e6ac5f1773e5449a0b0"],"b3d08461c77d39c25ea6ff0cd05b32f948fa2a33":["346d5897e4c4e77ed5dbd31f7730ff30973d5971"],"775efee7f959e0dd3df7960b93767d9e00b78751":["fa27b750ee9a51ec4bed93ef328aef9ca1e2153d"],"d736930237c54e1516a9e3bae803c92ff19ec4e5":["a3a7f858bb80cdcedf1e39222961d53f11e1f6ed"],"c7b6cdc70e097da94da79a655ed8f94477ff69f5":["4630870c8c0e172c76b902376619806cc2ec5e38"],"6177f0f28ace66d1538b1e6ac5f1773e5449a0b0":["770281b8a8459cafcdd2354b6a06078fea2d83c9"],"a3a7f858bb80cdcedf1e39222961d53f11e1f6ed":["c7b6cdc70e097da94da79a655ed8f94477ff69f5"],"84acdfa12c18361ff932244db20470fce117e52d":["da3e8c2fef4ea558379c4c0879b3bcdecde41bcd"],"66f3dadb253a44f4cccc81c8a21b685b18b201fb":["d736930237c54e1516a9e3bae803c92ff19ec4e5"],"770281b8a8459cafcdd2354b6a06078fea2d83c9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"fa27b750ee9a51ec4bed93ef328aef9ca1e2153d":["55ddd0022d1d26b52ed711e90ab77de2b36130c6"],"55ddd0022d1d26b52ed711e90ab77de2b36130c6":["66f3dadb253a44f4cccc81c8a21b685b18b201fb"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"da3e8c2fef4ea558379c4c0879b3bcdecde41bcd":["b3d08461c77d39c25ea6ff0cd05b32f948fa2a33"],"4630870c8c0e172c76b902376619806cc2ec5e38":["84acdfa12c18361ff932244db20470fce117e52d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["775efee7f959e0dd3df7960b93767d9e00b78751"]},"commit2Childs":{"346d5897e4c4e77ed5dbd31f7730ff30973d5971":["b3d08461c77d39c25ea6ff0cd05b32f948fa2a33"],"b3d08461c77d39c25ea6ff0cd05b32f948fa2a33":["da3e8c2fef4ea558379c4c0879b3bcdecde41bcd"],"775efee7f959e0dd3df7960b93767d9e00b78751":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"d736930237c54e1516a9e3bae803c92ff19ec4e5":["66f3dadb253a44f4cccc81c8a21b685b18b201fb"],"c7b6cdc70e097da94da79a655ed8f94477ff69f5":["a3a7f858bb80cdcedf1e39222961d53f11e1f6ed"],"6177f0f28ace66d1538b1e6ac5f1773e5449a0b0":["346d5897e4c4e77ed5dbd31f7730ff30973d5971"],"a3a7f858bb80cdcedf1e39222961d53f11e1f6ed":["d736930237c54e1516a9e3bae803c92ff19ec4e5"],"84acdfa12c18361ff932244db20470fce117e52d":["4630870c8c0e172c76b902376619806cc2ec5e38"],"66f3dadb253a44f4cccc81c8a21b685b18b201fb":["55ddd0022d1d26b52ed711e90ab77de2b36130c6"],"770281b8a8459cafcdd2354b6a06078fea2d83c9":["6177f0f28ace66d1538b1e6ac5f1773e5449a0b0"],"fa27b750ee9a51ec4bed93ef328aef9ca1e2153d":["775efee7f959e0dd3df7960b93767d9e00b78751"],"55ddd0022d1d26b52ed711e90ab77de2b36130c6":["fa27b750ee9a51ec4bed93ef328aef9ca1e2153d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["770281b8a8459cafcdd2354b6a06078fea2d83c9"],"da3e8c2fef4ea558379c4c0879b3bcdecde41bcd":["84acdfa12c18361ff932244db20470fce117e52d"],"4630870c8c0e172c76b902376619806cc2ec5e38":["c7b6cdc70e097da94da79a655ed8f94477ff69f5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}