{"path":"solr/core/src/test/org/apache/solr/cloud/TestTlogReplayVsRecovery#testManyDocsInTlogReplayWhileReplicaIsTryingToRecover().mjava","commits":[{"id":"df06aa21e6f41b678afd8f30568ebadd781be717","date":1578089543,"type":0,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestTlogReplayVsRecovery#testManyDocsInTlogReplayWhileReplicaIsTryingToRecover().mjava","pathOld":"/dev/null","sourceNew":"  public void testManyDocsInTlogReplayWhileReplicaIsTryingToRecover() throws Exception {\n    final int committedDocs = 3;\n    final int uncommittedDocs = 50;\n    \n    log.info(\"Create Collection...\");\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.createCollection(COLLECTION, 1, 2)\n                 .setCreateNodeSet(\"\")\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.addReplicaToShard(COLLECTION, \"shard1\")\n                 .setNode(NODE0.getNodeName())\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    \n    waitForState(\"Timeout waiting for shard leader\", COLLECTION, clusterShape(1, 1));\n\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.addReplicaToShard(COLLECTION, \"shard1\")\n                 .setNode(NODE1.getNodeName())\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    \n    cluster.waitForActiveCollection(COLLECTION, 1, 2);\n    \n    waitForState(\"Timeout waiting for 1x2 collection\", COLLECTION, clusterShape(1, 2));\n    \n    final Replica leader = getCollectionState(COLLECTION).getSlice(\"shard1\").getLeader();\n    assertEquals(\"Sanity check failed\", NODE0.getNodeName(), leader.getNodeName());\n\n    log.info(\"Add and commit a {} docs...\", committedDocs);\n    addDocs(true, committedDocs, 1);\n\n    log.info(\"Partition nodes...\");\n    proxies.get(NODE0).close();\n    proxies.get(NODE1).close();\n\n    log.info(\"Adding {} (uncommitted) docs during network partition....\", uncommittedDocs);\n    addDocs(false, uncommittedDocs, committedDocs + 1);\n\n    log.info(\"Stopping leader node...\");\n    assertEquals(\"Something broke our expected commitOnClose\", false, DirectUpdateHandler2.commitOnClose);\n    NODE0.stop();\n    cluster.waitForJettyToStop(NODE0);\n\n    log.info(\"Un-Partition replica (NODE1)...\");\n    proxies.get(NODE1).reopen();\n    \n    waitForState(\"Timeout waiting for leader goes DOWN\", COLLECTION, (liveNodes, collectionState)\n                 -> collectionState.getReplica(leader.getName()).getState() == Replica.State.DOWN);\n\n    TimeOut timeOut = new TimeOut(10, TimeUnit.SECONDS, TimeSource.NANO_TIME);\n    while (!timeOut.hasTimedOut()) {\n      Replica newLeader = getCollectionState(COLLECTION).getLeader(\"shard1\");\n      if (newLeader != null && !newLeader.getName().equals(leader.getName()) && newLeader.getState() == Replica.State.ACTIVE) {\n        fail(\"Out of sync replica became leader \" + newLeader);\n      }\n    }\n\n    log.info(\"Enabling TestInjection.updateLogReplayRandomPause\");\n    TestInjection.updateLogReplayRandomPause = \"true:100\";\n      \n    log.info(\"Un-Partition & restart leader (NODE0)...\");\n    proxies.get(NODE0).reopen();\n    NODE0.start();\n\n    log.info(\"Waiting for all nodes and active collection...\");\n    \n    cluster.waitForAllNodes(30);;\n    waitForState(\"Timeout waiting for leader\", COLLECTION, (liveNodes, collectionState) -> {\n      Replica newLeader = collectionState.getLeader(\"shard1\");\n      return newLeader != null && newLeader.getName().equals(leader.getName());\n    });\n    waitForState(\"Timeout waiting for active collection\", COLLECTION, clusterShape(1, 2));\n    \n    cluster.waitForActiveCollection(COLLECTION, 1, 2);\n\n    log.info(\"Check docs on both replicas...\");\n    assertDocsExistInBothReplicas(1, uncommittedDocs + uncommittedDocs);\n    \n    log.info(\"Test ok, delete collection...\");\n    CollectionAdminRequest.deleteCollection(COLLECTION).process(cluster.getSolrClient());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ca62564055241632cd20d65b5ecb8c8e93bd60c4","date":1578383112,"type":0,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestTlogReplayVsRecovery#testManyDocsInTlogReplayWhileReplicaIsTryingToRecover().mjava","pathOld":"/dev/null","sourceNew":"  public void testManyDocsInTlogReplayWhileReplicaIsTryingToRecover() throws Exception {\n    final int committedDocs = 3;\n    final int uncommittedDocs = 50;\n    \n    log.info(\"Create Collection...\");\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.createCollection(COLLECTION, 1, 2)\n                 .setCreateNodeSet(\"\")\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.addReplicaToShard(COLLECTION, \"shard1\")\n                 .setNode(NODE0.getNodeName())\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    \n    waitForState(\"Timeout waiting for shard leader\", COLLECTION, clusterShape(1, 1));\n\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.addReplicaToShard(COLLECTION, \"shard1\")\n                 .setNode(NODE1.getNodeName())\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    \n    cluster.waitForActiveCollection(COLLECTION, 1, 2);\n    \n    waitForState(\"Timeout waiting for 1x2 collection\", COLLECTION, clusterShape(1, 2));\n    \n    final Replica leader = getCollectionState(COLLECTION).getSlice(\"shard1\").getLeader();\n    assertEquals(\"Sanity check failed\", NODE0.getNodeName(), leader.getNodeName());\n\n    log.info(\"Add and commit a {} docs...\", committedDocs);\n    addDocs(true, committedDocs, 1);\n\n    log.info(\"Partition nodes...\");\n    proxies.get(NODE0).close();\n    proxies.get(NODE1).close();\n\n    log.info(\"Adding {} (uncommitted) docs during network partition....\", uncommittedDocs);\n    addDocs(false, uncommittedDocs, committedDocs + 1);\n\n    log.info(\"Stopping leader node...\");\n    assertEquals(\"Something broke our expected commitOnClose\", false, DirectUpdateHandler2.commitOnClose);\n    NODE0.stop();\n    cluster.waitForJettyToStop(NODE0);\n\n    log.info(\"Un-Partition replica (NODE1)...\");\n    proxies.get(NODE1).reopen();\n    \n    waitForState(\"Timeout waiting for leader goes DOWN\", COLLECTION, (liveNodes, collectionState)\n                 -> collectionState.getReplica(leader.getName()).getState() == Replica.State.DOWN);\n\n    TimeOut timeOut = new TimeOut(10, TimeUnit.SECONDS, TimeSource.NANO_TIME);\n    while (!timeOut.hasTimedOut()) {\n      Replica newLeader = getCollectionState(COLLECTION).getLeader(\"shard1\");\n      if (newLeader != null && !newLeader.getName().equals(leader.getName()) && newLeader.getState() == Replica.State.ACTIVE) {\n        fail(\"Out of sync replica became leader \" + newLeader);\n      }\n    }\n\n    log.info(\"Enabling TestInjection.updateLogReplayRandomPause\");\n    TestInjection.updateLogReplayRandomPause = \"true:100\";\n      \n    log.info(\"Un-Partition & restart leader (NODE0)...\");\n    proxies.get(NODE0).reopen();\n    NODE0.start();\n\n    log.info(\"Waiting for all nodes and active collection...\");\n    \n    cluster.waitForAllNodes(30);;\n    waitForState(\"Timeout waiting for leader\", COLLECTION, (liveNodes, collectionState) -> {\n      Replica newLeader = collectionState.getLeader(\"shard1\");\n      return newLeader != null && newLeader.getName().equals(leader.getName());\n    });\n    waitForState(\"Timeout waiting for active collection\", COLLECTION, clusterShape(1, 2));\n    \n    cluster.waitForActiveCollection(COLLECTION, 1, 2);\n\n    log.info(\"Check docs on both replicas...\");\n    assertDocsExistInBothReplicas(1, uncommittedDocs + uncommittedDocs);\n    \n    log.info(\"Test ok, delete collection...\");\n    CollectionAdminRequest.deleteCollection(COLLECTION).process(cluster.getSolrClient());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e18458cbca975852db0911f1a7f9a0a2fcd493f1","date":1578786485,"type":3,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestTlogReplayVsRecovery#testManyDocsInTlogReplayWhileReplicaIsTryingToRecover().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestTlogReplayVsRecovery#testManyDocsInTlogReplayWhileReplicaIsTryingToRecover().mjava","sourceNew":"  public void testManyDocsInTlogReplayWhileReplicaIsTryingToRecover() throws Exception {\n    // TODO: One the basic problem in SOLR-13486 is fixed, this test can be made more robust by:\n    // 1) randomizing the number of committedDocs (pre net split) & uncommittedDocs (post net split)\n    //    to trigger diff recovery strategies & shutdown behavior\n    // 2) replace \"committedDocs + uncommittedDocs\" with 4 variables:\n    //    a: docs committed before network split (add + commit)\n    //    b: docs not committed before network split (add w/o commit)\n    //    c: docs committed after network split (add + commit)\n    //    d: docs not committed after network split (add w/o commit)\n    final int committedDocs = 3;\n    final int uncommittedDocs = 50;\n    \n    log.info(\"Create Collection...\");\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.createCollection(COLLECTION, 1, 2)\n                 .setCreateNodeSet(\"\")\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.addReplicaToShard(COLLECTION, \"shard1\")\n                 .setNode(NODE0.getNodeName())\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    \n    waitForState(\"Timeout waiting for shard leader\", COLLECTION, clusterShape(1, 1));\n\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.addReplicaToShard(COLLECTION, \"shard1\")\n                 .setNode(NODE1.getNodeName())\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    \n    cluster.waitForActiveCollection(COLLECTION, 1, 2);\n    \n    waitForState(\"Timeout waiting for 1x2 collection\", COLLECTION, clusterShape(1, 2));\n    \n    final Replica leader = getCollectionState(COLLECTION).getSlice(\"shard1\").getLeader();\n    assertEquals(\"Sanity check failed\", NODE0.getNodeName(), leader.getNodeName());\n\n    log.info(\"Add and commit {} docs...\", committedDocs);\n    addDocs(true, committedDocs, 1);\n    assertDocsExistInBothReplicas(1, committedDocs);\n\n    log.info(\"Partition nodes...\");\n    proxies.get(NODE0).close();\n    proxies.get(NODE1).close();\n\n    log.info(\"Adding {} (uncommitted) docs during network partition....\", uncommittedDocs);\n    addDocs(false, uncommittedDocs, committedDocs + 1);\n\n    log.info(\"Stopping leader node...\");\n    assertEquals(\"Something broke our expected commitOnClose\",\n                 TEST_VALUE_FOR_COMMIT_ON_CLOSE, DirectUpdateHandler2.commitOnClose);\n    NODE0.stop();\n    cluster.waitForJettyToStop(NODE0);\n\n    log.info(\"Un-Partition replica (NODE1)...\");\n    proxies.get(NODE1).reopen();\n    \n    waitForState(\"Timeout waiting for leader goes DOWN\", COLLECTION, (liveNodes, collectionState)\n                 -> collectionState.getReplica(leader.getName()).getState() == Replica.State.DOWN);\n\n    TimeOut timeOut = new TimeOut(10, TimeUnit.SECONDS, TimeSource.NANO_TIME);\n    while (!timeOut.hasTimedOut()) {\n      Replica newLeader = getCollectionState(COLLECTION).getLeader(\"shard1\");\n      if (newLeader != null && !newLeader.getName().equals(leader.getName()) && newLeader.getState() == Replica.State.ACTIVE) {\n        fail(\"Out of sync replica became leader \" + newLeader);\n      }\n    }\n\n    log.info(\"Enabling TestInjection.updateLogReplayRandomPause\");\n    TestInjection.updateLogReplayRandomPause = \"true:100\";\n      \n    log.info(\"Un-Partition & restart leader (NODE0)...\");\n    proxies.get(NODE0).reopen();\n    NODE0.start();\n\n    log.info(\"Waiting for all nodes and active collection...\");\n    \n    cluster.waitForAllNodes(30);;\n    waitForState(\"Timeout waiting for leader\", COLLECTION, (liveNodes, collectionState) -> {\n      Replica newLeader = collectionState.getLeader(\"shard1\");\n      return newLeader != null && newLeader.getName().equals(leader.getName());\n    });\n    waitForState(\"Timeout waiting for active collection\", COLLECTION, clusterShape(1, 2));\n    \n    cluster.waitForActiveCollection(COLLECTION, 1, 2);\n\n    log.info(\"Check docs on both replicas...\");\n    assertDocsExistInBothReplicas(1, committedDocs + uncommittedDocs);\n    \n    log.info(\"Test ok, delete collection...\");\n    CollectionAdminRequest.deleteCollection(COLLECTION).process(cluster.getSolrClient());\n  }\n\n","sourceOld":"  public void testManyDocsInTlogReplayWhileReplicaIsTryingToRecover() throws Exception {\n    final int committedDocs = 3;\n    final int uncommittedDocs = 50;\n    \n    log.info(\"Create Collection...\");\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.createCollection(COLLECTION, 1, 2)\n                 .setCreateNodeSet(\"\")\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.addReplicaToShard(COLLECTION, \"shard1\")\n                 .setNode(NODE0.getNodeName())\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    \n    waitForState(\"Timeout waiting for shard leader\", COLLECTION, clusterShape(1, 1));\n\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.addReplicaToShard(COLLECTION, \"shard1\")\n                 .setNode(NODE1.getNodeName())\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    \n    cluster.waitForActiveCollection(COLLECTION, 1, 2);\n    \n    waitForState(\"Timeout waiting for 1x2 collection\", COLLECTION, clusterShape(1, 2));\n    \n    final Replica leader = getCollectionState(COLLECTION).getSlice(\"shard1\").getLeader();\n    assertEquals(\"Sanity check failed\", NODE0.getNodeName(), leader.getNodeName());\n\n    log.info(\"Add and commit a {} docs...\", committedDocs);\n    addDocs(true, committedDocs, 1);\n\n    log.info(\"Partition nodes...\");\n    proxies.get(NODE0).close();\n    proxies.get(NODE1).close();\n\n    log.info(\"Adding {} (uncommitted) docs during network partition....\", uncommittedDocs);\n    addDocs(false, uncommittedDocs, committedDocs + 1);\n\n    log.info(\"Stopping leader node...\");\n    assertEquals(\"Something broke our expected commitOnClose\", false, DirectUpdateHandler2.commitOnClose);\n    NODE0.stop();\n    cluster.waitForJettyToStop(NODE0);\n\n    log.info(\"Un-Partition replica (NODE1)...\");\n    proxies.get(NODE1).reopen();\n    \n    waitForState(\"Timeout waiting for leader goes DOWN\", COLLECTION, (liveNodes, collectionState)\n                 -> collectionState.getReplica(leader.getName()).getState() == Replica.State.DOWN);\n\n    TimeOut timeOut = new TimeOut(10, TimeUnit.SECONDS, TimeSource.NANO_TIME);\n    while (!timeOut.hasTimedOut()) {\n      Replica newLeader = getCollectionState(COLLECTION).getLeader(\"shard1\");\n      if (newLeader != null && !newLeader.getName().equals(leader.getName()) && newLeader.getState() == Replica.State.ACTIVE) {\n        fail(\"Out of sync replica became leader \" + newLeader);\n      }\n    }\n\n    log.info(\"Enabling TestInjection.updateLogReplayRandomPause\");\n    TestInjection.updateLogReplayRandomPause = \"true:100\";\n      \n    log.info(\"Un-Partition & restart leader (NODE0)...\");\n    proxies.get(NODE0).reopen();\n    NODE0.start();\n\n    log.info(\"Waiting for all nodes and active collection...\");\n    \n    cluster.waitForAllNodes(30);;\n    waitForState(\"Timeout waiting for leader\", COLLECTION, (liveNodes, collectionState) -> {\n      Replica newLeader = collectionState.getLeader(\"shard1\");\n      return newLeader != null && newLeader.getName().equals(leader.getName());\n    });\n    waitForState(\"Timeout waiting for active collection\", COLLECTION, clusterShape(1, 2));\n    \n    cluster.waitForActiveCollection(COLLECTION, 1, 2);\n\n    log.info(\"Check docs on both replicas...\");\n    assertDocsExistInBothReplicas(1, uncommittedDocs + uncommittedDocs);\n    \n    log.info(\"Test ok, delete collection...\");\n    CollectionAdminRequest.deleteCollection(COLLECTION).process(cluster.getSolrClient());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"94f2d4492fe7f2ad392dfb81b309ee9afa8a32ac","date":1578901035,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestTlogReplayVsRecovery#testManyDocsInTlogReplayWhileReplicaIsTryingToRecover().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestTlogReplayVsRecovery#testManyDocsInTlogReplayWhileReplicaIsTryingToRecover().mjava","sourceNew":"  public void testManyDocsInTlogReplayWhileReplicaIsTryingToRecover() throws Exception {\n    // TODO: One the basic problem in SOLR-13486 is fixed, this test can be made more robust by:\n    // 1) randomizing the number of committedDocs (pre net split) & uncommittedDocs (post net split)\n    //    to trigger diff recovery strategies & shutdown behavior\n    // 2) replace \"committedDocs + uncommittedDocs\" with 4 variables:\n    //    a: docs committed before network split (add + commit)\n    //    b: docs not committed before network split (add w/o commit)\n    //    c: docs committed after network split (add + commit)\n    //    d: docs not committed after network split (add w/o commit)\n    final int committedDocs = 3;\n    final int uncommittedDocs = 50;\n    \n    log.info(\"Create Collection...\");\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.createCollection(COLLECTION, 1, 2)\n                 .setCreateNodeSet(\"\")\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.addReplicaToShard(COLLECTION, \"shard1\")\n                 .setNode(NODE0.getNodeName())\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    \n    waitForState(\"Timeout waiting for shard leader\", COLLECTION, clusterShape(1, 1));\n\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.addReplicaToShard(COLLECTION, \"shard1\")\n                 .setNode(NODE1.getNodeName())\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    \n    cluster.waitForActiveCollection(COLLECTION, 1, 2);\n    \n    waitForState(\"Timeout waiting for 1x2 collection\", COLLECTION, clusterShape(1, 2));\n    \n    final Replica leader = getCollectionState(COLLECTION).getSlice(\"shard1\").getLeader();\n    assertEquals(\"Sanity check failed\", NODE0.getNodeName(), leader.getNodeName());\n\n    log.info(\"Add and commit {} docs...\", committedDocs);\n    addDocs(true, committedDocs, 1);\n    assertDocsExistInBothReplicas(1, committedDocs);\n\n    log.info(\"Partition nodes...\");\n    proxies.get(NODE0).close();\n    proxies.get(NODE1).close();\n\n    log.info(\"Adding {} (uncommitted) docs during network partition....\", uncommittedDocs);\n    addDocs(false, uncommittedDocs, committedDocs + 1);\n\n    log.info(\"Stopping leader node...\");\n    assertEquals(\"Something broke our expected commitOnClose\",\n                 TEST_VALUE_FOR_COMMIT_ON_CLOSE, DirectUpdateHandler2.commitOnClose);\n    NODE0.stop();\n    cluster.waitForJettyToStop(NODE0);\n\n    log.info(\"Un-Partition replica (NODE1)...\");\n    proxies.get(NODE1).reopen();\n    \n    waitForState(\"Timeout waiting for leader goes DOWN\", COLLECTION, (liveNodes, collectionState)\n                 -> collectionState.getReplica(leader.getName()).getState() == Replica.State.DOWN);\n\n    TimeOut timeOut = new TimeOut(10, TimeUnit.SECONDS, TimeSource.NANO_TIME);\n    while (!timeOut.hasTimedOut()) {\n      Replica newLeader = getCollectionState(COLLECTION).getLeader(\"shard1\");\n      if (newLeader != null && !newLeader.getName().equals(leader.getName()) && newLeader.getState() == Replica.State.ACTIVE) {\n        fail(\"Out of sync replica became leader \" + newLeader);\n      }\n    }\n\n    log.info(\"Enabling TestInjection.updateLogReplayRandomPause\");\n    TestInjection.updateLogReplayRandomPause = \"true:100\";\n      \n    log.info(\"Un-Partition & restart leader (NODE0)...\");\n    proxies.get(NODE0).reopen();\n    NODE0.start();\n\n    log.info(\"Waiting for all nodes and active collection...\");\n    \n    cluster.waitForAllNodes(30);;\n    waitForState(\"Timeout waiting for leader\", COLLECTION, (liveNodes, collectionState) -> {\n      Replica newLeader = collectionState.getLeader(\"shard1\");\n      return newLeader != null && newLeader.getName().equals(leader.getName());\n    });\n    waitForState(\"Timeout waiting for active collection\", COLLECTION, clusterShape(1, 2));\n    \n    cluster.waitForActiveCollection(COLLECTION, 1, 2);\n\n    log.info(\"Check docs on both replicas...\");\n    assertDocsExistInBothReplicas(1, committedDocs + uncommittedDocs);\n    \n    log.info(\"Test ok, delete collection...\");\n    CollectionAdminRequest.deleteCollection(COLLECTION).process(cluster.getSolrClient());\n  }\n\n","sourceOld":"  public void testManyDocsInTlogReplayWhileReplicaIsTryingToRecover() throws Exception {\n    final int committedDocs = 3;\n    final int uncommittedDocs = 50;\n    \n    log.info(\"Create Collection...\");\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.createCollection(COLLECTION, 1, 2)\n                 .setCreateNodeSet(\"\")\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.addReplicaToShard(COLLECTION, \"shard1\")\n                 .setNode(NODE0.getNodeName())\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    \n    waitForState(\"Timeout waiting for shard leader\", COLLECTION, clusterShape(1, 1));\n\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.addReplicaToShard(COLLECTION, \"shard1\")\n                 .setNode(NODE1.getNodeName())\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    \n    cluster.waitForActiveCollection(COLLECTION, 1, 2);\n    \n    waitForState(\"Timeout waiting for 1x2 collection\", COLLECTION, clusterShape(1, 2));\n    \n    final Replica leader = getCollectionState(COLLECTION).getSlice(\"shard1\").getLeader();\n    assertEquals(\"Sanity check failed\", NODE0.getNodeName(), leader.getNodeName());\n\n    log.info(\"Add and commit a {} docs...\", committedDocs);\n    addDocs(true, committedDocs, 1);\n\n    log.info(\"Partition nodes...\");\n    proxies.get(NODE0).close();\n    proxies.get(NODE1).close();\n\n    log.info(\"Adding {} (uncommitted) docs during network partition....\", uncommittedDocs);\n    addDocs(false, uncommittedDocs, committedDocs + 1);\n\n    log.info(\"Stopping leader node...\");\n    assertEquals(\"Something broke our expected commitOnClose\", false, DirectUpdateHandler2.commitOnClose);\n    NODE0.stop();\n    cluster.waitForJettyToStop(NODE0);\n\n    log.info(\"Un-Partition replica (NODE1)...\");\n    proxies.get(NODE1).reopen();\n    \n    waitForState(\"Timeout waiting for leader goes DOWN\", COLLECTION, (liveNodes, collectionState)\n                 -> collectionState.getReplica(leader.getName()).getState() == Replica.State.DOWN);\n\n    TimeOut timeOut = new TimeOut(10, TimeUnit.SECONDS, TimeSource.NANO_TIME);\n    while (!timeOut.hasTimedOut()) {\n      Replica newLeader = getCollectionState(COLLECTION).getLeader(\"shard1\");\n      if (newLeader != null && !newLeader.getName().equals(leader.getName()) && newLeader.getState() == Replica.State.ACTIVE) {\n        fail(\"Out of sync replica became leader \" + newLeader);\n      }\n    }\n\n    log.info(\"Enabling TestInjection.updateLogReplayRandomPause\");\n    TestInjection.updateLogReplayRandomPause = \"true:100\";\n      \n    log.info(\"Un-Partition & restart leader (NODE0)...\");\n    proxies.get(NODE0).reopen();\n    NODE0.start();\n\n    log.info(\"Waiting for all nodes and active collection...\");\n    \n    cluster.waitForAllNodes(30);;\n    waitForState(\"Timeout waiting for leader\", COLLECTION, (liveNodes, collectionState) -> {\n      Replica newLeader = collectionState.getLeader(\"shard1\");\n      return newLeader != null && newLeader.getName().equals(leader.getName());\n    });\n    waitForState(\"Timeout waiting for active collection\", COLLECTION, clusterShape(1, 2));\n    \n    cluster.waitForActiveCollection(COLLECTION, 1, 2);\n\n    log.info(\"Check docs on both replicas...\");\n    assertDocsExistInBothReplicas(1, uncommittedDocs + uncommittedDocs);\n    \n    log.info(\"Test ok, delete collection...\");\n    CollectionAdminRequest.deleteCollection(COLLECTION).process(cluster.getSolrClient());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b6a0ad05ae2af8aa028b1a6099a8222fad0bc8c1","date":1579200426,"type":3,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestTlogReplayVsRecovery#testManyDocsInTlogReplayWhileReplicaIsTryingToRecover().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestTlogReplayVsRecovery#testManyDocsInTlogReplayWhileReplicaIsTryingToRecover().mjava","sourceNew":"  public void testManyDocsInTlogReplayWhileReplicaIsTryingToRecover() throws Exception {\n    // TODO: One the basic problem in SOLR-13486 is fixed, this test can be made more robust by:\n    // 1) randomizing the number of committedDocs (pre net split) & uncommittedDocs (post net split)\n    //    to trigger diff recovery strategies & shutdown behavior\n    // 2) replace \"committedDocs + uncommittedDocs\" with 4 variables:\n    //    a: docs committed before network split (add + commit)\n    //    b: docs not committed before network split (add w/o commit)\n    //    c: docs committed after network split (add + commit)\n    //    d: docs not committed after network split (add w/o commit)\n    final int committedDocs = 3;\n    final int uncommittedDocs = 50;\n    \n    log.info(\"Create Collection...\");\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.createCollection(COLLECTION, 1, 2)\n                 .setCreateNodeSet(\"\")\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.addReplicaToShard(COLLECTION, \"shard1\")\n                 .setNode(NODE0.getNodeName())\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    \n    waitForState(\"Timeout waiting for shard leader\", COLLECTION, clusterShape(1, 1));\n\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.addReplicaToShard(COLLECTION, \"shard1\")\n                 .setNode(NODE1.getNodeName())\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    \n    cluster.waitForActiveCollection(COLLECTION, 1, 2);\n    \n    waitForState(\"Timeout waiting for 1x2 collection\", COLLECTION, clusterShape(1, 2));\n    \n    final Replica leader = getCollectionState(COLLECTION).getSlice(\"shard1\").getLeader();\n    assertEquals(\"Sanity check failed\", NODE0.getNodeName(), leader.getNodeName());\n\n    log.info(\"Add and commit {} docs...\", committedDocs);\n    addDocs(true, committedDocs, 1);\n    assertDocsExistInBothReplicas(1, committedDocs);\n\n    log.info(\"Partition nodes...\");\n    proxies.get(NODE0).close();\n    proxies.get(NODE1).close();\n\n    log.info(\"Adding {} (uncommitted) docs during network partition....\", uncommittedDocs);\n    addDocs(false, uncommittedDocs, committedDocs + 1);\n\n    log.info(\"Stopping leader node...\");\n    assertEquals(\"Something broke our expected skipIndexWriterCommitOnClose\",\n                 TEST_VALUE_FOR_SKIP_COMMIT_ON_CLOSE, TestInjection.skipIndexWriterCommitOnClose);\n    NODE0.stop();\n    cluster.waitForJettyToStop(NODE0);\n\n    log.info(\"Un-Partition replica (NODE1)...\");\n    proxies.get(NODE1).reopen();\n    \n    waitForState(\"Timeout waiting for leader goes DOWN\", COLLECTION, (liveNodes, collectionState)\n                 -> collectionState.getReplica(leader.getName()).getState() == Replica.State.DOWN);\n\n    TimeOut timeOut = new TimeOut(10, TimeUnit.SECONDS, TimeSource.NANO_TIME);\n    while (!timeOut.hasTimedOut()) {\n      Replica newLeader = getCollectionState(COLLECTION).getLeader(\"shard1\");\n      if (newLeader != null && !newLeader.getName().equals(leader.getName()) && newLeader.getState() == Replica.State.ACTIVE) {\n        fail(\"Out of sync replica became leader \" + newLeader);\n      }\n    }\n\n    log.info(\"Enabling TestInjection.updateLogReplayRandomPause\");\n    TestInjection.updateLogReplayRandomPause = \"true:100\";\n      \n    log.info(\"Un-Partition & restart leader (NODE0)...\");\n    proxies.get(NODE0).reopen();\n    NODE0.start();\n\n    log.info(\"Waiting for all nodes and active collection...\");\n    \n    cluster.waitForAllNodes(30);;\n    waitForState(\"Timeout waiting for leader\", COLLECTION, (liveNodes, collectionState) -> {\n      Replica newLeader = collectionState.getLeader(\"shard1\");\n      return newLeader != null && newLeader.getName().equals(leader.getName());\n    });\n    waitForState(\"Timeout waiting for active collection\", COLLECTION, clusterShape(1, 2));\n    \n    cluster.waitForActiveCollection(COLLECTION, 1, 2);\n\n    log.info(\"Check docs on both replicas...\");\n    assertDocsExistInBothReplicas(1, committedDocs + uncommittedDocs);\n    \n    log.info(\"Test ok, delete collection...\");\n    CollectionAdminRequest.deleteCollection(COLLECTION).process(cluster.getSolrClient());\n  }\n\n","sourceOld":"  public void testManyDocsInTlogReplayWhileReplicaIsTryingToRecover() throws Exception {\n    // TODO: One the basic problem in SOLR-13486 is fixed, this test can be made more robust by:\n    // 1) randomizing the number of committedDocs (pre net split) & uncommittedDocs (post net split)\n    //    to trigger diff recovery strategies & shutdown behavior\n    // 2) replace \"committedDocs + uncommittedDocs\" with 4 variables:\n    //    a: docs committed before network split (add + commit)\n    //    b: docs not committed before network split (add w/o commit)\n    //    c: docs committed after network split (add + commit)\n    //    d: docs not committed after network split (add w/o commit)\n    final int committedDocs = 3;\n    final int uncommittedDocs = 50;\n    \n    log.info(\"Create Collection...\");\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.createCollection(COLLECTION, 1, 2)\n                 .setCreateNodeSet(\"\")\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.addReplicaToShard(COLLECTION, \"shard1\")\n                 .setNode(NODE0.getNodeName())\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    \n    waitForState(\"Timeout waiting for shard leader\", COLLECTION, clusterShape(1, 1));\n\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.addReplicaToShard(COLLECTION, \"shard1\")\n                 .setNode(NODE1.getNodeName())\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    \n    cluster.waitForActiveCollection(COLLECTION, 1, 2);\n    \n    waitForState(\"Timeout waiting for 1x2 collection\", COLLECTION, clusterShape(1, 2));\n    \n    final Replica leader = getCollectionState(COLLECTION).getSlice(\"shard1\").getLeader();\n    assertEquals(\"Sanity check failed\", NODE0.getNodeName(), leader.getNodeName());\n\n    log.info(\"Add and commit {} docs...\", committedDocs);\n    addDocs(true, committedDocs, 1);\n    assertDocsExistInBothReplicas(1, committedDocs);\n\n    log.info(\"Partition nodes...\");\n    proxies.get(NODE0).close();\n    proxies.get(NODE1).close();\n\n    log.info(\"Adding {} (uncommitted) docs during network partition....\", uncommittedDocs);\n    addDocs(false, uncommittedDocs, committedDocs + 1);\n\n    log.info(\"Stopping leader node...\");\n    assertEquals(\"Something broke our expected commitOnClose\",\n                 TEST_VALUE_FOR_COMMIT_ON_CLOSE, DirectUpdateHandler2.commitOnClose);\n    NODE0.stop();\n    cluster.waitForJettyToStop(NODE0);\n\n    log.info(\"Un-Partition replica (NODE1)...\");\n    proxies.get(NODE1).reopen();\n    \n    waitForState(\"Timeout waiting for leader goes DOWN\", COLLECTION, (liveNodes, collectionState)\n                 -> collectionState.getReplica(leader.getName()).getState() == Replica.State.DOWN);\n\n    TimeOut timeOut = new TimeOut(10, TimeUnit.SECONDS, TimeSource.NANO_TIME);\n    while (!timeOut.hasTimedOut()) {\n      Replica newLeader = getCollectionState(COLLECTION).getLeader(\"shard1\");\n      if (newLeader != null && !newLeader.getName().equals(leader.getName()) && newLeader.getState() == Replica.State.ACTIVE) {\n        fail(\"Out of sync replica became leader \" + newLeader);\n      }\n    }\n\n    log.info(\"Enabling TestInjection.updateLogReplayRandomPause\");\n    TestInjection.updateLogReplayRandomPause = \"true:100\";\n      \n    log.info(\"Un-Partition & restart leader (NODE0)...\");\n    proxies.get(NODE0).reopen();\n    NODE0.start();\n\n    log.info(\"Waiting for all nodes and active collection...\");\n    \n    cluster.waitForAllNodes(30);;\n    waitForState(\"Timeout waiting for leader\", COLLECTION, (liveNodes, collectionState) -> {\n      Replica newLeader = collectionState.getLeader(\"shard1\");\n      return newLeader != null && newLeader.getName().equals(leader.getName());\n    });\n    waitForState(\"Timeout waiting for active collection\", COLLECTION, clusterShape(1, 2));\n    \n    cluster.waitForActiveCollection(COLLECTION, 1, 2);\n\n    log.info(\"Check docs on both replicas...\");\n    assertDocsExistInBothReplicas(1, committedDocs + uncommittedDocs);\n    \n    log.info(\"Test ok, delete collection...\");\n    CollectionAdminRequest.deleteCollection(COLLECTION).process(cluster.getSolrClient());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"82917f9ea58e77078cb7b18e9195b383aeec1b60","date":1579729496,"type":3,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestTlogReplayVsRecovery#testManyDocsInTlogReplayWhileReplicaIsTryingToRecover().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestTlogReplayVsRecovery#testManyDocsInTlogReplayWhileReplicaIsTryingToRecover().mjava","sourceNew":"  public void testManyDocsInTlogReplayWhileReplicaIsTryingToRecover() throws Exception {\n    // TODO: One the basic problem in SOLR-13486 is fixed, this test can be made more robust by:\n    // 1) randomizing the number of committedDocs (pre net split) & uncommittedDocs (post net split)\n    //    to trigger diff recovery strategies & shutdown behavior\n    // 2) replace \"committedDocs + uncommittedDocs\" with 4 variables:\n    //    a: docs committed before network split (add + commit)\n    //    b: docs not committed before network split (add w/o commit)\n    //    c: docs committed after network split (add + commit)\n    //    d: docs not committed after network split (add w/o commit)\n    final int committedDocs = 3;\n    final int uncommittedDocs = 50;\n    \n    log.info(\"Create Collection...\");\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.createCollection(COLLECTION, 1, 2)\n                 .setCreateNodeSet(\"\")\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.addReplicaToShard(COLLECTION, \"shard1\")\n                 .setNode(NODE0.getNodeName())\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    \n    waitForState(\"Timeout waiting for shard leader\", COLLECTION, clusterShape(1, 1));\n\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.addReplicaToShard(COLLECTION, \"shard1\")\n                 .setNode(NODE1.getNodeName())\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    \n    cluster.waitForActiveCollection(COLLECTION, 1, 2);\n    \n    waitForState(\"Timeout waiting for 1x2 collection\", COLLECTION, clusterShape(1, 2));\n    \n    final Replica leader = getCollectionState(COLLECTION).getSlice(\"shard1\").getLeader();\n    assertEquals(\"Sanity check failed\", NODE0.getNodeName(), leader.getNodeName());\n\n    log.info(\"Add and commit {} docs...\", committedDocs);\n    addDocs(true, committedDocs, 1);\n    assertDocsExistInBothReplicas(1, committedDocs);\n\n    log.info(\"Partition nodes...\");\n    proxies.get(NODE0).close();\n    proxies.get(NODE1).close();\n\n    log.info(\"Adding {} (uncommitted) docs during network partition....\", uncommittedDocs);\n    addDocs(false, uncommittedDocs, committedDocs + 1);\n\n    log.info(\"Stopping leader node...\");\n    assertEquals(\"Something broke our expected skipIndexWriterCommitOnClose\",\n                 TEST_VALUE_FOR_SKIP_COMMIT_ON_CLOSE, TestInjection.skipIndexWriterCommitOnClose);\n    NODE0.stop();\n    cluster.waitForJettyToStop(NODE0);\n\n    log.info(\"Un-Partition replica (NODE1)...\");\n    proxies.get(NODE1).reopen();\n    \n    waitForState(\"Timeout waiting for leader goes DOWN\", COLLECTION, (liveNodes, collectionState)\n                 -> collectionState.getReplica(leader.getName()).getState() == Replica.State.DOWN);\n\n    // Sanity check that a new (out of sync) replica doesn't come up in our place...\n    expectThrows(TimeoutException.class,\n                 \"Did not time out waiting for new leader, out of sync replica became leader\",\n                 () -> {\n                   cluster.getSolrClient().waitForState(COLLECTION, 10, TimeUnit.SECONDS, (state) -> {\n            Replica newLeader = state.getSlice(\"shard1\").getLeader();\n            if (newLeader != null && !newLeader.getName().equals(leader.getName()) && newLeader.getState() == Replica.State.ACTIVE) {\n              // this is is the bad case, our \"bad\" state was found before timeout\n              log.error(\"WTF: New Leader={}\", newLeader);\n              return true;\n            }\n            return false; // still no bad state, wait for timeout\n          });\n      });\n\n    log.info(\"Enabling TestInjection.updateLogReplayRandomPause\");\n    TestInjection.updateLogReplayRandomPause = \"true:100\";\n      \n    log.info(\"Un-Partition & restart leader (NODE0)...\");\n    proxies.get(NODE0).reopen();\n    NODE0.start();\n\n    log.info(\"Waiting for all nodes and active collection...\");\n    \n    cluster.waitForAllNodes(30);;\n    waitForState(\"Timeout waiting for leader\", COLLECTION, (liveNodes, collectionState) -> {\n      Replica newLeader = collectionState.getLeader(\"shard1\");\n      return newLeader != null && newLeader.getName().equals(leader.getName());\n    });\n    waitForState(\"Timeout waiting for active collection\", COLLECTION, clusterShape(1, 2));\n    \n    cluster.waitForActiveCollection(COLLECTION, 1, 2);\n\n    log.info(\"Check docs on both replicas...\");\n    assertDocsExistInBothReplicas(1, committedDocs + uncommittedDocs);\n    \n    log.info(\"Test ok, delete collection...\");\n    CollectionAdminRequest.deleteCollection(COLLECTION).process(cluster.getSolrClient());\n  }\n\n","sourceOld":"  public void testManyDocsInTlogReplayWhileReplicaIsTryingToRecover() throws Exception {\n    // TODO: One the basic problem in SOLR-13486 is fixed, this test can be made more robust by:\n    // 1) randomizing the number of committedDocs (pre net split) & uncommittedDocs (post net split)\n    //    to trigger diff recovery strategies & shutdown behavior\n    // 2) replace \"committedDocs + uncommittedDocs\" with 4 variables:\n    //    a: docs committed before network split (add + commit)\n    //    b: docs not committed before network split (add w/o commit)\n    //    c: docs committed after network split (add + commit)\n    //    d: docs not committed after network split (add w/o commit)\n    final int committedDocs = 3;\n    final int uncommittedDocs = 50;\n    \n    log.info(\"Create Collection...\");\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.createCollection(COLLECTION, 1, 2)\n                 .setCreateNodeSet(\"\")\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.addReplicaToShard(COLLECTION, \"shard1\")\n                 .setNode(NODE0.getNodeName())\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    \n    waitForState(\"Timeout waiting for shard leader\", COLLECTION, clusterShape(1, 1));\n\n    assertEquals(RequestStatusState.COMPLETED,\n                 CollectionAdminRequest.addReplicaToShard(COLLECTION, \"shard1\")\n                 .setNode(NODE1.getNodeName())\n                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));\n    \n    cluster.waitForActiveCollection(COLLECTION, 1, 2);\n    \n    waitForState(\"Timeout waiting for 1x2 collection\", COLLECTION, clusterShape(1, 2));\n    \n    final Replica leader = getCollectionState(COLLECTION).getSlice(\"shard1\").getLeader();\n    assertEquals(\"Sanity check failed\", NODE0.getNodeName(), leader.getNodeName());\n\n    log.info(\"Add and commit {} docs...\", committedDocs);\n    addDocs(true, committedDocs, 1);\n    assertDocsExistInBothReplicas(1, committedDocs);\n\n    log.info(\"Partition nodes...\");\n    proxies.get(NODE0).close();\n    proxies.get(NODE1).close();\n\n    log.info(\"Adding {} (uncommitted) docs during network partition....\", uncommittedDocs);\n    addDocs(false, uncommittedDocs, committedDocs + 1);\n\n    log.info(\"Stopping leader node...\");\n    assertEquals(\"Something broke our expected skipIndexWriterCommitOnClose\",\n                 TEST_VALUE_FOR_SKIP_COMMIT_ON_CLOSE, TestInjection.skipIndexWriterCommitOnClose);\n    NODE0.stop();\n    cluster.waitForJettyToStop(NODE0);\n\n    log.info(\"Un-Partition replica (NODE1)...\");\n    proxies.get(NODE1).reopen();\n    \n    waitForState(\"Timeout waiting for leader goes DOWN\", COLLECTION, (liveNodes, collectionState)\n                 -> collectionState.getReplica(leader.getName()).getState() == Replica.State.DOWN);\n\n    TimeOut timeOut = new TimeOut(10, TimeUnit.SECONDS, TimeSource.NANO_TIME);\n    while (!timeOut.hasTimedOut()) {\n      Replica newLeader = getCollectionState(COLLECTION).getLeader(\"shard1\");\n      if (newLeader != null && !newLeader.getName().equals(leader.getName()) && newLeader.getState() == Replica.State.ACTIVE) {\n        fail(\"Out of sync replica became leader \" + newLeader);\n      }\n    }\n\n    log.info(\"Enabling TestInjection.updateLogReplayRandomPause\");\n    TestInjection.updateLogReplayRandomPause = \"true:100\";\n      \n    log.info(\"Un-Partition & restart leader (NODE0)...\");\n    proxies.get(NODE0).reopen();\n    NODE0.start();\n\n    log.info(\"Waiting for all nodes and active collection...\");\n    \n    cluster.waitForAllNodes(30);;\n    waitForState(\"Timeout waiting for leader\", COLLECTION, (liveNodes, collectionState) -> {\n      Replica newLeader = collectionState.getLeader(\"shard1\");\n      return newLeader != null && newLeader.getName().equals(leader.getName());\n    });\n    waitForState(\"Timeout waiting for active collection\", COLLECTION, clusterShape(1, 2));\n    \n    cluster.waitForActiveCollection(COLLECTION, 1, 2);\n\n    log.info(\"Check docs on both replicas...\");\n    assertDocsExistInBothReplicas(1, committedDocs + uncommittedDocs);\n    \n    log.info(\"Test ok, delete collection...\");\n    CollectionAdminRequest.deleteCollection(COLLECTION).process(cluster.getSolrClient());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"94f2d4492fe7f2ad392dfb81b309ee9afa8a32ac":["ca62564055241632cd20d65b5ecb8c8e93bd60c4","e18458cbca975852db0911f1a7f9a0a2fcd493f1"],"82917f9ea58e77078cb7b18e9195b383aeec1b60":["b6a0ad05ae2af8aa028b1a6099a8222fad0bc8c1"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"e18458cbca975852db0911f1a7f9a0a2fcd493f1":["df06aa21e6f41b678afd8f30568ebadd781be717"],"b6a0ad05ae2af8aa028b1a6099a8222fad0bc8c1":["e18458cbca975852db0911f1a7f9a0a2fcd493f1"],"df06aa21e6f41b678afd8f30568ebadd781be717":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"ca62564055241632cd20d65b5ecb8c8e93bd60c4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","df06aa21e6f41b678afd8f30568ebadd781be717"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["82917f9ea58e77078cb7b18e9195b383aeec1b60"]},"commit2Childs":{"94f2d4492fe7f2ad392dfb81b309ee9afa8a32ac":[],"82917f9ea58e77078cb7b18e9195b383aeec1b60":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["df06aa21e6f41b678afd8f30568ebadd781be717","ca62564055241632cd20d65b5ecb8c8e93bd60c4"],"e18458cbca975852db0911f1a7f9a0a2fcd493f1":["94f2d4492fe7f2ad392dfb81b309ee9afa8a32ac","b6a0ad05ae2af8aa028b1a6099a8222fad0bc8c1"],"b6a0ad05ae2af8aa028b1a6099a8222fad0bc8c1":["82917f9ea58e77078cb7b18e9195b383aeec1b60"],"ca62564055241632cd20d65b5ecb8c8e93bd60c4":["94f2d4492fe7f2ad392dfb81b309ee9afa8a32ac"],"df06aa21e6f41b678afd8f30568ebadd781be717":["e18458cbca975852db0911f1a7f9a0a2fcd493f1","ca62564055241632cd20d65b5ecb8c8e93bd60c4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["94f2d4492fe7f2ad392dfb81b309ee9afa8a32ac","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}