{"path":"lucene/analysis/common/src/java/org/apache/lucene/analysis/minhash/MinHashFilter#incrementToken().mjava","commits":[{"id":"af241f05539d0b41cd28a8051ad03e3d7fb051cf","date":1465897793,"type":0,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/minhash/MinHashFilter#incrementToken().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    // Pull the underlying stream of tokens\n    // Hash each token found\n    // Generate the required number of variants of this hash\n    // Keep the minimum hash value found so far of each variant\n\n    int positionIncrement = 0;\n    if (requiresInitialisation) {\n      requiresInitialisation = false;\n      boolean found = false;\n      // First time through so we pull and hash everything\n      while (input.incrementToken()) {\n        found = true;\n        String current = new String(termAttribute.buffer(), 0, termAttribute.length());\n\n        for (int i = 0; i < hashCount; i++) {\n          byte[] bytes = current.getBytes(\"UTF-16LE\");\n          LongPair hash = new LongPair();\n          murmurhash3_x64_128(bytes, 0, bytes.length, 0, hash);\n          LongPair rehashed = combineOrdered(hash, getIntHash(i));\n          minHashSets.get(i).get((int) ((rehashed.val2 >>> 32) / bucketSize)).add(rehashed);\n        }\n        endOffset = offsetAttribute.endOffset();\n      }\n      exhausted = true;\n      input.end();\n      // We need the end state so an underlying shingle filter can have its state restored correctly.\n      endState = captureState();\n      if (!found) {\n        return false;\n      }\n      \n      positionIncrement = 1;\n      // fix up any wrap around bucket values. ...\n      if (withRotation && (hashSetSize == 1)) {\n        for (int hashLoop = 0; hashLoop < hashCount; hashLoop++) {\n          for (int bucketLoop = 0; bucketLoop < bucketCount; bucketLoop++) {\n            if (minHashSets.get(hashLoop).get(bucketLoop).size() == 0) {\n              for (int bucketOffset = 1; bucketOffset < bucketCount; bucketOffset++) {\n                if (minHashSets.get(hashLoop).get((bucketLoop + bucketOffset) % bucketCount).size() > 0) {\n                  LongPair replacementHash = minHashSets.get(hashLoop).get((bucketLoop + bucketOffset) % bucketCount)\n                      .first();\n                  minHashSets.get(hashLoop).get(bucketLoop).add(replacementHash);\n                  break;\n                }\n              }\n            }\n          }\n        }\n      }\n\n    }\n   \n    clearAttributes();\n\n    while (hashPosition < hashCount) {\n      if (hashPosition == -1) {\n        hashPosition++;\n      } else {\n        while (bucketPosition < bucketCount) {\n          if (bucketPosition == -1) {\n            bucketPosition++;\n          } else {\n            LongPair hash = minHashSets.get(hashPosition).get(bucketPosition).pollFirst();\n            if (hash != null) {\n              termAttribute.setEmpty();\n              if (hashCount > 1) {\n                termAttribute.append(int0(hashPosition));\n                termAttribute.append(int1(hashPosition));\n              }\n              long high = hash.val2;\n              termAttribute.append(long0(high));\n              termAttribute.append(long1(high));\n              termAttribute.append(long2(high));\n              termAttribute.append(long3(high));\n              long low = hash.val1;\n              termAttribute.append(long0(low));\n              termAttribute.append(long1(low));\n              if (hashCount == 1) {\n                termAttribute.append(long2(low));\n                termAttribute.append(long3(low));\n              }\n              posIncAttribute.setPositionIncrement(positionIncrement);\n              offsetAttribute.setOffset(0, endOffset);\n              typeAttribute.setType(MIN_HASH_TYPE);\n              posLenAttribute.setPositionLength(1);\n              return true;\n            } else {\n              bucketPosition++;\n            }\n          }\n\n        }\n        bucketPosition = -1;\n        hashPosition++;\n      }\n    }\n    return false;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"57dc82c7b33cd580e7ab5179019bc78f3d7f8e79","date":1465913303,"type":0,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/minhash/MinHashFilter#incrementToken().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    // Pull the underlying stream of tokens\n    // Hash each token found\n    // Generate the required number of variants of this hash\n    // Keep the minimum hash value found so far of each variant\n\n    int positionIncrement = 0;\n    if (requiresInitialisation) {\n      requiresInitialisation = false;\n      boolean found = false;\n      // First time through so we pull and hash everything\n      while (input.incrementToken()) {\n        found = true;\n        String current = new String(termAttribute.buffer(), 0, termAttribute.length());\n\n        for (int i = 0; i < hashCount; i++) {\n          byte[] bytes = current.getBytes(\"UTF-16LE\");\n          LongPair hash = new LongPair();\n          murmurhash3_x64_128(bytes, 0, bytes.length, 0, hash);\n          LongPair rehashed = combineOrdered(hash, getIntHash(i));\n          minHashSets.get(i).get((int) ((rehashed.val2 >>> 32) / bucketSize)).add(rehashed);\n        }\n        endOffset = offsetAttribute.endOffset();\n      }\n      exhausted = true;\n      input.end();\n      // We need the end state so an underlying shingle filter can have its state restored correctly.\n      endState = captureState();\n      if (!found) {\n        return false;\n      }\n      \n      positionIncrement = 1;\n      // fix up any wrap around bucket values. ...\n      if (withRotation && (hashSetSize == 1)) {\n        for (int hashLoop = 0; hashLoop < hashCount; hashLoop++) {\n          for (int bucketLoop = 0; bucketLoop < bucketCount; bucketLoop++) {\n            if (minHashSets.get(hashLoop).get(bucketLoop).size() == 0) {\n              for (int bucketOffset = 1; bucketOffset < bucketCount; bucketOffset++) {\n                if (minHashSets.get(hashLoop).get((bucketLoop + bucketOffset) % bucketCount).size() > 0) {\n                  LongPair replacementHash = minHashSets.get(hashLoop).get((bucketLoop + bucketOffset) % bucketCount)\n                      .first();\n                  minHashSets.get(hashLoop).get(bucketLoop).add(replacementHash);\n                  break;\n                }\n              }\n            }\n          }\n        }\n      }\n\n    }\n   \n    clearAttributes();\n\n    while (hashPosition < hashCount) {\n      if (hashPosition == -1) {\n        hashPosition++;\n      } else {\n        while (bucketPosition < bucketCount) {\n          if (bucketPosition == -1) {\n            bucketPosition++;\n          } else {\n            LongPair hash = minHashSets.get(hashPosition).get(bucketPosition).pollFirst();\n            if (hash != null) {\n              termAttribute.setEmpty();\n              if (hashCount > 1) {\n                termAttribute.append(int0(hashPosition));\n                termAttribute.append(int1(hashPosition));\n              }\n              long high = hash.val2;\n              termAttribute.append(long0(high));\n              termAttribute.append(long1(high));\n              termAttribute.append(long2(high));\n              termAttribute.append(long3(high));\n              long low = hash.val1;\n              termAttribute.append(long0(low));\n              termAttribute.append(long1(low));\n              if (hashCount == 1) {\n                termAttribute.append(long2(low));\n                termAttribute.append(long3(low));\n              }\n              posIncAttribute.setPositionIncrement(positionIncrement);\n              offsetAttribute.setOffset(0, endOffset);\n              typeAttribute.setType(MIN_HASH_TYPE);\n              posLenAttribute.setPositionLength(1);\n              return true;\n            } else {\n              bucketPosition++;\n            }\n          }\n\n        }\n        bucketPosition = -1;\n        hashPosition++;\n      }\n    }\n    return false;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/minhash/MinHashFilter#incrementToken().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    // Pull the underlying stream of tokens\n    // Hash each token found\n    // Generate the required number of variants of this hash\n    // Keep the minimum hash value found so far of each variant\n\n    int positionIncrement = 0;\n    if (requiresInitialisation) {\n      requiresInitialisation = false;\n      boolean found = false;\n      // First time through so we pull and hash everything\n      while (input.incrementToken()) {\n        found = true;\n        String current = new String(termAttribute.buffer(), 0, termAttribute.length());\n\n        for (int i = 0; i < hashCount; i++) {\n          byte[] bytes = current.getBytes(\"UTF-16LE\");\n          LongPair hash = new LongPair();\n          murmurhash3_x64_128(bytes, 0, bytes.length, 0, hash);\n          LongPair rehashed = combineOrdered(hash, getIntHash(i));\n          minHashSets.get(i).get((int) ((rehashed.val2 >>> 32) / bucketSize)).add(rehashed);\n        }\n        endOffset = offsetAttribute.endOffset();\n      }\n      exhausted = true;\n      input.end();\n      // We need the end state so an underlying shingle filter can have its state restored correctly.\n      endState = captureState();\n      if (!found) {\n        return false;\n      }\n      \n      positionIncrement = 1;\n      // fix up any wrap around bucket values. ...\n      if (withRotation && (hashSetSize == 1)) {\n        for (int hashLoop = 0; hashLoop < hashCount; hashLoop++) {\n          for (int bucketLoop = 0; bucketLoop < bucketCount; bucketLoop++) {\n            if (minHashSets.get(hashLoop).get(bucketLoop).size() == 0) {\n              for (int bucketOffset = 1; bucketOffset < bucketCount; bucketOffset++) {\n                if (minHashSets.get(hashLoop).get((bucketLoop + bucketOffset) % bucketCount).size() > 0) {\n                  LongPair replacementHash = minHashSets.get(hashLoop).get((bucketLoop + bucketOffset) % bucketCount)\n                      .first();\n                  minHashSets.get(hashLoop).get(bucketLoop).add(replacementHash);\n                  break;\n                }\n              }\n            }\n          }\n        }\n      }\n\n    }\n   \n    clearAttributes();\n\n    while (hashPosition < hashCount) {\n      if (hashPosition == -1) {\n        hashPosition++;\n      } else {\n        while (bucketPosition < bucketCount) {\n          if (bucketPosition == -1) {\n            bucketPosition++;\n          } else {\n            LongPair hash = minHashSets.get(hashPosition).get(bucketPosition).pollFirst();\n            if (hash != null) {\n              termAttribute.setEmpty();\n              if (hashCount > 1) {\n                termAttribute.append(int0(hashPosition));\n                termAttribute.append(int1(hashPosition));\n              }\n              long high = hash.val2;\n              termAttribute.append(long0(high));\n              termAttribute.append(long1(high));\n              termAttribute.append(long2(high));\n              termAttribute.append(long3(high));\n              long low = hash.val1;\n              termAttribute.append(long0(low));\n              termAttribute.append(long1(low));\n              if (hashCount == 1) {\n                termAttribute.append(long2(low));\n                termAttribute.append(long3(low));\n              }\n              posIncAttribute.setPositionIncrement(positionIncrement);\n              offsetAttribute.setOffset(0, endOffset);\n              typeAttribute.setType(MIN_HASH_TYPE);\n              posLenAttribute.setPositionLength(1);\n              return true;\n            } else {\n              bucketPosition++;\n            }\n          }\n\n        }\n        bucketPosition = -1;\n        hashPosition++;\n      }\n    }\n    return false;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"57dc82c7b33cd580e7ab5179019bc78f3d7f8e79":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","af241f05539d0b41cd28a8051ad03e3d7fb051cf"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","57dc82c7b33cd580e7ab5179019bc78f3d7f8e79"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["57dc82c7b33cd580e7ab5179019bc78f3d7f8e79"],"af241f05539d0b41cd28a8051ad03e3d7fb051cf":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["57dc82c7b33cd580e7ab5179019bc78f3d7f8e79","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","af241f05539d0b41cd28a8051ad03e3d7fb051cf"],"57dc82c7b33cd580e7ab5179019bc78f3d7f8e79":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"af241f05539d0b41cd28a8051ad03e3d7fb051cf":["57dc82c7b33cd580e7ab5179019bc78f3d7f8e79"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}