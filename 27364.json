{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest#testNGrams(int,int,int).mjava","commits":[{"id":"4c807c4005aae1acaf5cebc9af40883985fb89a8","date":1366974206,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest#testNGrams(int,int,int).mjava","pathOld":"/dev/null","sourceNew":"  private void testNGrams(int minGram, int maxGram, int length) throws IOException {\n    final String s = RandomStrings.randomAsciiOfLength(random(), length);\n    final TokenStream grams = new NGramTokenizer(TEST_VERSION_CURRENT, new StringReader(s), minGram, maxGram);\n    final CharTermAttribute termAtt = grams.addAttribute(CharTermAttribute.class);\n    final PositionIncrementAttribute posIncAtt = grams.addAttribute(PositionIncrementAttribute.class);\n    final PositionLengthAttribute posLenAtt = grams.addAttribute(PositionLengthAttribute.class);\n    final OffsetAttribute offsetAtt = grams.addAttribute(OffsetAttribute.class);\n    grams.reset();\n    for (int start = 0; start < s.length(); ++start) {\n      for (int end = start + minGram; end <= start + maxGram && end <= s.length(); ++end) {\n        assertTrue(grams.incrementToken());\n        assertEquals(s.substring(start, end), termAtt.toString());\n        assertEquals(1, posIncAtt.getPositionIncrement());\n        assertEquals(start, offsetAtt.startOffset());\n        assertEquals(end, offsetAtt.endOffset());\n      }\n    }\n    grams.end();\n    assertEquals(s.length(), offsetAtt.startOffset());\n    assertEquals(s.length(), offsetAtt.endOffset());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704","date":1371043069,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest#testNGrams(int,int,String,String,boolean).mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest#testNGrams(int,int,int).mjava","sourceNew":"  static void testNGrams(int minGram, int maxGram, String s, final String nonTokenChars, boolean edgesOnly) throws IOException {\n    // convert the string to code points\n    final int[] codePoints = toCodePoints(s);\n    final int[] offsets = new int[codePoints.length + 1];\n    for (int i = 0; i < codePoints.length; ++i) {\n      offsets[i+1] = offsets[i] + Character.charCount(codePoints[i]);\n    }\n    final TokenStream grams = new NGramTokenizer(TEST_VERSION_CURRENT, new StringReader(s), minGram, maxGram, edgesOnly) {\n      @Override\n      protected boolean isTokenChar(int chr) {\n        return nonTokenChars.indexOf(chr) < 0;\n      }\n    };\n    final CharTermAttribute termAtt = grams.addAttribute(CharTermAttribute.class);\n    final PositionIncrementAttribute posIncAtt = grams.addAttribute(PositionIncrementAttribute.class);\n    final PositionLengthAttribute posLenAtt = grams.addAttribute(PositionLengthAttribute.class);\n    final OffsetAttribute offsetAtt = grams.addAttribute(OffsetAttribute.class);\n    grams.reset();\n    for (int start = 0; start < codePoints.length; ++start) {\n      nextGram:\n      for (int end = start + minGram; end <= start + maxGram && end <= codePoints.length; ++end) {\n        if (edgesOnly && start > 0 && isTokenChar(nonTokenChars, codePoints[start - 1])) {\n          // not on an edge\n          continue nextGram;\n        }\n        for (int j = start; j < end; ++j) {\n          if (!isTokenChar(nonTokenChars, codePoints[j])) {\n            continue nextGram;\n          }\n        }\n        assertTrue(grams.incrementToken());\n        assertArrayEquals(Arrays.copyOfRange(codePoints, start, end), toCodePoints(termAtt));\n        assertEquals(1, posIncAtt.getPositionIncrement());\n        assertEquals(1, posLenAtt.getPositionLength());\n        assertEquals(offsets[start], offsetAtt.startOffset());\n        assertEquals(offsets[end], offsetAtt.endOffset());\n      }\n    }\n    assertFalse(grams.incrementToken());\n    grams.end();\n    assertEquals(s.length(), offsetAtt.startOffset());\n    assertEquals(s.length(), offsetAtt.endOffset());\n  }\n\n","sourceOld":"  private void testNGrams(int minGram, int maxGram, int length) throws IOException {\n    final String s = RandomStrings.randomAsciiOfLength(random(), length);\n    final TokenStream grams = new NGramTokenizer(TEST_VERSION_CURRENT, new StringReader(s), minGram, maxGram);\n    final CharTermAttribute termAtt = grams.addAttribute(CharTermAttribute.class);\n    final PositionIncrementAttribute posIncAtt = grams.addAttribute(PositionIncrementAttribute.class);\n    final PositionLengthAttribute posLenAtt = grams.addAttribute(PositionLengthAttribute.class);\n    final OffsetAttribute offsetAtt = grams.addAttribute(OffsetAttribute.class);\n    grams.reset();\n    for (int start = 0; start < s.length(); ++start) {\n      for (int end = start + minGram; end <= start + maxGram && end <= s.length(); ++end) {\n        assertTrue(grams.incrementToken());\n        assertEquals(s.substring(start, end), termAtt.toString());\n        assertEquals(1, posIncAtt.getPositionIncrement());\n        assertEquals(start, offsetAtt.startOffset());\n        assertEquals(end, offsetAtt.endOffset());\n      }\n    }\n    grams.end();\n    assertEquals(s.length(), offsetAtt.startOffset());\n    assertEquals(s.length(), offsetAtt.endOffset());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704":["4c807c4005aae1acaf5cebc9af40883985fb89a8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4c807c4005aae1acaf5cebc9af40883985fb89a8":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704"]},"commit2Childs":{"bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4c807c4005aae1acaf5cebc9af40883985fb89a8"],"4c807c4005aae1acaf5cebc9af40883985fb89a8":["bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}