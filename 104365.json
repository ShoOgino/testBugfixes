{"path":"lucene/core/src/java/org/apache/lucene/codecs/lucene50/CompoundFileDirectory#readEntries(Directory,String).mjava","commits":[{"id":"989d940c4bf402188f4f0ae13736836885227383","date":1412263633,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene50/CompoundFileDirectory#readEntries(Directory,String).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/store/CompoundFileDirectory#readEntries(Directory,String).mjava","sourceNew":"  /** Helper method that reads CFS entries from an input stream */\n  private final Map<String, FileEntry> readEntries(Directory dir, String name) throws IOException {\n    ChecksumIndexInput entriesStream = null;\n    Map<String,FileEntry> mapping = null;\n    boolean success = false;\n    try {\n      final String entriesFileName = IndexFileNames.segmentFileName(\n                                            IndexFileNames.stripExtension(name), \"\",\n                                             IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION);\n      entriesStream = dir.openChecksumInput(entriesFileName, IOContext.READONCE);\n      version = CodecUtil.checkHeader(entriesStream, CompoundFileWriter.ENTRY_CODEC, CompoundFileWriter.VERSION_START, CompoundFileWriter.VERSION_CURRENT);\n      if (version >= CompoundFileWriter.VERSION_SEGMENTHEADER) {\n        byte id[] = new byte[StringHelper.ID_LENGTH];\n        entriesStream.readBytes(id, 0, id.length);\n        // nocommit: remove this null \"hack\", its because old rw test codecs cant properly impersonate\n        if (segmentID != null && !Arrays.equals(id, segmentID)) {\n          throw new CorruptIndexException(\"file mismatch, expected segment id=\" + StringHelper.idToString(segmentID) \n                                                                     + \", got=\" + StringHelper.idToString(id), entriesStream);\n        }\n        byte suffixLength = entriesStream.readByte();\n        if (suffixLength != 0) {\n          throw new CorruptIndexException(\"unexpected segment suffix, expected zero-length, got=\" + (suffixLength & 0xFF), entriesStream);\n        }\n      }\n      final int numEntries = entriesStream.readVInt();\n      mapping = new HashMap<>(numEntries);\n      for (int i = 0; i < numEntries; i++) {\n        final FileEntry fileEntry = new FileEntry();\n        final String id = entriesStream.readString();\n        FileEntry previous = mapping.put(id, fileEntry);\n        if (previous != null) {\n          throw new CorruptIndexException(\"Duplicate cfs entry id=\" + id + \" in CFS \", entriesStream);\n        }\n        fileEntry.offset = entriesStream.readLong();\n        fileEntry.length = entriesStream.readLong();\n      }\n      if (version >= CompoundFileWriter.VERSION_CHECKSUM) {\n        CodecUtil.checkFooter(entriesStream);\n      } else {\n        CodecUtil.checkEOF(entriesStream);\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(entriesStream);\n      } else {\n        IOUtils.closeWhileHandlingException(entriesStream);\n      }\n    }\n    return mapping;\n  }\n\n","sourceOld":"  /** Helper method that reads CFS entries from an input stream */\n  private final Map<String, FileEntry> readEntries(Directory dir, String name) throws IOException {\n    ChecksumIndexInput entriesStream = null;\n    Map<String,FileEntry> mapping = null;\n    boolean success = false;\n    try {\n      final String entriesFileName = IndexFileNames.segmentFileName(\n                                            IndexFileNames.stripExtension(name), \"\",\n                                             IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION);\n      entriesStream = dir.openChecksumInput(entriesFileName, IOContext.READONCE);\n      version = CodecUtil.checkHeader(entriesStream, CompoundFileWriter.ENTRY_CODEC, CompoundFileWriter.VERSION_START, CompoundFileWriter.VERSION_CURRENT);\n      if (version >= CompoundFileWriter.VERSION_SEGMENTHEADER) {\n        byte id[] = new byte[StringHelper.ID_LENGTH];\n        entriesStream.readBytes(id, 0, id.length);\n        // nocommit: remove this null \"hack\", its because old rw test codecs cant properly impersonate\n        if (segmentID != null && !Arrays.equals(id, segmentID)) {\n          throw new CorruptIndexException(\"file mismatch, expected segment id=\" + StringHelper.idToString(segmentID) \n                                                                     + \", got=\" + StringHelper.idToString(id), entriesStream);\n        }\n        byte suffixLength = entriesStream.readByte();\n        if (suffixLength != 0) {\n          throw new CorruptIndexException(\"unexpected segment suffix, expected zero-length, got=\" + (suffixLength & 0xFF), entriesStream);\n        }\n      }\n      final int numEntries = entriesStream.readVInt();\n      mapping = new HashMap<>(numEntries);\n      for (int i = 0; i < numEntries; i++) {\n        final FileEntry fileEntry = new FileEntry();\n        final String id = entriesStream.readString();\n        FileEntry previous = mapping.put(id, fileEntry);\n        if (previous != null) {\n          throw new CorruptIndexException(\"Duplicate cfs entry id=\" + id + \" in CFS \", entriesStream);\n        }\n        fileEntry.offset = entriesStream.readLong();\n        fileEntry.length = entriesStream.readLong();\n      }\n      if (version >= CompoundFileWriter.VERSION_CHECKSUM) {\n        CodecUtil.checkFooter(entriesStream);\n      } else {\n        CodecUtil.checkEOF(entriesStream);\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(entriesStream);\n      } else {\n        IOUtils.closeWhileHandlingException(entriesStream);\n      }\n    }\n    return mapping;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c550967fa12e5ac7ea0a4134124f18280da0ebb1","date":1412264295,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene50/CompoundFileDirectory#readEntries(Directory,String).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene50/CompoundFileDirectory#readEntries(Directory,String).mjava","sourceNew":"  /** Helper method that reads CFS entries from an input stream */\n  private final Map<String, FileEntry> readEntries(Directory dir, String name) throws IOException {\n    Map<String,FileEntry> mapping = null;\n    final String entriesFileName = IndexFileNames.segmentFileName(IndexFileNames.stripExtension(name), \"\",\n                                                                  IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION);\n    try (ChecksumIndexInput entriesStream = dir.openChecksumInput(entriesFileName, IOContext.READONCE)) {\n      Throwable priorE = null;\n      try {\n        version = CodecUtil.checkSegmentHeader(entriesStream, CompoundFileWriter.ENTRY_CODEC, \n                                                              CompoundFileWriter.VERSION_START, \n                                                              CompoundFileWriter.VERSION_CURRENT, segmentID, \"\");\n        final int numEntries = entriesStream.readVInt();\n        mapping = new HashMap<>(numEntries);\n        for (int i = 0; i < numEntries; i++) {\n          final FileEntry fileEntry = new FileEntry();\n          final String id = entriesStream.readString();\n          FileEntry previous = mapping.put(id, fileEntry);\n          if (previous != null) {\n            throw new CorruptIndexException(\"Duplicate cfs entry id=\" + id + \" in CFS \", entriesStream);\n          }\n          fileEntry.offset = entriesStream.readLong();\n          fileEntry.length = entriesStream.readLong();\n        }\n      } catch (Throwable exception) {\n        priorE = exception;\n      } finally {\n        CodecUtil.checkFooter(entriesStream, priorE);\n      }\n    }\n    return mapping;\n  }\n\n","sourceOld":"  /** Helper method that reads CFS entries from an input stream */\n  private final Map<String, FileEntry> readEntries(Directory dir, String name) throws IOException {\n    ChecksumIndexInput entriesStream = null;\n    Map<String,FileEntry> mapping = null;\n    boolean success = false;\n    try {\n      final String entriesFileName = IndexFileNames.segmentFileName(\n                                            IndexFileNames.stripExtension(name), \"\",\n                                             IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION);\n      entriesStream = dir.openChecksumInput(entriesFileName, IOContext.READONCE);\n      version = CodecUtil.checkHeader(entriesStream, CompoundFileWriter.ENTRY_CODEC, CompoundFileWriter.VERSION_START, CompoundFileWriter.VERSION_CURRENT);\n      if (version >= CompoundFileWriter.VERSION_SEGMENTHEADER) {\n        byte id[] = new byte[StringHelper.ID_LENGTH];\n        entriesStream.readBytes(id, 0, id.length);\n        // nocommit: remove this null \"hack\", its because old rw test codecs cant properly impersonate\n        if (segmentID != null && !Arrays.equals(id, segmentID)) {\n          throw new CorruptIndexException(\"file mismatch, expected segment id=\" + StringHelper.idToString(segmentID) \n                                                                     + \", got=\" + StringHelper.idToString(id), entriesStream);\n        }\n        byte suffixLength = entriesStream.readByte();\n        if (suffixLength != 0) {\n          throw new CorruptIndexException(\"unexpected segment suffix, expected zero-length, got=\" + (suffixLength & 0xFF), entriesStream);\n        }\n      }\n      final int numEntries = entriesStream.readVInt();\n      mapping = new HashMap<>(numEntries);\n      for (int i = 0; i < numEntries; i++) {\n        final FileEntry fileEntry = new FileEntry();\n        final String id = entriesStream.readString();\n        FileEntry previous = mapping.put(id, fileEntry);\n        if (previous != null) {\n          throw new CorruptIndexException(\"Duplicate cfs entry id=\" + id + \" in CFS \", entriesStream);\n        }\n        fileEntry.offset = entriesStream.readLong();\n        fileEntry.length = entriesStream.readLong();\n      }\n      if (version >= CompoundFileWriter.VERSION_CHECKSUM) {\n        CodecUtil.checkFooter(entriesStream);\n      } else {\n        CodecUtil.checkEOF(entriesStream);\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(entriesStream);\n      } else {\n        IOUtils.closeWhileHandlingException(entriesStream);\n      }\n    }\n    return mapping;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7f936b67ab4a872d22231aae4f63608e7f411071","date":1412266152,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50CompoundReader#readEntries(byte[],Directory,String).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene50/CompoundFileDirectory#readEntries(Directory,String).mjava","sourceNew":"  /** Helper method that reads CFS entries from an input stream */\n  private final Map<String, FileEntry> readEntries(byte[] segmentID, Directory dir, String name) throws IOException {\n    Map<String,FileEntry> mapping = null;\n    final String entriesFileName = IndexFileNames.segmentFileName(IndexFileNames.stripExtension(name), \"\",\n                                                                  IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION);\n    try (ChecksumIndexInput entriesStream = dir.openChecksumInput(entriesFileName, IOContext.READONCE)) {\n      Throwable priorE = null;\n      try {\n        version = CodecUtil.checkSegmentHeader(entriesStream, Lucene50CompoundFormat.ENTRY_CODEC, \n                                                              Lucene50CompoundFormat.VERSION_START, \n                                                              Lucene50CompoundFormat.VERSION_CURRENT, segmentID, \"\");\n        final int numEntries = entriesStream.readVInt();\n        mapping = new HashMap<>(numEntries);\n        for (int i = 0; i < numEntries; i++) {\n          final FileEntry fileEntry = new FileEntry();\n          final String id = entriesStream.readString();\n          FileEntry previous = mapping.put(id, fileEntry);\n          if (previous != null) {\n            throw new CorruptIndexException(\"Duplicate cfs entry id=\" + id + \" in CFS \", entriesStream);\n          }\n          fileEntry.offset = entriesStream.readLong();\n          fileEntry.length = entriesStream.readLong();\n        }\n      } catch (Throwable exception) {\n        priorE = exception;\n      } finally {\n        CodecUtil.checkFooter(entriesStream, priorE);\n      }\n    }\n    return Collections.unmodifiableMap(mapping);\n  }\n\n","sourceOld":"  /** Helper method that reads CFS entries from an input stream */\n  private final Map<String, FileEntry> readEntries(Directory dir, String name) throws IOException {\n    Map<String,FileEntry> mapping = null;\n    final String entriesFileName = IndexFileNames.segmentFileName(IndexFileNames.stripExtension(name), \"\",\n                                                                  IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION);\n    try (ChecksumIndexInput entriesStream = dir.openChecksumInput(entriesFileName, IOContext.READONCE)) {\n      Throwable priorE = null;\n      try {\n        version = CodecUtil.checkSegmentHeader(entriesStream, CompoundFileWriter.ENTRY_CODEC, \n                                                              CompoundFileWriter.VERSION_START, \n                                                              CompoundFileWriter.VERSION_CURRENT, segmentID, \"\");\n        final int numEntries = entriesStream.readVInt();\n        mapping = new HashMap<>(numEntries);\n        for (int i = 0; i < numEntries; i++) {\n          final FileEntry fileEntry = new FileEntry();\n          final String id = entriesStream.readString();\n          FileEntry previous = mapping.put(id, fileEntry);\n          if (previous != null) {\n            throw new CorruptIndexException(\"Duplicate cfs entry id=\" + id + \" in CFS \", entriesStream);\n          }\n          fileEntry.offset = entriesStream.readLong();\n          fileEntry.length = entriesStream.readLong();\n        }\n      } catch (Throwable exception) {\n        priorE = exception;\n      } finally {\n        CodecUtil.checkFooter(entriesStream, priorE);\n      }\n    }\n    return mapping;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"989d940c4bf402188f4f0ae13736836885227383":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"7f936b67ab4a872d22231aae4f63608e7f411071":["c550967fa12e5ac7ea0a4134124f18280da0ebb1"],"c550967fa12e5ac7ea0a4134124f18280da0ebb1":["989d940c4bf402188f4f0ae13736836885227383"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"989d940c4bf402188f4f0ae13736836885227383":["c550967fa12e5ac7ea0a4134124f18280da0ebb1"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["989d940c4bf402188f4f0ae13736836885227383","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"7f936b67ab4a872d22231aae4f63608e7f411071":[],"c550967fa12e5ac7ea0a4134124f18280da0ebb1":["7f936b67ab4a872d22231aae4f63608e7f411071"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["7f936b67ab4a872d22231aae4f63608e7f411071","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}