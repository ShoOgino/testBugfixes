{"path":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random);\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    if (Codec.getDefault().getName().equals(\"Lucene40\")) {\n      // pulsing etc are not implemented\n      iwc.setCodec(_TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat()));\n    }\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term), true);\n      int doc;\n      while((doc = dp.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random, 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"), true);\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random);\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    if (Codec.getDefault().getName().equals(\"Lucene40\")) {\n      // pulsing etc are not implemented\n      iwc.setCodec(_TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat()));\n    }\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term), true);\n      int doc;\n      while((doc = dp.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random, 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"), true);\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f08557cdb6c60ac7b88a9342c983a20cd236e74f","date":1330954480,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random);\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    if (Codec.getDefault().getName().equals(\"Lucene40\")) {\n      // pulsing etc are not implemented\n      iwc.setCodec(_TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat()));\n    }\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term), true);\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random, 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"), true);\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random);\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    if (Codec.getDefault().getName().equals(\"Lucene40\")) {\n      // pulsing etc are not implemented\n      iwc.setCodec(_TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat()));\n    }\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term), true);\n      int doc;\n      while((doc = dp.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random, 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"), true);\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","date":1331075828,"type":3,"author":"Ryan McKinley","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random);\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    if (Codec.getDefault().getName().equals(\"Lucene40\")) {\n      // pulsing etc are not implemented\n      iwc.setCodec(_TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat()));\n    }\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term), true);\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random, 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"), true);\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random);\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    if (Codec.getDefault().getName().equals(\"Lucene40\")) {\n      // pulsing etc are not implemented\n      iwc.setCodec(_TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat()));\n    }\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term), true);\n      int doc;\n      while((doc = dp.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random, 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"), true);\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"06e9e87a586e724774d060a39c60ed1178637952","date":1331555866,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random);\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    if (Codec.getDefault().getName().equals(\"Lucene40\")) {\n      // pulsing etc are not implemented\n      if (random.nextBoolean()) {\n        iwc.setCodec(_TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat()));\n      } else {\n        iwc.setCodec(_TestUtil.alwaysPostingsFormat(new MemoryPostingsFormat()));\n      }\n    }\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term), true);\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random, 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"), true);\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random);\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    if (Codec.getDefault().getName().equals(\"Lucene40\")) {\n      // pulsing etc are not implemented\n      iwc.setCodec(_TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat()));\n    }\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term), true);\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random, 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"), true);\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"38e3b736c7ca086d61b7dbb841c905ee115490da","date":1331657018,"type":3,"author":"Ryan McKinley","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random);\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    if (Codec.getDefault().getName().equals(\"Lucene40\")) {\n      // pulsing etc are not implemented\n      if (random.nextBoolean()) {\n        iwc.setCodec(_TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat()));\n      } else {\n        iwc.setCodec(_TestUtil.alwaysPostingsFormat(new MemoryPostingsFormat()));\n      }\n    }\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term), true);\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random, 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"), true);\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random);\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    if (Codec.getDefault().getName().equals(\"Lucene40\")) {\n      // pulsing etc are not implemented\n      iwc.setCodec(_TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat()));\n    }\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term), true);\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random, 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"), true);\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    if (Codec.getDefault().getName().equals(\"Lucene40\")) {\n      // pulsing etc are not implemented\n      if (random().nextBoolean()) {\n        iwc.setCodec(_TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat()));\n      } else {\n        iwc.setCodec(_TestUtil.alwaysPostingsFormat(new MemoryPostingsFormat()));\n      }\n    }\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term), true);\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random(), 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"), true);\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random);\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    if (Codec.getDefault().getName().equals(\"Lucene40\")) {\n      // pulsing etc are not implemented\n      if (random.nextBoolean()) {\n        iwc.setCodec(_TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat()));\n      } else {\n        iwc.setCodec(_TestUtil.alwaysPostingsFormat(new MemoryPostingsFormat()));\n      }\n    }\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term), true);\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random, 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"), true);\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7424161ac990ef8f959f09ee516148e4d12c48bc","date":1336236796,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    if (Codec.getDefault().getName().equals(\"Lucene40\")) {\n      // sep etc are not implemented\n      switch(random().nextInt(4)) {\n        case 0: iwc.setCodec(_TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat())); break;\n        case 1: iwc.setCodec(_TestUtil.alwaysPostingsFormat(new MemoryPostingsFormat())); break;\n        case 2: iwc.setCodec(_TestUtil.alwaysPostingsFormat(\n            new Pulsing40PostingsFormat(_TestUtil.nextInt(random(), 1, 3)))); break;\n        case 3: iwc.setCodec(_TestUtil.alwaysPostingsFormat(new NestedPulsingPostingsFormat())); break;\n      }\n    }\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term), true);\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random(), 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"), true);\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    if (Codec.getDefault().getName().equals(\"Lucene40\")) {\n      // pulsing etc are not implemented\n      if (random().nextBoolean()) {\n        iwc.setCodec(_TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat()));\n      } else {\n        iwc.setCodec(_TestUtil.alwaysPostingsFormat(new MemoryPostingsFormat()));\n      }\n    }\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term), true);\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random(), 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"), true);\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"74d3eb76711bd95b2051ecfcf7a10ca9069577bc","date":1338324080,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term), true);\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random(), 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"), true);\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    if (Codec.getDefault().getName().equals(\"Lucene40\")) {\n      // sep etc are not implemented\n      switch(random().nextInt(4)) {\n        case 0: iwc.setCodec(_TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat())); break;\n        case 1: iwc.setCodec(_TestUtil.alwaysPostingsFormat(new MemoryPostingsFormat())); break;\n        case 2: iwc.setCodec(_TestUtil.alwaysPostingsFormat(\n            new Pulsing40PostingsFormat(_TestUtil.nextInt(random(), 1, 3)))); break;\n        case 3: iwc.setCodec(_TestUtil.alwaysPostingsFormat(new NestedPulsingPostingsFormat())); break;\n      }\n    }\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term), true);\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random(), 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"), true);\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term), true);\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random(), 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"), true);\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term), true);\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random(), 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"), true);\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":["a44b232879361a7ace3520b5b313094a9a35e044"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"322360ac5185a8446d3e0b530b2068bef67cd3d5","date":1343669494,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random(), 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term), true);\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random(), 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"), true);\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"02331260bb246364779cb6f04919ca47900d01bb","date":1343749884,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random(), 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), 0);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random(), 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":["a44b232879361a7ace3520b5b313094a9a35e044"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","date":1343768312,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random(), 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), 0);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term), true);\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random(), 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"), true);\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random(), 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), 0);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term), true);\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random(), 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"), true);\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), false);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552","date":1344797146,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertNotNull(dp.getPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random(), 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertNotNull(dp.getPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), 0);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random(), 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), 0);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c7869f64c874ebf7f317d22c00baf2b6857797a6","date":1344856617,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertNotNull(dp.getPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random(), 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertNotNull(dp.getPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), 0);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random(), 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), 0);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9","date":1344867506,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertNotNull(dp.getPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random(), 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertNotNull(dp.getPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), 0);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertTrue(dp.hasPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random(), 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertTrue(dp.hasPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), 0);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertNotNull(dp.getPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = TestUtil.nextInt(random(), 100, Math.min(numDocs - 1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertNotNull(dp.getPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), 0);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertNotNull(dp.getPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = _TestUtil.nextInt(random(), 100, Math.min(numDocs-1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertNotNull(dp.getPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), 0);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.shutdown();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertNotNull(dp.getPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = TestUtil.nextInt(random(), 100, Math.min(numDocs - 1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertNotNull(dp.getPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), 0);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertNotNull(dp.getPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = TestUtil.nextInt(random(), 100, Math.min(numDocs - 1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertNotNull(dp.getPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), 0);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.shutdown();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertNotNull(dp.getPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = TestUtil.nextInt(random(), 100, Math.min(numDocs - 1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertNotNull(dp.getPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), 0);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.shutdown();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertNotNull(dp.getPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = TestUtil.nextInt(random(), 100, Math.min(numDocs - 1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertNotNull(dp.getPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), 0);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertNotNull(dp.getPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = TestUtil.nextInt(random(), 100, Math.min(numDocs - 1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertNotNull(dp.getPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), 0);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.shutdown();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertNotNull(dp.getPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = TestUtil.nextInt(random(), 100, Math.min(numDocs - 1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertNotNull(dp.getPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), 0);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2bb2842e561df4e8e9ad89010605fc86ac265465","date":1414768208,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertNotNull(dp.getPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = TestUtil.nextInt(random(), 100, Math.min(numDocs - 1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertNotNull(dp.getPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), 0);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertNotNull(dp.getPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = TestUtil.nextInt(random(), 100, Math.min(numDocs - 1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertNotNull(dp.getPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), 0);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      PostingsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertNotNull(dp.getPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = TestUtil.nextInt(random(), 100, Math.min(numDocs - 1, 999));\n      PostingsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertNotNull(dp.getPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      PostingsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), 0);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertNotNull(dp.getPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = TestUtil.nextInt(random(), 100, Math.min(numDocs - 1, 999));\n      DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertNotNull(dp.getPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      DocsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), 0);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      PostingsEnum dp = MultiFields.getTermPositionsEnum(reader, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertNotNull(dp.getPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = TestUtil.nextInt(random(), 100, Math.min(numDocs - 1, 999));\n      PostingsEnum dp = MultiFields.getTermPositionsEnum(reader, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertNotNull(dp.getPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      PostingsEnum dp = MultiFields.getTermDocsEnum(reader, \"id\", new BytesRef(\"\" + i), 0);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      PostingsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertNotNull(dp.getPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = TestUtil.nextInt(random(), 100, Math.min(numDocs - 1, 999));\n      PostingsEnum dp = MultiFields.getTermPositionsEnum(reader, null, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertNotNull(dp.getPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      PostingsEnum dp = MultiFields.getTermDocsEnum(reader, null, \"id\", new BytesRef(\"\" + i), 0);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"04e775de416dd2d8067b10db1c8af975a1d5017e","date":1539906554,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#doTestNumbers(boolean).mjava","sourceNew":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      PostingsEnum dp = MultiTerms.getTermPostingsEnum(reader, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertNotNull(dp.getPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = TestUtil.nextInt(random(), 100, Math.min(numDocs - 1, 999));\n      PostingsEnum dp = MultiTerms.getTermPostingsEnum(reader, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertNotNull(dp.getPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      PostingsEnum dp = MultiTerms.getTermPostingsEnum(reader, \"id\", new BytesRef(\"\" + i), 0);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void doTestNumbers(boolean withPayloads) throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = withPayloads ? new MockPayloadAnalyzer() : new MockAnalyzer(random());\n    iwc = newIndexWriterConfig(analyzer);\n    iwc.setMergePolicy(newLogMergePolicy()); // will rely on docids a bit for skipping\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n    \n    FieldType ft = new FieldType(TextField.TYPE_STORED);\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n    \n    int numDocs = atLeast(500);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"numbers\", English.intToEnglish(i), ft));\n      doc.add(new Field(\"oddeven\", (i % 2) == 0 ? \"even\" : \"odd\", ft));\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    IndexReader reader = w.getReader();\n    w.close();\n    \n    String terms[] = { \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"hundred\" };\n    \n    for (String term : terms) {\n      PostingsEnum dp = MultiFields.getTermPositionsEnum(reader, \"numbers\", new BytesRef(term));\n      int doc;\n      while((doc = dp.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        int freq = dp.freq();\n        for (int i = 0; i < freq; i++) {\n          dp.nextPosition();\n          int start = dp.startOffset();\n          assert start >= 0;\n          int end = dp.endOffset();\n          assert end >= 0 && end >= start;\n          // check that the offsets correspond to the term in the src text\n          assertTrue(storedNumbers.substring(start, end).equals(term));\n          if (withPayloads) {\n            // check that we have a payload and it starts with \"pos\"\n            assertNotNull(dp.getPayload());\n            BytesRef payload = dp.getPayload();\n            assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n          } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n        }\n      }\n    }\n    \n    // check we can skip correctly\n    int numSkippingTests = atLeast(50);\n    \n    for (int j = 0; j < numSkippingTests; j++) {\n      int num = TestUtil.nextInt(random(), 100, Math.min(numDocs - 1, 999));\n      PostingsEnum dp = MultiFields.getTermPositionsEnum(reader, \"numbers\", new BytesRef(\"hundred\"));\n      int doc = dp.advance(num);\n      assertEquals(num, doc);\n      int freq = dp.freq();\n      for (int i = 0; i < freq; i++) {\n        String storedNumbers = reader.document(doc).get(\"numbers\");\n        dp.nextPosition();\n        int start = dp.startOffset();\n        assert start >= 0;\n        int end = dp.endOffset();\n        assert end >= 0 && end >= start;\n        // check that the offsets correspond to the term in the src text\n        assertTrue(storedNumbers.substring(start, end).equals(\"hundred\"));\n        if (withPayloads) {\n          // check that we have a payload and it starts with \"pos\"\n          assertNotNull(dp.getPayload());\n          BytesRef payload = dp.getPayload();\n          assertTrue(payload.utf8ToString().startsWith(\"pos:\"));\n        } // note: withPayloads=false doesnt necessarily mean we dont have them from MockAnalyzer!\n      }\n    }\n    \n    // check that other fields (without offsets) work correctly\n    \n    for (int i = 0; i < numDocs; i++) {\n      PostingsEnum dp = MultiFields.getTermDocsEnum(reader, \"id\", new BytesRef(\"\" + i), 0);\n      assertEquals(i, dp.nextDoc());\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, dp.nextDoc());\n    }\n    \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"2bb2842e561df4e8e9ad89010605fc86ac265465":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["74d3eb76711bd95b2051ecfcf7a10ca9069577bc"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552"],"2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552":["02331260bb246364779cb6f04919ca47900d01bb"],"7424161ac990ef8f959f09ee516148e4d12c48bc":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["6613659748fe4411a7dcf85266e55db1f95f7315"],"322360ac5185a8446d3e0b530b2068bef67cd3d5":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9":["d6f074e73200c07d54f242d3880a8da5a35ff97b","2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552"],"38e3b736c7ca086d61b7dbb841c905ee115490da":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","06e9e87a586e724774d060a39c60ed1178637952"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"6613659748fe4411a7dcf85266e55db1f95f7315":["2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552"],"f08557cdb6c60ac7b88a9342c983a20cd236e74f":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"51f5280f31484820499077f41fcdfe92d527d9dc":["2bb2842e561df4e8e9ad89010605fc86ac265465"],"74d3eb76711bd95b2051ecfcf7a10ca9069577bc":["7424161ac990ef8f959f09ee516148e4d12c48bc"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["51f5280f31484820499077f41fcdfe92d527d9dc"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["04f07771a2a7dd3a395700665ed839c3dae2def2","02331260bb246364779cb6f04919ca47900d01bb"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["04f07771a2a7dd3a395700665ed839c3dae2def2","02331260bb246364779cb6f04919ca47900d01bb"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"06e9e87a586e724774d060a39c60ed1178637952":["f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["06e9e87a586e724774d060a39c60ed1178637952"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["04e775de416dd2d8067b10db1c8af975a1d5017e"],"02331260bb246364779cb6f04919ca47900d01bb":["322360ac5185a8446d3e0b530b2068bef67cd3d5"]},"commit2Childs":{"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":["38e3b736c7ca086d61b7dbb841c905ee115490da"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"2bb2842e561df4e8e9ad89010605fc86ac265465":["51f5280f31484820499077f41fcdfe92d527d9dc"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["322360ac5185a8446d3e0b530b2068bef67cd3d5","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":[],"2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552":["c7869f64c874ebf7f317d22c00baf2b6857797a6","d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9","6613659748fe4411a7dcf85266e55db1f95f7315"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"7424161ac990ef8f959f09ee516148e4d12c48bc":["74d3eb76711bd95b2051ecfcf7a10ca9069577bc"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"322360ac5185a8446d3e0b530b2068bef67cd3d5":["02331260bb246364779cb6f04919ca47900d01bb"],"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9":[],"38e3b736c7ca086d61b7dbb841c905ee115490da":[],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"f08557cdb6c60ac7b88a9342c983a20cd236e74f":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","06e9e87a586e724774d060a39c60ed1178637952"],"6613659748fe4411a7dcf85266e55db1f95f7315":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"74d3eb76711bd95b2051ecfcf7a10ca9069577bc":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"51f5280f31484820499077f41fcdfe92d527d9dc":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["04e775de416dd2d8067b10db1c8af975a1d5017e"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["c7869f64c874ebf7f317d22c00baf2b6857797a6"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["2bb2842e561df4e8e9ad89010605fc86ac265465"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9"],"06e9e87a586e724774d060a39c60ed1178637952":["38e3b736c7ca086d61b7dbb841c905ee115490da","629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["7424161ac990ef8f959f09ee516148e4d12c48bc"],"02331260bb246364779cb6f04919ca47900d01bb":["2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c7869f64c874ebf7f317d22c00baf2b6857797a6","d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9","38e3b736c7ca086d61b7dbb841c905ee115490da","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}