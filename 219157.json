{"path":"lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader#getReindexerSameDVField(Path,AtomicLong,AtomicLong).mjava","commits":[{"id":"4d9aa91d3fdd25528bac3b2e6115d54fc2f28753","date":1416999434,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader#getReindexerSameDVField(Path,AtomicLong,AtomicLong).mjava","pathOld":"/dev/null","sourceNew":"  /** Schema change by adding changing how the same \"number\" DV field is indexed. */\n  private ReindexingReader getReindexerSameDVField(Path root, final AtomicLong currentSchemaGen, final AtomicLong mergingSchemaGen) throws IOException {\n    return new ReindexingReader(root) {\n      @Override\n      protected IndexWriterConfig getIndexWriterConfig() throws IOException {\n        return newIndexWriterConfig();\n      }\n\n      @Override\n      protected Directory openDirectory(Path path) throws IOException {\n        MockDirectoryWrapper dir = newMockFSDirectory(path);\n        dir.setUseSlowOpenClosers(false);\n        dir.setThrottling(Throttling.NEVER);\n        return dir;\n      }\n\n      @Override\n      protected void reindex(long oldSchemaGen, long newSchemaGen, LeafReader reader, Directory parallelDir) throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n\n        // The order of our docIDs must precisely matching incoming reader:\n        iwc.setMergePolicy(new LogByteSizeMergePolicy());\n        IndexWriter w = new IndexWriter(parallelDir, iwc);\n        int maxDoc = reader.maxDoc();\n\n        if (oldSchemaGen <= 0) {\n          // Must slowly parse the stored field into a new doc values field:\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            StoredDocument oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*value));\n            newDoc.add(new LongField(\"number\", value, Field.Store.NO));\n            w.addDocument(newDoc);\n          }\n        } else {\n          // Just carry over doc values from previous field:\n          NumericDocValues oldValues = reader.getNumericDocValues(\"number\");\n          assertNotNull(\"oldSchemaGen=\" + oldSchemaGen, oldValues);\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            StoredDocument oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*(oldValues.get(i)/oldSchemaGen)));\n            w.addDocument(newDoc);\n          }\n        }\n\n        if (random().nextBoolean()) {\n          w.forceMerge(1);\n        }\n\n        w.close();\n      }\n\n      @Override\n      protected long getCurrentSchemaGen() {\n        return currentSchemaGen.get();\n      }\n\n      @Override\n      protected long getMergingSchemaGen() {\n        return mergingSchemaGen.get();\n      }\n\n      @Override\n      protected void checkParallelReader(LeafReader r, LeafReader parR, long schemaGen) throws IOException {\n        if (DEBUG) System.out.println(Thread.currentThread().getName() + \": TEST: now check parallel number DVs r=\" + r + \" parR=\" + parR);\n        NumericDocValues numbers = parR.getNumericDocValues(\"numbers\");\n        if (numbers == null) {\n          return;\n        }\n        int maxDoc = r.maxDoc();\n        boolean failed = false;\n        for(int i=0;i<maxDoc;i++) {\n          StoredDocument oldDoc = r.document(i);\n          long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n          value *= schemaGen;\n          if (value != numbers.get(i)) {\n            System.out.println(\"FAIL: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i) + \" numbers=\" + numbers);\n            failed = true;\n          } else if (failed) {\n            System.out.println(\"OK: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i));\n          }\n        }\n        assertFalse(\"FAILED r=\" + r, failed);\n      }\n    };\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f6450c8e5ae2fc3df8d5de3bce074cb72847ae23","date":1417512615,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader#getReindexerSameDVField(Path,AtomicLong,AtomicLong).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader#getReindexerSameDVField(Path,AtomicLong,AtomicLong).mjava","sourceNew":"  /** Schema change by adding changing how the same \"number\" DV field is indexed. */\n  private ReindexingReader getReindexerSameDVField(Path root, final AtomicLong currentSchemaGen, final AtomicLong mergingSchemaGen) throws IOException {\n    return new ReindexingReader(root) {\n      @Override\n      protected IndexWriterConfig getIndexWriterConfig() throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n        TieredMergePolicy tmp = new TieredMergePolicy();\n        // We write tiny docs, so we need tiny floor to avoid O(N^2) merging:\n        tmp.setFloorSegmentMB(.01);\n        iwc.setMergePolicy(tmp);\n        return iwc;\n      }\n\n      @Override\n      protected Directory openDirectory(Path path) throws IOException {\n        MockDirectoryWrapper dir = newMockFSDirectory(path);\n        dir.setUseSlowOpenClosers(false);\n        dir.setThrottling(Throttling.NEVER);\n        return dir;\n      }\n\n      @Override\n      protected void reindex(long oldSchemaGen, long newSchemaGen, LeafReader reader, Directory parallelDir) throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n\n        // The order of our docIDs must precisely matching incoming reader:\n        iwc.setMergePolicy(new LogByteSizeMergePolicy());\n        IndexWriter w = new IndexWriter(parallelDir, iwc);\n        int maxDoc = reader.maxDoc();\n\n        if (oldSchemaGen <= 0) {\n          // Must slowly parse the stored field into a new doc values field:\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            StoredDocument oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*value));\n            newDoc.add(new LongField(\"number\", value, Field.Store.NO));\n            w.addDocument(newDoc);\n          }\n        } else {\n          // Just carry over doc values from previous field:\n          NumericDocValues oldValues = reader.getNumericDocValues(\"number\");\n          assertNotNull(\"oldSchemaGen=\" + oldSchemaGen, oldValues);\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            StoredDocument oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*(oldValues.get(i)/oldSchemaGen)));\n            w.addDocument(newDoc);\n          }\n        }\n\n        if (random().nextBoolean()) {\n          w.forceMerge(1);\n        }\n\n        w.close();\n      }\n\n      @Override\n      protected long getCurrentSchemaGen() {\n        return currentSchemaGen.get();\n      }\n\n      @Override\n      protected long getMergingSchemaGen() {\n        return mergingSchemaGen.get();\n      }\n\n      @Override\n      protected void checkParallelReader(LeafReader r, LeafReader parR, long schemaGen) throws IOException {\n        if (DEBUG) System.out.println(Thread.currentThread().getName() + \": TEST: now check parallel number DVs r=\" + r + \" parR=\" + parR);\n        NumericDocValues numbers = parR.getNumericDocValues(\"numbers\");\n        if (numbers == null) {\n          return;\n        }\n        int maxDoc = r.maxDoc();\n        boolean failed = false;\n        for(int i=0;i<maxDoc;i++) {\n          StoredDocument oldDoc = r.document(i);\n          long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n          value *= schemaGen;\n          if (value != numbers.get(i)) {\n            System.out.println(\"FAIL: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i) + \" numbers=\" + numbers);\n            failed = true;\n          } else if (failed) {\n            System.out.println(\"OK: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i));\n          }\n        }\n        assertFalse(\"FAILED r=\" + r, failed);\n      }\n    };\n  }\n\n","sourceOld":"  /** Schema change by adding changing how the same \"number\" DV field is indexed. */\n  private ReindexingReader getReindexerSameDVField(Path root, final AtomicLong currentSchemaGen, final AtomicLong mergingSchemaGen) throws IOException {\n    return new ReindexingReader(root) {\n      @Override\n      protected IndexWriterConfig getIndexWriterConfig() throws IOException {\n        return newIndexWriterConfig();\n      }\n\n      @Override\n      protected Directory openDirectory(Path path) throws IOException {\n        MockDirectoryWrapper dir = newMockFSDirectory(path);\n        dir.setUseSlowOpenClosers(false);\n        dir.setThrottling(Throttling.NEVER);\n        return dir;\n      }\n\n      @Override\n      protected void reindex(long oldSchemaGen, long newSchemaGen, LeafReader reader, Directory parallelDir) throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n\n        // The order of our docIDs must precisely matching incoming reader:\n        iwc.setMergePolicy(new LogByteSizeMergePolicy());\n        IndexWriter w = new IndexWriter(parallelDir, iwc);\n        int maxDoc = reader.maxDoc();\n\n        if (oldSchemaGen <= 0) {\n          // Must slowly parse the stored field into a new doc values field:\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            StoredDocument oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*value));\n            newDoc.add(new LongField(\"number\", value, Field.Store.NO));\n            w.addDocument(newDoc);\n          }\n        } else {\n          // Just carry over doc values from previous field:\n          NumericDocValues oldValues = reader.getNumericDocValues(\"number\");\n          assertNotNull(\"oldSchemaGen=\" + oldSchemaGen, oldValues);\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            StoredDocument oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*(oldValues.get(i)/oldSchemaGen)));\n            w.addDocument(newDoc);\n          }\n        }\n\n        if (random().nextBoolean()) {\n          w.forceMerge(1);\n        }\n\n        w.close();\n      }\n\n      @Override\n      protected long getCurrentSchemaGen() {\n        return currentSchemaGen.get();\n      }\n\n      @Override\n      protected long getMergingSchemaGen() {\n        return mergingSchemaGen.get();\n      }\n\n      @Override\n      protected void checkParallelReader(LeafReader r, LeafReader parR, long schemaGen) throws IOException {\n        if (DEBUG) System.out.println(Thread.currentThread().getName() + \": TEST: now check parallel number DVs r=\" + r + \" parR=\" + parR);\n        NumericDocValues numbers = parR.getNumericDocValues(\"numbers\");\n        if (numbers == null) {\n          return;\n        }\n        int maxDoc = r.maxDoc();\n        boolean failed = false;\n        for(int i=0;i<maxDoc;i++) {\n          StoredDocument oldDoc = r.document(i);\n          long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n          value *= schemaGen;\n          if (value != numbers.get(i)) {\n            System.out.println(\"FAIL: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i) + \" numbers=\" + numbers);\n            failed = true;\n          } else if (failed) {\n            System.out.println(\"OK: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i));\n          }\n        }\n        assertFalse(\"FAILED r=\" + r, failed);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"17afc0a594655cd4e902b4fdb9a0238f302f264c","date":1420921879,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader#getReindexerSameDVField(Path,AtomicLong,AtomicLong).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader#getReindexerSameDVField(Path,AtomicLong,AtomicLong).mjava","sourceNew":"  /** Schema change by adding changing how the same \"number\" DV field is indexed. */\n  private ReindexingReader getReindexerSameDVField(Path root, final AtomicLong currentSchemaGen, final AtomicLong mergingSchemaGen) throws IOException {\n    return new ReindexingReader(root) {\n      @Override\n      protected IndexWriterConfig getIndexWriterConfig() throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n        TieredMergePolicy tmp = new TieredMergePolicy();\n        // We write tiny docs, so we need tiny floor to avoid O(N^2) merging:\n        tmp.setFloorSegmentMB(.01);\n        iwc.setMergePolicy(tmp);\n        if (TEST_NIGHTLY) {\n          // during nightly tests, we might use too many files if we arent careful\n          iwc.setUseCompoundFile(true);\n        }\n        return iwc;\n      }\n\n      @Override\n      protected Directory openDirectory(Path path) throws IOException {\n        MockDirectoryWrapper dir = newMockFSDirectory(path);\n        dir.setUseSlowOpenClosers(false);\n        dir.setThrottling(Throttling.NEVER);\n        return dir;\n      }\n\n      @Override\n      protected void reindex(long oldSchemaGen, long newSchemaGen, LeafReader reader, Directory parallelDir) throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n\n        // The order of our docIDs must precisely matching incoming reader:\n        iwc.setMergePolicy(new LogByteSizeMergePolicy());\n        IndexWriter w = new IndexWriter(parallelDir, iwc);\n        int maxDoc = reader.maxDoc();\n\n        if (oldSchemaGen <= 0) {\n          // Must slowly parse the stored field into a new doc values field:\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            StoredDocument oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*value));\n            newDoc.add(new LongField(\"number\", value, Field.Store.NO));\n            w.addDocument(newDoc);\n          }\n        } else {\n          // Just carry over doc values from previous field:\n          NumericDocValues oldValues = reader.getNumericDocValues(\"number\");\n          assertNotNull(\"oldSchemaGen=\" + oldSchemaGen, oldValues);\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            StoredDocument oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*(oldValues.get(i)/oldSchemaGen)));\n            w.addDocument(newDoc);\n          }\n        }\n\n        if (random().nextBoolean()) {\n          w.forceMerge(1);\n        }\n\n        w.close();\n      }\n\n      @Override\n      protected long getCurrentSchemaGen() {\n        return currentSchemaGen.get();\n      }\n\n      @Override\n      protected long getMergingSchemaGen() {\n        return mergingSchemaGen.get();\n      }\n\n      @Override\n      protected void checkParallelReader(LeafReader r, LeafReader parR, long schemaGen) throws IOException {\n        if (DEBUG) System.out.println(Thread.currentThread().getName() + \": TEST: now check parallel number DVs r=\" + r + \" parR=\" + parR);\n        NumericDocValues numbers = parR.getNumericDocValues(\"numbers\");\n        if (numbers == null) {\n          return;\n        }\n        int maxDoc = r.maxDoc();\n        boolean failed = false;\n        for(int i=0;i<maxDoc;i++) {\n          StoredDocument oldDoc = r.document(i);\n          long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n          value *= schemaGen;\n          if (value != numbers.get(i)) {\n            System.out.println(\"FAIL: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i) + \" numbers=\" + numbers);\n            failed = true;\n          } else if (failed) {\n            System.out.println(\"OK: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i));\n          }\n        }\n        assertFalse(\"FAILED r=\" + r, failed);\n      }\n    };\n  }\n\n","sourceOld":"  /** Schema change by adding changing how the same \"number\" DV field is indexed. */\n  private ReindexingReader getReindexerSameDVField(Path root, final AtomicLong currentSchemaGen, final AtomicLong mergingSchemaGen) throws IOException {\n    return new ReindexingReader(root) {\n      @Override\n      protected IndexWriterConfig getIndexWriterConfig() throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n        TieredMergePolicy tmp = new TieredMergePolicy();\n        // We write tiny docs, so we need tiny floor to avoid O(N^2) merging:\n        tmp.setFloorSegmentMB(.01);\n        iwc.setMergePolicy(tmp);\n        return iwc;\n      }\n\n      @Override\n      protected Directory openDirectory(Path path) throws IOException {\n        MockDirectoryWrapper dir = newMockFSDirectory(path);\n        dir.setUseSlowOpenClosers(false);\n        dir.setThrottling(Throttling.NEVER);\n        return dir;\n      }\n\n      @Override\n      protected void reindex(long oldSchemaGen, long newSchemaGen, LeafReader reader, Directory parallelDir) throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n\n        // The order of our docIDs must precisely matching incoming reader:\n        iwc.setMergePolicy(new LogByteSizeMergePolicy());\n        IndexWriter w = new IndexWriter(parallelDir, iwc);\n        int maxDoc = reader.maxDoc();\n\n        if (oldSchemaGen <= 0) {\n          // Must slowly parse the stored field into a new doc values field:\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            StoredDocument oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*value));\n            newDoc.add(new LongField(\"number\", value, Field.Store.NO));\n            w.addDocument(newDoc);\n          }\n        } else {\n          // Just carry over doc values from previous field:\n          NumericDocValues oldValues = reader.getNumericDocValues(\"number\");\n          assertNotNull(\"oldSchemaGen=\" + oldSchemaGen, oldValues);\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            StoredDocument oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*(oldValues.get(i)/oldSchemaGen)));\n            w.addDocument(newDoc);\n          }\n        }\n\n        if (random().nextBoolean()) {\n          w.forceMerge(1);\n        }\n\n        w.close();\n      }\n\n      @Override\n      protected long getCurrentSchemaGen() {\n        return currentSchemaGen.get();\n      }\n\n      @Override\n      protected long getMergingSchemaGen() {\n        return mergingSchemaGen.get();\n      }\n\n      @Override\n      protected void checkParallelReader(LeafReader r, LeafReader parR, long schemaGen) throws IOException {\n        if (DEBUG) System.out.println(Thread.currentThread().getName() + \": TEST: now check parallel number DVs r=\" + r + \" parR=\" + parR);\n        NumericDocValues numbers = parR.getNumericDocValues(\"numbers\");\n        if (numbers == null) {\n          return;\n        }\n        int maxDoc = r.maxDoc();\n        boolean failed = false;\n        for(int i=0;i<maxDoc;i++) {\n          StoredDocument oldDoc = r.document(i);\n          long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n          value *= schemaGen;\n          if (value != numbers.get(i)) {\n            System.out.println(\"FAIL: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i) + \" numbers=\" + numbers);\n            failed = true;\n          } else if (failed) {\n            System.out.println(\"OK: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i));\n          }\n        }\n        assertFalse(\"FAILED r=\" + r, failed);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"770342641f7b505eaa8dccdc666158bff2419109","date":1449868421,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader#getReindexerSameDVField(Path,AtomicLong,AtomicLong).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader#getReindexerSameDVField(Path,AtomicLong,AtomicLong).mjava","sourceNew":"  /** Schema change by adding changing how the same \"number\" DV field is indexed. */\n  private ReindexingReader getReindexerSameDVField(Path root, final AtomicLong currentSchemaGen, final AtomicLong mergingSchemaGen) throws IOException {\n    return new ReindexingReader(root) {\n      @Override\n      protected IndexWriterConfig getIndexWriterConfig() throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n        TieredMergePolicy tmp = new TieredMergePolicy();\n        // We write tiny docs, so we need tiny floor to avoid O(N^2) merging:\n        tmp.setFloorSegmentMB(.01);\n        iwc.setMergePolicy(tmp);\n        if (TEST_NIGHTLY) {\n          // during nightly tests, we might use too many files if we arent careful\n          iwc.setUseCompoundFile(true);\n        }\n        return iwc;\n      }\n\n      @Override\n      protected Directory openDirectory(Path path) throws IOException {\n        MockDirectoryWrapper dir = newMockFSDirectory(path);\n        dir.setUseSlowOpenClosers(false);\n        dir.setThrottling(Throttling.NEVER);\n        return dir;\n      }\n\n      @Override\n      protected void reindex(long oldSchemaGen, long newSchemaGen, LeafReader reader, Directory parallelDir) throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n\n        // The order of our docIDs must precisely matching incoming reader:\n        iwc.setMergePolicy(new LogByteSizeMergePolicy());\n        IndexWriter w = new IndexWriter(parallelDir, iwc);\n        int maxDoc = reader.maxDoc();\n\n        if (oldSchemaGen <= 0) {\n          // Must slowly parse the stored field into a new doc values field:\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            StoredDocument oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*value));\n            newDoc.add(new DimensionalLongField(\"number\", value));\n            w.addDocument(newDoc);\n          }\n        } else {\n          // Just carry over doc values from previous field:\n          NumericDocValues oldValues = reader.getNumericDocValues(\"number\");\n          assertNotNull(\"oldSchemaGen=\" + oldSchemaGen, oldValues);\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            StoredDocument oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*(oldValues.get(i)/oldSchemaGen)));\n            w.addDocument(newDoc);\n          }\n        }\n\n        if (random().nextBoolean()) {\n          w.forceMerge(1);\n        }\n\n        w.close();\n      }\n\n      @Override\n      protected long getCurrentSchemaGen() {\n        return currentSchemaGen.get();\n      }\n\n      @Override\n      protected long getMergingSchemaGen() {\n        return mergingSchemaGen.get();\n      }\n\n      @Override\n      protected void checkParallelReader(LeafReader r, LeafReader parR, long schemaGen) throws IOException {\n        if (DEBUG) System.out.println(Thread.currentThread().getName() + \": TEST: now check parallel number DVs r=\" + r + \" parR=\" + parR);\n        NumericDocValues numbers = parR.getNumericDocValues(\"numbers\");\n        if (numbers == null) {\n          return;\n        }\n        int maxDoc = r.maxDoc();\n        boolean failed = false;\n        for(int i=0;i<maxDoc;i++) {\n          StoredDocument oldDoc = r.document(i);\n          long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n          value *= schemaGen;\n          if (value != numbers.get(i)) {\n            System.out.println(\"FAIL: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i) + \" numbers=\" + numbers);\n            failed = true;\n          } else if (failed) {\n            System.out.println(\"OK: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i));\n          }\n        }\n        assertFalse(\"FAILED r=\" + r, failed);\n      }\n    };\n  }\n\n","sourceOld":"  /** Schema change by adding changing how the same \"number\" DV field is indexed. */\n  private ReindexingReader getReindexerSameDVField(Path root, final AtomicLong currentSchemaGen, final AtomicLong mergingSchemaGen) throws IOException {\n    return new ReindexingReader(root) {\n      @Override\n      protected IndexWriterConfig getIndexWriterConfig() throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n        TieredMergePolicy tmp = new TieredMergePolicy();\n        // We write tiny docs, so we need tiny floor to avoid O(N^2) merging:\n        tmp.setFloorSegmentMB(.01);\n        iwc.setMergePolicy(tmp);\n        if (TEST_NIGHTLY) {\n          // during nightly tests, we might use too many files if we arent careful\n          iwc.setUseCompoundFile(true);\n        }\n        return iwc;\n      }\n\n      @Override\n      protected Directory openDirectory(Path path) throws IOException {\n        MockDirectoryWrapper dir = newMockFSDirectory(path);\n        dir.setUseSlowOpenClosers(false);\n        dir.setThrottling(Throttling.NEVER);\n        return dir;\n      }\n\n      @Override\n      protected void reindex(long oldSchemaGen, long newSchemaGen, LeafReader reader, Directory parallelDir) throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n\n        // The order of our docIDs must precisely matching incoming reader:\n        iwc.setMergePolicy(new LogByteSizeMergePolicy());\n        IndexWriter w = new IndexWriter(parallelDir, iwc);\n        int maxDoc = reader.maxDoc();\n\n        if (oldSchemaGen <= 0) {\n          // Must slowly parse the stored field into a new doc values field:\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            StoredDocument oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*value));\n            newDoc.add(new LongField(\"number\", value, Field.Store.NO));\n            w.addDocument(newDoc);\n          }\n        } else {\n          // Just carry over doc values from previous field:\n          NumericDocValues oldValues = reader.getNumericDocValues(\"number\");\n          assertNotNull(\"oldSchemaGen=\" + oldSchemaGen, oldValues);\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            StoredDocument oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*(oldValues.get(i)/oldSchemaGen)));\n            w.addDocument(newDoc);\n          }\n        }\n\n        if (random().nextBoolean()) {\n          w.forceMerge(1);\n        }\n\n        w.close();\n      }\n\n      @Override\n      protected long getCurrentSchemaGen() {\n        return currentSchemaGen.get();\n      }\n\n      @Override\n      protected long getMergingSchemaGen() {\n        return mergingSchemaGen.get();\n      }\n\n      @Override\n      protected void checkParallelReader(LeafReader r, LeafReader parR, long schemaGen) throws IOException {\n        if (DEBUG) System.out.println(Thread.currentThread().getName() + \": TEST: now check parallel number DVs r=\" + r + \" parR=\" + parR);\n        NumericDocValues numbers = parR.getNumericDocValues(\"numbers\");\n        if (numbers == null) {\n          return;\n        }\n        int maxDoc = r.maxDoc();\n        boolean failed = false;\n        for(int i=0;i<maxDoc;i++) {\n          StoredDocument oldDoc = r.document(i);\n          long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n          value *= schemaGen;\n          if (value != numbers.get(i)) {\n            System.out.println(\"FAIL: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i) + \" numbers=\" + numbers);\n            failed = true;\n          } else if (failed) {\n            System.out.println(\"OK: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i));\n          }\n        }\n        assertFalse(\"FAILED r=\" + r, failed);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","date":1453060490,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader#getReindexerSameDVField(Path,AtomicLong,AtomicLong).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader#getReindexerSameDVField(Path,AtomicLong,AtomicLong).mjava","sourceNew":"  /** Schema change by adding changing how the same \"number\" DV field is indexed. */\n  private ReindexingReader getReindexerSameDVField(Path root, final AtomicLong currentSchemaGen, final AtomicLong mergingSchemaGen) throws IOException {\n    return new ReindexingReader(root) {\n      @Override\n      protected IndexWriterConfig getIndexWriterConfig() throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n        TieredMergePolicy tmp = new TieredMergePolicy();\n        // We write tiny docs, so we need tiny floor to avoid O(N^2) merging:\n        tmp.setFloorSegmentMB(.01);\n        iwc.setMergePolicy(tmp);\n        if (TEST_NIGHTLY) {\n          // during nightly tests, we might use too many files if we arent careful\n          iwc.setUseCompoundFile(true);\n        }\n        return iwc;\n      }\n\n      @Override\n      protected Directory openDirectory(Path path) throws IOException {\n        MockDirectoryWrapper dir = newMockFSDirectory(path);\n        dir.setUseSlowOpenClosers(false);\n        dir.setThrottling(Throttling.NEVER);\n        return dir;\n      }\n\n      @Override\n      protected void reindex(long oldSchemaGen, long newSchemaGen, LeafReader reader, Directory parallelDir) throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n\n        // The order of our docIDs must precisely matching incoming reader:\n        iwc.setMergePolicy(new LogByteSizeMergePolicy());\n        IndexWriter w = new IndexWriter(parallelDir, iwc);\n        int maxDoc = reader.maxDoc();\n\n        if (oldSchemaGen <= 0) {\n          // Must slowly parse the stored field into a new doc values field:\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            Document oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*value));\n            newDoc.add(new DimensionalLongField(\"number\", value));\n            w.addDocument(newDoc);\n          }\n        } else {\n          // Just carry over doc values from previous field:\n          NumericDocValues oldValues = reader.getNumericDocValues(\"number\");\n          assertNotNull(\"oldSchemaGen=\" + oldSchemaGen, oldValues);\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            Document oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*(oldValues.get(i)/oldSchemaGen)));\n            w.addDocument(newDoc);\n          }\n        }\n\n        if (random().nextBoolean()) {\n          w.forceMerge(1);\n        }\n\n        w.close();\n      }\n\n      @Override\n      protected long getCurrentSchemaGen() {\n        return currentSchemaGen.get();\n      }\n\n      @Override\n      protected long getMergingSchemaGen() {\n        return mergingSchemaGen.get();\n      }\n\n      @Override\n      protected void checkParallelReader(LeafReader r, LeafReader parR, long schemaGen) throws IOException {\n        if (DEBUG) System.out.println(Thread.currentThread().getName() + \": TEST: now check parallel number DVs r=\" + r + \" parR=\" + parR);\n        NumericDocValues numbers = parR.getNumericDocValues(\"numbers\");\n        if (numbers == null) {\n          return;\n        }\n        int maxDoc = r.maxDoc();\n        boolean failed = false;\n        for(int i=0;i<maxDoc;i++) {\n          Document oldDoc = r.document(i);\n          long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n          value *= schemaGen;\n          if (value != numbers.get(i)) {\n            System.out.println(\"FAIL: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i) + \" numbers=\" + numbers);\n            failed = true;\n          } else if (failed) {\n            System.out.println(\"OK: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i));\n          }\n        }\n        assertFalse(\"FAILED r=\" + r, failed);\n      }\n    };\n  }\n\n","sourceOld":"  /** Schema change by adding changing how the same \"number\" DV field is indexed. */\n  private ReindexingReader getReindexerSameDVField(Path root, final AtomicLong currentSchemaGen, final AtomicLong mergingSchemaGen) throws IOException {\n    return new ReindexingReader(root) {\n      @Override\n      protected IndexWriterConfig getIndexWriterConfig() throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n        TieredMergePolicy tmp = new TieredMergePolicy();\n        // We write tiny docs, so we need tiny floor to avoid O(N^2) merging:\n        tmp.setFloorSegmentMB(.01);\n        iwc.setMergePolicy(tmp);\n        if (TEST_NIGHTLY) {\n          // during nightly tests, we might use too many files if we arent careful\n          iwc.setUseCompoundFile(true);\n        }\n        return iwc;\n      }\n\n      @Override\n      protected Directory openDirectory(Path path) throws IOException {\n        MockDirectoryWrapper dir = newMockFSDirectory(path);\n        dir.setUseSlowOpenClosers(false);\n        dir.setThrottling(Throttling.NEVER);\n        return dir;\n      }\n\n      @Override\n      protected void reindex(long oldSchemaGen, long newSchemaGen, LeafReader reader, Directory parallelDir) throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n\n        // The order of our docIDs must precisely matching incoming reader:\n        iwc.setMergePolicy(new LogByteSizeMergePolicy());\n        IndexWriter w = new IndexWriter(parallelDir, iwc);\n        int maxDoc = reader.maxDoc();\n\n        if (oldSchemaGen <= 0) {\n          // Must slowly parse the stored field into a new doc values field:\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            StoredDocument oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*value));\n            newDoc.add(new DimensionalLongField(\"number\", value));\n            w.addDocument(newDoc);\n          }\n        } else {\n          // Just carry over doc values from previous field:\n          NumericDocValues oldValues = reader.getNumericDocValues(\"number\");\n          assertNotNull(\"oldSchemaGen=\" + oldSchemaGen, oldValues);\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            StoredDocument oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*(oldValues.get(i)/oldSchemaGen)));\n            w.addDocument(newDoc);\n          }\n        }\n\n        if (random().nextBoolean()) {\n          w.forceMerge(1);\n        }\n\n        w.close();\n      }\n\n      @Override\n      protected long getCurrentSchemaGen() {\n        return currentSchemaGen.get();\n      }\n\n      @Override\n      protected long getMergingSchemaGen() {\n        return mergingSchemaGen.get();\n      }\n\n      @Override\n      protected void checkParallelReader(LeafReader r, LeafReader parR, long schemaGen) throws IOException {\n        if (DEBUG) System.out.println(Thread.currentThread().getName() + \": TEST: now check parallel number DVs r=\" + r + \" parR=\" + parR);\n        NumericDocValues numbers = parR.getNumericDocValues(\"numbers\");\n        if (numbers == null) {\n          return;\n        }\n        int maxDoc = r.maxDoc();\n        boolean failed = false;\n        for(int i=0;i<maxDoc;i++) {\n          StoredDocument oldDoc = r.document(i);\n          long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n          value *= schemaGen;\n          if (value != numbers.get(i)) {\n            System.out.println(\"FAIL: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i) + \" numbers=\" + numbers);\n            failed = true;\n          } else if (failed) {\n            System.out.println(\"OK: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i));\n          }\n        }\n        assertFalse(\"FAILED r=\" + r, failed);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cab7a79353f33d1a94cd307bf33aa5148601ebe6","date":1453391888,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader#getReindexerSameDVField(Path,AtomicLong,AtomicLong).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader#getReindexerSameDVField(Path,AtomicLong,AtomicLong).mjava","sourceNew":"  /** Schema change by adding changing how the same \"number\" DV field is indexed. */\n  private ReindexingReader getReindexerSameDVField(Path root, final AtomicLong currentSchemaGen, final AtomicLong mergingSchemaGen) throws IOException {\n    return new ReindexingReader(root) {\n      @Override\n      protected IndexWriterConfig getIndexWriterConfig() throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n        TieredMergePolicy tmp = new TieredMergePolicy();\n        // We write tiny docs, so we need tiny floor to avoid O(N^2) merging:\n        tmp.setFloorSegmentMB(.01);\n        iwc.setMergePolicy(tmp);\n        if (TEST_NIGHTLY) {\n          // during nightly tests, we might use too many files if we arent careful\n          iwc.setUseCompoundFile(true);\n        }\n        return iwc;\n      }\n\n      @Override\n      protected Directory openDirectory(Path path) throws IOException {\n        MockDirectoryWrapper dir = newMockFSDirectory(path);\n        dir.setUseSlowOpenClosers(false);\n        dir.setThrottling(Throttling.NEVER);\n        return dir;\n      }\n\n      @Override\n      protected void reindex(long oldSchemaGen, long newSchemaGen, LeafReader reader, Directory parallelDir) throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n\n        // The order of our docIDs must precisely matching incoming reader:\n        iwc.setMergePolicy(new LogByteSizeMergePolicy());\n        IndexWriter w = new IndexWriter(parallelDir, iwc);\n        int maxDoc = reader.maxDoc();\n\n        if (oldSchemaGen <= 0) {\n          // Must slowly parse the stored field into a new doc values field:\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            Document oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*value));\n            newDoc.add(new LongPoint(\"number\", value));\n            w.addDocument(newDoc);\n          }\n        } else {\n          // Just carry over doc values from previous field:\n          NumericDocValues oldValues = reader.getNumericDocValues(\"number\");\n          assertNotNull(\"oldSchemaGen=\" + oldSchemaGen, oldValues);\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            Document oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*(oldValues.get(i)/oldSchemaGen)));\n            w.addDocument(newDoc);\n          }\n        }\n\n        if (random().nextBoolean()) {\n          w.forceMerge(1);\n        }\n\n        w.close();\n      }\n\n      @Override\n      protected long getCurrentSchemaGen() {\n        return currentSchemaGen.get();\n      }\n\n      @Override\n      protected long getMergingSchemaGen() {\n        return mergingSchemaGen.get();\n      }\n\n      @Override\n      protected void checkParallelReader(LeafReader r, LeafReader parR, long schemaGen) throws IOException {\n        if (DEBUG) System.out.println(Thread.currentThread().getName() + \": TEST: now check parallel number DVs r=\" + r + \" parR=\" + parR);\n        NumericDocValues numbers = parR.getNumericDocValues(\"numbers\");\n        if (numbers == null) {\n          return;\n        }\n        int maxDoc = r.maxDoc();\n        boolean failed = false;\n        for(int i=0;i<maxDoc;i++) {\n          Document oldDoc = r.document(i);\n          long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n          value *= schemaGen;\n          if (value != numbers.get(i)) {\n            System.out.println(\"FAIL: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i) + \" numbers=\" + numbers);\n            failed = true;\n          } else if (failed) {\n            System.out.println(\"OK: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i));\n          }\n        }\n        assertFalse(\"FAILED r=\" + r, failed);\n      }\n    };\n  }\n\n","sourceOld":"  /** Schema change by adding changing how the same \"number\" DV field is indexed. */\n  private ReindexingReader getReindexerSameDVField(Path root, final AtomicLong currentSchemaGen, final AtomicLong mergingSchemaGen) throws IOException {\n    return new ReindexingReader(root) {\n      @Override\n      protected IndexWriterConfig getIndexWriterConfig() throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n        TieredMergePolicy tmp = new TieredMergePolicy();\n        // We write tiny docs, so we need tiny floor to avoid O(N^2) merging:\n        tmp.setFloorSegmentMB(.01);\n        iwc.setMergePolicy(tmp);\n        if (TEST_NIGHTLY) {\n          // during nightly tests, we might use too many files if we arent careful\n          iwc.setUseCompoundFile(true);\n        }\n        return iwc;\n      }\n\n      @Override\n      protected Directory openDirectory(Path path) throws IOException {\n        MockDirectoryWrapper dir = newMockFSDirectory(path);\n        dir.setUseSlowOpenClosers(false);\n        dir.setThrottling(Throttling.NEVER);\n        return dir;\n      }\n\n      @Override\n      protected void reindex(long oldSchemaGen, long newSchemaGen, LeafReader reader, Directory parallelDir) throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n\n        // The order of our docIDs must precisely matching incoming reader:\n        iwc.setMergePolicy(new LogByteSizeMergePolicy());\n        IndexWriter w = new IndexWriter(parallelDir, iwc);\n        int maxDoc = reader.maxDoc();\n\n        if (oldSchemaGen <= 0) {\n          // Must slowly parse the stored field into a new doc values field:\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            Document oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*value));\n            newDoc.add(new DimensionalLongField(\"number\", value));\n            w.addDocument(newDoc);\n          }\n        } else {\n          // Just carry over doc values from previous field:\n          NumericDocValues oldValues = reader.getNumericDocValues(\"number\");\n          assertNotNull(\"oldSchemaGen=\" + oldSchemaGen, oldValues);\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            Document oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*(oldValues.get(i)/oldSchemaGen)));\n            w.addDocument(newDoc);\n          }\n        }\n\n        if (random().nextBoolean()) {\n          w.forceMerge(1);\n        }\n\n        w.close();\n      }\n\n      @Override\n      protected long getCurrentSchemaGen() {\n        return currentSchemaGen.get();\n      }\n\n      @Override\n      protected long getMergingSchemaGen() {\n        return mergingSchemaGen.get();\n      }\n\n      @Override\n      protected void checkParallelReader(LeafReader r, LeafReader parR, long schemaGen) throws IOException {\n        if (DEBUG) System.out.println(Thread.currentThread().getName() + \": TEST: now check parallel number DVs r=\" + r + \" parR=\" + parR);\n        NumericDocValues numbers = parR.getNumericDocValues(\"numbers\");\n        if (numbers == null) {\n          return;\n        }\n        int maxDoc = r.maxDoc();\n        boolean failed = false;\n        for(int i=0;i<maxDoc;i++) {\n          Document oldDoc = r.document(i);\n          long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n          value *= schemaGen;\n          if (value != numbers.get(i)) {\n            System.out.println(\"FAIL: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i) + \" numbers=\" + numbers);\n            failed = true;\n          } else if (failed) {\n            System.out.println(\"OK: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i));\n          }\n        }\n        assertFalse(\"FAILED r=\" + r, failed);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1","date":1457644139,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader#getReindexerSameDVField(Path,AtomicLong,AtomicLong).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader#getReindexerSameDVField(Path,AtomicLong,AtomicLong).mjava","sourceNew":"  /** Schema change by adding changing how the same \"number\" DV field is indexed. */\n  private ReindexingReader getReindexerSameDVField(Path root, final AtomicLong currentSchemaGen, final AtomicLong mergingSchemaGen) throws IOException {\n    return new ReindexingReader(root) {\n      @Override\n      protected IndexWriterConfig getIndexWriterConfig() throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n        TieredMergePolicy tmp = new TieredMergePolicy();\n        // We write tiny docs, so we need tiny floor to avoid O(N^2) merging:\n        tmp.setFloorSegmentMB(.01);\n        iwc.setMergePolicy(tmp);\n        if (TEST_NIGHTLY) {\n          // during nightly tests, we might use too many files if we arent careful\n          iwc.setUseCompoundFile(true);\n        }\n        return iwc;\n      }\n\n      @Override\n      protected Directory openDirectory(Path path) throws IOException {\n        MockDirectoryWrapper dir = newMockFSDirectory(path);\n        dir.setUseSlowOpenClosers(false);\n        dir.setThrottling(Throttling.NEVER);\n        return dir;\n      }\n\n      @Override\n      protected void reindex(long oldSchemaGen, long newSchemaGen, LeafReader reader, Directory parallelDir) throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n\n        // The order of our docIDs must precisely matching incoming reader:\n        iwc.setMergePolicy(new LogByteSizeMergePolicy());\n        IndexWriter w = new IndexWriter(parallelDir, iwc);\n        int maxDoc = reader.maxDoc();\n\n        if (oldSchemaGen <= 0) {\n          // Must slowly parse the stored field into a new doc values field:\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            Document oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*value));\n            newDoc.add(new LongPoint(\"number\", value));\n            w.addDocument(newDoc);\n          }\n        } else {\n          // Just carry over doc values from previous field:\n          NumericDocValues oldValues = reader.getNumericDocValues(\"number\");\n          assertNotNull(\"oldSchemaGen=\" + oldSchemaGen, oldValues);\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            Document oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*(oldValues.get(i)/oldSchemaGen)));\n            w.addDocument(newDoc);\n          }\n        }\n\n        w.forceMerge(1);\n\n        w.close();\n      }\n\n      @Override\n      protected long getCurrentSchemaGen() {\n        return currentSchemaGen.get();\n      }\n\n      @Override\n      protected long getMergingSchemaGen() {\n        return mergingSchemaGen.get();\n      }\n\n      @Override\n      protected void checkParallelReader(LeafReader r, LeafReader parR, long schemaGen) throws IOException {\n        if (DEBUG) System.out.println(Thread.currentThread().getName() + \": TEST: now check parallel number DVs r=\" + r + \" parR=\" + parR);\n        NumericDocValues numbers = parR.getNumericDocValues(\"numbers\");\n        if (numbers == null) {\n          return;\n        }\n        int maxDoc = r.maxDoc();\n        boolean failed = false;\n        for(int i=0;i<maxDoc;i++) {\n          Document oldDoc = r.document(i);\n          long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n          value *= schemaGen;\n          if (value != numbers.get(i)) {\n            System.out.println(\"FAIL: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i) + \" numbers=\" + numbers);\n            failed = true;\n          } else if (failed) {\n            System.out.println(\"OK: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i));\n          }\n        }\n        assertFalse(\"FAILED r=\" + r, failed);\n      }\n    };\n  }\n\n","sourceOld":"  /** Schema change by adding changing how the same \"number\" DV field is indexed. */\n  private ReindexingReader getReindexerSameDVField(Path root, final AtomicLong currentSchemaGen, final AtomicLong mergingSchemaGen) throws IOException {\n    return new ReindexingReader(root) {\n      @Override\n      protected IndexWriterConfig getIndexWriterConfig() throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n        TieredMergePolicy tmp = new TieredMergePolicy();\n        // We write tiny docs, so we need tiny floor to avoid O(N^2) merging:\n        tmp.setFloorSegmentMB(.01);\n        iwc.setMergePolicy(tmp);\n        if (TEST_NIGHTLY) {\n          // during nightly tests, we might use too many files if we arent careful\n          iwc.setUseCompoundFile(true);\n        }\n        return iwc;\n      }\n\n      @Override\n      protected Directory openDirectory(Path path) throws IOException {\n        MockDirectoryWrapper dir = newMockFSDirectory(path);\n        dir.setUseSlowOpenClosers(false);\n        dir.setThrottling(Throttling.NEVER);\n        return dir;\n      }\n\n      @Override\n      protected void reindex(long oldSchemaGen, long newSchemaGen, LeafReader reader, Directory parallelDir) throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n\n        // The order of our docIDs must precisely matching incoming reader:\n        iwc.setMergePolicy(new LogByteSizeMergePolicy());\n        IndexWriter w = new IndexWriter(parallelDir, iwc);\n        int maxDoc = reader.maxDoc();\n\n        if (oldSchemaGen <= 0) {\n          // Must slowly parse the stored field into a new doc values field:\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            Document oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*value));\n            newDoc.add(new LongPoint(\"number\", value));\n            w.addDocument(newDoc);\n          }\n        } else {\n          // Just carry over doc values from previous field:\n          NumericDocValues oldValues = reader.getNumericDocValues(\"number\");\n          assertNotNull(\"oldSchemaGen=\" + oldSchemaGen, oldValues);\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            Document oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*(oldValues.get(i)/oldSchemaGen)));\n            w.addDocument(newDoc);\n          }\n        }\n\n        if (random().nextBoolean()) {\n          w.forceMerge(1);\n        }\n\n        w.close();\n      }\n\n      @Override\n      protected long getCurrentSchemaGen() {\n        return currentSchemaGen.get();\n      }\n\n      @Override\n      protected long getMergingSchemaGen() {\n        return mergingSchemaGen.get();\n      }\n\n      @Override\n      protected void checkParallelReader(LeafReader r, LeafReader parR, long schemaGen) throws IOException {\n        if (DEBUG) System.out.println(Thread.currentThread().getName() + \": TEST: now check parallel number DVs r=\" + r + \" parR=\" + parR);\n        NumericDocValues numbers = parR.getNumericDocValues(\"numbers\");\n        if (numbers == null) {\n          return;\n        }\n        int maxDoc = r.maxDoc();\n        boolean failed = false;\n        for(int i=0;i<maxDoc;i++) {\n          Document oldDoc = r.document(i);\n          long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n          value *= schemaGen;\n          if (value != numbers.get(i)) {\n            System.out.println(\"FAIL: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i) + \" numbers=\" + numbers);\n            failed = true;\n          } else if (failed) {\n            System.out.println(\"OK: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i));\n          }\n        }\n        assertFalse(\"FAILED r=\" + r, failed);\n      }\n    };\n  }\n\n","bugFix":["4d9aa91d3fdd25528bac3b2e6115d54fc2f28753"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6652c74b2358a0b13223817a6a793bf1c9d0749d","date":1474465301,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader#getReindexerSameDVField(Path,AtomicLong,AtomicLong).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader#getReindexerSameDVField(Path,AtomicLong,AtomicLong).mjava","sourceNew":"  /** Schema change by adding changing how the same \"number\" DV field is indexed. */\n  private ReindexingReader getReindexerSameDVField(Path root, final AtomicLong currentSchemaGen, final AtomicLong mergingSchemaGen) throws IOException {\n    return new ReindexingReader(root) {\n      @Override\n      protected IndexWriterConfig getIndexWriterConfig() throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n        TieredMergePolicy tmp = new TieredMergePolicy();\n        // We write tiny docs, so we need tiny floor to avoid O(N^2) merging:\n        tmp.setFloorSegmentMB(.01);\n        iwc.setMergePolicy(tmp);\n        if (TEST_NIGHTLY) {\n          // during nightly tests, we might use too many files if we arent careful\n          iwc.setUseCompoundFile(true);\n        }\n        return iwc;\n      }\n\n      @Override\n      protected Directory openDirectory(Path path) throws IOException {\n        MockDirectoryWrapper dir = newMockFSDirectory(path);\n        dir.setUseSlowOpenClosers(false);\n        dir.setThrottling(Throttling.NEVER);\n        return dir;\n      }\n\n      @Override\n      protected void reindex(long oldSchemaGen, long newSchemaGen, LeafReader reader, Directory parallelDir) throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n\n        // The order of our docIDs must precisely matching incoming reader:\n        iwc.setMergePolicy(new LogByteSizeMergePolicy());\n        IndexWriter w = new IndexWriter(parallelDir, iwc);\n        int maxDoc = reader.maxDoc();\n\n        if (oldSchemaGen <= 0) {\n          // Must slowly parse the stored field into a new doc values field:\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            Document oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*value));\n            newDoc.add(new LongPoint(\"number\", value));\n            w.addDocument(newDoc);\n          }\n        } else {\n          // Just carry over doc values from previous field:\n          NumericDocValues oldValues = reader.getNumericDocValues(\"number\");\n          assertNotNull(\"oldSchemaGen=\" + oldSchemaGen, oldValues);\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            Document oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            assertEquals(i, oldValues.nextDoc());\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*(oldValues.longValue()/oldSchemaGen)));\n            w.addDocument(newDoc);\n          }\n        }\n\n        w.forceMerge(1);\n\n        w.close();\n      }\n\n      @Override\n      protected long getCurrentSchemaGen() {\n        return currentSchemaGen.get();\n      }\n\n      @Override\n      protected long getMergingSchemaGen() {\n        return mergingSchemaGen.get();\n      }\n\n      @Override\n      protected void checkParallelReader(LeafReader r, LeafReader parR, long schemaGen) throws IOException {\n        if (DEBUG) System.out.println(Thread.currentThread().getName() + \": TEST: now check parallel number DVs r=\" + r + \" parR=\" + parR);\n        NumericDocValues numbers = parR.getNumericDocValues(\"numbers\");\n        if (numbers == null) {\n          return;\n        }\n        int maxDoc = r.maxDoc();\n        boolean failed = false;\n        for(int i=0;i<maxDoc;i++) {\n          Document oldDoc = r.document(i);\n          long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n          value *= schemaGen;\n          assertEquals(i, numbers.nextDoc());\n          if (value != numbers.longValue()) {\n            System.out.println(\"FAIL: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.longValue() + \" numbers=\" + numbers);\n            failed = true;\n          } else if (failed) {\n            System.out.println(\"OK: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.longValue());\n          }\n        }\n        assertFalse(\"FAILED r=\" + r, failed);\n      }\n    };\n  }\n\n","sourceOld":"  /** Schema change by adding changing how the same \"number\" DV field is indexed. */\n  private ReindexingReader getReindexerSameDVField(Path root, final AtomicLong currentSchemaGen, final AtomicLong mergingSchemaGen) throws IOException {\n    return new ReindexingReader(root) {\n      @Override\n      protected IndexWriterConfig getIndexWriterConfig() throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n        TieredMergePolicy tmp = new TieredMergePolicy();\n        // We write tiny docs, so we need tiny floor to avoid O(N^2) merging:\n        tmp.setFloorSegmentMB(.01);\n        iwc.setMergePolicy(tmp);\n        if (TEST_NIGHTLY) {\n          // during nightly tests, we might use too many files if we arent careful\n          iwc.setUseCompoundFile(true);\n        }\n        return iwc;\n      }\n\n      @Override\n      protected Directory openDirectory(Path path) throws IOException {\n        MockDirectoryWrapper dir = newMockFSDirectory(path);\n        dir.setUseSlowOpenClosers(false);\n        dir.setThrottling(Throttling.NEVER);\n        return dir;\n      }\n\n      @Override\n      protected void reindex(long oldSchemaGen, long newSchemaGen, LeafReader reader, Directory parallelDir) throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n\n        // The order of our docIDs must precisely matching incoming reader:\n        iwc.setMergePolicy(new LogByteSizeMergePolicy());\n        IndexWriter w = new IndexWriter(parallelDir, iwc);\n        int maxDoc = reader.maxDoc();\n\n        if (oldSchemaGen <= 0) {\n          // Must slowly parse the stored field into a new doc values field:\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            Document oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*value));\n            newDoc.add(new LongPoint(\"number\", value));\n            w.addDocument(newDoc);\n          }\n        } else {\n          // Just carry over doc values from previous field:\n          NumericDocValues oldValues = reader.getNumericDocValues(\"number\");\n          assertNotNull(\"oldSchemaGen=\" + oldSchemaGen, oldValues);\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            Document oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*(oldValues.get(i)/oldSchemaGen)));\n            w.addDocument(newDoc);\n          }\n        }\n\n        w.forceMerge(1);\n\n        w.close();\n      }\n\n      @Override\n      protected long getCurrentSchemaGen() {\n        return currentSchemaGen.get();\n      }\n\n      @Override\n      protected long getMergingSchemaGen() {\n        return mergingSchemaGen.get();\n      }\n\n      @Override\n      protected void checkParallelReader(LeafReader r, LeafReader parR, long schemaGen) throws IOException {\n        if (DEBUG) System.out.println(Thread.currentThread().getName() + \": TEST: now check parallel number DVs r=\" + r + \" parR=\" + parR);\n        NumericDocValues numbers = parR.getNumericDocValues(\"numbers\");\n        if (numbers == null) {\n          return;\n        }\n        int maxDoc = r.maxDoc();\n        boolean failed = false;\n        for(int i=0;i<maxDoc;i++) {\n          Document oldDoc = r.document(i);\n          long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n          value *= schemaGen;\n          if (value != numbers.get(i)) {\n            System.out.println(\"FAIL: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i) + \" numbers=\" + numbers);\n            failed = true;\n          } else if (failed) {\n            System.out.println(\"OK: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i));\n          }\n        }\n        assertFalse(\"FAILED r=\" + r, failed);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader#getReindexerSameDVField(Path,AtomicLong,AtomicLong).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader#getReindexerSameDVField(Path,AtomicLong,AtomicLong).mjava","sourceNew":"  /** Schema change by adding changing how the same \"number\" DV field is indexed. */\n  private ReindexingReader getReindexerSameDVField(Path root, final AtomicLong currentSchemaGen, final AtomicLong mergingSchemaGen) throws IOException {\n    return new ReindexingReader(root) {\n      @Override\n      protected IndexWriterConfig getIndexWriterConfig() throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n        TieredMergePolicy tmp = new TieredMergePolicy();\n        // We write tiny docs, so we need tiny floor to avoid O(N^2) merging:\n        tmp.setFloorSegmentMB(.01);\n        iwc.setMergePolicy(tmp);\n        if (TEST_NIGHTLY) {\n          // during nightly tests, we might use too many files if we arent careful\n          iwc.setUseCompoundFile(true);\n        }\n        return iwc;\n      }\n\n      @Override\n      protected Directory openDirectory(Path path) throws IOException {\n        MockDirectoryWrapper dir = newMockFSDirectory(path);\n        dir.setUseSlowOpenClosers(false);\n        dir.setThrottling(Throttling.NEVER);\n        return dir;\n      }\n\n      @Override\n      protected void reindex(long oldSchemaGen, long newSchemaGen, LeafReader reader, Directory parallelDir) throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n\n        // The order of our docIDs must precisely matching incoming reader:\n        iwc.setMergePolicy(new LogByteSizeMergePolicy());\n        IndexWriter w = new IndexWriter(parallelDir, iwc);\n        int maxDoc = reader.maxDoc();\n\n        if (oldSchemaGen <= 0) {\n          // Must slowly parse the stored field into a new doc values field:\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            Document oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*value));\n            newDoc.add(new LongPoint(\"number\", value));\n            w.addDocument(newDoc);\n          }\n        } else {\n          // Just carry over doc values from previous field:\n          NumericDocValues oldValues = reader.getNumericDocValues(\"number\");\n          assertNotNull(\"oldSchemaGen=\" + oldSchemaGen, oldValues);\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            Document oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            assertEquals(i, oldValues.nextDoc());\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*(oldValues.longValue()/oldSchemaGen)));\n            w.addDocument(newDoc);\n          }\n        }\n\n        w.forceMerge(1);\n\n        w.close();\n      }\n\n      @Override\n      protected long getCurrentSchemaGen() {\n        return currentSchemaGen.get();\n      }\n\n      @Override\n      protected long getMergingSchemaGen() {\n        return mergingSchemaGen.get();\n      }\n\n      @Override\n      protected void checkParallelReader(LeafReader r, LeafReader parR, long schemaGen) throws IOException {\n        if (DEBUG) System.out.println(Thread.currentThread().getName() + \": TEST: now check parallel number DVs r=\" + r + \" parR=\" + parR);\n        NumericDocValues numbers = parR.getNumericDocValues(\"numbers\");\n        if (numbers == null) {\n          return;\n        }\n        int maxDoc = r.maxDoc();\n        boolean failed = false;\n        for(int i=0;i<maxDoc;i++) {\n          Document oldDoc = r.document(i);\n          long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n          value *= schemaGen;\n          assertEquals(i, numbers.nextDoc());\n          if (value != numbers.longValue()) {\n            System.out.println(\"FAIL: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.longValue() + \" numbers=\" + numbers);\n            failed = true;\n          } else if (failed) {\n            System.out.println(\"OK: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.longValue());\n          }\n        }\n        assertFalse(\"FAILED r=\" + r, failed);\n      }\n    };\n  }\n\n","sourceOld":"  /** Schema change by adding changing how the same \"number\" DV field is indexed. */\n  private ReindexingReader getReindexerSameDVField(Path root, final AtomicLong currentSchemaGen, final AtomicLong mergingSchemaGen) throws IOException {\n    return new ReindexingReader(root) {\n      @Override\n      protected IndexWriterConfig getIndexWriterConfig() throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n        TieredMergePolicy tmp = new TieredMergePolicy();\n        // We write tiny docs, so we need tiny floor to avoid O(N^2) merging:\n        tmp.setFloorSegmentMB(.01);\n        iwc.setMergePolicy(tmp);\n        if (TEST_NIGHTLY) {\n          // during nightly tests, we might use too many files if we arent careful\n          iwc.setUseCompoundFile(true);\n        }\n        return iwc;\n      }\n\n      @Override\n      protected Directory openDirectory(Path path) throws IOException {\n        MockDirectoryWrapper dir = newMockFSDirectory(path);\n        dir.setUseSlowOpenClosers(false);\n        dir.setThrottling(Throttling.NEVER);\n        return dir;\n      }\n\n      @Override\n      protected void reindex(long oldSchemaGen, long newSchemaGen, LeafReader reader, Directory parallelDir) throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n\n        // The order of our docIDs must precisely matching incoming reader:\n        iwc.setMergePolicy(new LogByteSizeMergePolicy());\n        IndexWriter w = new IndexWriter(parallelDir, iwc);\n        int maxDoc = reader.maxDoc();\n\n        if (oldSchemaGen <= 0) {\n          // Must slowly parse the stored field into a new doc values field:\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            Document oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*value));\n            newDoc.add(new LongPoint(\"number\", value));\n            w.addDocument(newDoc);\n          }\n        } else {\n          // Just carry over doc values from previous field:\n          NumericDocValues oldValues = reader.getNumericDocValues(\"number\");\n          assertNotNull(\"oldSchemaGen=\" + oldSchemaGen, oldValues);\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            Document oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*(oldValues.get(i)/oldSchemaGen)));\n            w.addDocument(newDoc);\n          }\n        }\n\n        w.forceMerge(1);\n\n        w.close();\n      }\n\n      @Override\n      protected long getCurrentSchemaGen() {\n        return currentSchemaGen.get();\n      }\n\n      @Override\n      protected long getMergingSchemaGen() {\n        return mergingSchemaGen.get();\n      }\n\n      @Override\n      protected void checkParallelReader(LeafReader r, LeafReader parR, long schemaGen) throws IOException {\n        if (DEBUG) System.out.println(Thread.currentThread().getName() + \": TEST: now check parallel number DVs r=\" + r + \" parR=\" + parR);\n        NumericDocValues numbers = parR.getNumericDocValues(\"numbers\");\n        if (numbers == null) {\n          return;\n        }\n        int maxDoc = r.maxDoc();\n        boolean failed = false;\n        for(int i=0;i<maxDoc;i++) {\n          Document oldDoc = r.document(i);\n          long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n          value *= schemaGen;\n          if (value != numbers.get(i)) {\n            System.out.println(\"FAIL: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i) + \" numbers=\" + numbers);\n            failed = true;\n          } else if (failed) {\n            System.out.println(\"OK: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i));\n          }\n        }\n        assertFalse(\"FAILED r=\" + r, failed);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader#getReindexerSameDVField(Path,AtomicLong,AtomicLong).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader#getReindexerSameDVField(Path,AtomicLong,AtomicLong).mjava","sourceNew":"  /** Schema change by adding changing how the same \"number\" DV field is indexed. */\n  private ReindexingReader getReindexerSameDVField(Path root, final AtomicLong currentSchemaGen, final AtomicLong mergingSchemaGen) throws IOException {\n    return new ReindexingReader(root) {\n      @Override\n      protected IndexWriterConfig getIndexWriterConfig() throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n        TieredMergePolicy tmp = new TieredMergePolicy();\n        // We write tiny docs, so we need tiny floor to avoid O(N^2) merging:\n        tmp.setFloorSegmentMB(.01);\n        iwc.setMergePolicy(tmp);\n        if (TEST_NIGHTLY) {\n          // during nightly tests, we might use too many files if we arent careful\n          iwc.setUseCompoundFile(true);\n        }\n        return iwc;\n      }\n\n      @Override\n      protected Directory openDirectory(Path path) throws IOException {\n        MockDirectoryWrapper dir = newMockFSDirectory(path);\n        dir.setUseSlowOpenClosers(false);\n        dir.setThrottling(Throttling.NEVER);\n        return dir;\n      }\n\n      @Override\n      protected void reindex(long oldSchemaGen, long newSchemaGen, LeafReader reader, Directory parallelDir) throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n\n        // The order of our docIDs must precisely matching incoming reader:\n        iwc.setMergePolicy(new LogByteSizeMergePolicy());\n        IndexWriter w = new IndexWriter(parallelDir, iwc);\n        int maxDoc = reader.maxDoc();\n\n        if (oldSchemaGen <= 0) {\n          // Must slowly parse the stored field into a new doc values field:\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            Document oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*value));\n            newDoc.add(new LongPoint(\"number\", value));\n            w.addDocument(newDoc);\n          }\n        } else {\n          // Just carry over doc values from previous field:\n          NumericDocValues oldValues = reader.getNumericDocValues(\"number\");\n          assertNotNull(\"oldSchemaGen=\" + oldSchemaGen, oldValues);\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            Document oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            assertEquals(i, oldValues.nextDoc());\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*(oldValues.longValue()/oldSchemaGen)));\n            w.addDocument(newDoc);\n          }\n        }\n\n        w.forceMerge(1);\n\n        w.close();\n      }\n\n      @Override\n      protected long getCurrentSchemaGen() {\n        return currentSchemaGen.get();\n      }\n\n      @Override\n      protected long getMergingSchemaGen() {\n        return mergingSchemaGen.get();\n      }\n\n      @Override\n      protected void checkParallelReader(LeafReader r, LeafReader parR, long schemaGen) throws IOException {\n        if (DEBUG) System.out.println(Thread.currentThread().getName() + \": TEST: now check parallel number DVs r=\" + r + \" parR=\" + parR);\n        NumericDocValues numbers = parR.getNumericDocValues(\"numbers\");\n        if (numbers == null) {\n          return;\n        }\n        int maxDoc = r.maxDoc();\n        boolean failed = false;\n        for(int i=0;i<maxDoc;i++) {\n          Document oldDoc = r.document(i);\n          long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n          value *= schemaGen;\n          assertEquals(i, numbers.nextDoc());\n          if (value != numbers.longValue()) {\n            System.out.println(\"FAIL: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.longValue() + \" numbers=\" + numbers);\n            failed = true;\n          } else if (failed) {\n            System.out.println(\"OK: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.longValue());\n          }\n        }\n        assertFalse(\"FAILED r=\" + r, failed);\n      }\n    };\n  }\n\n","sourceOld":"  /** Schema change by adding changing how the same \"number\" DV field is indexed. */\n  private ReindexingReader getReindexerSameDVField(Path root, final AtomicLong currentSchemaGen, final AtomicLong mergingSchemaGen) throws IOException {\n    return new ReindexingReader(root) {\n      @Override\n      protected IndexWriterConfig getIndexWriterConfig() throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n        TieredMergePolicy tmp = new TieredMergePolicy();\n        // We write tiny docs, so we need tiny floor to avoid O(N^2) merging:\n        tmp.setFloorSegmentMB(.01);\n        iwc.setMergePolicy(tmp);\n        if (TEST_NIGHTLY) {\n          // during nightly tests, we might use too many files if we arent careful\n          iwc.setUseCompoundFile(true);\n        }\n        return iwc;\n      }\n\n      @Override\n      protected Directory openDirectory(Path path) throws IOException {\n        MockDirectoryWrapper dir = newMockFSDirectory(path);\n        dir.setUseSlowOpenClosers(false);\n        dir.setThrottling(Throttling.NEVER);\n        return dir;\n      }\n\n      @Override\n      protected void reindex(long oldSchemaGen, long newSchemaGen, LeafReader reader, Directory parallelDir) throws IOException {\n        IndexWriterConfig iwc = newIndexWriterConfig();\n\n        // The order of our docIDs must precisely matching incoming reader:\n        iwc.setMergePolicy(new LogByteSizeMergePolicy());\n        IndexWriter w = new IndexWriter(parallelDir, iwc);\n        int maxDoc = reader.maxDoc();\n\n        if (oldSchemaGen <= 0) {\n          // Must slowly parse the stored field into a new doc values field:\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            Document oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*value));\n            newDoc.add(new LongPoint(\"number\", value));\n            w.addDocument(newDoc);\n          }\n        } else {\n          // Just carry over doc values from previous field:\n          NumericDocValues oldValues = reader.getNumericDocValues(\"number\");\n          assertNotNull(\"oldSchemaGen=\" + oldSchemaGen, oldValues);\n          for(int i=0;i<maxDoc;i++) {\n            // TODO: is this still O(blockSize^2)?\n            Document oldDoc = reader.document(i);\n            Document newDoc = new Document();\n            newDoc.add(new NumericDocValuesField(\"number\", newSchemaGen*(oldValues.get(i)/oldSchemaGen)));\n            w.addDocument(newDoc);\n          }\n        }\n\n        w.forceMerge(1);\n\n        w.close();\n      }\n\n      @Override\n      protected long getCurrentSchemaGen() {\n        return currentSchemaGen.get();\n      }\n\n      @Override\n      protected long getMergingSchemaGen() {\n        return mergingSchemaGen.get();\n      }\n\n      @Override\n      protected void checkParallelReader(LeafReader r, LeafReader parR, long schemaGen) throws IOException {\n        if (DEBUG) System.out.println(Thread.currentThread().getName() + \": TEST: now check parallel number DVs r=\" + r + \" parR=\" + parR);\n        NumericDocValues numbers = parR.getNumericDocValues(\"numbers\");\n        if (numbers == null) {\n          return;\n        }\n        int maxDoc = r.maxDoc();\n        boolean failed = false;\n        for(int i=0;i<maxDoc;i++) {\n          Document oldDoc = r.document(i);\n          long value = Long.parseLong(oldDoc.get(\"text\").split(\" \")[1]);\n          value *= schemaGen;\n          if (value != numbers.get(i)) {\n            System.out.println(\"FAIL: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i) + \" numbers=\" + numbers);\n            failed = true;\n          } else if (failed) {\n            System.out.println(\"OK: docID=\" + i + \" \" + oldDoc+ \" value=\" + value + \" number=\" + numbers.get(i));\n          }\n        }\n        assertFalse(\"FAILED r=\" + r, failed);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1":["cab7a79353f33d1a94cd307bf33aa5148601ebe6"],"cab7a79353f33d1a94cd307bf33aa5148601ebe6":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"4d9aa91d3fdd25528bac3b2e6115d54fc2f28753":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1"],"f6450c8e5ae2fc3df8d5de3bce074cb72847ae23":["4d9aa91d3fdd25528bac3b2e6115d54fc2f28753"],"17afc0a594655cd4e902b4fdb9a0238f302f264c":["f6450c8e5ae2fc3df8d5de3bce074cb72847ae23"],"770342641f7b505eaa8dccdc666158bff2419109":["17afc0a594655cd4e902b4fdb9a0238f302f264c"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["770342641f7b505eaa8dccdc666158bff2419109"]},"commit2Childs":{"0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","6652c74b2358a0b13223817a6a793bf1c9d0749d","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"cab7a79353f33d1a94cd307bf33aa5148601ebe6":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1"],"4d9aa91d3fdd25528bac3b2e6115d54fc2f28753":["f6450c8e5ae2fc3df8d5de3bce074cb72847ae23"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4d9aa91d3fdd25528bac3b2e6115d54fc2f28753"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"f6450c8e5ae2fc3df8d5de3bce074cb72847ae23":["17afc0a594655cd4e902b4fdb9a0238f302f264c"],"17afc0a594655cd4e902b4fdb9a0238f302f264c":["770342641f7b505eaa8dccdc666158bff2419109"],"770342641f7b505eaa8dccdc666158bff2419109":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["cab7a79353f33d1a94cd307bf33aa5148601ebe6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}