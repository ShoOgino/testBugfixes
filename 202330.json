{"path":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","commits":[{"id":"8d73510b39df24d6b65de48e56f8dccb136b9d01","date":1483971905,"type":1,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              if (singleField) {\n                // common case: all terms are in the same field\n                // use an optimized builder that leverages terms stats to be more efficient\n                builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              } else {\n                // corner case: different fields\n                // don't make assumptions about the docs we will get\n                builder = new DocIdSetBuilder(reader.maxDoc());\n              }\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              if (singleField) {\n                // common case: all terms are in the same field\n                // use an optimized builder that leverages terms stats to be more efficient\n                builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              } else {\n                // corner case: different fields\n                // don't make assumptions about the docs we will get\n                builder = new DocIdSetBuilder(reader.maxDoc());\n              }\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"09ab8ee44ca898536770d0106a7c0ee4be4f0eb7","date":1484239864,"type":1,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              if (singleField) {\n                // common case: all terms are in the same field\n                // use an optimized builder that leverages terms stats to be more efficient\n                builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              } else {\n                // corner case: different fields\n                // don't make assumptions about the docs we will get\n                builder = new DocIdSetBuilder(reader.maxDoc());\n              }\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              if (singleField) {\n                // common case: all terms are in the same field\n                // use an optimized builder that leverages terms stats to be more efficient\n                builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              } else {\n                // corner case: different fields\n                // don't make assumptions about the docs we will get\n                builder = new DocIdSetBuilder(reader.maxDoc());\n              }\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"830a7b9be2f498e249b346796921c94cdd39d98f","date":1484639518,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        final Fields fields = reader.fields();\n        Terms terms = fields.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              if (singleField) {\n                // common case: all terms are in the same field\n                // use an optimized builder that leverages terms stats to be more efficient\n                builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              } else {\n                // corner case: different fields\n                // don't make assumptions about the docs we will get\n                builder = new DocIdSetBuilder(reader.maxDoc());\n              }\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"302d34f2c66e8d489ee13078305c330cbf67b226","date":1484754357,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        final Fields fields = reader.fields();\n        Terms terms = fields.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              if (singleField) {\n                // common case: all terms are in the same field\n                // use an optimized builder that leverages terms stats to be more efficient\n                builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              } else {\n                // corner case: different fields\n                // don't make assumptions about the docs we will get\n                builder = new DocIdSetBuilder(reader.maxDoc());\n              }\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b","date":1497408244,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        final Fields fields = reader.fields();\n        Terms terms = fields.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","date":1498028748,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        final Fields fields = reader.fields();\n        Terms terms = fields.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        final Fields fields = reader.fields();\n        Terms terms = fields.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"81b48d33779fd9654522a21c624c0f27e503f376","date":1504629245,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // Here we partition postings based on cost: longer ones will be consumed\n        // lazily while shorter ones are consumed eagerly into a bitset. Compared to\n        // putting everything into a bitset, this should help skip over unnecessary doc\n        // ids in the longer postings lists. This should be especially useful if\n        // document frequencies have a zipfian distribution.\n        final PriorityQueue<PostingsEnum> longestPostingsLists = new PriorityQueue<PostingsEnum>(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD) {\n          @Override\n          protected boolean lessThan(PostingsEnum a, PostingsEnum b) {\n            return a.cost() < b.cost();\n          }\n        };\n        DocIdSetBuilder shortestPostingsLists = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            docs = termsEnum.postings(docs, PostingsEnum.NONE);\n            docs = longestPostingsLists.insertWithOverflow(docs);\n            if (docs != null) { // the pq is full\n              if (shortestPostingsLists == null) {\n                shortestPostingsLists = new DocIdSetBuilder(reader.maxDoc());\n              }\n              shortestPostingsLists.add(docs);\n            }\n          }\n        }\n\n        final int numClauses = longestPostingsLists.size() + (shortestPostingsLists == null ? 0 : 1);\n        if (numClauses == 0) {\n          return null;\n        }\n\n        DisiPriorityQueue queue = new DisiPriorityQueue(numClauses);\n        for (PostingsEnum postings : longestPostingsLists) {\n          queue.add(new DisiWrapper(postings));\n        }\n        if (shortestPostingsLists != null) {\n          queue.add(new DisiWrapper(shortestPostingsLists.build().iterator()));\n        }\n        final DocIdSetIterator disi = new DisjunctionDISIApproximation(queue);\n        return new ConstantScoreScorer(this, boost, disi);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3c3d09560d479581ea79da290aeda6fa4135a6b0","date":1504639023,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // Here we partition postings based on cost: longer ones will be consumed\n        // lazily while shorter ones are consumed eagerly into a bitset. Compared to\n        // putting everything into a bitset, this should help skip over unnecessary doc\n        // ids in the longer postings lists. This should be especially useful if\n        // document frequencies have a zipfian distribution.\n        final PriorityQueue<PostingsEnum> longestPostingsLists = new PriorityQueue<PostingsEnum>(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD) {\n          @Override\n          protected boolean lessThan(PostingsEnum a, PostingsEnum b) {\n            return a.cost() < b.cost();\n          }\n        };\n        DocIdSetBuilder shortestPostingsLists = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            docs = termsEnum.postings(docs, PostingsEnum.NONE);\n            docs = longestPostingsLists.insertWithOverflow(docs);\n            if (docs != null) { // the pq is full\n              if (shortestPostingsLists == null) {\n                shortestPostingsLists = new DocIdSetBuilder(reader.maxDoc());\n              }\n              shortestPostingsLists.add(docs);\n            }\n          }\n        }\n\n        final int numClauses = longestPostingsLists.size() + (shortestPostingsLists == null ? 0 : 1);\n        if (numClauses == 0) {\n          return null;\n        }\n\n        DisiPriorityQueue queue = new DisiPriorityQueue(numClauses);\n        for (PostingsEnum postings : longestPostingsLists) {\n          queue.add(new DisiWrapper(postings));\n        }\n        if (shortestPostingsLists != null) {\n          queue.add(new DisiWrapper(shortestPostingsLists.build().iterator()));\n        }\n        final DocIdSetIterator disi = new DisjunctionDISIApproximation(queue);\n        return new ConstantScoreScorer(this, boost, disi);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"706a7a3396c030cc66dda92a0492eb492131c4c0","date":1509705614,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public IndexReader.CacheHelper getCacheHelper(LeafReaderContext context) {\n        return context.reader().getCoreCacheHelper();\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d523b8189b211dd1630166aa77b8c88bb48b3fcc","date":1510144168,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public IndexReader.CacheHelper getCacheHelper(LeafReaderContext context) {\n        return context.reader().getCoreCacheHelper();\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"da1460d7a5dea2658e7b8e4f6e632e53ade440ac","date":1510316270,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        return true;\n      }\n\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public IndexReader.CacheHelper getCacheHelper(LeafReaderContext context) {\n        return context.reader().getCoreCacheHelper();\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b31781f87f2e572b5c28caf2f83400abe6c05fe7","date":1511944192,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        return true;\n      }\n\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1aad05eeff7818b0833c02ac6b743aa72054963b","date":1512093122,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        return true;\n      }\n\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9fc47cb7b4346802411bb432f501ed0673d7119e","date":1512640179,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, scoreMode, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"417142ff08fda9cf0b72d5133e63097a166c6458","date":1512729693,"type":5,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,ScoreMode,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/TermInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, scoreMode, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        Terms terms = reader.terms(field);\n        if (terms == null) {\n          return null;\n        }\n        TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          assert field.equals(iterator.field());\n          if (termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet == null) {\n          return null;\n        } else if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n\n      @Override\n      public boolean isCacheable(LeafReaderContext ctx) {\n        // Only cache instances that have a reasonable size. Otherwise it might cause memory issues\n        // with the query cache if most memory ends up being spent on queries rather than doc id sets.\n        return ramBytesUsed() <= LRUQueryCache.QUERY_DEFAULT_RAM_BYTES_USED;\n      }\n\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"706a7a3396c030cc66dda92a0492eb492131c4c0":["28288370235ed02234a64753cdbf0c6ec096304a"],"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b":["830a7b9be2f498e249b346796921c94cdd39d98f"],"b31781f87f2e572b5c28caf2f83400abe6c05fe7":["da1460d7a5dea2658e7b8e4f6e632e53ade440ac"],"da1460d7a5dea2658e7b8e4f6e632e53ade440ac":["d523b8189b211dd1630166aa77b8c88bb48b3fcc"],"d523b8189b211dd1630166aa77b8c88bb48b3fcc":["28288370235ed02234a64753cdbf0c6ec096304a","706a7a3396c030cc66dda92a0492eb492131c4c0"],"09ab8ee44ca898536770d0106a7c0ee4be4f0eb7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","8d73510b39df24d6b65de48e56f8dccb136b9d01"],"28288370235ed02234a64753cdbf0c6ec096304a":["830a7b9be2f498e249b346796921c94cdd39d98f","e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b"],"417142ff08fda9cf0b72d5133e63097a166c6458":["1aad05eeff7818b0833c02ac6b743aa72054963b","9fc47cb7b4346802411bb432f501ed0673d7119e"],"302d34f2c66e8d489ee13078305c330cbf67b226":["09ab8ee44ca898536770d0106a7c0ee4be4f0eb7","830a7b9be2f498e249b346796921c94cdd39d98f"],"1aad05eeff7818b0833c02ac6b743aa72054963b":["da1460d7a5dea2658e7b8e4f6e632e53ade440ac","b31781f87f2e572b5c28caf2f83400abe6c05fe7"],"830a7b9be2f498e249b346796921c94cdd39d98f":["8d73510b39df24d6b65de48e56f8dccb136b9d01"],"81b48d33779fd9654522a21c624c0f27e503f376":["28288370235ed02234a64753cdbf0c6ec096304a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8d73510b39df24d6b65de48e56f8dccb136b9d01":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3c3d09560d479581ea79da290aeda6fa4135a6b0":["81b48d33779fd9654522a21c624c0f27e503f376"],"9fc47cb7b4346802411bb432f501ed0673d7119e":["1aad05eeff7818b0833c02ac6b743aa72054963b"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":["830a7b9be2f498e249b346796921c94cdd39d98f","e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["417142ff08fda9cf0b72d5133e63097a166c6458"]},"commit2Childs":{"706a7a3396c030cc66dda92a0492eb492131c4c0":["d523b8189b211dd1630166aa77b8c88bb48b3fcc"],"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b":["28288370235ed02234a64753cdbf0c6ec096304a","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"b31781f87f2e572b5c28caf2f83400abe6c05fe7":["1aad05eeff7818b0833c02ac6b743aa72054963b"],"da1460d7a5dea2658e7b8e4f6e632e53ade440ac":["b31781f87f2e572b5c28caf2f83400abe6c05fe7","1aad05eeff7818b0833c02ac6b743aa72054963b"],"d523b8189b211dd1630166aa77b8c88bb48b3fcc":["da1460d7a5dea2658e7b8e4f6e632e53ade440ac"],"09ab8ee44ca898536770d0106a7c0ee4be4f0eb7":["302d34f2c66e8d489ee13078305c330cbf67b226"],"28288370235ed02234a64753cdbf0c6ec096304a":["706a7a3396c030cc66dda92a0492eb492131c4c0","d523b8189b211dd1630166aa77b8c88bb48b3fcc","81b48d33779fd9654522a21c624c0f27e503f376"],"417142ff08fda9cf0b72d5133e63097a166c6458":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"302d34f2c66e8d489ee13078305c330cbf67b226":[],"1aad05eeff7818b0833c02ac6b743aa72054963b":["417142ff08fda9cf0b72d5133e63097a166c6458","9fc47cb7b4346802411bb432f501ed0673d7119e"],"830a7b9be2f498e249b346796921c94cdd39d98f":["e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b","28288370235ed02234a64753cdbf0c6ec096304a","302d34f2c66e8d489ee13078305c330cbf67b226","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"81b48d33779fd9654522a21c624c0f27e503f376":["3c3d09560d479581ea79da290aeda6fa4135a6b0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["09ab8ee44ca898536770d0106a7c0ee4be4f0eb7","8d73510b39df24d6b65de48e56f8dccb136b9d01"],"8d73510b39df24d6b65de48e56f8dccb136b9d01":["09ab8ee44ca898536770d0106a7c0ee4be4f0eb7","830a7b9be2f498e249b346796921c94cdd39d98f"],"3c3d09560d479581ea79da290aeda6fa4135a6b0":[],"9fc47cb7b4346802411bb432f501ed0673d7119e":["417142ff08fda9cf0b72d5133e63097a166c6458"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["302d34f2c66e8d489ee13078305c330cbf67b226","3c3d09560d479581ea79da290aeda6fa4135a6b0","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}