{"path":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/FieldReader.SegmentTermsEnum.Frame#loadBlock().mjava","commits":[{"id":"d289450fa5030c34b5e378c04bcb03a026aad9f4","date":1400178141,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/FieldReader.SegmentTermsEnum.Frame#loadBlock().mjava","pathOld":"/dev/null","sourceNew":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      void loadBlock() throws IOException {\n\n        // Clone the IndexInput lazily, so that consumers\n        // that just pull a TermsEnum to\n        // seekExact(TermState) don't pay this cost:\n        initIndexInput();\n\n        if (nextEnt != -1) {\n          // Already loaded\n          return;\n        }\n        //System.out.println(\"blc=\" + blockLoadCount);\n\n        in.seek(fp);\n        int code = in.readVInt();\n        entCount = code >>> 1;\n        assert entCount > 0;\n        isLastInFloor = (code & 1) != 0;\n        assert arc == null || (isLastInFloor || isFloor);\n\n        // TODO: if suffixes were stored in random-access\n        // array structure, then we could do binary search\n        // instead of linear scan to find target term; eg\n        // we could have simple array of offsets\n\n        // term suffixes:\n        code = in.readVInt();\n        isLeafBlock = (code & 1) != 0;\n        int numBytes = code >>> 1;\n        if (suffixBytes.length < numBytes) {\n          suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n        }\n        in.readBytes(suffixBytes, 0, numBytes);\n        suffixesReader.reset(suffixBytes, 0, numBytes);\n\n        /*if (DEBUG) {\n          if (arc == null) {\n          System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n          } else {\n          System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n          }\n          }*/\n\n        // stats\n        numBytes = in.readVInt();\n        if (statBytes.length < numBytes) {\n          statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n        }\n        in.readBytes(statBytes, 0, numBytes);\n        statsReader.reset(statBytes, 0, numBytes);\n        metaDataUpto = 0;\n\n        state.termBlockOrd = 0;\n        nextEnt = 0;\n        lastSubFP = -1;\n\n        // TODO: we could skip this if !hasTerms; but\n        // that's rare so won't help much\n        // metadata\n        numBytes = in.readVInt();\n        if (bytes == null) {\n          bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n          bytesReader = new ByteArrayDataInput();\n        } else if (bytes.length < numBytes) {\n          bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n        }\n        in.readBytes(bytes, 0, numBytes);\n        bytesReader.reset(bytes, 0, numBytes);\n\n\n        // Sub-blocks of a single floor block are always\n        // written one after another -- tail recurse:\n        fpEnd = in.getFilePointer();\n        // if (DEBUG) {\n        //   System.out.println(\"      fpEnd=\" + fpEnd);\n        // }\n      }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e50f0da93c6dcee20d8792637b1786e2c34975e9","date":1400182475,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/FieldReader.SegmentTermsEnum.Frame#loadBlock().mjava","sourceNew":null,"sourceOld":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      void loadBlock() throws IOException {\n\n        // Clone the IndexInput lazily, so that consumers\n        // that just pull a TermsEnum to\n        // seekExact(TermState) don't pay this cost:\n        initIndexInput();\n\n        if (nextEnt != -1) {\n          // Already loaded\n          return;\n        }\n        //System.out.println(\"blc=\" + blockLoadCount);\n\n        in.seek(fp);\n        int code = in.readVInt();\n        entCount = code >>> 1;\n        assert entCount > 0;\n        isLastInFloor = (code & 1) != 0;\n        assert arc == null || (isLastInFloor || isFloor);\n\n        // TODO: if suffixes were stored in random-access\n        // array structure, then we could do binary search\n        // instead of linear scan to find target term; eg\n        // we could have simple array of offsets\n\n        // term suffixes:\n        code = in.readVInt();\n        isLeafBlock = (code & 1) != 0;\n        int numBytes = code >>> 1;\n        if (suffixBytes.length < numBytes) {\n          suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n        }\n        in.readBytes(suffixBytes, 0, numBytes);\n        suffixesReader.reset(suffixBytes, 0, numBytes);\n\n        /*if (DEBUG) {\n          if (arc == null) {\n          System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n          } else {\n          System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n          }\n          }*/\n\n        // stats\n        numBytes = in.readVInt();\n        if (statBytes.length < numBytes) {\n          statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n        }\n        in.readBytes(statBytes, 0, numBytes);\n        statsReader.reset(statBytes, 0, numBytes);\n        metaDataUpto = 0;\n\n        state.termBlockOrd = 0;\n        nextEnt = 0;\n        lastSubFP = -1;\n\n        // TODO: we could skip this if !hasTerms; but\n        // that's rare so won't help much\n        // metadata\n        numBytes = in.readVInt();\n        if (bytes == null) {\n          bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n          bytesReader = new ByteArrayDataInput();\n        } else if (bytes.length < numBytes) {\n          bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n        }\n        in.readBytes(bytes, 0, numBytes);\n        bytesReader.reset(bytes, 0, numBytes);\n\n\n        // Sub-blocks of a single floor block are always\n        // written one after another -- tail recurse:\n        fpEnd = in.getFilePointer();\n        // if (DEBUG) {\n        //   System.out.println(\"      fpEnd=\" + fpEnd);\n        // }\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"e50f0da93c6dcee20d8792637b1786e2c34975e9":["d289450fa5030c34b5e378c04bcb03a026aad9f4"],"d289450fa5030c34b5e378c04bcb03a026aad9f4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"e50f0da93c6dcee20d8792637b1786e2c34975e9":[],"d289450fa5030c34b5e378c04bcb03a026aad9f4":["e50f0da93c6dcee20d8792637b1786e2c34975e9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d289450fa5030c34b5e378c04bcb03a026aad9f4","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["e50f0da93c6dcee20d8792637b1786e2c34975e9","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}