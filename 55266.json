{"path":"lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis#addTermFrequencies(Reader,Map[String,Map[String,Int]],String).mjava","commits":[{"id":"e562682007e295029696e354ac6947531b083c79","date":1459152450,"type":1,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis#addTermFrequencies(Reader,Map[String,Map[String,Int]],String).mjava","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","sourceNew":"  /**\n   * Adds term frequencies found by tokenizing text from reader into the Map words\n   *\n   * @param r a source of text to be tokenized\n   * @param perFieldTermFrequencies a Map of terms and their frequencies per field\n   * @param fieldName Used by analyzer for any special per-field analysis\n   */\n  private void addTermFrequencies(Reader r, Map<String, Map<String, Int>> perFieldTermFrequencies, String fieldName)\n      throws IOException {\n    if (analyzer == null) {\n      throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n          \"term vectors, you must provide an Analyzer\");\n    }\n    Map<String, Int> termFreqMap = perFieldTermFrequencies.get(fieldName);\n    if (termFreqMap == null) {\n      termFreqMap = new HashMap<>();\n      perFieldTermFrequencies.put(fieldName, termFreqMap);\n    }\n    try (TokenStream ts = analyzer.tokenStream(fieldName, r)) {\n      int tokenCount = 0;\n      // for every token\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      ts.reset();\n      while (ts.incrementToken()) {\n        String word = termAtt.toString();\n        tokenCount++;\n        if (tokenCount > maxNumTokensParsed) {\n          break;\n        }\n        if (isNoiseWord(word)) {\n          continue;\n        }\n\n        // increment frequency\n        Int cnt = termFreqMap.get(word);\n        if (cnt == null) {\n          termFreqMap.put(word, new Int());\n        } else {\n          cnt.x++;\n        }\n      }\n      ts.end();\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds term frequencies found by tokenizing text from reader into the Map words\n   *\n   * @param r a source of text to be tokenized\n   * @param termFreqMap a Map of terms and their frequencies\n   * @param fieldName Used by analyzer for any special per-field analysis\n   */\n  private void addTermFrequencies(Reader r, Map<String, Int> termFreqMap, String fieldName)\n      throws IOException {\n    if (analyzer == null) {\n      throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n          \"term vectors, you must provide an Analyzer\");\n    }\n    try (TokenStream ts = analyzer.tokenStream(fieldName, r)) {\n      int tokenCount = 0;\n      // for every token\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      ts.reset();\n      while (ts.incrementToken()) {\n        String word = termAtt.toString();\n        tokenCount++;\n        if (tokenCount > maxNumTokensParsed) {\n          break;\n        }\n        if (isNoiseWord(word)) {\n          continue;\n        }\n\n        // increment frequency\n        Int cnt = termFreqMap.get(word);\n        if (cnt == null) {\n          termFreqMap.put(word, new Int());\n        } else {\n          cnt.x++;\n        }\n      }\n      ts.end();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6a2ec5c4aea62b49cb9b5986524f8aee431470a3","date":1506692092,"type":3,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis#addTermFrequencies(Reader,Map[String,Map[String,Int]],String).mjava","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis#addTermFrequencies(Reader,Map[String,Map[String,Int]],String).mjava","sourceNew":"  /**\n   * Adds term frequencies found by tokenizing text from reader into the Map words\n   *\n   * @param r a source of text to be tokenized\n   * @param perFieldTermFrequencies a Map of terms and their frequencies per field\n   * @param fieldName Used by analyzer for any special per-field analysis\n   */\n  private void addTermFrequencies(Reader r, Map<String, Map<String, Int>> perFieldTermFrequencies, String fieldName)\n      throws IOException {\n    if (analyzer == null) {\n      throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n          \"term vectors, you must provide an Analyzer\");\n    }\n    Map<String, Int> termFreqMap = perFieldTermFrequencies.computeIfAbsent(fieldName, k -> new HashMap<>());\n    try (TokenStream ts = analyzer.tokenStream(fieldName, r)) {\n      int tokenCount = 0;\n      // for every token\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      ts.reset();\n      while (ts.incrementToken()) {\n        String word = termAtt.toString();\n        tokenCount++;\n        if (tokenCount > maxNumTokensParsed) {\n          break;\n        }\n        if (isNoiseWord(word)) {\n          continue;\n        }\n\n        // increment frequency\n        Int cnt = termFreqMap.get(word);\n        if (cnt == null) {\n          termFreqMap.put(word, new Int());\n        } else {\n          cnt.x++;\n        }\n      }\n      ts.end();\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds term frequencies found by tokenizing text from reader into the Map words\n   *\n   * @param r a source of text to be tokenized\n   * @param perFieldTermFrequencies a Map of terms and their frequencies per field\n   * @param fieldName Used by analyzer for any special per-field analysis\n   */\n  private void addTermFrequencies(Reader r, Map<String, Map<String, Int>> perFieldTermFrequencies, String fieldName)\n      throws IOException {\n    if (analyzer == null) {\n      throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n          \"term vectors, you must provide an Analyzer\");\n    }\n    Map<String, Int> termFreqMap = perFieldTermFrequencies.get(fieldName);\n    if (termFreqMap == null) {\n      termFreqMap = new HashMap<>();\n      perFieldTermFrequencies.put(fieldName, termFreqMap);\n    }\n    try (TokenStream ts = analyzer.tokenStream(fieldName, r)) {\n      int tokenCount = 0;\n      // for every token\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      ts.reset();\n      while (ts.incrementToken()) {\n        String word = termAtt.toString();\n        tokenCount++;\n        if (tokenCount > maxNumTokensParsed) {\n          break;\n        }\n        if (isNoiseWord(word)) {\n          continue;\n        }\n\n        // increment frequency\n        Int cnt = termFreqMap.get(word);\n        if (cnt == null) {\n          termFreqMap.put(word, new Int());\n        } else {\n          cnt.x++;\n        }\n      }\n      ts.end();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6240b74b884c5587f2a4062dd27d6c32bf228889","date":1507037235,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis#addTermFrequencies(Reader,Map[String,Map[String,Int]],String).mjava","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis#addTermFrequencies(Reader,Map[String,Map[String,Int]],String).mjava","sourceNew":"  /**\n   * Adds term frequencies found by tokenizing text from reader into the Map words\n   *\n   * @param r a source of text to be tokenized\n   * @param perFieldTermFrequencies a Map of terms and their frequencies per field\n   * @param fieldName Used by analyzer for any special per-field analysis\n   */\n  private void addTermFrequencies(Reader r, Map<String, Map<String, Int>> perFieldTermFrequencies, String fieldName)\n      throws IOException {\n    if (analyzer == null) {\n      throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n          \"term vectors, you must provide an Analyzer\");\n    }\n    Map<String, Int> termFreqMap = perFieldTermFrequencies.computeIfAbsent(fieldName, k -> new HashMap<>());\n    try (TokenStream ts = analyzer.tokenStream(fieldName, r)) {\n      int tokenCount = 0;\n      // for every token\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      ts.reset();\n      while (ts.incrementToken()) {\n        String word = termAtt.toString();\n        tokenCount++;\n        if (tokenCount > maxNumTokensParsed) {\n          break;\n        }\n        if (isNoiseWord(word)) {\n          continue;\n        }\n\n        // increment frequency\n        Int cnt = termFreqMap.get(word);\n        if (cnt == null) {\n          termFreqMap.put(word, new Int());\n        } else {\n          cnt.x++;\n        }\n      }\n      ts.end();\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds term frequencies found by tokenizing text from reader into the Map words\n   *\n   * @param r a source of text to be tokenized\n   * @param perFieldTermFrequencies a Map of terms and their frequencies per field\n   * @param fieldName Used by analyzer for any special per-field analysis\n   */\n  private void addTermFrequencies(Reader r, Map<String, Map<String, Int>> perFieldTermFrequencies, String fieldName)\n      throws IOException {\n    if (analyzer == null) {\n      throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n          \"term vectors, you must provide an Analyzer\");\n    }\n    Map<String, Int> termFreqMap = perFieldTermFrequencies.get(fieldName);\n    if (termFreqMap == null) {\n      termFreqMap = new HashMap<>();\n      perFieldTermFrequencies.put(fieldName, termFreqMap);\n    }\n    try (TokenStream ts = analyzer.tokenStream(fieldName, r)) {\n      int tokenCount = 0;\n      // for every token\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      ts.reset();\n      while (ts.incrementToken()) {\n        String word = termAtt.toString();\n        tokenCount++;\n        if (tokenCount > maxNumTokensParsed) {\n          break;\n        }\n        if (isNoiseWord(word)) {\n          continue;\n        }\n\n        // increment frequency\n        Int cnt = termFreqMap.get(word);\n        if (cnt == null) {\n          termFreqMap.put(word, new Int());\n        } else {\n          cnt.x++;\n        }\n      }\n      ts.end();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f3ff85fc12f4290c14ae4652270f0216b9cec578","date":1556640829,"type":3,"author":"Olli Kuonanoja","isMerge":false,"pathNew":"lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis#addTermFrequencies(Reader,Map[String,Map[String,Int]],String).mjava","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis#addTermFrequencies(Reader,Map[String,Map[String,Int]],String).mjava","sourceNew":"  /**\n   * Adds term frequencies found by tokenizing text from reader into the Map words\n   *\n   * @param r a source of text to be tokenized\n   * @param perFieldTermFrequencies a Map of terms and their frequencies per field\n   * @param fieldName Used by analyzer for any special per-field analysis\n   */\n  private void addTermFrequencies(Reader r, Map<String, Map<String, Int>> perFieldTermFrequencies, String fieldName)\n      throws IOException {\n    if (analyzer == null) {\n      throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n          \"term vectors, you must provide an Analyzer\");\n    }\n    Map<String, Int> termFreqMap = perFieldTermFrequencies.computeIfAbsent(fieldName, k -> new HashMap<>());\n    try (TokenStream ts = analyzer.tokenStream(fieldName, r)) {\n      int tokenCount = 0;\n      // for every token\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      TermFrequencyAttribute tfAtt = ts.addAttribute(TermFrequencyAttribute.class);\n      ts.reset();\n      while (ts.incrementToken()) {\n        String word = termAtt.toString();\n        tokenCount++;\n        if (tokenCount > maxNumTokensParsed) {\n          break;\n        }\n        if (isNoiseWord(word)) {\n          continue;\n        }\n\n        // increment frequency\n        Int cnt = termFreqMap.get(word);\n        if (cnt == null) {\n          termFreqMap.put(word, new Int(tfAtt.getTermFrequency()));\n        } else {\n          cnt.x += tfAtt.getTermFrequency();\n        }\n      }\n      ts.end();\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds term frequencies found by tokenizing text from reader into the Map words\n   *\n   * @param r a source of text to be tokenized\n   * @param perFieldTermFrequencies a Map of terms and their frequencies per field\n   * @param fieldName Used by analyzer for any special per-field analysis\n   */\n  private void addTermFrequencies(Reader r, Map<String, Map<String, Int>> perFieldTermFrequencies, String fieldName)\n      throws IOException {\n    if (analyzer == null) {\n      throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n          \"term vectors, you must provide an Analyzer\");\n    }\n    Map<String, Int> termFreqMap = perFieldTermFrequencies.computeIfAbsent(fieldName, k -> new HashMap<>());\n    try (TokenStream ts = analyzer.tokenStream(fieldName, r)) {\n      int tokenCount = 0;\n      // for every token\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      ts.reset();\n      while (ts.incrementToken()) {\n        String word = termAtt.toString();\n        tokenCount++;\n        if (tokenCount > maxNumTokensParsed) {\n          break;\n        }\n        if (isNoiseWord(word)) {\n          continue;\n        }\n\n        // increment frequency\n        Int cnt = termFreqMap.get(word);\n        if (cnt == null) {\n          termFreqMap.put(word, new Int());\n        } else {\n          cnt.x++;\n        }\n      }\n      ts.end();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"f3ff85fc12f4290c14ae4652270f0216b9cec578":["6a2ec5c4aea62b49cb9b5986524f8aee431470a3"],"6240b74b884c5587f2a4062dd27d6c32bf228889":["e562682007e295029696e354ac6947531b083c79","6a2ec5c4aea62b49cb9b5986524f8aee431470a3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"6a2ec5c4aea62b49cb9b5986524f8aee431470a3":["e562682007e295029696e354ac6947531b083c79"],"e562682007e295029696e354ac6947531b083c79":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["f3ff85fc12f4290c14ae4652270f0216b9cec578"]},"commit2Childs":{"f3ff85fc12f4290c14ae4652270f0216b9cec578":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"6240b74b884c5587f2a4062dd27d6c32bf228889":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["e562682007e295029696e354ac6947531b083c79"],"6a2ec5c4aea62b49cb9b5986524f8aee431470a3":["f3ff85fc12f4290c14ae4652270f0216b9cec578","6240b74b884c5587f2a4062dd27d6c32bf228889"],"e562682007e295029696e354ac6947531b083c79":["6240b74b884c5587f2a4062dd27d6c32bf228889","6a2ec5c4aea62b49cb9b5986524f8aee431470a3"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["6240b74b884c5587f2a4062dd27d6c32bf228889","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}