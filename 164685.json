{"path":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","commits":[{"id":"e147cce225492338f15a94a427f51f867da574ee","date":1346365916,"type":0,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"/dev/null","sourceNew":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    List<OpenBitSet[]> segmentDocSets = new ArrayList<OpenBitSet[]>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + ranges.size() + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      OpenBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n\n    IndexReader[] subReaders = new IndexReader[leaves.size()];\n    for (int partitionNumber=0; partitionNumber<ranges.size(); partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" range=\" + ranges.get(partitionNumber));\n\n      for (int segmentNumber = 0; segmentNumber<subReaders.length; segmentNumber++) {\n        subReaders[segmentNumber] = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n      }\n\n      String path = paths.get(partitionNumber);\n      boolean success = false;\n      SolrCore core = searcher.getCore();\n      IndexWriter iw = new SolrIndexWriter(\"SplittingIndexWriter\"+partitionNumber + \" \" + ranges.get(partitionNumber), path,\n          core.getDirectoryFactory(), true, core.getSchema(),\n          core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec(), true);\n\n      try {\n        iw.addIndexes(subReaders);\n        // TODO: will many deletes have been removed, or should we optimize?\n        success = true;\n      } finally {\n        if (success) {\n          IOUtils.close(iw);\n        } else {\n          IOUtils.closeWhileHandlingException(iw);\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"05a14b2611ead08655a2b2bdc61632eb31316e57","date":1346366621,"type":0,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"/dev/null","sourceNew":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    List<OpenBitSet[]> segmentDocSets = new ArrayList<OpenBitSet[]>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + ranges.size() + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      OpenBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n\n    IndexReader[] subReaders = new IndexReader[leaves.size()];\n    for (int partitionNumber=0; partitionNumber<ranges.size(); partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" range=\" + ranges.get(partitionNumber));\n\n      for (int segmentNumber = 0; segmentNumber<subReaders.length; segmentNumber++) {\n        subReaders[segmentNumber] = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n      }\n\n      String path = paths.get(partitionNumber);\n      boolean success = false;\n      SolrCore core = searcher.getCore();\n      IndexWriter iw = new SolrIndexWriter(\"SplittingIndexWriter\"+partitionNumber + \" \" + ranges.get(partitionNumber), path,\n          core.getDirectoryFactory(), true, core.getSchema(),\n          core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec(), true);\n\n      try {\n        iw.addIndexes(subReaders);\n        // TODO: will many deletes have been removed, or should we optimize?\n        success = true;\n      } finally {\n        if (success) {\n          IOUtils.close(iw);\n        } else {\n          IOUtils.closeWhileHandlingException(iw);\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c2d1ec821a2d311389d0085f20e2b2698e4b3442","date":1346690180,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","sourceNew":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    List<OpenBitSet[]> segmentDocSets = new ArrayList<OpenBitSet[]>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + ranges.size() + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      OpenBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    IndexReader[] subReaders = new IndexReader[leaves.size()];\n    for (int partitionNumber=0; partitionNumber<ranges.size(); partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" range=\" + ranges.get(partitionNumber));\n\n      for (int segmentNumber = 0; segmentNumber<subReaders.length; segmentNumber++) {\n        subReaders[segmentNumber] = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n      }\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = new SolrIndexWriter(\"SplittingIndexWriter\"+partitionNumber + \" \" + ranges.get(partitionNumber), path,\n                                 core.getDirectoryFactory(), true, core.getSchema(),\n                                 core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec(), true);\n      }\n\n      try {\n        // This merges the subreaders and will thus remove deletions (i.e. no optimize needed)\n        iw.addIndexes(subReaders);\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            IOUtils.close(iw);\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    List<OpenBitSet[]> segmentDocSets = new ArrayList<OpenBitSet[]>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + ranges.size() + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      OpenBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n\n    IndexReader[] subReaders = new IndexReader[leaves.size()];\n    for (int partitionNumber=0; partitionNumber<ranges.size(); partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" range=\" + ranges.get(partitionNumber));\n\n      for (int segmentNumber = 0; segmentNumber<subReaders.length; segmentNumber++) {\n        subReaders[segmentNumber] = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n      }\n\n      String path = paths.get(partitionNumber);\n      boolean success = false;\n      SolrCore core = searcher.getCore();\n      IndexWriter iw = new SolrIndexWriter(\"SplittingIndexWriter\"+partitionNumber + \" \" + ranges.get(partitionNumber), path,\n          core.getDirectoryFactory(), true, core.getSchema(),\n          core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec(), true);\n\n      try {\n        iw.addIndexes(subReaders);\n        // TODO: will many deletes have been removed, or should we optimize?\n        success = true;\n      } finally {\n        if (success) {\n          IOUtils.close(iw);\n        } else {\n          IOUtils.closeWhileHandlingException(iw);\n        }\n      }\n\n    }\n\n  }\n\n","bugFix":null,"bugIntro":["e4d5fc8284faca306256427bbbb86017a32002cf"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e4d5fc8284faca306256427bbbb86017a32002cf","date":1347055110,"type":3,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","sourceNew":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    List<OpenBitSet[]> segmentDocSets = new ArrayList<OpenBitSet[]>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + ranges.size() + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      OpenBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    IndexReader[] subReaders = new IndexReader[leaves.size()];\n    for (int partitionNumber=0; partitionNumber<ranges.size(); partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" range=\" + ranges.get(partitionNumber));\n\n      for (int segmentNumber = 0; segmentNumber<subReaders.length; segmentNumber++) {\n        subReaders[segmentNumber] = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n      }\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + \" \" + ranges.get(partitionNumber), path,\n                                    core.getDirectoryFactory(), true, core.getSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec(), true);\n      }\n\n      try {\n        // This merges the subreaders and will thus remove deletions (i.e. no optimize needed)\n        iw.addIndexes(subReaders);\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            IOUtils.close(iw);\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    List<OpenBitSet[]> segmentDocSets = new ArrayList<OpenBitSet[]>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + ranges.size() + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      OpenBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    IndexReader[] subReaders = new IndexReader[leaves.size()];\n    for (int partitionNumber=0; partitionNumber<ranges.size(); partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" range=\" + ranges.get(partitionNumber));\n\n      for (int segmentNumber = 0; segmentNumber<subReaders.length; segmentNumber++) {\n        subReaders[segmentNumber] = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n      }\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = new SolrIndexWriter(\"SplittingIndexWriter\"+partitionNumber + \" \" + ranges.get(partitionNumber), path,\n                                 core.getDirectoryFactory(), true, core.getSchema(),\n                                 core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec(), true);\n      }\n\n      try {\n        // This merges the subreaders and will thus remove deletions (i.e. no optimize needed)\n        iw.addIndexes(subReaders);\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            IOUtils.close(iw);\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","bugFix":["c2d1ec821a2d311389d0085f20e2b2698e4b3442"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54f9a16329716fa08b653c53f1c3dfc6b284d2cc","date":1363905854,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","sourceNew":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    List<OpenBitSet[]> segmentDocSets = new ArrayList<OpenBitSet[]>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + ranges.size() + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      OpenBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    IndexReader[] subReaders = new IndexReader[leaves.size()];\n    for (int partitionNumber=0; partitionNumber<ranges.size(); partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" range=\" + ranges.get(partitionNumber));\n\n      for (int segmentNumber = 0; segmentNumber<subReaders.length; segmentNumber++) {\n        subReaders[segmentNumber] = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n      }\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + \" \" + ranges.get(partitionNumber), path,\n                                    core.getDirectoryFactory(), true, core.getSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This merges the subreaders and will thus remove deletions (i.e. no optimize needed)\n        iw.addIndexes(subReaders);\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            IOUtils.close(iw);\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    List<OpenBitSet[]> segmentDocSets = new ArrayList<OpenBitSet[]>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + ranges.size() + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      OpenBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    IndexReader[] subReaders = new IndexReader[leaves.size()];\n    for (int partitionNumber=0; partitionNumber<ranges.size(); partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" range=\" + ranges.get(partitionNumber));\n\n      for (int segmentNumber = 0; segmentNumber<subReaders.length; segmentNumber++) {\n        subReaders[segmentNumber] = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n      }\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + \" \" + ranges.get(partitionNumber), path,\n                                    core.getDirectoryFactory(), true, core.getSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec(), true);\n      }\n\n      try {\n        // This merges the subreaders and will thus remove deletions (i.e. no optimize needed)\n        iw.addIndexes(subReaders);\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            IOUtils.close(iw);\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7ba13b1e8eb54daafdac40183a898b820ac9f73b","date":1365684595,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","sourceNew":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    List<OpenBitSet[]> segmentDocSets = new ArrayList<OpenBitSet[]>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      OpenBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    IndexReader[] subReaders = new IndexReader[leaves.size()];\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      for (int segmentNumber = 0; segmentNumber<subReaders.length; segmentNumber++) {\n        subReaders[segmentNumber] = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n      }\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This merges the subreaders and will thus remove deletions (i.e. no optimize needed)\n        iw.addIndexes(subReaders);\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            IOUtils.close(iw);\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    List<OpenBitSet[]> segmentDocSets = new ArrayList<OpenBitSet[]>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + ranges.size() + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      OpenBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    IndexReader[] subReaders = new IndexReader[leaves.size()];\n    for (int partitionNumber=0; partitionNumber<ranges.size(); partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" range=\" + ranges.get(partitionNumber));\n\n      for (int segmentNumber = 0; segmentNumber<subReaders.length; segmentNumber++) {\n        subReaders[segmentNumber] = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n      }\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + \" \" + ranges.get(partitionNumber), path,\n                                    core.getDirectoryFactory(), true, core.getSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This merges the subreaders and will thus remove deletions (i.e. no optimize needed)\n        iw.addIndexes(subReaders);\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            IOUtils.close(iw);\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"08970e5b8411182a29412c177eff67ec1110095b","date":1366640815,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","sourceNew":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    List<OpenBitSet[]> segmentDocSets = new ArrayList<OpenBitSet[]>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      OpenBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    IndexReader[] subReaders = new IndexReader[leaves.size()];\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      for (int segmentNumber = 0; segmentNumber<subReaders.length; segmentNumber++) {\n        subReaders[segmentNumber] = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n      }\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This merges the subreaders and will thus remove deletions (i.e. no optimize needed)\n        iw.addIndexes(subReaders);\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            IOUtils.close(iw);\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    List<OpenBitSet[]> segmentDocSets = new ArrayList<OpenBitSet[]>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      OpenBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    IndexReader[] subReaders = new IndexReader[leaves.size()];\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      for (int segmentNumber = 0; segmentNumber<subReaders.length; segmentNumber++) {\n        subReaders[segmentNumber] = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n      }\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This merges the subreaders and will thus remove deletions (i.e. no optimize needed)\n        iw.addIndexes(subReaders);\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            IOUtils.close(iw);\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8d8d21b87efb5fd6128e128460cdccacb929eb31","date":1388133383,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","sourceNew":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    List<OpenBitSet[]> segmentDocSets = new ArrayList<OpenBitSet[]>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      OpenBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          IndexReader subReader = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n          iw.addIndexes(subReader);\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            IOUtils.close(iw);\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    List<OpenBitSet[]> segmentDocSets = new ArrayList<OpenBitSet[]>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      OpenBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    IndexReader[] subReaders = new IndexReader[leaves.size()];\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      for (int segmentNumber = 0; segmentNumber<subReaders.length; segmentNumber++) {\n        subReaders[segmentNumber] = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n      }\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This merges the subreaders and will thus remove deletions (i.e. no optimize needed)\n        iw.addIndexes(subReaders);\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            IOUtils.close(iw);\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","bugFix":null,"bugIntro":["0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa","505bff044e47a553f461b6f4484d1d08faf4ac85"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a69cf7f1b4cac5d5b1363402b565cd535f13e6a1","date":1392536197,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","sourceNew":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<FixedBitSet[]>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          IndexReader subReader = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n          iw.addIndexes(subReader);\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            IOUtils.close(iw);\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    List<OpenBitSet[]> segmentDocSets = new ArrayList<OpenBitSet[]>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      OpenBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          IndexReader subReader = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n          iw.addIndexes(subReader);\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            IOUtils.close(iw);\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","sourceNew":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          IndexReader subReader = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n          iw.addIndexes(subReader);\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            IOUtils.close(iw);\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<FixedBitSet[]>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          IndexReader subReader = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n          iw.addIndexes(subReader);\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            IOUtils.close(iw);\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","sourceNew":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          IndexReader subReader = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n          iw.addIndexes(subReader);\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.shutdown();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          IndexReader subReader = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n          iw.addIndexes(subReader);\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            IOUtils.close(iw);\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"30b6ad849a21206db510322a3f583ca70ae20a2f","date":1399996150,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","sourceNew":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          IndexReader subReader = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n          iw.addIndexes(subReader);\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.shutdown();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          IndexReader subReader = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n          iw.addIndexes(subReader);\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.shutdown();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"93dd449115a9247533e44bab47e8429e5dccbc6d","date":1400258396,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","sourceNew":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          IndexReader subReader = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n          iw.addIndexes(subReader);\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.shutdown();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          IndexReader subReader = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n          iw.addIndexes(subReader);\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.shutdown();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"56572ec06f1407c066d6b7399413178b33176cd8","date":1400495675,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","sourceNew":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          IndexReader subReader = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n          iw.addIndexes(subReader);\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.shutdown();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          IndexReader subReader = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n          iw.addIndexes(subReader);\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.shutdown();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","sourceNew":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          IndexReader subReader = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n          iw.addIndexes(subReader);\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.close();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          IndexReader subReader = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n          iw.addIndexes(subReader);\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.shutdown();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","sourceNew":"  public void split() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (LeafReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          IndexReader subReader = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n          iw.addIndexes(subReader);\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.close();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":"  public void split() throws IOException {\n\n    List<AtomicReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (AtomicReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          IndexReader subReader = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n          iw.addIndexes(subReader);\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.close();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa","date":1420599177,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","sourceNew":"  public void split() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (LeafReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          LeafReader subReader = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n          iw.addIndexes(subReader);\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.close();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":"  public void split() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (LeafReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          IndexReader subReader = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n          iw.addIndexes(subReader);\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.close();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","bugFix":["8d8d21b87efb5fd6128e128460cdccacb929eb31"],"bugIntro":["505bff044e47a553f461b6f4484d1d08faf4ac85"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"505bff044e47a553f461b6f4484d1d08faf4ac85","date":1420728783,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","sourceNew":"  public void split() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (LeafReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n          iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.close();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":"  public void split() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (LeafReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          LeafReader subReader = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );\n          iw.addIndexes(subReader);\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.close();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","bugFix":["0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa","8d8d21b87efb5fd6128e128460cdccacb929eb31"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5c1c5aa8e88aa52c9e1cbfc696b611d3a56223c0","date":1422781929,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","sourceNew":"  public void split() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (LeafReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(core, \"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n          iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.close();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":"  public void split() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (LeafReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(\"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n          iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.close();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"138bba875d696cd48f61b681050026222022e937","date":1473262610,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","sourceNew":"  public void split() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (LeafReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(core, \"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n          iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n        }\n        // we commit explicitly instead of sending a CommitUpdateCommand through the processor chain\n        // because the sub-shard cores will just ignore such a commit because the update log is not\n        // in active state at this time.\n        setCommitData(iw);\n        iw.commit();\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.close();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":"  public void split() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (LeafReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(core, \"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n          iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.close();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","bugFix":null,"bugIntro":["13734b36bfd631ed6a46b961df376f679e8a3f57"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"13734b36bfd631ed6a46b961df376f679e8a3f57","date":1473743967,"type":3,"author":"Varun Thacker","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","sourceNew":"  public void split() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (LeafReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(core, \"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n          iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n        }\n        // we commit explicitly instead of sending a CommitUpdateCommand through the processor chain\n        // because the sub-shard cores will just ignore such a commit because the update log is not\n        // in active state at this time.\n        SolrIndexWriter.setCommitData(iw);\n        iw.commit();\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.close();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":"  public void split() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (LeafReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(core, \"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n          iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n        }\n        // we commit explicitly instead of sending a CommitUpdateCommand through the processor chain\n        // because the sub-shard cores will just ignore such a commit because the update log is not\n        // in active state at this time.\n        setCommitData(iw);\n        iw.commit();\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.close();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","bugFix":["138bba875d696cd48f61b681050026222022e937"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"89424def13674ea17829b41c5883c54ecc31a132","date":1473767373,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","sourceNew":"  public void split() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (LeafReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(core, \"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n          iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n        }\n        // we commit explicitly instead of sending a CommitUpdateCommand through the processor chain\n        // because the sub-shard cores will just ignore such a commit because the update log is not\n        // in active state at this time.\n        SolrIndexWriter.setCommitData(iw);\n        iw.commit();\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.close();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":"  public void split() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (LeafReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(core, \"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n          iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.close();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","sourceNew":"  public void split() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (LeafReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(core, \"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n          iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n        }\n        // we commit explicitly instead of sending a CommitUpdateCommand through the processor chain\n        // because the sub-shard cores will just ignore such a commit because the update log is not\n        // in active state at this time.\n        SolrIndexWriter.setCommitData(iw);\n        iw.commit();\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.close();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":"  public void split() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (LeafReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(core, \"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n          iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.close();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","sourceNew":"  public void split() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (LeafReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(core, \"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n          iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n        }\n        // we commit explicitly instead of sending a CommitUpdateCommand through the processor chain\n        // because the sub-shard cores will just ignore such a commit because the update log is not\n        // in active state at this time.\n        SolrIndexWriter.setCommitData(iw);\n        iw.commit();\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.close();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":"  public void split() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (LeafReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(core, \"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n          iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n        }\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.close();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"be320990bdc77e643388fa801e75017f19289c42","date":1489477067,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","sourceNew":"  public void split() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (LeafReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(core, \"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n          iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n        }\n        // we commit explicitly instead of sending a CommitUpdateCommand through the processor chain\n        // because the sub-shard cores will just ignore such a commit because the update log is not\n        // in active state at this time.\n        //TODO no commitUpdateCommand\n        SolrIndexWriter.setCommitData(iw, -1);\n        iw.commit();\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.close();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":"  public void split() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (LeafReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(core, \"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n          iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n        }\n        // we commit explicitly instead of sending a CommitUpdateCommand through the processor chain\n        // because the sub-shard cores will just ignore such a commit because the update log is not\n        // in active state at this time.\n        SolrIndexWriter.setCommitData(iw);\n        iw.commit();\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.close();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f996f8177b9204bdc92f7164460c6cefad9ac99a","date":1489482690,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","sourceNew":"  public void split() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (LeafReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(core, \"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n          iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n        }\n        // we commit explicitly instead of sending a CommitUpdateCommand through the processor chain\n        // because the sub-shard cores will just ignore such a commit because the update log is not\n        // in active state at this time.\n        //TODO no commitUpdateCommand\n        SolrIndexWriter.setCommitData(iw, -1);\n        iw.commit();\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.close();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":"  public void split() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (LeafReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(core, \"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n          iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n        }\n        // we commit explicitly instead of sending a CommitUpdateCommand through the processor chain\n        // because the sub-shard cores will just ignore such a commit because the update log is not\n        // in active state at this time.\n        SolrIndexWriter.setCommitData(iw);\n        iw.commit();\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.close();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab68488225b6a6c357dda72ed11dedca9914a192","date":1490013111,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","sourceNew":"  public void split() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (LeafReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(core, \"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n          iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n        }\n        // we commit explicitly instead of sending a CommitUpdateCommand through the processor chain\n        // because the sub-shard cores will just ignore such a commit because the update log is not\n        // in active state at this time.\n        //TODO no commitUpdateCommand\n        SolrIndexWriter.setCommitData(iw, -1);\n        iw.commit();\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.close();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","sourceOld":"  public void split() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (LeafReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(core, \"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n          iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n        }\n        // we commit explicitly instead of sending a CommitUpdateCommand through the processor chain\n        // because the sub-shard cores will just ignore such a commit because the update log is not\n        // in active state at this time.\n        SolrIndexWriter.setCommitData(iw);\n        iw.commit();\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.close();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"20c968c14aace7cf49843bf2c1fafc7fd3845659","date":1533133859,"type":4,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split().mjava","sourceNew":null,"sourceOld":"  public void split() throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();\n    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());\n\n    log.info(\"SolrIndexSplitter: partitions=\" + numPieces + \" segments=\"+leaves.size());\n\n    for (LeafReaderContext readerContext : leaves) {\n      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order\n      FixedBitSet[] docSets = split(readerContext);\n      segmentDocSets.add( docSets );\n    }\n\n\n    // would it be more efficient to write segment-at-a-time to each new index?\n    // - need to worry about number of open descriptors\n    // - need to worry about if IW.addIndexes does a sync or not...\n    // - would be more efficient on the read side, but prob less efficient merging\n\n    for (int partitionNumber=0; partitionNumber<numPieces; partitionNumber++) {\n      log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\"));\n\n      boolean success = false;\n\n      RefCounted<IndexWriter> iwRef = null;\n      IndexWriter iw = null;\n      if (cores != null) {\n        SolrCore subCore = cores.get(partitionNumber);\n        iwRef = subCore.getUpdateHandler().getSolrCoreState().getIndexWriter(subCore);\n        iw = iwRef.get();\n      } else {\n        SolrCore core = searcher.getCore();\n        String path = paths.get(partitionNumber);\n        iw = SolrIndexWriter.create(core, \"SplittingIndexWriter\"+partitionNumber + (ranges != null ? \" \" + ranges.get(partitionNumber) : \"\"), path,\n                                    core.getDirectoryFactory(), true, core.getLatestSchema(),\n                                    core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec());\n      }\n\n      try {\n        // This removes deletions but optimize might still be needed because sub-shards will have the same number of segments as the parent shard.\n        for (int segmentNumber = 0; segmentNumber<leaves.size(); segmentNumber++) {\n          log.info(\"SolrIndexSplitter: partition #\" + partitionNumber + \" partitionCount=\" + numPieces + (ranges != null ? \" range=\" + ranges.get(partitionNumber) : \"\") + \" segment #\"+segmentNumber + \" segmentCount=\" + leaves.size());\n          CodecReader subReader = SlowCodecReaderWrapper.wrap(leaves.get(segmentNumber).reader());\n          iw.addIndexes(new LiveDocsReader(subReader, segmentDocSets.get(segmentNumber)[partitionNumber]));\n        }\n        // we commit explicitly instead of sending a CommitUpdateCommand through the processor chain\n        // because the sub-shard cores will just ignore such a commit because the update log is not\n        // in active state at this time.\n        //TODO no commitUpdateCommand\n        SolrIndexWriter.setCommitData(iw, -1);\n        iw.commit();\n        success = true;\n      } finally {\n        if (iwRef != null) {\n          iwRef.decref();\n        } else {\n          if (success) {\n            iw.close();\n          } else {\n            IOUtils.closeWhileHandlingException(iw);\n          }\n        }\n      }\n\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"c2d1ec821a2d311389d0085f20e2b2698e4b3442":["e147cce225492338f15a94a427f51f867da574ee"],"56572ec06f1407c066d6b7399413178b33176cd8":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a","93dd449115a9247533e44bab47e8429e5dccbc6d"],"20c968c14aace7cf49843bf2c1fafc7fd3845659":["be320990bdc77e643388fa801e75017f19289c42"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"5c1c5aa8e88aa52c9e1cbfc696b611d3a56223c0":["505bff044e47a553f461b6f4484d1d08faf4ac85"],"08970e5b8411182a29412c177eff67ec1110095b":["7ba13b1e8eb54daafdac40183a898b820ac9f73b"],"89424def13674ea17829b41c5883c54ecc31a132":["5c1c5aa8e88aa52c9e1cbfc696b611d3a56223c0","13734b36bfd631ed6a46b961df376f679e8a3f57"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["5c1c5aa8e88aa52c9e1cbfc696b611d3a56223c0","89424def13674ea17829b41c5883c54ecc31a132"],"54f9a16329716fa08b653c53f1c3dfc6b284d2cc":["e4d5fc8284faca306256427bbbb86017a32002cf"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"30b6ad849a21206db510322a3f583ca70ae20a2f":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"be320990bdc77e643388fa801e75017f19289c42":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"ab68488225b6a6c357dda72ed11dedca9914a192":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","f996f8177b9204bdc92f7164460c6cefad9ac99a"],"7ba13b1e8eb54daafdac40183a898b820ac9f73b":["54f9a16329716fa08b653c53f1c3dfc6b284d2cc"],"8d8d21b87efb5fd6128e128460cdccacb929eb31":["08970e5b8411182a29412c177eff67ec1110095b"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["a69cf7f1b4cac5d5b1363402b565cd535f13e6a1"],"13734b36bfd631ed6a46b961df376f679e8a3f57":["138bba875d696cd48f61b681050026222022e937"],"138bba875d696cd48f61b681050026222022e937":["5c1c5aa8e88aa52c9e1cbfc696b611d3a56223c0"],"0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"e147cce225492338f15a94a427f51f867da574ee":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"05a14b2611ead08655a2b2bdc61632eb31316e57":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","e147cce225492338f15a94a427f51f867da574ee"],"505bff044e47a553f461b6f4484d1d08faf4ac85":["0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["5c1c5aa8e88aa52c9e1cbfc696b611d3a56223c0","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"e4d5fc8284faca306256427bbbb86017a32002cf":["c2d1ec821a2d311389d0085f20e2b2698e4b3442"],"f996f8177b9204bdc92f7164460c6cefad9ac99a":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"a69cf7f1b4cac5d5b1363402b565cd535f13e6a1":["8d8d21b87efb5fd6128e128460cdccacb929eb31"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a","30b6ad849a21206db510322a3f583ca70ae20a2f"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["20c968c14aace7cf49843bf2c1fafc7fd3845659"]},"commit2Childs":{"c2d1ec821a2d311389d0085f20e2b2698e4b3442":["e4d5fc8284faca306256427bbbb86017a32002cf"],"56572ec06f1407c066d6b7399413178b33176cd8":[],"20c968c14aace7cf49843bf2c1fafc7fd3845659":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa"],"5c1c5aa8e88aa52c9e1cbfc696b611d3a56223c0":["89424def13674ea17829b41c5883c54ecc31a132","17e5da53e4e5bd659e22add9bba1cfa222e7e30d","138bba875d696cd48f61b681050026222022e937","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"08970e5b8411182a29412c177eff67ec1110095b":["8d8d21b87efb5fd6128e128460cdccacb929eb31"],"89424def13674ea17829b41c5883c54ecc31a132":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["e147cce225492338f15a94a427f51f867da574ee","05a14b2611ead08655a2b2bdc61632eb31316e57"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["be320990bdc77e643388fa801e75017f19289c42","ab68488225b6a6c357dda72ed11dedca9914a192","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","f996f8177b9204bdc92f7164460c6cefad9ac99a"],"54f9a16329716fa08b653c53f1c3dfc6b284d2cc":["7ba13b1e8eb54daafdac40183a898b820ac9f73b"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["56572ec06f1407c066d6b7399413178b33176cd8","30b6ad849a21206db510322a3f583ca70ae20a2f","93dd449115a9247533e44bab47e8429e5dccbc6d"],"30b6ad849a21206db510322a3f583ca70ae20a2f":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"be320990bdc77e643388fa801e75017f19289c42":["20c968c14aace7cf49843bf2c1fafc7fd3845659"],"ab68488225b6a6c357dda72ed11dedca9914a192":[],"7ba13b1e8eb54daafdac40183a898b820ac9f73b":["08970e5b8411182a29412c177eff67ec1110095b"],"8d8d21b87efb5fd6128e128460cdccacb929eb31":["a69cf7f1b4cac5d5b1363402b565cd535f13e6a1"],"13734b36bfd631ed6a46b961df376f679e8a3f57":["89424def13674ea17829b41c5883c54ecc31a132"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"138bba875d696cd48f61b681050026222022e937":["13734b36bfd631ed6a46b961df376f679e8a3f57"],"0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa":["505bff044e47a553f461b6f4484d1d08faf4ac85"],"e147cce225492338f15a94a427f51f867da574ee":["c2d1ec821a2d311389d0085f20e2b2698e4b3442","05a14b2611ead08655a2b2bdc61632eb31316e57"],"05a14b2611ead08655a2b2bdc61632eb31316e57":[],"505bff044e47a553f461b6f4484d1d08faf4ac85":["5c1c5aa8e88aa52c9e1cbfc696b611d3a56223c0"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"e4d5fc8284faca306256427bbbb86017a32002cf":["54f9a16329716fa08b653c53f1c3dfc6b284d2cc"],"f996f8177b9204bdc92f7164460c6cefad9ac99a":["ab68488225b6a6c357dda72ed11dedca9914a192"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["56572ec06f1407c066d6b7399413178b33176cd8","d0ef034a4f10871667ae75181537775ddcf8ade4"],"a69cf7f1b4cac5d5b1363402b565cd535f13e6a1":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["56572ec06f1407c066d6b7399413178b33176cd8","ab68488225b6a6c357dda72ed11dedca9914a192","05a14b2611ead08655a2b2bdc61632eb31316e57","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}