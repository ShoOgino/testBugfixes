{"path":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriter#appendPostings(String,SegmentWriteState,FreqProxTermsWriterPerField[],FieldsConsumer).mjava","commits":[{"id":"69a6d2d525aeab53c867ed26934185e5bb627d0e","date":1296516902,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriter#appendPostings(String,SegmentWriteState,FreqProxTermsWriterPerField[],FieldsConsumer).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriter#appendPostings(FreqProxTermsWriterPerField[],FieldsConsumer).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void appendPostings(String fieldName, SegmentWriteState state,\n                      FreqProxTermsWriterPerField[] fields,\n                      FieldsConsumer consumer)\n    throws CorruptIndexException, IOException {\n\n    int numFields = fields.length;\n\n    final BytesRef text = new BytesRef();\n\n    final FreqProxFieldMergeState[] mergeStates = new FreqProxFieldMergeState[numFields];\n\n    final TermsConsumer termsConsumer = consumer.addField(fields[0].fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    for(int i=0;i<numFields;i++) {\n      FreqProxFieldMergeState fms = mergeStates[i] = new FreqProxFieldMergeState(fields[i], termComp);\n\n      assert fms.field.fieldInfo == fields[0].fieldInfo;\n\n      // Should always be true\n      boolean result = fms.nextTerm();\n      assert result;\n    }\n\n    final Term protoTerm = new Term(fieldName);\n\n    FreqProxFieldMergeState[] termStates = new FreqProxFieldMergeState[numFields];\n\n    final boolean currentFieldOmitTermFreqAndPositions = fields[0].fieldInfo.omitTermFreqAndPositions;\n    //System.out.println(\"flush terms field=\" + fields[0].fieldInfo.name);\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    // TODO: really TermsHashPerField should take over most\n    // of this loop, including merge sort of terms from\n    // multiple threads and interacting with the\n    // TermsConsumer, only calling out to us (passing us the\n    // DocsConsumer) to handle delivery of docs/positions\n    long sumTotalTermFreq = 0;\n    while(numFields > 0) {\n\n      // Get the next term to merge\n      termStates[0] = mergeStates[0];\n      int numToMerge = 1;\n\n      // TODO: pqueue\n      for(int i=1;i<numFields;i++) {\n        final int cmp = termComp.compare(mergeStates[i].text, termStates[0].text);\n        if (cmp < 0) {\n          termStates[0] = mergeStates[i];\n          numToMerge = 1;\n        } else if (cmp == 0) {\n          termStates[numToMerge++] = mergeStates[i];\n        }\n      }\n\n      // Need shallow copy here because termStates[0].text\n      // changes by the time we call finishTerm\n      text.bytes = termStates[0].text.bytes;\n      text.offset = termStates[0].text.offset;\n      text.length = termStates[0].text.length;  \n\n      //System.out.println(\"  term=\" + text.toUnicodeString());\n      //System.out.println(\"  term=\" + text.toString());\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(protoTerm.createTerm(text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      while(numToMerge > 0) {\n        \n        FreqProxFieldMergeState minState = termStates[0];\n        for(int i=1;i<numToMerge;i++) {\n          if (termStates[i].docID < minState.docID) {\n            minState = termStates[i];\n          }\n        }\n\n        final int termDocFreq = minState.termFreq;\n        numDocs++;\n\n        assert minState.docID < flushedDocCount: \"doc=\" + minState.docID + \" maxDoc=\" + flushedDocCount;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n\n        postingsConsumer.startDoc(minState.docID, termDocFreq);\n        if (minState.docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.deletedDocs == null) {\n            state.deletedDocs = new BitVector(state.numDocs);\n          }\n          state.deletedDocs.set(minState.docID);\n        }\n\n        final ByteSliceReader prox = minState.prox;\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload          \n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n            //System.out.println(\"    pos=\" + position);\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();  \n              \n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          } //End for\n\n          postingsConsumer.finishDoc();\n        }\n\n        if (!minState.nextDoc()) {\n\n          // Remove from termStates\n          int upto = 0;\n          // TODO: inefficient O(N) where N = number of\n          // threads that had seen this term:\n          for(int i=0;i<numToMerge;i++) {\n            if (termStates[i] != minState) {\n              termStates[upto++] = termStates[i];\n            }\n          }\n          numToMerge--;\n          assert upto == numToMerge;\n\n          // Advance this state to the next term\n\n          if (!minState.nextTerm()) {\n            // OK, no more terms, so remove from mergeStates\n            // as well\n            upto = 0;\n            for(int i=0;i<numFields;i++)\n              if (mergeStates[i] != minState)\n                mergeStates[upto++] = mergeStates[i];\n            numFields--;\n            assert upto == numFields;\n          }\n        }\n      }\n\n      assert numDocs > 0;\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void appendPostings(FreqProxTermsWriterPerField[] fields,\n                      FieldsConsumer consumer)\n    throws CorruptIndexException, IOException {\n\n    int numFields = fields.length;\n\n    final BytesRef text = new BytesRef();\n\n    final FreqProxFieldMergeState[] mergeStates = new FreqProxFieldMergeState[numFields];\n\n    final TermsConsumer termsConsumer = consumer.addField(fields[0].fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    for(int i=0;i<numFields;i++) {\n      FreqProxFieldMergeState fms = mergeStates[i] = new FreqProxFieldMergeState(fields[i], termComp);\n\n      assert fms.field.fieldInfo == fields[0].fieldInfo;\n\n      // Should always be true\n      boolean result = fms.nextTerm();\n      assert result;\n    }\n\n    FreqProxFieldMergeState[] termStates = new FreqProxFieldMergeState[numFields];\n\n    final boolean currentFieldOmitTermFreqAndPositions = fields[0].fieldInfo.omitTermFreqAndPositions;\n    //System.out.println(\"flush terms field=\" + fields[0].fieldInfo.name);\n\n    // TODO: really TermsHashPerField should take over most\n    // of this loop, including merge sort of terms from\n    // multiple threads and interacting with the\n    // TermsConsumer, only calling out to us (passing us the\n    // DocsConsumer) to handle delivery of docs/positions\n    long sumTotalTermFreq = 0;\n    while(numFields > 0) {\n\n      // Get the next term to merge\n      termStates[0] = mergeStates[0];\n      int numToMerge = 1;\n\n      // TODO: pqueue\n      for(int i=1;i<numFields;i++) {\n        final int cmp = termComp.compare(mergeStates[i].text, termStates[0].text);\n        if (cmp < 0) {\n          termStates[0] = mergeStates[i];\n          numToMerge = 1;\n        } else if (cmp == 0) {\n          termStates[numToMerge++] = mergeStates[i];\n        }\n      }\n\n      // Need shallow copy here because termStates[0].text\n      // changes by the time we call finishTerm\n      text.bytes = termStates[0].text.bytes;\n      text.offset = termStates[0].text.offset;\n      text.length = termStates[0].text.length;  \n\n      //System.out.println(\"  term=\" + text.toUnicodeString());\n      //System.out.println(\"  term=\" + text.toString());\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      while(numToMerge > 0) {\n        \n        FreqProxFieldMergeState minState = termStates[0];\n        for(int i=1;i<numToMerge;i++) {\n          if (termStates[i].docID < minState.docID) {\n            minState = termStates[i];\n          }\n        }\n\n        final int termDocFreq = minState.termFreq;\n        numDocs++;\n\n        assert minState.docID < flushedDocCount: \"doc=\" + minState.docID + \" maxDoc=\" + flushedDocCount;\n\n        postingsConsumer.startDoc(minState.docID, termDocFreq);\n\n        final ByteSliceReader prox = minState.prox;\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload          \n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n            //System.out.println(\"    pos=\" + position);\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();  \n              \n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          } //End for\n\n          postingsConsumer.finishDoc();\n        }\n\n        if (!minState.nextDoc()) {\n\n          // Remove from termStates\n          int upto = 0;\n          // TODO: inefficient O(N) where N = number of\n          // threads that had seen this term:\n          for(int i=0;i<numToMerge;i++) {\n            if (termStates[i] != minState) {\n              termStates[upto++] = termStates[i];\n            }\n          }\n          numToMerge--;\n          assert upto == numToMerge;\n\n          // Advance this state to the next term\n\n          if (!minState.nextTerm()) {\n            // OK, no more terms, so remove from mergeStates\n            // as well\n            upto = 0;\n            for(int i=0;i<numFields;i++)\n              if (mergeStates[i] != minState)\n                mergeStates[upto++] = mergeStates[i];\n            numFields--;\n            assert upto == numFields;\n          }\n        }\n      }\n\n      assert numDocs > 0;\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriter#appendPostings(String,SegmentWriteState,FreqProxTermsWriterPerField[],FieldsConsumer).mjava","pathOld":"/dev/null","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void appendPostings(String fieldName, SegmentWriteState state,\n                      FreqProxTermsWriterPerField[] fields,\n                      FieldsConsumer consumer)\n    throws CorruptIndexException, IOException {\n\n    int numFields = fields.length;\n\n    final BytesRef text = new BytesRef();\n\n    final FreqProxFieldMergeState[] mergeStates = new FreqProxFieldMergeState[numFields];\n\n    final TermsConsumer termsConsumer = consumer.addField(fields[0].fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    for(int i=0;i<numFields;i++) {\n      FreqProxFieldMergeState fms = mergeStates[i] = new FreqProxFieldMergeState(fields[i], termComp);\n\n      assert fms.field.fieldInfo == fields[0].fieldInfo;\n\n      // Should always be true\n      boolean result = fms.nextTerm();\n      assert result;\n    }\n\n    final Term protoTerm = new Term(fieldName);\n\n    FreqProxFieldMergeState[] termStates = new FreqProxFieldMergeState[numFields];\n\n    final boolean currentFieldOmitTermFreqAndPositions = fields[0].fieldInfo.omitTermFreqAndPositions;\n    //System.out.println(\"flush terms field=\" + fields[0].fieldInfo.name);\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    // TODO: really TermsHashPerField should take over most\n    // of this loop, including merge sort of terms from\n    // multiple threads and interacting with the\n    // TermsConsumer, only calling out to us (passing us the\n    // DocsConsumer) to handle delivery of docs/positions\n    long sumTotalTermFreq = 0;\n    while(numFields > 0) {\n\n      // Get the next term to merge\n      termStates[0] = mergeStates[0];\n      int numToMerge = 1;\n\n      // TODO: pqueue\n      for(int i=1;i<numFields;i++) {\n        final int cmp = termComp.compare(mergeStates[i].text, termStates[0].text);\n        if (cmp < 0) {\n          termStates[0] = mergeStates[i];\n          numToMerge = 1;\n        } else if (cmp == 0) {\n          termStates[numToMerge++] = mergeStates[i];\n        }\n      }\n\n      // Need shallow copy here because termStates[0].text\n      // changes by the time we call finishTerm\n      text.bytes = termStates[0].text.bytes;\n      text.offset = termStates[0].text.offset;\n      text.length = termStates[0].text.length;  \n\n      //System.out.println(\"  term=\" + text.toUnicodeString());\n      //System.out.println(\"  term=\" + text.toString());\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(protoTerm.createTerm(text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      while(numToMerge > 0) {\n        \n        FreqProxFieldMergeState minState = termStates[0];\n        for(int i=1;i<numToMerge;i++) {\n          if (termStates[i].docID < minState.docID) {\n            minState = termStates[i];\n          }\n        }\n\n        final int termDocFreq = minState.termFreq;\n        numDocs++;\n\n        assert minState.docID < flushedDocCount: \"doc=\" + minState.docID + \" maxDoc=\" + flushedDocCount;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n\n        postingsConsumer.startDoc(minState.docID, termDocFreq);\n        if (minState.docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.deletedDocs == null) {\n            state.deletedDocs = new BitVector(state.numDocs);\n          }\n          state.deletedDocs.set(minState.docID);\n        }\n\n        final ByteSliceReader prox = minState.prox;\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload          \n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n            //System.out.println(\"    pos=\" + position);\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();  \n              \n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          } //End for\n\n          postingsConsumer.finishDoc();\n        }\n\n        if (!minState.nextDoc()) {\n\n          // Remove from termStates\n          int upto = 0;\n          // TODO: inefficient O(N) where N = number of\n          // threads that had seen this term:\n          for(int i=0;i<numToMerge;i++) {\n            if (termStates[i] != minState) {\n              termStates[upto++] = termStates[i];\n            }\n          }\n          numToMerge--;\n          assert upto == numToMerge;\n\n          // Advance this state to the next term\n\n          if (!minState.nextTerm()) {\n            // OK, no more terms, so remove from mergeStates\n            // as well\n            upto = 0;\n            for(int i=0;i<numFields;i++)\n              if (mergeStates[i] != minState)\n                mergeStates[upto++] = mergeStates[i];\n            numFields--;\n            assert upto == numFields;\n          }\n        }\n      }\n\n      assert numDocs > 0;\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3e06be49006ecac364d39d12b9c9f74882f9b9f","date":1304289513,"type":5,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriter#appendPostings(String,SegmentWriteState,FreqProxTermsWriterPerField[],FieldsConsumer).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final Term protoTerm = new Term(fieldName);\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    long sumTotalTermFreq = 0;\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(protoTerm.createTerm(text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.deletedDocs == null) {\n            state.deletedDocs = new BitVector(state.numDocs);\n          }\n          state.deletedDocs.set(docID);\n        }\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload\n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void appendPostings(String fieldName, SegmentWriteState state,\n                      FreqProxTermsWriterPerField[] fields,\n                      FieldsConsumer consumer)\n    throws CorruptIndexException, IOException {\n\n    int numFields = fields.length;\n\n    final BytesRef text = new BytesRef();\n\n    final FreqProxFieldMergeState[] mergeStates = new FreqProxFieldMergeState[numFields];\n\n    final TermsConsumer termsConsumer = consumer.addField(fields[0].fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    for(int i=0;i<numFields;i++) {\n      FreqProxFieldMergeState fms = mergeStates[i] = new FreqProxFieldMergeState(fields[i], termComp);\n\n      assert fms.field.fieldInfo == fields[0].fieldInfo;\n\n      // Should always be true\n      boolean result = fms.nextTerm();\n      assert result;\n    }\n\n    final Term protoTerm = new Term(fieldName);\n\n    FreqProxFieldMergeState[] termStates = new FreqProxFieldMergeState[numFields];\n\n    final boolean currentFieldOmitTermFreqAndPositions = fields[0].fieldInfo.omitTermFreqAndPositions;\n    //System.out.println(\"flush terms field=\" + fields[0].fieldInfo.name);\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    // TODO: really TermsHashPerField should take over most\n    // of this loop, including merge sort of terms from\n    // multiple threads and interacting with the\n    // TermsConsumer, only calling out to us (passing us the\n    // DocsConsumer) to handle delivery of docs/positions\n    long sumTotalTermFreq = 0;\n    while(numFields > 0) {\n\n      // Get the next term to merge\n      termStates[0] = mergeStates[0];\n      int numToMerge = 1;\n\n      // TODO: pqueue\n      for(int i=1;i<numFields;i++) {\n        final int cmp = termComp.compare(mergeStates[i].text, termStates[0].text);\n        if (cmp < 0) {\n          termStates[0] = mergeStates[i];\n          numToMerge = 1;\n        } else if (cmp == 0) {\n          termStates[numToMerge++] = mergeStates[i];\n        }\n      }\n\n      // Need shallow copy here because termStates[0].text\n      // changes by the time we call finishTerm\n      text.bytes = termStates[0].text.bytes;\n      text.offset = termStates[0].text.offset;\n      text.length = termStates[0].text.length;  \n\n      //System.out.println(\"  term=\" + text.toUnicodeString());\n      //System.out.println(\"  term=\" + text.toString());\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(protoTerm.createTerm(text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      while(numToMerge > 0) {\n        \n        FreqProxFieldMergeState minState = termStates[0];\n        for(int i=1;i<numToMerge;i++) {\n          if (termStates[i].docID < minState.docID) {\n            minState = termStates[i];\n          }\n        }\n\n        final int termDocFreq = minState.termFreq;\n        numDocs++;\n\n        assert minState.docID < flushedDocCount: \"doc=\" + minState.docID + \" maxDoc=\" + flushedDocCount;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n\n        postingsConsumer.startDoc(minState.docID, termDocFreq);\n        if (minState.docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.deletedDocs == null) {\n            state.deletedDocs = new BitVector(state.numDocs);\n          }\n          state.deletedDocs.set(minState.docID);\n        }\n\n        final ByteSliceReader prox = minState.prox;\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload          \n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n            //System.out.println(\"    pos=\" + position);\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();  \n              \n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          } //End for\n\n          postingsConsumer.finishDoc();\n        }\n\n        if (!minState.nextDoc()) {\n\n          // Remove from termStates\n          int upto = 0;\n          // TODO: inefficient O(N) where N = number of\n          // threads that had seen this term:\n          for(int i=0;i<numToMerge;i++) {\n            if (termStates[i] != minState) {\n              termStates[upto++] = termStates[i];\n            }\n          }\n          numToMerge--;\n          assert upto == numToMerge;\n\n          // Advance this state to the next term\n\n          if (!minState.nextTerm()) {\n            // OK, no more terms, so remove from mergeStates\n            // as well\n            upto = 0;\n            for(int i=0;i<numFields;i++)\n              if (mergeStates[i] != minState)\n                mergeStates[upto++] = mergeStates[i];\n            numFields--;\n            assert upto == numFields;\n          }\n        }\n      }\n\n      assert numDocs > 0;\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":4,"author":"Simon Willnauer","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriter#appendPostings(String,SegmentWriteState,FreqProxTermsWriterPerField[],FieldsConsumer).mjava","sourceNew":null,"sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void appendPostings(String fieldName, SegmentWriteState state,\n                      FreqProxTermsWriterPerField[] fields,\n                      FieldsConsumer consumer)\n    throws CorruptIndexException, IOException {\n\n    int numFields = fields.length;\n\n    final BytesRef text = new BytesRef();\n\n    final FreqProxFieldMergeState[] mergeStates = new FreqProxFieldMergeState[numFields];\n\n    final TermsConsumer termsConsumer = consumer.addField(fields[0].fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    for(int i=0;i<numFields;i++) {\n      FreqProxFieldMergeState fms = mergeStates[i] = new FreqProxFieldMergeState(fields[i], termComp);\n\n      assert fms.field.fieldInfo == fields[0].fieldInfo;\n\n      // Should always be true\n      boolean result = fms.nextTerm();\n      assert result;\n    }\n\n    final Term protoTerm = new Term(fieldName);\n\n    FreqProxFieldMergeState[] termStates = new FreqProxFieldMergeState[numFields];\n\n    final boolean currentFieldOmitTermFreqAndPositions = fields[0].fieldInfo.omitTermFreqAndPositions;\n    //System.out.println(\"flush terms field=\" + fields[0].fieldInfo.name);\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    // TODO: really TermsHashPerField should take over most\n    // of this loop, including merge sort of terms from\n    // multiple threads and interacting with the\n    // TermsConsumer, only calling out to us (passing us the\n    // DocsConsumer) to handle delivery of docs/positions\n    long sumTotalTermFreq = 0;\n    while(numFields > 0) {\n\n      // Get the next term to merge\n      termStates[0] = mergeStates[0];\n      int numToMerge = 1;\n\n      // TODO: pqueue\n      for(int i=1;i<numFields;i++) {\n        final int cmp = termComp.compare(mergeStates[i].text, termStates[0].text);\n        if (cmp < 0) {\n          termStates[0] = mergeStates[i];\n          numToMerge = 1;\n        } else if (cmp == 0) {\n          termStates[numToMerge++] = mergeStates[i];\n        }\n      }\n\n      // Need shallow copy here because termStates[0].text\n      // changes by the time we call finishTerm\n      text.bytes = termStates[0].text.bytes;\n      text.offset = termStates[0].text.offset;\n      text.length = termStates[0].text.length;  \n\n      //System.out.println(\"  term=\" + text.toUnicodeString());\n      //System.out.println(\"  term=\" + text.toString());\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(protoTerm.createTerm(text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      while(numToMerge > 0) {\n        \n        FreqProxFieldMergeState minState = termStates[0];\n        for(int i=1;i<numToMerge;i++) {\n          if (termStates[i].docID < minState.docID) {\n            minState = termStates[i];\n          }\n        }\n\n        final int termDocFreq = minState.termFreq;\n        numDocs++;\n\n        assert minState.docID < flushedDocCount: \"doc=\" + minState.docID + \" maxDoc=\" + flushedDocCount;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n\n        postingsConsumer.startDoc(minState.docID, termDocFreq);\n        if (minState.docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.deletedDocs == null) {\n            state.deletedDocs = new BitVector(state.numDocs);\n          }\n          state.deletedDocs.set(minState.docID);\n        }\n\n        final ByteSliceReader prox = minState.prox;\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload          \n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n            //System.out.println(\"    pos=\" + position);\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();  \n              \n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          } //End for\n\n          postingsConsumer.finishDoc();\n        }\n\n        if (!minState.nextDoc()) {\n\n          // Remove from termStates\n          int upto = 0;\n          // TODO: inefficient O(N) where N = number of\n          // threads that had seen this term:\n          for(int i=0;i<numToMerge;i++) {\n            if (termStates[i] != minState) {\n              termStates[upto++] = termStates[i];\n            }\n          }\n          numToMerge--;\n          assert upto == numToMerge;\n\n          // Advance this state to the next term\n\n          if (!minState.nextTerm()) {\n            // OK, no more terms, so remove from mergeStates\n            // as well\n            upto = 0;\n            for(int i=0;i<numFields;i++)\n              if (mergeStates[i] != minState)\n                mergeStates[upto++] = mergeStates[i];\n            numFields--;\n            assert upto == numFields;\n          }\n        }\n      }\n\n      assert numDocs > 0;\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":4,"author":"Steven Rowe","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriter#appendPostings(String,SegmentWriteState,FreqProxTermsWriterPerField[],FieldsConsumer).mjava","sourceNew":null,"sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void appendPostings(String fieldName, SegmentWriteState state,\n                      FreqProxTermsWriterPerField[] fields,\n                      FieldsConsumer consumer)\n    throws CorruptIndexException, IOException {\n\n    int numFields = fields.length;\n\n    final BytesRef text = new BytesRef();\n\n    final FreqProxFieldMergeState[] mergeStates = new FreqProxFieldMergeState[numFields];\n\n    final TermsConsumer termsConsumer = consumer.addField(fields[0].fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    for(int i=0;i<numFields;i++) {\n      FreqProxFieldMergeState fms = mergeStates[i] = new FreqProxFieldMergeState(fields[i], termComp);\n\n      assert fms.field.fieldInfo == fields[0].fieldInfo;\n\n      // Should always be true\n      boolean result = fms.nextTerm();\n      assert result;\n    }\n\n    final Term protoTerm = new Term(fieldName);\n\n    FreqProxFieldMergeState[] termStates = new FreqProxFieldMergeState[numFields];\n\n    final boolean currentFieldOmitTermFreqAndPositions = fields[0].fieldInfo.omitTermFreqAndPositions;\n    //System.out.println(\"flush terms field=\" + fields[0].fieldInfo.name);\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    // TODO: really TermsHashPerField should take over most\n    // of this loop, including merge sort of terms from\n    // multiple threads and interacting with the\n    // TermsConsumer, only calling out to us (passing us the\n    // DocsConsumer) to handle delivery of docs/positions\n    long sumTotalTermFreq = 0;\n    while(numFields > 0) {\n\n      // Get the next term to merge\n      termStates[0] = mergeStates[0];\n      int numToMerge = 1;\n\n      // TODO: pqueue\n      for(int i=1;i<numFields;i++) {\n        final int cmp = termComp.compare(mergeStates[i].text, termStates[0].text);\n        if (cmp < 0) {\n          termStates[0] = mergeStates[i];\n          numToMerge = 1;\n        } else if (cmp == 0) {\n          termStates[numToMerge++] = mergeStates[i];\n        }\n      }\n\n      // Need shallow copy here because termStates[0].text\n      // changes by the time we call finishTerm\n      text.bytes = termStates[0].text.bytes;\n      text.offset = termStates[0].text.offset;\n      text.length = termStates[0].text.length;  \n\n      //System.out.println(\"  term=\" + text.toUnicodeString());\n      //System.out.println(\"  term=\" + text.toString());\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(protoTerm.createTerm(text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      while(numToMerge > 0) {\n        \n        FreqProxFieldMergeState minState = termStates[0];\n        for(int i=1;i<numToMerge;i++) {\n          if (termStates[i].docID < minState.docID) {\n            minState = termStates[i];\n          }\n        }\n\n        final int termDocFreq = minState.termFreq;\n        numDocs++;\n\n        assert minState.docID < flushedDocCount: \"doc=\" + minState.docID + \" maxDoc=\" + flushedDocCount;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n\n        postingsConsumer.startDoc(minState.docID, termDocFreq);\n        if (minState.docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.deletedDocs == null) {\n            state.deletedDocs = new BitVector(state.numDocs);\n          }\n          state.deletedDocs.set(minState.docID);\n        }\n\n        final ByteSliceReader prox = minState.prox;\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload          \n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n            //System.out.println(\"    pos=\" + position);\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();  \n              \n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          } //End for\n\n          postingsConsumer.finishDoc();\n        }\n\n        if (!minState.nextDoc()) {\n\n          // Remove from termStates\n          int upto = 0;\n          // TODO: inefficient O(N) where N = number of\n          // threads that had seen this term:\n          for(int i=0;i<numToMerge;i++) {\n            if (termStates[i] != minState) {\n              termStates[upto++] = termStates[i];\n            }\n          }\n          numToMerge--;\n          assert upto == numToMerge;\n\n          // Advance this state to the next term\n\n          if (!minState.nextTerm()) {\n            // OK, no more terms, so remove from mergeStates\n            // as well\n            upto = 0;\n            for(int i=0;i<numFields;i++)\n              if (mergeStates[i] != minState)\n                mergeStates[upto++] = mergeStates[i];\n            numFields--;\n            assert upto == numFields;\n          }\n        }\n      }\n\n      assert numDocs > 0;\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a3776dccca01c11e7046323cfad46a3b4a471233":["69a6d2d525aeab53c867ed26934185e5bb627d0e","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","69a6d2d525aeab53c867ed26934185e5bb627d0e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["29ef99d61cda9641b6250bf9567329a6e65f901d","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["69a6d2d525aeab53c867ed26934185e5bb627d0e","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"69a6d2d525aeab53c867ed26934185e5bb627d0e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"]},"commit2Childs":{"a3776dccca01c11e7046323cfad46a3b4a471233":[],"29ef99d61cda9641b6250bf9567329a6e65f901d":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["29ef99d61cda9641b6250bf9567329a6e65f901d","b3e06be49006ecac364d39d12b9c9f74882f9b9f","69a6d2d525aeab53c867ed26934185e5bb627d0e"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["a3776dccca01c11e7046323cfad46a3b4a471233","135621f3a0670a9394eb563224a3b76cc4dddc0f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"69a6d2d525aeab53c867ed26934185e5bb627d0e":["a3776dccca01c11e7046323cfad46a3b4a471233","29ef99d61cda9641b6250bf9567329a6e65f901d","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a3776dccca01c11e7046323cfad46a3b4a471233","135621f3a0670a9394eb563224a3b76cc4dddc0f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}