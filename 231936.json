{"path":"src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","commits":[{"id":"e5a95ce1d7a3779af6db59b6b39d3b89172d7445","date":1228620032,"type":1,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,boolean,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    FieldCache.StringIndex si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), fieldName);\n    final String[] terms = si.lookup;\n    final int[] termNum = si.order;\n\n    if (prefix!=null && prefix.length()==0) prefix=null;\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = Arrays.binarySearch(terms,prefix,nullStrComparator);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n      // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n      endTermIndex = Arrays.binarySearch(terms,prefix+\"\\uffff\\uffff\\uffff\\uffff\",nullStrComparator);\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=1;\n      endTermIndex=terms.length;\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int term = termNum[iter.nextDoc()];\n        int arrIdx = term-startTermIndex;\n        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        final BoundedTreeSet<CountPair<String,Integer>> queue = new BoundedTreeSet<CountPair<String,Integer>>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n            queue.add(new CountPair<String,Integer>(terms[startTermIndex+i], c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        }\n        // now select the right page from the results\n        for (CountPair<String,Integer> p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          res.add(ft.indexedToReadable(p.key), p.val);\n        }\n      } else {\n        // add results in index order\n        int i=0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          res.add(ft.indexedToReadable(terms[startTermIndex+i]), c);\n        }\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,fieldName));\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, boolean sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    FieldCache.StringIndex si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), fieldName);\n    final String[] terms = si.lookup;\n    final int[] termNum = si.order;\n\n    if (prefix!=null && prefix.length()==0) prefix=null;\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = Arrays.binarySearch(terms,prefix,nullStrComparator);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n      // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n      endTermIndex = Arrays.binarySearch(terms,prefix+\"\\uffff\\uffff\\uffff\\uffff\",nullStrComparator);\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=1;\n      endTermIndex=terms.length;\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int term = termNum[iter.nextDoc()];\n        int arrIdx = term-startTermIndex;\n        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        final BoundedTreeSet<CountPair<String,Integer>> queue = new BoundedTreeSet<CountPair<String,Integer>>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n            queue.add(new CountPair<String,Integer>(terms[startTermIndex+i], c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        }\n        // now select the right page from the results\n        for (CountPair<String,Integer> p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          res.add(ft.indexedToReadable(p.key), p.val);\n        }\n      } else {\n        // add results in index order\n        int i=0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          res.add(ft.indexedToReadable(terms[startTermIndex+i]), c);\n        }\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,fieldName));\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    FieldCache.StringIndex si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), fieldName);\n    final String[] terms = si.lookup;\n    final int[] termNum = si.order;\n\n    if (prefix!=null && prefix.length()==0) prefix=null;\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = Arrays.binarySearch(terms,prefix,nullStrComparator);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n      // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n      endTermIndex = Arrays.binarySearch(terms,prefix+\"\\uffff\\uffff\\uffff\\uffff\",nullStrComparator);\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=1;\n      endTermIndex=terms.length;\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int term = termNum[iter.nextDoc()];\n        int arrIdx = term-startTermIndex;\n        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        final BoundedTreeSet<CountPair<String,Integer>> queue = new BoundedTreeSet<CountPair<String,Integer>>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n            queue.add(new CountPair<String,Integer>(terms[startTermIndex+i], c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        }\n        // now select the right page from the results\n        for (CountPair<String,Integer> p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          res.add(ft.indexedToReadable(p.key), p.val);\n        }\n      } else {\n        // add results in index order\n        int i=0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          res.add(ft.indexedToReadable(terms[startTermIndex+i]), c);\n        }\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,fieldName));\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    FieldCache.StringIndex si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), fieldName);\n    final String[] terms = si.lookup;\n    final int[] termNum = si.order;\n\n    if (prefix!=null && prefix.length()==0) prefix=null;\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = Arrays.binarySearch(terms,prefix,nullStrComparator);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n      // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n      endTermIndex = Arrays.binarySearch(terms,prefix+\"\\uffff\\uffff\\uffff\\uffff\",nullStrComparator);\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=1;\n      endTermIndex=terms.length;\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int term = termNum[iter.nextDoc()];\n        int arrIdx = term-startTermIndex;\n        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        final BoundedTreeSet<CountPair<String,Integer>> queue = new BoundedTreeSet<CountPair<String,Integer>>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n            queue.add(new CountPair<String,Integer>(terms[startTermIndex+i], c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        }\n        // now select the right page from the results\n        for (CountPair<String,Integer> p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          res.add(ft.indexedToReadable(p.key), p.val);\n        }\n      } else {\n        // add results in index order\n        int i=0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          res.add(ft.indexedToReadable(terms[startTermIndex+i]), c);\n        }\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,fieldName));\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"ad94625fb8d088209f46650c8097196fec67f00c":["e5a95ce1d7a3779af6db59b6b39d3b89172d7445"],"e5a95ce1d7a3779af6db59b6b39d3b89172d7445":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["e5a95ce1d7a3779af6db59b6b39d3b89172d7445"],"ad94625fb8d088209f46650c8097196fec67f00c":[],"e5a95ce1d7a3779af6db59b6b39d3b89172d7445":["ad94625fb8d088209f46650c8097196fec67f00c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ad94625fb8d088209f46650c8097196fec67f00c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"pathCommit":null}