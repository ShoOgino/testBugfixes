{"path":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","commits":[{"id":"29baaefef1b62d76a3370ff72a0fe5f9bd84e365","date":1348949582,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, codec, null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? fieldMaxIndexOption : random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, new IOContext(new FlushInfo(maxDocID, bytes)));\n    FieldsConsumer fieldsConsumer = codec.postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, doFreq ? posting.positions.size() : -1);\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, doOffsets ? pos.startOffset : -1, doOffsets ? pos.endOffset : -1);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, codec, null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, new IOContext(new FlushInfo(maxDocID, bytes)));\n    FieldsConsumer fieldsConsumer = codec.postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, doFreq ? posting.positions.size() : -1);\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, doOffsets ? pos.startOffset : -1, doOffsets ? pos.endOffset : -1);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c4108b7b6cc881d8b7c6ba2df50010b7bc581a39","date":1348960504,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, codec, null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? fieldMaxIndexOption : random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, new IOContext(new FlushInfo(1+maxDocID, bytes)));\n    FieldsConsumer fieldsConsumer = codec.postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, doFreq ? posting.positions.size() : -1);\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, doOffsets ? pos.startOffset : -1, doOffsets ? pos.endOffset : -1);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, codec, null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? fieldMaxIndexOption : random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, new IOContext(new FlushInfo(maxDocID, bytes)));\n    FieldsConsumer fieldsConsumer = codec.postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, doFreq ? posting.positions.size() : -1);\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, doOffsets ? pos.startOffset : -1, doOffsets ? pos.endOffset : -1);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2fe4b12f0dcf02b1690143f2ad02d8f89625eb36","date":1349174553,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? fieldMaxIndexOption : random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, new IOContext(new FlushInfo(maxDoc, bytes)));\n    FieldsConsumer fieldsConsumer = codec.postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,Long> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDoc);\n      for(Map.Entry<BytesRef,Long> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        SeedPostings postings = getSeedPostings(term.utf8ToString(), termEnt.getValue(), false, maxAllowed);\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.docFreq + \" seed=\" + termEnt.getValue());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docID = 0;\n        while((docID = postings.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          final int freq = postings.freq();\n          if (VERBOSE) {\n            System.out.println(\"    \" + postings.upto + \": docID=\" + docID + \" freq=\" + postings.freq);\n          }\n          postingsConsumer.startDoc(docID, doFreq ? postings.freq : -1);\n          seenDocs.set(docID);\n          if (doPos) {\n            totalTF += postings.freq;\n            for(int posUpto=0;posUpto<freq;posUpto++) {\n              int pos = postings.nextPosition();\n              BytesRef payload = postings.getPayload();\n\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos + \" payload=\" + (payload == null ? \"null\" : payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos);\n                }\n              }\n              postingsConsumer.addPosition(pos, doPayloads ? payload : null,\n                                           doOffsets ? postings.startOffset() : -1,\n                                           doOffsets ? postings.endOffset() : -1);\n            }\n          } else if (doFreq) {\n            totalTF += freq;\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.docFreq, doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.docFreq;\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", 1+maxDocID, false, codec, null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? fieldMaxIndexOption : random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, new IOContext(new FlushInfo(1+maxDocID, bytes)));\n    FieldsConsumer fieldsConsumer = codec.postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,List<Posting>> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDocID+1);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        List<Posting> postings = termEnt.getValue();\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.size());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docCount = 0;\n        for(Posting posting : postings) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + docCount + \": docID=\" + posting.docID + \" freq=\" + posting.positions.size());\n          }\n          postingsConsumer.startDoc(posting.docID, doFreq ? posting.positions.size() : -1);\n          seenDocs.set(posting.docID);\n          if (doPos) {\n            totalTF += posting.positions.size();\n            for(Position pos : posting.positions) {\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos.position + \" payload=\" + (pos.payload == null ? \"null\" : pos.payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos.position);\n                }\n              }\n              postingsConsumer.addPosition(pos.position, (doPayloads && pos.payload != null) ? new BytesRef(pos.payload) : null, doOffsets ? pos.startOffset : -1, doOffsets ? pos.endOffset : -1);\n            }\n          } else if (doFreq) {\n            totalTF += posting.positions.size();\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n          docCount++;\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.size(), doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.size();\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0837ab0472feecb3a54260729d845f839e1cbd72","date":1358283639,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? fieldMaxIndexOption : random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, new IOContext(new FlushInfo(maxDoc, bytes)));\n    FieldsConsumer fieldsConsumer = codec.postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,Long> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDoc);\n      for(Map.Entry<BytesRef,Long> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        SeedPostings postings = getSeedPostings(term.utf8ToString(), termEnt.getValue(), false, maxAllowed);\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.docFreq + \" seed=\" + termEnt.getValue());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docID = 0;\n        while((docID = postings.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          final int freq = postings.freq();\n          if (VERBOSE) {\n            System.out.println(\"    \" + postings.upto + \": docID=\" + docID + \" freq=\" + postings.freq);\n          }\n          postingsConsumer.startDoc(docID, doFreq ? postings.freq : -1);\n          seenDocs.set(docID);\n          if (doPos) {\n            totalTF += postings.freq;\n            for(int posUpto=0;posUpto<freq;posUpto++) {\n              int pos = postings.nextPosition();\n              BytesRef payload = postings.getPayload();\n\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos + \" payload=\" + (payload == null ? \"null\" : payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos);\n                }\n              }\n              postingsConsumer.addPosition(pos, doPayloads ? payload : null,\n                                           doOffsets ? postings.startOffset() : -1,\n                                           doOffsets ? postings.endOffset() : -1);\n            }\n          } else if (doFreq) {\n            totalTF += freq;\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.docFreq, doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.docFreq;\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? fieldMaxIndexOption : random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, new IOContext(new FlushInfo(maxDoc, bytes)));\n    FieldsConsumer fieldsConsumer = codec.postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,Long> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDoc);\n      for(Map.Entry<BytesRef,Long> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        SeedPostings postings = getSeedPostings(term.utf8ToString(), termEnt.getValue(), false, maxAllowed);\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.docFreq + \" seed=\" + termEnt.getValue());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docID = 0;\n        while((docID = postings.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          final int freq = postings.freq();\n          if (VERBOSE) {\n            System.out.println(\"    \" + postings.upto + \": docID=\" + docID + \" freq=\" + postings.freq);\n          }\n          postingsConsumer.startDoc(docID, doFreq ? postings.freq : -1);\n          seenDocs.set(docID);\n          if (doPos) {\n            totalTF += postings.freq;\n            for(int posUpto=0;posUpto<freq;posUpto++) {\n              int pos = postings.nextPosition();\n              BytesRef payload = postings.getPayload();\n\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos + \" payload=\" + (payload == null ? \"null\" : payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos);\n                }\n              }\n              postingsConsumer.addPosition(pos, doPayloads ? payload : null,\n                                           doOffsets ? postings.startOffset() : -1,\n                                           doOffsets ? postings.endOffset() : -1);\n            }\n          } else if (doFreq) {\n            totalTF += freq;\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.docFreq, doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.docFreq;\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d4d69c535930b5cce125cff868d40f6373dc27d4","date":1360270101,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? fieldMaxIndexOption : random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, new IOContext(new FlushInfo(maxDoc, bytes)));\n    FieldsConsumer fieldsConsumer = codec.postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,Long> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDoc);\n      for(Map.Entry<BytesRef,Long> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        SeedPostings postings = getSeedPostings(term.utf8ToString(), termEnt.getValue(), false, maxAllowed);\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.docFreq + \" seed=\" + termEnt.getValue());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docID = 0;\n        while((docID = postings.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          final int freq = postings.freq();\n          if (VERBOSE) {\n            System.out.println(\"    \" + postings.upto + \": docID=\" + docID + \" freq=\" + postings.freq);\n          }\n          postingsConsumer.startDoc(docID, doFreq ? postings.freq : -1);\n          seenDocs.set(docID);\n          if (doPos) {\n            totalTF += postings.freq;\n            for(int posUpto=0;posUpto<freq;posUpto++) {\n              int pos = postings.nextPosition();\n              BytesRef payload = postings.getPayload();\n\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos + \" payload=\" + (payload == null ? \"null\" : payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos);\n                }\n              }\n              postingsConsumer.addPosition(pos, doPayloads ? payload : null,\n                                           doOffsets ? postings.startOffset() : -1,\n                                           doOffsets ? postings.endOffset() : -1);\n            }\n          } else if (doFreq) {\n            totalTF += freq;\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.docFreq, doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.docFreq;\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? fieldMaxIndexOption : random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValues.Type.FIXED_INTS_8,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, new IOContext(new FlushInfo(maxDoc, bytes)));\n    FieldsConsumer fieldsConsumer = codec.postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,Long> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDoc);\n      for(Map.Entry<BytesRef,Long> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        SeedPostings postings = getSeedPostings(term.utf8ToString(), termEnt.getValue(), false, maxAllowed);\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.docFreq + \" seed=\" + termEnt.getValue());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docID = 0;\n        while((docID = postings.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          final int freq = postings.freq();\n          if (VERBOSE) {\n            System.out.println(\"    \" + postings.upto + \": docID=\" + docID + \" freq=\" + postings.freq);\n          }\n          postingsConsumer.startDoc(docID, doFreq ? postings.freq : -1);\n          seenDocs.set(docID);\n          if (doPos) {\n            totalTF += postings.freq;\n            for(int posUpto=0;posUpto<freq;posUpto++) {\n              int pos = postings.nextPosition();\n              BytesRef payload = postings.getPayload();\n\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos + \" payload=\" + (payload == null ? \"null\" : payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos);\n                }\n              }\n              postingsConsumer.addPosition(pos, doPayloads ? payload : null,\n                                           doOffsets ? postings.startOffset() : -1,\n                                           doOffsets ? postings.endOffset() : -1);\n            }\n          } else if (doFreq) {\n            totalTF += freq;\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.docFreq, doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.docFreq;\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":["ddf662c415c0d0ad543e5314fcdf8396cd2f1b8d"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a45bec74b98f6fc05f52770cfb425739e6563960","date":1375119292,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? fieldMaxIndexOption : random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n    FieldsConsumer fieldsConsumer = codec.postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,Long> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDoc);\n      for(Map.Entry<BytesRef,Long> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        SeedPostings postings = getSeedPostings(term.utf8ToString(), termEnt.getValue(), false, maxAllowed);\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.docFreq + \" seed=\" + termEnt.getValue());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docID = 0;\n        while((docID = postings.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          final int freq = postings.freq();\n          if (VERBOSE) {\n            System.out.println(\"    \" + postings.upto + \": docID=\" + docID + \" freq=\" + postings.freq);\n          }\n          postingsConsumer.startDoc(docID, doFreq ? postings.freq : -1);\n          seenDocs.set(docID);\n          if (doPos) {\n            totalTF += postings.freq;\n            for(int posUpto=0;posUpto<freq;posUpto++) {\n              int pos = postings.nextPosition();\n              BytesRef payload = postings.getPayload();\n\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos + \" payload=\" + (payload == null ? \"null\" : payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos);\n                }\n              }\n              postingsConsumer.addPosition(pos, doPayloads ? payload : null,\n                                           doOffsets ? postings.startOffset() : -1,\n                                           doOffsets ? postings.endOffset() : -1);\n            }\n          } else if (doFreq) {\n            totalTF += freq;\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.docFreq, doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.docFreq;\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? fieldMaxIndexOption : random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, new IOContext(new FlushInfo(maxDoc, bytes)));\n    FieldsConsumer fieldsConsumer = codec.postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,Long> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDoc);\n      for(Map.Entry<BytesRef,Long> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        SeedPostings postings = getSeedPostings(term.utf8ToString(), termEnt.getValue(), false, maxAllowed);\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.docFreq + \" seed=\" + termEnt.getValue());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docID = 0;\n        while((docID = postings.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          final int freq = postings.freq();\n          if (VERBOSE) {\n            System.out.println(\"    \" + postings.upto + \": docID=\" + docID + \" freq=\" + postings.freq);\n          }\n          postingsConsumer.startDoc(docID, doFreq ? postings.freq : -1);\n          seenDocs.set(docID);\n          if (doPos) {\n            totalTF += postings.freq;\n            for(int posUpto=0;posUpto<freq;posUpto++) {\n              int pos = postings.nextPosition();\n              BytesRef payload = postings.getPayload();\n\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos + \" payload=\" + (payload == null ? \"null\" : payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos);\n                }\n              }\n              postingsConsumer.addPosition(pos, doPayloads ? payload : null,\n                                           doOffsets ? postings.startOffset() : -1,\n                                           doOffsets ? postings.endOffset() : -1);\n            }\n          } else if (doFreq) {\n            totalTF += freq;\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.docFreq, doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.docFreq;\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","date":1376366778,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? fieldMaxIndexOption : random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n    FieldsConsumer fieldsConsumer = codec.postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,Long> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDoc);\n      for(Map.Entry<BytesRef,Long> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        SeedPostings postings = getSeedPostings(term.utf8ToString(), termEnt.getValue(), false, maxAllowed);\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.docFreq + \" seed=\" + termEnt.getValue());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docID = 0;\n        while((docID = postings.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          final int freq = postings.freq();\n          if (VERBOSE) {\n            System.out.println(\"    \" + postings.upto + \": docID=\" + docID + \" freq=\" + postings.freq);\n          }\n          postingsConsumer.startDoc(docID, doFreq ? postings.freq : -1);\n          seenDocs.set(docID);\n          if (doPos) {\n            totalTF += postings.freq;\n            for(int posUpto=0;posUpto<freq;posUpto++) {\n              int pos = postings.nextPosition();\n              BytesRef payload = postings.getPayload();\n\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos + \" payload=\" + (payload == null ? \"null\" : payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos);\n                }\n              }\n              postingsConsumer.addPosition(pos, doPayloads ? payload : null,\n                                           doOffsets ? postings.startOffset() : -1,\n                                           doOffsets ? postings.endOffset() : -1);\n            }\n          } else if (doFreq) {\n            totalTF += freq;\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.docFreq, doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.docFreq;\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? fieldMaxIndexOption : random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         32, null, new IOContext(new FlushInfo(maxDoc, bytes)));\n    FieldsConsumer fieldsConsumer = codec.postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,Long> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDoc);\n      for(Map.Entry<BytesRef,Long> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        SeedPostings postings = getSeedPostings(term.utf8ToString(), termEnt.getValue(), false, maxAllowed);\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.docFreq + \" seed=\" + termEnt.getValue());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docID = 0;\n        while((docID = postings.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          final int freq = postings.freq();\n          if (VERBOSE) {\n            System.out.println(\"    \" + postings.upto + \": docID=\" + docID + \" freq=\" + postings.freq);\n          }\n          postingsConsumer.startDoc(docID, doFreq ? postings.freq : -1);\n          seenDocs.set(docID);\n          if (doPos) {\n            totalTF += postings.freq;\n            for(int posUpto=0;posUpto<freq;posUpto++) {\n              int pos = postings.nextPosition();\n              BytesRef payload = postings.getPayload();\n\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos + \" payload=\" + (payload == null ? \"null\" : payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos);\n                }\n              }\n              postingsConsumer.addPosition(pos, doPayloads ? payload : null,\n                                           doOffsets ? postings.startOffset() : -1,\n                                           doOffsets ? postings.endOffset() : -1);\n            }\n          } else if (doFreq) {\n            totalTF += freq;\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.docFreq, doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.docFreq;\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"82ffd58510acfc0e2e788a90a10002e689ec9145","date":1379018753,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? fieldMaxIndexOption : random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n    FieldsConsumer fieldsConsumer = codec.postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,Long> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDoc);\n      for(Map.Entry<BytesRef,Long> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        SeedPostings postings = getSeedPostings(term.utf8ToString(), termEnt.getValue(), false, maxAllowed);\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.docFreq + \" seed=\" + termEnt.getValue());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docID = 0;\n        while((docID = postings.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          final int freq = postings.freq();\n          if (VERBOSE) {\n            System.out.println(\"    \" + postings.upto + \": docID=\" + docID + \" freq=\" + postings.freq);\n          }\n          postingsConsumer.startDoc(docID, doFreq ? postings.freq : -1);\n          seenDocs.set(docID);\n          if (doPos) {\n            totalTF += postings.freq;\n            for(int posUpto=0;posUpto<freq;posUpto++) {\n              int pos = postings.nextPosition();\n              BytesRef payload = postings.getPayload();\n\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos + \" payload=\" + (payload == null ? \"null\" : payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos);\n                }\n              }\n              postingsConsumer.addPosition(pos, doPayloads ? payload : null,\n                                           doOffsets ? postings.startOffset() : -1,\n                                           doOffsets ? postings.endOffset() : -1);\n            }\n          } else if (doFreq) {\n            totalTF += freq;\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.docFreq, doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.docFreq;\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? fieldMaxIndexOption : random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n    FieldsConsumer fieldsConsumer = codec.postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,Long> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDoc);\n      for(Map.Entry<BytesRef,Long> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        SeedPostings postings = getSeedPostings(term.utf8ToString(), termEnt.getValue(), false, maxAllowed);\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.docFreq + \" seed=\" + termEnt.getValue());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docID = 0;\n        while((docID = postings.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          final int freq = postings.freq();\n          if (VERBOSE) {\n            System.out.println(\"    \" + postings.upto + \": docID=\" + docID + \" freq=\" + postings.freq);\n          }\n          postingsConsumer.startDoc(docID, doFreq ? postings.freq : -1);\n          seenDocs.set(docID);\n          if (doPos) {\n            totalTF += postings.freq;\n            for(int posUpto=0;posUpto<freq;posUpto++) {\n              int pos = postings.nextPosition();\n              BytesRef payload = postings.getPayload();\n\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos + \" payload=\" + (payload == null ? \"null\" : payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos);\n                }\n              }\n              postingsConsumer.addPosition(pos, doPayloads ? payload : null,\n                                           doOffsets ? postings.startOffset() : -1,\n                                           doOffsets ? postings.endOffset() : -1);\n            }\n          } else if (doFreq) {\n            totalTF += freq;\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.docFreq, doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.docFreq;\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5","date":1379624229,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? fieldMaxIndexOption : random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    codec.postingsFormat().fieldsConsumer(writeState).write(seedFields);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? fieldMaxIndexOption : random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n    FieldsConsumer fieldsConsumer = codec.postingsFormat().fieldsConsumer(writeState);\n\n    for(Map.Entry<String,Map<BytesRef,Long>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      Map<BytesRef,Long> terms = fieldEnt.getValue();\n\n      FieldInfo fieldInfo = newFieldInfos.fieldInfo(field);\n\n      IndexOptions indexOptions = fieldInfo.getIndexOptions();\n\n      if (VERBOSE) {\n        System.out.println(\"field=\" + field + \" indexOtions=\" + indexOptions);\n      }\n\n      boolean doFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n      boolean doPos = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n      boolean doOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n      \n      TermsConsumer termsConsumer = fieldsConsumer.addField(fieldInfo);\n      long sumTotalTF = 0;\n      long sumDF = 0;\n      FixedBitSet seenDocs = new FixedBitSet(maxDoc);\n      for(Map.Entry<BytesRef,Long> termEnt : terms.entrySet()) {\n        BytesRef term = termEnt.getKey();\n        SeedPostings postings = getSeedPostings(term.utf8ToString(), termEnt.getValue(), false, maxAllowed);\n        if (VERBOSE) {\n          System.out.println(\"  term=\" + field + \":\" + term.utf8ToString() + \" docFreq=\" + postings.docFreq + \" seed=\" + termEnt.getValue());\n        }\n        \n        PostingsConsumer postingsConsumer = termsConsumer.startTerm(term);\n        long totalTF = 0;\n        int docID = 0;\n        while((docID = postings.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n          final int freq = postings.freq();\n          if (VERBOSE) {\n            System.out.println(\"    \" + postings.upto + \": docID=\" + docID + \" freq=\" + postings.freq);\n          }\n          postingsConsumer.startDoc(docID, doFreq ? postings.freq : -1);\n          seenDocs.set(docID);\n          if (doPos) {\n            totalTF += postings.freq;\n            for(int posUpto=0;posUpto<freq;posUpto++) {\n              int pos = postings.nextPosition();\n              BytesRef payload = postings.getPayload();\n\n              if (VERBOSE) {\n                if (doPayloads) {\n                  System.out.println(\"      pos=\" + pos + \" payload=\" + (payload == null ? \"null\" : payload.length + \" bytes\"));\n                } else {\n                  System.out.println(\"      pos=\" + pos);\n                }\n              }\n              postingsConsumer.addPosition(pos, doPayloads ? payload : null,\n                                           doOffsets ? postings.startOffset() : -1,\n                                           doOffsets ? postings.endOffset() : -1);\n            }\n          } else if (doFreq) {\n            totalTF += freq;\n          } else {\n            totalTF++;\n          }\n          postingsConsumer.finishDoc();\n        }\n        termsConsumer.finishTerm(term, new TermStats(postings.docFreq, doFreq ? totalTF : -1));\n        sumTotalTF += totalTF;\n        sumDF += postings.docFreq;\n      }\n\n      termsConsumer.finish(doFreq ? sumTotalTF : -1, sumDF, seenDocs.cardinality());\n    }\n\n    fieldsConsumer.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"77f264c55cbf75404f8601ae7290d69157273a56","date":1380484282,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? fieldMaxIndexOption : random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    codec.postingsFormat().fieldsConsumer(writeState).write(seedFields);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? fieldMaxIndexOption : random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    codec.postingsFormat().fieldsConsumer(writeState).write(seedFields);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? fieldMaxIndexOption : random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    codec.postingsFormat().fieldsConsumer(writeState).write(seedFields);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = _TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? fieldMaxIndexOption : random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    codec.postingsFormat().fieldsConsumer(writeState).write(seedFields);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"91d86ebcdb45ce6a1b2584e2603f76db47523d0a","date":1396466913,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    codec.postingsFormat().fieldsConsumer(writeState).write(seedFields);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? fieldMaxIndexOption : random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    codec.postingsFormat().fieldsConsumer(writeState).write(seedFields);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5eb2511ababf862ea11e10761c70ee560cd84510","date":1396607225,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    codec.postingsFormat().fieldsConsumer(writeState).write(seedFields);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n\n      String pf = TestUtil.getPostingsFormat(codec, oldFieldInfo.name);\n      int fieldMaxIndexOption;\n      if (doesntSupportOffsets.contains(pf)) {\n        fieldMaxIndexOption = Math.min(maxIndexOptionNoOffsets, maxIndexOption);\n      } else {\n        fieldMaxIndexOption = maxIndexOption;\n      }\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? fieldMaxIndexOption : random().nextInt(1+fieldMaxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    codec.postingsFormat().fieldsConsumer(writeState).write(seedFields);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7afb64ff3a701f68b2689cafff6c5bdeb4f67f63","date":1398957288,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    codec.postingsFormat().fieldsConsumer(writeState).write(seedFields);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ca02c6cbb1ba28fd0bd36bd0f8a2ac84be5f0e71","date":1400675008,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a656b32c3aa151037a8c52e9b134acc3cbf482bc","date":1400688195,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7605579001505896d48b07160075a5c8b8e128e","date":1400758727,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2e0cd078dfbe2df4d73a3db81ed598a118caf5fe","date":1405588742,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    int maxIndexOptionNoOffsets = Arrays.asList(IndexOptions.values()).indexOf(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"057a1793765d068ea9302f1a29e21734ee58d41e","date":1408130117,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, \"_0\", maxDoc, false, codec, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":["5f6bd27530a2846413fe2d00030493c0e2d3a072"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c65d2864d936ccf22dc7ec14dd48b4dff7bacceb","date":1411653326,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, null, StringHelper.randomId());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":["8521d944f9dfb45692ec28235dbf116d47ef69ba"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5f6bd27530a2846413fe2d00030493c0e2d3a072","date":1411811855,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, null, StringHelper.randomId());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":["057a1793765d068ea9302f1a29e21734ee58d41e"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"05da2d758a6089e737cdfc230e57a51b472b94b6","date":1413392310,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, null, StringHelper.randomId());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, null, StringHelper.randomId());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c93b0dbaa6abe99bc8d1b476bcacc27b324b2b84","date":1413458798,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, null, StringHelper.randomId());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, null, StringHelper.randomId());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   DocValuesType.NUMERIC,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3184874f7f3aca850248483485b4995343066875","date":1413876758,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, null, StringHelper.randomId());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, null, StringHelper.randomId());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0a22eafe3f72a4c2945eaad9547e6c78816978f4","date":1413956657,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, null, StringHelper.randomId());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, null, StringHelper.randomId());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   true,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2bb2842e561df4e8e9ad89010605fc86ac265465","date":1414768208,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, null, StringHelper.randomId());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random(), 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NO,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, null, StringHelper.randomId());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : random().nextInt(1+maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   null,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f95ce1375367b92d411a06175eab3915fe93c6bc","date":1414788502,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, null, StringHelper.randomId());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random(), 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, null, StringHelper.randomId());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random(), 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NO,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8521d944f9dfb45692ec28235dbf116d47ef69ba","date":1417535150,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random(), 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, null, StringHelper.randomId());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random(), 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":["c65d2864d936ccf22dc7ec14dd48b4dff7bacceb"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"79700663e164dece87bed4adfd3e28bab6cb1385","date":1425241849,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random(), 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>());\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random(), 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"299a2348fa24151d150182211b6208a38e5e3450","date":1425304608,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random(), 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>());\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random(), 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random(), 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>());\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random(), 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   null);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3e8715d826e588419327562287d5d6a8040d63d6","date":1427987148,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>());\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random(), 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>());\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d2638f781be724518ff6c2263d14a48cf6e68017","date":1427989059,"type":5,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>());\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random(), 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>());\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"29baaefef1b62d76a3370ff72a0fe5f9bd84e365":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"82ffd58510acfc0e2e788a90a10002e689ec9145":["a45bec74b98f6fc05f52770cfb425739e6563960"],"91d86ebcdb45ce6a1b2584e2603f76db47523d0a":["6613659748fe4411a7dcf85266e55db1f95f7315"],"5f6bd27530a2846413fe2d00030493c0e2d3a072":["057a1793765d068ea9302f1a29e21734ee58d41e","c65d2864d936ccf22dc7ec14dd48b4dff7bacceb"],"c4108b7b6cc881d8b7c6ba2df50010b7bc581a39":["29baaefef1b62d76a3370ff72a0fe5f9bd84e365"],"2bb2842e561df4e8e9ad89010605fc86ac265465":["3184874f7f3aca850248483485b4995343066875"],"2e0cd078dfbe2df4d73a3db81ed598a118caf5fe":["ca02c6cbb1ba28fd0bd36bd0f8a2ac84be5f0e71"],"b7605579001505896d48b07160075a5c8b8e128e":["7afb64ff3a701f68b2689cafff6c5bdeb4f67f63","ca02c6cbb1ba28fd0bd36bd0f8a2ac84be5f0e71"],"057a1793765d068ea9302f1a29e21734ee58d41e":["2e0cd078dfbe2df4d73a3db81ed598a118caf5fe"],"7afb64ff3a701f68b2689cafff6c5bdeb4f67f63":["91d86ebcdb45ce6a1b2584e2603f76db47523d0a"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["8521d944f9dfb45692ec28235dbf116d47ef69ba","79700663e164dece87bed4adfd3e28bab6cb1385"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d2638f781be724518ff6c2263d14a48cf6e68017":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","3e8715d826e588419327562287d5d6a8040d63d6"],"79700663e164dece87bed4adfd3e28bab6cb1385":["8521d944f9dfb45692ec28235dbf116d47ef69ba"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"0837ab0472feecb3a54260729d845f839e1cbd72":["2fe4b12f0dcf02b1690143f2ad02d8f89625eb36"],"77f264c55cbf75404f8601ae7290d69157273a56":["519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5"],"299a2348fa24151d150182211b6208a38e5e3450":["8521d944f9dfb45692ec28235dbf116d47ef69ba","79700663e164dece87bed4adfd3e28bab6cb1385"],"2fe4b12f0dcf02b1690143f2ad02d8f89625eb36":["c4108b7b6cc881d8b7c6ba2df50010b7bc581a39"],"05da2d758a6089e737cdfc230e57a51b472b94b6":["5f6bd27530a2846413fe2d00030493c0e2d3a072"],"5eb2511ababf862ea11e10761c70ee560cd84510":["6613659748fe4411a7dcf85266e55db1f95f7315","91d86ebcdb45ce6a1b2584e2603f76db47523d0a"],"c93b0dbaa6abe99bc8d1b476bcacc27b324b2b84":["c65d2864d936ccf22dc7ec14dd48b4dff7bacceb","05da2d758a6089e737cdfc230e57a51b472b94b6"],"6613659748fe4411a7dcf85266e55db1f95f7315":["77f264c55cbf75404f8601ae7290d69157273a56"],"a45bec74b98f6fc05f52770cfb425739e6563960":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"f95ce1375367b92d411a06175eab3915fe93c6bc":["2bb2842e561df4e8e9ad89010605fc86ac265465"],"3184874f7f3aca850248483485b4995343066875":["05da2d758a6089e737cdfc230e57a51b472b94b6"],"8521d944f9dfb45692ec28235dbf116d47ef69ba":["f95ce1375367b92d411a06175eab3915fe93c6bc"],"0a22eafe3f72a4c2945eaad9547e6c78816978f4":["c93b0dbaa6abe99bc8d1b476bcacc27b324b2b84","3184874f7f3aca850248483485b4995343066875"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["2fe4b12f0dcf02b1690143f2ad02d8f89625eb36","0837ab0472feecb3a54260729d845f839e1cbd72"],"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5":["82ffd58510acfc0e2e788a90a10002e689ec9145"],"a656b32c3aa151037a8c52e9b134acc3cbf482bc":["7afb64ff3a701f68b2689cafff6c5bdeb4f67f63","ca02c6cbb1ba28fd0bd36bd0f8a2ac84be5f0e71"],"3e8715d826e588419327562287d5d6a8040d63d6":["79700663e164dece87bed4adfd3e28bab6cb1385"],"ca02c6cbb1ba28fd0bd36bd0f8a2ac84be5f0e71":["7afb64ff3a701f68b2689cafff6c5bdeb4f67f63"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3e8715d826e588419327562287d5d6a8040d63d6"],"c65d2864d936ccf22dc7ec14dd48b4dff7bacceb":["057a1793765d068ea9302f1a29e21734ee58d41e"]},"commit2Childs":{"29baaefef1b62d76a3370ff72a0fe5f9bd84e365":["c4108b7b6cc881d8b7c6ba2df50010b7bc581a39"],"82ffd58510acfc0e2e788a90a10002e689ec9145":["519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5"],"91d86ebcdb45ce6a1b2584e2603f76db47523d0a":["7afb64ff3a701f68b2689cafff6c5bdeb4f67f63","5eb2511ababf862ea11e10761c70ee560cd84510"],"5f6bd27530a2846413fe2d00030493c0e2d3a072":["05da2d758a6089e737cdfc230e57a51b472b94b6"],"c4108b7b6cc881d8b7c6ba2df50010b7bc581a39":["2fe4b12f0dcf02b1690143f2ad02d8f89625eb36"],"2bb2842e561df4e8e9ad89010605fc86ac265465":["f95ce1375367b92d411a06175eab3915fe93c6bc"],"2e0cd078dfbe2df4d73a3db81ed598a118caf5fe":["057a1793765d068ea9302f1a29e21734ee58d41e"],"b7605579001505896d48b07160075a5c8b8e128e":[],"057a1793765d068ea9302f1a29e21734ee58d41e":["5f6bd27530a2846413fe2d00030493c0e2d3a072","c65d2864d936ccf22dc7ec14dd48b4dff7bacceb"],"7afb64ff3a701f68b2689cafff6c5bdeb4f67f63":["b7605579001505896d48b07160075a5c8b8e128e","a656b32c3aa151037a8c52e9b134acc3cbf482bc","ca02c6cbb1ba28fd0bd36bd0f8a2ac84be5f0e71"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["d2638f781be724518ff6c2263d14a48cf6e68017"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["29baaefef1b62d76a3370ff72a0fe5f9bd84e365"],"d2638f781be724518ff6c2263d14a48cf6e68017":[],"79700663e164dece87bed4adfd3e28bab6cb1385":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","299a2348fa24151d150182211b6208a38e5e3450","3e8715d826e588419327562287d5d6a8040d63d6"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":[],"0837ab0472feecb3a54260729d845f839e1cbd72":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"77f264c55cbf75404f8601ae7290d69157273a56":["6613659748fe4411a7dcf85266e55db1f95f7315"],"299a2348fa24151d150182211b6208a38e5e3450":[],"2fe4b12f0dcf02b1690143f2ad02d8f89625eb36":["0837ab0472feecb3a54260729d845f839e1cbd72","d4d69c535930b5cce125cff868d40f6373dc27d4"],"05da2d758a6089e737cdfc230e57a51b472b94b6":["c93b0dbaa6abe99bc8d1b476bcacc27b324b2b84","3184874f7f3aca850248483485b4995343066875"],"5eb2511ababf862ea11e10761c70ee560cd84510":[],"c93b0dbaa6abe99bc8d1b476bcacc27b324b2b84":["0a22eafe3f72a4c2945eaad9547e6c78816978f4"],"a45bec74b98f6fc05f52770cfb425739e6563960":["82ffd58510acfc0e2e788a90a10002e689ec9145"],"6613659748fe4411a7dcf85266e55db1f95f7315":["91d86ebcdb45ce6a1b2584e2603f76db47523d0a","5eb2511ababf862ea11e10761c70ee560cd84510"],"3184874f7f3aca850248483485b4995343066875":["2bb2842e561df4e8e9ad89010605fc86ac265465","0a22eafe3f72a4c2945eaad9547e6c78816978f4"],"f95ce1375367b92d411a06175eab3915fe93c6bc":["8521d944f9dfb45692ec28235dbf116d47ef69ba"],"8521d944f9dfb45692ec28235dbf116d47ef69ba":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","79700663e164dece87bed4adfd3e28bab6cb1385","299a2348fa24151d150182211b6208a38e5e3450"],"0a22eafe3f72a4c2945eaad9547e6c78816978f4":[],"d4d69c535930b5cce125cff868d40f6373dc27d4":["8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","a45bec74b98f6fc05f52770cfb425739e6563960"],"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5":["77f264c55cbf75404f8601ae7290d69157273a56"],"a656b32c3aa151037a8c52e9b134acc3cbf482bc":[],"3e8715d826e588419327562287d5d6a8040d63d6":["d2638f781be724518ff6c2263d14a48cf6e68017","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ca02c6cbb1ba28fd0bd36bd0f8a2ac84be5f0e71":["2e0cd078dfbe2df4d73a3db81ed598a118caf5fe","b7605579001505896d48b07160075a5c8b8e128e","a656b32c3aa151037a8c52e9b134acc3cbf482bc"],"c65d2864d936ccf22dc7ec14dd48b4dff7bacceb":["5f6bd27530a2846413fe2d00030493c0e2d3a072","c93b0dbaa6abe99bc8d1b476bcacc27b324b2b84"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b7605579001505896d48b07160075a5c8b8e128e","d2638f781be724518ff6c2263d14a48cf6e68017","8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","299a2348fa24151d150182211b6208a38e5e3450","5eb2511ababf862ea11e10761c70ee560cd84510","0a22eafe3f72a4c2945eaad9547e6c78816978f4","a656b32c3aa151037a8c52e9b134acc3cbf482bc","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}