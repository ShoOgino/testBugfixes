{"path":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.BlockState#document(int).mjava","commits":[{"id":"1f09f483a0844bb9dc34fb10380cb053aa96219b","date":1418894001,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.BlockState#document(int).mjava","pathOld":"/dev/null","sourceNew":"    /**\n     * Get the serialized representation of the given docID. This docID has\n     * to be contained in the current block.\n     */\n    SerializedDocument document(int docID) throws IOException {\n      if (contains(docID) == false) {\n        throw new IllegalArgumentException();\n      }\n\n      final int index = docID - docBase;\n      final int offset = offsets[index];\n      final int length = offsets[index+1] - offset;\n      final int totalLength = offsets[chunkDocs];\n      final int numStoredFields = this.numStoredFields[index];\n\n      fieldsStream.seek(startPointer);\n\n      final DataInput documentInput;\n      if (length == 0) {\n        // empty\n        documentInput = new ByteArrayDataInput();\n      } else if (merging) {\n        // already decompressed\n        documentInput = new ByteArrayDataInput(bytes.bytes, bytes.offset + offset, length);\n      } else if (sliced) {\n        decompressor.decompress(fieldsStream, chunkSize, offset, Math.min(length, chunkSize - offset), bytes);\n        documentInput = new DataInput() {\n\n          int decompressed = bytes.length;\n\n          void fillBuffer() throws IOException {\n            assert decompressed <= length;\n            if (decompressed == length) {\n              throw new EOFException();\n            }\n            final int toDecompress = Math.min(length - decompressed, chunkSize);\n            decompressor.decompress(fieldsStream, toDecompress, 0, toDecompress, bytes);\n            decompressed += toDecompress;\n          }\n\n          @Override\n          public byte readByte() throws IOException {\n            if (bytes.length == 0) {\n              fillBuffer();\n            }\n            --bytes.length;\n            return bytes.bytes[bytes.offset++];\n          }\n\n          @Override\n          public void readBytes(byte[] b, int offset, int len) throws IOException {\n            while (len > bytes.length) {\n              System.arraycopy(bytes.bytes, bytes.offset, b, offset, bytes.length);\n              len -= bytes.length;\n              offset += bytes.length;\n              fillBuffer();\n            }\n            System.arraycopy(bytes.bytes, bytes.offset, b, offset, len);\n            bytes.offset += len;\n            bytes.length -= len;\n          }\n\n        };\n      } else {\n        decompressor.decompress(fieldsStream, totalLength, offset, length, bytes);\n        assert bytes.length == length;\n        documentInput = new ByteArrayDataInput(bytes.bytes, bytes.offset, bytes.length);\n      }\n\n      return new SerializedDocument(documentInput, length, numStoredFields);\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bd7962f4da329a4e559727022b752c5cefaee5da","date":1421356185,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.BlockState#document(int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.BlockState#document(int).mjava","sourceNew":"    /**\n     * Get the serialized representation of the given docID. This docID has\n     * to be contained in the current block.\n     */\n    SerializedDocument document(int docID) throws IOException {\n      if (contains(docID) == false) {\n        throw new IllegalArgumentException();\n      }\n\n      final int index = docID - docBase;\n      final int offset = offsets[index];\n      final int length = offsets[index+1] - offset;\n      final int totalLength = offsets[chunkDocs];\n      final int numStoredFields = this.numStoredFields[index];\n\n      final DataInput documentInput;\n      if (length == 0) {\n        // empty\n        documentInput = new ByteArrayDataInput();\n      } else if (merging) {\n        // already decompressed\n        documentInput = new ByteArrayDataInput(bytes.bytes, bytes.offset + offset, length);\n      } else if (sliced) {\n        fieldsStream.seek(startPointer);\n        decompressor.decompress(fieldsStream, chunkSize, offset, Math.min(length, chunkSize - offset), bytes);\n        documentInput = new DataInput() {\n\n          int decompressed = bytes.length;\n\n          void fillBuffer() throws IOException {\n            assert decompressed <= length;\n            if (decompressed == length) {\n              throw new EOFException();\n            }\n            final int toDecompress = Math.min(length - decompressed, chunkSize);\n            decompressor.decompress(fieldsStream, toDecompress, 0, toDecompress, bytes);\n            decompressed += toDecompress;\n          }\n\n          @Override\n          public byte readByte() throws IOException {\n            if (bytes.length == 0) {\n              fillBuffer();\n            }\n            --bytes.length;\n            return bytes.bytes[bytes.offset++];\n          }\n\n          @Override\n          public void readBytes(byte[] b, int offset, int len) throws IOException {\n            while (len > bytes.length) {\n              System.arraycopy(bytes.bytes, bytes.offset, b, offset, bytes.length);\n              len -= bytes.length;\n              offset += bytes.length;\n              fillBuffer();\n            }\n            System.arraycopy(bytes.bytes, bytes.offset, b, offset, len);\n            bytes.offset += len;\n            bytes.length -= len;\n          }\n\n        };\n      } else {\n        fieldsStream.seek(startPointer);\n        decompressor.decompress(fieldsStream, totalLength, offset, length, bytes);\n        assert bytes.length == length;\n        documentInput = new ByteArrayDataInput(bytes.bytes, bytes.offset, bytes.length);\n      }\n\n      return new SerializedDocument(documentInput, length, numStoredFields);\n    }\n\n","sourceOld":"    /**\n     * Get the serialized representation of the given docID. This docID has\n     * to be contained in the current block.\n     */\n    SerializedDocument document(int docID) throws IOException {\n      if (contains(docID) == false) {\n        throw new IllegalArgumentException();\n      }\n\n      final int index = docID - docBase;\n      final int offset = offsets[index];\n      final int length = offsets[index+1] - offset;\n      final int totalLength = offsets[chunkDocs];\n      final int numStoredFields = this.numStoredFields[index];\n\n      fieldsStream.seek(startPointer);\n\n      final DataInput documentInput;\n      if (length == 0) {\n        // empty\n        documentInput = new ByteArrayDataInput();\n      } else if (merging) {\n        // already decompressed\n        documentInput = new ByteArrayDataInput(bytes.bytes, bytes.offset + offset, length);\n      } else if (sliced) {\n        decompressor.decompress(fieldsStream, chunkSize, offset, Math.min(length, chunkSize - offset), bytes);\n        documentInput = new DataInput() {\n\n          int decompressed = bytes.length;\n\n          void fillBuffer() throws IOException {\n            assert decompressed <= length;\n            if (decompressed == length) {\n              throw new EOFException();\n            }\n            final int toDecompress = Math.min(length - decompressed, chunkSize);\n            decompressor.decompress(fieldsStream, toDecompress, 0, toDecompress, bytes);\n            decompressed += toDecompress;\n          }\n\n          @Override\n          public byte readByte() throws IOException {\n            if (bytes.length == 0) {\n              fillBuffer();\n            }\n            --bytes.length;\n            return bytes.bytes[bytes.offset++];\n          }\n\n          @Override\n          public void readBytes(byte[] b, int offset, int len) throws IOException {\n            while (len > bytes.length) {\n              System.arraycopy(bytes.bytes, bytes.offset, b, offset, bytes.length);\n              len -= bytes.length;\n              offset += bytes.length;\n              fillBuffer();\n            }\n            System.arraycopy(bytes.bytes, bytes.offset, b, offset, len);\n            bytes.offset += len;\n            bytes.length -= len;\n          }\n\n        };\n      } else {\n        decompressor.decompress(fieldsStream, totalLength, offset, length, bytes);\n        assert bytes.length == length;\n        documentInput = new ByteArrayDataInput(bytes.bytes, bytes.offset, bytes.length);\n      }\n\n      return new SerializedDocument(documentInput, length, numStoredFields);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d36bc83b1e3a6e15aaa57b2efbec0e950d3fa31c","date":1599125854,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.BlockState#document(int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.BlockState#document(int).mjava","sourceNew":"    /**\n     * Get the serialized representation of the given docID. This docID has\n     * to be contained in the current block.\n     */\n    SerializedDocument document(int docID) throws IOException {\n      if (contains(docID) == false) {\n        throw new IllegalArgumentException();\n      }\n\n      final int index = docID - docBase;\n      final int offset = offsets[index];\n      final int length = offsets[index+1] - offset;\n      final int totalLength = offsets[chunkDocs];\n      final int numStoredFields = this.numStoredFields[index];\n\n      final BytesRef bytes;\n      if (merging) {\n        bytes = this.bytes;\n      } else {\n        bytes = new BytesRef();\n      }\n\n      final DataInput documentInput;\n      if (length == 0) {\n        // empty\n        documentInput = new ByteArrayDataInput();\n      } else if (merging) {\n        // already decompressed\n        documentInput = new ByteArrayDataInput(bytes.bytes, bytes.offset + offset, length);\n      } else if (sliced) {\n        fieldsStream.seek(startPointer);\n        decompressor.decompress(fieldsStream, chunkSize, offset, Math.min(length, chunkSize - offset), bytes);\n        documentInput = new DataInput() {\n\n          int decompressed = bytes.length;\n\n          void fillBuffer() throws IOException {\n            assert decompressed <= length;\n            if (decompressed == length) {\n              throw new EOFException();\n            }\n            final int toDecompress = Math.min(length - decompressed, chunkSize);\n            decompressor.decompress(fieldsStream, toDecompress, 0, toDecompress, bytes);\n            decompressed += toDecompress;\n          }\n\n          @Override\n          public byte readByte() throws IOException {\n            if (bytes.length == 0) {\n              fillBuffer();\n            }\n            --bytes.length;\n            return bytes.bytes[bytes.offset++];\n          }\n\n          @Override\n          public void readBytes(byte[] b, int offset, int len) throws IOException {\n            while (len > bytes.length) {\n              System.arraycopy(bytes.bytes, bytes.offset, b, offset, bytes.length);\n              len -= bytes.length;\n              offset += bytes.length;\n              fillBuffer();\n            }\n            System.arraycopy(bytes.bytes, bytes.offset, b, offset, len);\n            bytes.offset += len;\n            bytes.length -= len;\n          }\n\n        };\n      } else {\n        fieldsStream.seek(startPointer);\n        decompressor.decompress(fieldsStream, totalLength, offset, length, bytes);\n        assert bytes.length == length;\n        documentInput = new ByteArrayDataInput(bytes.bytes, bytes.offset, bytes.length);\n      }\n\n      return new SerializedDocument(documentInput, length, numStoredFields);\n    }\n\n","sourceOld":"    /**\n     * Get the serialized representation of the given docID. This docID has\n     * to be contained in the current block.\n     */\n    SerializedDocument document(int docID) throws IOException {\n      if (contains(docID) == false) {\n        throw new IllegalArgumentException();\n      }\n\n      final int index = docID - docBase;\n      final int offset = offsets[index];\n      final int length = offsets[index+1] - offset;\n      final int totalLength = offsets[chunkDocs];\n      final int numStoredFields = this.numStoredFields[index];\n\n      final DataInput documentInput;\n      if (length == 0) {\n        // empty\n        documentInput = new ByteArrayDataInput();\n      } else if (merging) {\n        // already decompressed\n        documentInput = new ByteArrayDataInput(bytes.bytes, bytes.offset + offset, length);\n      } else if (sliced) {\n        fieldsStream.seek(startPointer);\n        decompressor.decompress(fieldsStream, chunkSize, offset, Math.min(length, chunkSize - offset), bytes);\n        documentInput = new DataInput() {\n\n          int decompressed = bytes.length;\n\n          void fillBuffer() throws IOException {\n            assert decompressed <= length;\n            if (decompressed == length) {\n              throw new EOFException();\n            }\n            final int toDecompress = Math.min(length - decompressed, chunkSize);\n            decompressor.decompress(fieldsStream, toDecompress, 0, toDecompress, bytes);\n            decompressed += toDecompress;\n          }\n\n          @Override\n          public byte readByte() throws IOException {\n            if (bytes.length == 0) {\n              fillBuffer();\n            }\n            --bytes.length;\n            return bytes.bytes[bytes.offset++];\n          }\n\n          @Override\n          public void readBytes(byte[] b, int offset, int len) throws IOException {\n            while (len > bytes.length) {\n              System.arraycopy(bytes.bytes, bytes.offset, b, offset, bytes.length);\n              len -= bytes.length;\n              offset += bytes.length;\n              fillBuffer();\n            }\n            System.arraycopy(bytes.bytes, bytes.offset, b, offset, len);\n            bytes.offset += len;\n            bytes.length -= len;\n          }\n\n        };\n      } else {\n        fieldsStream.seek(startPointer);\n        decompressor.decompress(fieldsStream, totalLength, offset, length, bytes);\n        assert bytes.length == length;\n        documentInput = new ByteArrayDataInput(bytes.bytes, bytes.offset, bytes.length);\n      }\n\n      return new SerializedDocument(documentInput, length, numStoredFields);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c84d994a0fa92153d37e334f71c26ca6f6be0272","date":1600360257,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.BlockState#document(int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.BlockState#document(int).mjava","sourceNew":"    /**\n     * Get the serialized representation of the given docID. This docID has\n     * to be contained in the current block.\n     */\n    SerializedDocument document(int docID) throws IOException {\n      if (contains(docID) == false) {\n        throw new IllegalArgumentException();\n      }\n\n      final int index = docID - docBase;\n      final int offset = Math.toIntExact(offsets[index]);\n      final int length = Math.toIntExact(offsets[index+1]) - offset;\n      final int totalLength = Math.toIntExact(offsets[chunkDocs]);\n      final int numStoredFields = Math.toIntExact(this.numStoredFields[index]);\n\n      final BytesRef bytes;\n      if (merging) {\n        bytes = this.bytes;\n      } else {\n        bytes = new BytesRef();\n      }\n\n      final DataInput documentInput;\n      if (length == 0) {\n        // empty\n        documentInput = new ByteArrayDataInput();\n      } else if (merging) {\n        // already decompressed\n        documentInput = new ByteArrayDataInput(bytes.bytes, bytes.offset + offset, length);\n      } else if (sliced) {\n        fieldsStream.seek(startPointer);\n        decompressor.decompress(fieldsStream, chunkSize, offset, Math.min(length, chunkSize - offset), bytes);\n        documentInput = new DataInput() {\n\n          int decompressed = bytes.length;\n\n          void fillBuffer() throws IOException {\n            assert decompressed <= length;\n            if (decompressed == length) {\n              throw new EOFException();\n            }\n            final int toDecompress = Math.min(length - decompressed, chunkSize);\n            decompressor.decompress(fieldsStream, toDecompress, 0, toDecompress, bytes);\n            decompressed += toDecompress;\n          }\n\n          @Override\n          public byte readByte() throws IOException {\n            if (bytes.length == 0) {\n              fillBuffer();\n            }\n            --bytes.length;\n            return bytes.bytes[bytes.offset++];\n          }\n\n          @Override\n          public void readBytes(byte[] b, int offset, int len) throws IOException {\n            while (len > bytes.length) {\n              System.arraycopy(bytes.bytes, bytes.offset, b, offset, bytes.length);\n              len -= bytes.length;\n              offset += bytes.length;\n              fillBuffer();\n            }\n            System.arraycopy(bytes.bytes, bytes.offset, b, offset, len);\n            bytes.offset += len;\n            bytes.length -= len;\n          }\n\n        };\n      } else {\n        fieldsStream.seek(startPointer);\n        decompressor.decompress(fieldsStream, totalLength, offset, length, bytes);\n        assert bytes.length == length;\n        documentInput = new ByteArrayDataInput(bytes.bytes, bytes.offset, bytes.length);\n      }\n\n      return new SerializedDocument(documentInput, length, numStoredFields);\n    }\n\n","sourceOld":"    /**\n     * Get the serialized representation of the given docID. This docID has\n     * to be contained in the current block.\n     */\n    SerializedDocument document(int docID) throws IOException {\n      if (contains(docID) == false) {\n        throw new IllegalArgumentException();\n      }\n\n      final int index = docID - docBase;\n      final int offset = offsets[index];\n      final int length = offsets[index+1] - offset;\n      final int totalLength = offsets[chunkDocs];\n      final int numStoredFields = this.numStoredFields[index];\n\n      final BytesRef bytes;\n      if (merging) {\n        bytes = this.bytes;\n      } else {\n        bytes = new BytesRef();\n      }\n\n      final DataInput documentInput;\n      if (length == 0) {\n        // empty\n        documentInput = new ByteArrayDataInput();\n      } else if (merging) {\n        // already decompressed\n        documentInput = new ByteArrayDataInput(bytes.bytes, bytes.offset + offset, length);\n      } else if (sliced) {\n        fieldsStream.seek(startPointer);\n        decompressor.decompress(fieldsStream, chunkSize, offset, Math.min(length, chunkSize - offset), bytes);\n        documentInput = new DataInput() {\n\n          int decompressed = bytes.length;\n\n          void fillBuffer() throws IOException {\n            assert decompressed <= length;\n            if (decompressed == length) {\n              throw new EOFException();\n            }\n            final int toDecompress = Math.min(length - decompressed, chunkSize);\n            decompressor.decompress(fieldsStream, toDecompress, 0, toDecompress, bytes);\n            decompressed += toDecompress;\n          }\n\n          @Override\n          public byte readByte() throws IOException {\n            if (bytes.length == 0) {\n              fillBuffer();\n            }\n            --bytes.length;\n            return bytes.bytes[bytes.offset++];\n          }\n\n          @Override\n          public void readBytes(byte[] b, int offset, int len) throws IOException {\n            while (len > bytes.length) {\n              System.arraycopy(bytes.bytes, bytes.offset, b, offset, bytes.length);\n              len -= bytes.length;\n              offset += bytes.length;\n              fillBuffer();\n            }\n            System.arraycopy(bytes.bytes, bytes.offset, b, offset, len);\n            bytes.offset += len;\n            bytes.length -= len;\n          }\n\n        };\n      } else {\n        fieldsStream.seek(startPointer);\n        decompressor.decompress(fieldsStream, totalLength, offset, length, bytes);\n        assert bytes.length == length;\n        documentInput = new ByteArrayDataInput(bytes.bytes, bytes.offset, bytes.length);\n      }\n\n      return new SerializedDocument(documentInput, length, numStoredFields);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"d36bc83b1e3a6e15aaa57b2efbec0e950d3fa31c":["bd7962f4da329a4e559727022b752c5cefaee5da"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1f09f483a0844bb9dc34fb10380cb053aa96219b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"bd7962f4da329a4e559727022b752c5cefaee5da":["1f09f483a0844bb9dc34fb10380cb053aa96219b"],"c84d994a0fa92153d37e334f71c26ca6f6be0272":["d36bc83b1e3a6e15aaa57b2efbec0e950d3fa31c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c84d994a0fa92153d37e334f71c26ca6f6be0272"]},"commit2Childs":{"d36bc83b1e3a6e15aaa57b2efbec0e950d3fa31c":["c84d994a0fa92153d37e334f71c26ca6f6be0272"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1f09f483a0844bb9dc34fb10380cb053aa96219b"],"1f09f483a0844bb9dc34fb10380cb053aa96219b":["bd7962f4da329a4e559727022b752c5cefaee5da"],"bd7962f4da329a4e559727022b752c5cefaee5da":["d36bc83b1e3a6e15aaa57b2efbec0e950d3fa31c"],"c84d994a0fa92153d37e334f71c26ca6f6be0272":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}