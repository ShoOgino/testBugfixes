{"path":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#doTest().mjava","commits":[{"id":"70f91c8322fbffe3a3a897ef20ea19119cac10cd","date":1386170124,"type":1,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#doTest().mjava","pathOld":"solr/contrib/solr-mr/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n    jobConf.setBoolean(\"ignoreTikaException\", false);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    HttpSolrServer server = new HttpSolrServer(cloudJettys.get(0).url);\n    String[] args = new String[]{};\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--log4j=\" + ExternalPaths.SOURCE_HOME + \"/core/src/test-files/log4j.properties\",\n        \"--mappers=3\",\n        ++numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n        \"--verbose\",\n        \"--go-live\"\n    };\n    args = prependInitialArgs(args);\n    List<String> argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      \n      res = ToolRunner.run(jobConf, tool, args);\n      \n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      results = server.query(new SolrQuery(\"*:*\"));\n      assertEquals(20, results.getResults().getNumFound());\n    }    \n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true); \n    assertTrue(fs.mkdirs(inDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\",\n        ++numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n    };\n    args = prependInitialArgs(args);\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());      \n      results = server.query(new SolrQuery(\"*:*\"));\n      \n      assertEquals(22, results.getResults().getNumFound());\n    }    \n    \n    // try using zookeeper\n    String collection = \"collection1\";\n    if (random().nextBoolean()) {\n      // sometimes, use an alias\n      createAlias(\"updatealias\", \"collection1\");\n      collection = \"updatealias\";\n    }\n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);    \n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=6\",\n        \"--verbose\",\n        \"--go-live\",\n        ++numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", collection\n    };\n    args = prependInitialArgs(args);\n\n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      results = server.query(new SolrQuery(\"*:*\"));      \n      assertEquals(2126, results.getResults().getNumFound());\n    }    \n    \n    server.shutdown();\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    createCollection(replicatedCollection, 2, 3, 2);\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=6\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      results = cloudClient.query(new SolrQuery(\"*:*\"));      \n      assertEquals(2104, results.getResults().getNumFound());\n      \n      checkConsistency(replicatedCollection);\n    }   \n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      results = cloudClient.query(new SolrQuery(\"*:*\"));      \n      assertEquals(2104, results.getResults().getNumFound());\n    }  \n    \n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n    jobConf.setBoolean(\"ignoreTikaException\", false);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    HttpSolrServer server = new HttpSolrServer(cloudJettys.get(0).url);\n    String[] args = new String[]{};\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--log4j=\" + ExternalPaths.SOURCE_HOME + \"/core/src/test-files/log4j.properties\",\n        \"--mappers=3\",\n        ++numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n        \"--verbose\",\n        \"--go-live\"\n    };\n    args = prependInitialArgs(args);\n    List<String> argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      \n      res = ToolRunner.run(jobConf, tool, args);\n      \n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      results = server.query(new SolrQuery(\"*:*\"));\n      assertEquals(20, results.getResults().getNumFound());\n    }    \n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true); \n    assertTrue(fs.mkdirs(inDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\",\n        ++numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n    };\n    args = prependInitialArgs(args);\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());      \n      results = server.query(new SolrQuery(\"*:*\"));\n      \n      assertEquals(22, results.getResults().getNumFound());\n    }    \n    \n    // try using zookeeper\n    String collection = \"collection1\";\n    if (random().nextBoolean()) {\n      // sometimes, use an alias\n      createAlias(\"updatealias\", \"collection1\");\n      collection = \"updatealias\";\n    }\n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);    \n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=6\",\n        \"--verbose\",\n        \"--go-live\",\n        ++numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", collection\n    };\n    args = prependInitialArgs(args);\n\n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      results = server.query(new SolrQuery(\"*:*\"));      \n      assertEquals(2126, results.getResults().getNumFound());\n    }    \n    \n    server.shutdown();\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    createCollection(replicatedCollection, 2, 3, 2);\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=6\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      results = cloudClient.query(new SolrQuery(\"*:*\"));      \n      assertEquals(2104, results.getResults().getNumFound());\n      \n      checkConsistency(replicatedCollection);\n    }   \n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      results = cloudClient.query(new SolrQuery(\"*:*\"));      \n      assertEquals(2104, results.getResults().getNumFound());\n    }  \n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"42d384b06aa87eae925b668b65f3246154f0b0fa","date":1386181725,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#doTest().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n    jobConf.setBoolean(ExtractingParams.IGNORE_TIKA_EXCEPTION, false);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    HttpSolrServer server = new HttpSolrServer(cloudJettys.get(0).url);\n    String[] args = new String[]{};\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--log4j=\" + ExternalPaths.SOURCE_HOME + \"/core/src/test-files/log4j.properties\",\n        \"--mappers=3\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),  \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n        \"--verbose\",\n        \"--go-live\"\n    };\n    args = prependInitialArgs(args);\n    List<String> argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      results = server.query(new SolrQuery(\"*:*\"));\n      assertEquals(20, results.getResults().getNumFound());\n    }    \n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true); \n    assertTrue(fs.mkdirs(inDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n    };\n    args = prependInitialArgs(args);\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());      \n      results = server.query(new SolrQuery(\"*:*\"));\n      \n      assertEquals(22, results.getResults().getNumFound());\n    }    \n    \n    // try using zookeeper\n    String collection = \"collection1\";\n    if (random().nextBoolean()) {\n      // sometimes, use an alias\n      createAlias(\"updatealias\", \"collection1\");\n      collection = \"updatealias\";\n    }\n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);    \n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n\n    args = new String[] {\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", collection\n    };\n    args = prependInitialArgs(args);\n\n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");      \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }    \n    \n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n    server.shutdown();\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }  \n    \n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n    jobConf.setBoolean(\"ignoreTikaException\", false);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    HttpSolrServer server = new HttpSolrServer(cloudJettys.get(0).url);\n    String[] args = new String[]{};\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--log4j=\" + ExternalPaths.SOURCE_HOME + \"/core/src/test-files/log4j.properties\",\n        \"--mappers=3\",\n        ++numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n        \"--verbose\",\n        \"--go-live\"\n    };\n    args = prependInitialArgs(args);\n    List<String> argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      \n      res = ToolRunner.run(jobConf, tool, args);\n      \n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      results = server.query(new SolrQuery(\"*:*\"));\n      assertEquals(20, results.getResults().getNumFound());\n    }    \n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true); \n    assertTrue(fs.mkdirs(inDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\",\n        ++numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n    };\n    args = prependInitialArgs(args);\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());      \n      results = server.query(new SolrQuery(\"*:*\"));\n      \n      assertEquals(22, results.getResults().getNumFound());\n    }    \n    \n    // try using zookeeper\n    String collection = \"collection1\";\n    if (random().nextBoolean()) {\n      // sometimes, use an alias\n      createAlias(\"updatealias\", \"collection1\");\n      collection = \"updatealias\";\n    }\n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);    \n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=6\",\n        \"--verbose\",\n        \"--go-live\",\n        ++numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", collection\n    };\n    args = prependInitialArgs(args);\n\n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      results = server.query(new SolrQuery(\"*:*\"));      \n      assertEquals(2126, results.getResults().getNumFound());\n    }    \n    \n    server.shutdown();\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    createCollection(replicatedCollection, 2, 3, 2);\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=6\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      results = cloudClient.query(new SolrQuery(\"*:*\"));      \n      assertEquals(2104, results.getResults().getNumFound());\n      \n      checkConsistency(replicatedCollection);\n    }   \n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      results = cloudClient.query(new SolrQuery(\"*:*\"));      \n      assertEquals(2104, results.getResults().getNumFound());\n    }  \n    \n  }\n\n","bugFix":null,"bugIntro":["da59c6d3748d1a2a9d1a58a69f70383622d68379"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d1806dbd0764a9ee0958f91e02dc8629b7a0ac02","date":1386199730,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#doTest().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    HttpSolrServer server = new HttpSolrServer(cloudJettys.get(0).url);\n    String[] args = new String[]{};\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--log4j=\" + ExternalPaths.SOURCE_HOME + \"/core/src/test-files/log4j.properties\",\n        \"--mappers=3\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),  \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n        \"--verbose\",\n        \"--go-live\"\n    };\n    args = prependInitialArgs(args);\n    List<String> argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      results = server.query(new SolrQuery(\"*:*\"));\n      assertEquals(20, results.getResults().getNumFound());\n    }    \n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true); \n    assertTrue(fs.mkdirs(inDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n    };\n    args = prependInitialArgs(args);\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());      \n      results = server.query(new SolrQuery(\"*:*\"));\n      \n      assertEquals(22, results.getResults().getNumFound());\n    }    \n    \n    // try using zookeeper\n    String collection = \"collection1\";\n    if (random().nextBoolean()) {\n      // sometimes, use an alias\n      createAlias(\"updatealias\", \"collection1\");\n      collection = \"updatealias\";\n    }\n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);    \n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n\n    args = new String[] {\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", collection\n    };\n    args = prependInitialArgs(args);\n\n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");      \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }    \n    \n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n    server.shutdown();\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }  \n    \n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n    jobConf.setBoolean(ExtractingParams.IGNORE_TIKA_EXCEPTION, false);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    HttpSolrServer server = new HttpSolrServer(cloudJettys.get(0).url);\n    String[] args = new String[]{};\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--log4j=\" + ExternalPaths.SOURCE_HOME + \"/core/src/test-files/log4j.properties\",\n        \"--mappers=3\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),  \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n        \"--verbose\",\n        \"--go-live\"\n    };\n    args = prependInitialArgs(args);\n    List<String> argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      results = server.query(new SolrQuery(\"*:*\"));\n      assertEquals(20, results.getResults().getNumFound());\n    }    \n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true); \n    assertTrue(fs.mkdirs(inDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n    };\n    args = prependInitialArgs(args);\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());      \n      results = server.query(new SolrQuery(\"*:*\"));\n      \n      assertEquals(22, results.getResults().getNumFound());\n    }    \n    \n    // try using zookeeper\n    String collection = \"collection1\";\n    if (random().nextBoolean()) {\n      // sometimes, use an alias\n      createAlias(\"updatealias\", \"collection1\");\n      collection = \"updatealias\";\n    }\n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);    \n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n\n    args = new String[] {\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", collection\n    };\n    args = prependInitialArgs(args);\n\n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");      \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }    \n    \n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n    server.shutdown();\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }  \n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"74f45af4339b0daf7a95c820ab88c1aea74fbce0","date":1387475327,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#doTest().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    HttpSolrServer server = new HttpSolrServer(cloudJettys.get(0).url);\n    String[] args = new String[]{};\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--log4j=\" + ExternalPaths.SOURCE_HOME + \"/core/src/test-files/log4j.properties\",\n        \"--mappers=3\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),  \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n        \"--verbose\",\n        \"--go-live\"\n    };\n    args = prependInitialArgs(args);\n    List<String> argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      results = server.query(new SolrQuery(\"*:*\"));\n      assertEquals(20, results.getResults().getNumFound());\n    }    \n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true); \n    assertTrue(fs.mkdirs(inDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n    };\n    args = prependInitialArgs(args);\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());      \n      results = server.query(new SolrQuery(\"*:*\"));\n      \n      assertEquals(22, results.getResults().getNumFound());\n    }    \n    \n    // try using zookeeper\n    String collection = \"collection1\";\n    if (random().nextBoolean()) {\n      // sometimes, use an alias\n      createAlias(\"updatealias\", \"collection1\");\n      collection = \"updatealias\";\n    }\n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);    \n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n\n    args = new String[] {\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", collection\n    };\n    args = prependInitialArgs(args);\n\n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");      \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }    \n    \n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n    server.shutdown();\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }  \n    \n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2b7f6e3ddc2af1fbb25d2161324367db5e977c9f","date":1388973780,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#doTest().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    HttpSolrServer server = new HttpSolrServer(cloudJettys.get(0).url);\n    String[] args = new String[]{};\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--log4j=\" + ExternalPaths.SOURCE_HOME + \"/core/src/test-files/log4j.properties\",\n        \"--mappers=3\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),  \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n        \"--verbose\",\n        \"--go-live\"\n    };\n    args = prependInitialArgs(args);\n    List<String> argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      results = server.query(new SolrQuery(\"*:*\"));\n      assertEquals(20, results.getResults().getNumFound());\n    }    \n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true); \n    assertTrue(fs.mkdirs(inDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n    };\n    args = prependInitialArgs(args);\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());      \n      results = server.query(new SolrQuery(\"*:*\"));\n      \n      assertEquals(22, results.getResults().getNumFound());\n    }    \n    \n    // try using zookeeper\n    String collection = \"collection1\";\n    if (random().nextBoolean()) {\n      // sometimes, use an alias\n      createAlias(\"updatealias\", \"collection1\");\n      collection = \"updatealias\";\n    }\n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);    \n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n\n    args = new String[] {\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", collection\n    };\n    args = prependInitialArgs(args);\n\n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");      \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }    \n    \n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n    server.shutdown();\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }  \n    \n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    HttpSolrServer server = new HttpSolrServer(cloudJettys.get(0).url);\n    String[] args = new String[]{};\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--log4j=\" + ExternalPaths.SOURCE_HOME + \"/core/src/test-files/log4j.properties\",\n        \"--mappers=3\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),  \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n        \"--verbose\",\n        \"--go-live\"\n    };\n    args = prependInitialArgs(args);\n    List<String> argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      results = server.query(new SolrQuery(\"*:*\"));\n      assertEquals(20, results.getResults().getNumFound());\n    }    \n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true); \n    assertTrue(fs.mkdirs(inDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n    };\n    args = prependInitialArgs(args);\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());      \n      results = server.query(new SolrQuery(\"*:*\"));\n      \n      assertEquals(22, results.getResults().getNumFound());\n    }    \n    \n    // try using zookeeper\n    String collection = \"collection1\";\n    if (random().nextBoolean()) {\n      // sometimes, use an alias\n      createAlias(\"updatealias\", \"collection1\");\n      collection = \"updatealias\";\n    }\n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);    \n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n\n    args = new String[] {\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", collection\n    };\n    args = prependInitialArgs(args);\n\n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");      \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }    \n    \n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n    server.shutdown();\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }  \n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a475a1c165e02515bff766b3b7be0c42edf1870c","date":1393099333,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#doTest().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    HttpSolrServer server = new HttpSolrServer(cloudJettys.get(0).url);\n    String[] args = new String[]{};\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n        \"--mappers=3\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),  \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n        \"--verbose\",\n        \"--go-live\"\n    };\n    args = prependInitialArgs(args);\n    List<String> argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      results = server.query(new SolrQuery(\"*:*\"));\n      assertEquals(20, results.getResults().getNumFound());\n    }    \n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true); \n    assertTrue(fs.mkdirs(inDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n    };\n    args = prependInitialArgs(args);\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());      \n      results = server.query(new SolrQuery(\"*:*\"));\n      \n      assertEquals(22, results.getResults().getNumFound());\n    }    \n    \n    // try using zookeeper\n    String collection = \"collection1\";\n    if (random().nextBoolean()) {\n      // sometimes, use an alias\n      createAlias(\"updatealias\", \"collection1\");\n      collection = \"updatealias\";\n    }\n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);    \n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n\n    args = new String[] {\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", collection\n    };\n    args = prependInitialArgs(args);\n\n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");      \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }    \n    \n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n    server.shutdown();\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }  \n    \n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    HttpSolrServer server = new HttpSolrServer(cloudJettys.get(0).url);\n    String[] args = new String[]{};\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--log4j=\" + ExternalPaths.SOURCE_HOME + \"/core/src/test-files/log4j.properties\",\n        \"--mappers=3\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),  \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n        \"--verbose\",\n        \"--go-live\"\n    };\n    args = prependInitialArgs(args);\n    List<String> argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      results = server.query(new SolrQuery(\"*:*\"));\n      assertEquals(20, results.getResults().getNumFound());\n    }    \n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true); \n    assertTrue(fs.mkdirs(inDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n    };\n    args = prependInitialArgs(args);\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());      \n      results = server.query(new SolrQuery(\"*:*\"));\n      \n      assertEquals(22, results.getResults().getNumFound());\n    }    \n    \n    // try using zookeeper\n    String collection = \"collection1\";\n    if (random().nextBoolean()) {\n      // sometimes, use an alias\n      createAlias(\"updatealias\", \"collection1\");\n      collection = \"updatealias\";\n    }\n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);    \n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n\n    args = new String[] {\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", collection\n    };\n    args = prependInitialArgs(args);\n\n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");      \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }    \n    \n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n    server.shutdown();\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }  \n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#doTest().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    HttpSolrServer server = new HttpSolrServer(cloudJettys.get(0).url);\n    String[] args = new String[]{};\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n        \"--mappers=3\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),  \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n        \"--verbose\",\n        \"--go-live\"\n    };\n    args = prependInitialArgs(args);\n    List<String> argList = new ArrayList<>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      results = server.query(new SolrQuery(\"*:*\"));\n      assertEquals(20, results.getResults().getNumFound());\n    }    \n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true); \n    assertTrue(fs.mkdirs(inDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n    };\n    args = prependInitialArgs(args);\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());      \n      results = server.query(new SolrQuery(\"*:*\"));\n      \n      assertEquals(22, results.getResults().getNumFound());\n    }    \n    \n    // try using zookeeper\n    String collection = \"collection1\";\n    if (random().nextBoolean()) {\n      // sometimes, use an alias\n      createAlias(\"updatealias\", \"collection1\");\n      collection = \"updatealias\";\n    }\n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);    \n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n\n    args = new String[] {\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", collection\n    };\n    args = prependInitialArgs(args);\n\n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");      \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }    \n    \n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n    server.shutdown();\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }  \n    \n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    HttpSolrServer server = new HttpSolrServer(cloudJettys.get(0).url);\n    String[] args = new String[]{};\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n        \"--mappers=3\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),  \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n        \"--verbose\",\n        \"--go-live\"\n    };\n    args = prependInitialArgs(args);\n    List<String> argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      results = server.query(new SolrQuery(\"*:*\"));\n      assertEquals(20, results.getResults().getNumFound());\n    }    \n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true); \n    assertTrue(fs.mkdirs(inDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n    };\n    args = prependInitialArgs(args);\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());      \n      results = server.query(new SolrQuery(\"*:*\"));\n      \n      assertEquals(22, results.getResults().getNumFound());\n    }    \n    \n    // try using zookeeper\n    String collection = \"collection1\";\n    if (random().nextBoolean()) {\n      // sometimes, use an alias\n      createAlias(\"updatealias\", \"collection1\");\n      collection = \"updatealias\";\n    }\n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);    \n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n\n    args = new String[] {\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", collection\n    };\n    args = prependInitialArgs(args);\n\n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");      \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }    \n    \n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n    server.shutdown();\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }  \n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0d29d666b8eaa2fe8efe1e0d42fc8e32876d7294","date":1408633409,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#doTest().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    HttpSolrServer server = new HttpSolrServer(cloudJettys.get(0).url);\n    String[] args = new String[]{};\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n        \"--mappers=3\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),  \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n        \"--verbose\",\n        \"--go-live\"\n    };\n    args = prependInitialArgs(args);\n    List<String> argList = new ArrayList<>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      results = server.query(new SolrQuery(\"*:*\"));\n      assertEquals(20, results.getResults().getNumFound());\n    }    \n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true); \n    assertTrue(fs.mkdirs(inDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n    };\n    args = prependInitialArgs(args);\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());      \n      results = server.query(new SolrQuery(\"*:*\"));\n      \n      assertEquals(22, results.getResults().getNumFound());\n    }    \n    \n    // try using zookeeper\n    String collection = \"collection1\";\n    if (random().nextBoolean()) {\n      // sometimes, use an alias\n      createAlias(\"updatealias\", \"collection1\");\n      collection = \"updatealias\";\n    }\n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);    \n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n\n    args = new String[] {\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", collection\n    };\n    args = prependInitialArgs(args);\n\n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");      \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }    \n    \n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n    server.shutdown();\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    HttpSolrServer server = new HttpSolrServer(cloudJettys.get(0).url);\n    String[] args = new String[]{};\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n        \"--mappers=3\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),  \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n        \"--verbose\",\n        \"--go-live\"\n    };\n    args = prependInitialArgs(args);\n    List<String> argList = new ArrayList<>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      results = server.query(new SolrQuery(\"*:*\"));\n      assertEquals(20, results.getResults().getNumFound());\n    }    \n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true); \n    assertTrue(fs.mkdirs(inDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n    };\n    args = prependInitialArgs(args);\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());      \n      results = server.query(new SolrQuery(\"*:*\"));\n      \n      assertEquals(22, results.getResults().getNumFound());\n    }    \n    \n    // try using zookeeper\n    String collection = \"collection1\";\n    if (random().nextBoolean()) {\n      // sometimes, use an alias\n      createAlias(\"updatealias\", \"collection1\");\n      collection = \"updatealias\";\n    }\n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);    \n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n\n    args = new String[] {\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", collection\n    };\n    args = prependInitialArgs(args);\n\n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");      \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }    \n    \n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n    server.shutdown();\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }  \n    \n  }\n\n","bugFix":["d6e604e9030fb0cabf0c5a85ae6039921a81419c"],"bugIntro":["da59c6d3748d1a2a9d1a58a69f70383622d68379"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bafca15d8e408346a67f4282ad1143b88023893b","date":1420034748,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#doTest().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    HttpSolrClient server = new HttpSolrClient(cloudJettys.get(0).url);\n    String[] args = new String[]{};\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n        \"--mappers=3\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),  \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n        \"--verbose\",\n        \"--go-live\"\n    };\n    args = prependInitialArgs(args);\n    List<String> argList = new ArrayList<>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      results = server.query(new SolrQuery(\"*:*\"));\n      assertEquals(20, results.getResults().getNumFound());\n    }    \n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true); \n    assertTrue(fs.mkdirs(inDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n    };\n    args = prependInitialArgs(args);\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());      \n      results = server.query(new SolrQuery(\"*:*\"));\n      \n      assertEquals(22, results.getResults().getNumFound());\n    }    \n    \n    // try using zookeeper\n    String collection = \"collection1\";\n    if (random().nextBoolean()) {\n      // sometimes, use an alias\n      createAlias(\"updatealias\", \"collection1\");\n      collection = \"updatealias\";\n    }\n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);    \n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n\n    args = new String[] {\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", collection\n    };\n    args = prependInitialArgs(args);\n\n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");      \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }    \n    \n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n    server.shutdown();\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    HttpSolrServer server = new HttpSolrServer(cloudJettys.get(0).url);\n    String[] args = new String[]{};\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n        \"--mappers=3\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),  \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n        \"--verbose\",\n        \"--go-live\"\n    };\n    args = prependInitialArgs(args);\n    List<String> argList = new ArrayList<>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      results = server.query(new SolrQuery(\"*:*\"));\n      assertEquals(20, results.getResults().getNumFound());\n    }    \n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true); \n    assertTrue(fs.mkdirs(inDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n    };\n    args = prependInitialArgs(args);\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());      \n      results = server.query(new SolrQuery(\"*:*\"));\n      \n      assertEquals(22, results.getResults().getNumFound());\n    }    \n    \n    // try using zookeeper\n    String collection = \"collection1\";\n    if (random().nextBoolean()) {\n      // sometimes, use an alias\n      createAlias(\"updatealias\", \"collection1\");\n      collection = \"updatealias\";\n    }\n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);    \n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n\n    args = new String[] {\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", collection\n    };\n    args = prependInitialArgs(args);\n\n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");      \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }    \n    \n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n    server.shutdown();\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"abb23fcc2461782ab204e61213240feb77d355aa","date":1422029612,"type":5,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#test().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#doTest().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    HttpSolrClient server = new HttpSolrClient(cloudJettys.get(0).url);\n    String[] args = new String[]{};\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n        \"--mappers=3\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),  \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n        \"--verbose\",\n        \"--go-live\"\n    };\n    args = prependInitialArgs(args);\n    List<String> argList = new ArrayList<>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      results = server.query(new SolrQuery(\"*:*\"));\n      assertEquals(20, results.getResults().getNumFound());\n    }    \n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true); \n    assertTrue(fs.mkdirs(inDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n    };\n    args = prependInitialArgs(args);\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());      \n      results = server.query(new SolrQuery(\"*:*\"));\n      \n      assertEquals(22, results.getResults().getNumFound());\n    }    \n    \n    // try using zookeeper\n    String collection = \"collection1\";\n    if (random().nextBoolean()) {\n      // sometimes, use an alias\n      createAlias(\"updatealias\", \"collection1\");\n      collection = \"updatealias\";\n    }\n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);    \n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n\n    args = new String[] {\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", collection\n    };\n    args = prependInitialArgs(args);\n\n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");      \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }    \n    \n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n    server.shutdown();\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    HttpSolrClient server = new HttpSolrClient(cloudJettys.get(0).url);\n    String[] args = new String[]{};\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--log4j=\" + getFile(\"log4j.properties\").getAbsolutePath(),\n        \"--mappers=3\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),  \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n        \"--verbose\",\n        \"--go-live\"\n    };\n    args = prependInitialArgs(args);\n    List<String> argList = new ArrayList<>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      results = server.query(new SolrQuery(\"*:*\"));\n      assertEquals(20, results.getResults().getNumFound());\n    }    \n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true); \n    assertTrue(fs.mkdirs(inDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n    };\n    args = prependInitialArgs(args);\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());      \n      results = server.query(new SolrQuery(\"*:*\"));\n      \n      assertEquals(22, results.getResults().getNumFound());\n    }    \n    \n    // try using zookeeper\n    String collection = \"collection1\";\n    if (random().nextBoolean()) {\n      // sometimes, use an alias\n      createAlias(\"updatealias\", \"collection1\");\n      collection = \"updatealias\";\n    }\n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);    \n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n\n    args = new String[] {\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=12\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        random().nextBoolean() ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", collection\n    };\n    args = prependInitialArgs(args);\n\n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");      \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFirstValue(\"id\"), doc2.getFirstValue(\"id\"));\n          assertEquals(\"Nadja\" + i, doc2.getFirstValue(\"user_screen_name\"));\n          assertEquals(doc.getFirstValue(\"text\"), doc2.getFirstValue(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }    \n    \n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());      \n    server.shutdown();\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=22\",\n        \"--fanout=2\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      SolrDocumentList resultDocs = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs.size());\n      \n      checkConsistency(replicatedCollection);\n      \n      // perform updates\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);          \n          SolrInputDocument update = new SolrInputDocument();\n          for (Map.Entry<String, Object> entry : doc.entrySet()) {\n              update.setField(entry.getKey(), entry.getValue());\n          }\n          update.setField(\"user_screen_name\", \"@Nadja\" + i);\n          update.removeField(\"_version_\");\n          cloudClient.add(update);\n      }\n      cloudClient.commit();\n      \n      // verify updates\n      SolrDocumentList resultDocs2 = executeSolrQuery(cloudClient, \"*:*\");   \n      assertEquals(RECORD_COUNT, resultDocs2.getNumFound());\n      assertEquals(RECORD_COUNT, resultDocs2.size());\n      for (int i = 0; i < RECORD_COUNT; i++) {\n          SolrDocument doc = resultDocs.get(i);\n          SolrDocument doc2 = resultDocs2.get(i);\n          assertEquals(doc.getFieldValues(\"id\"), doc2.getFieldValues(\"id\"));\n          assertEquals(1, doc.getFieldValues(\"id\").size());\n          assertEquals(Arrays.asList(\"@Nadja\" + i), doc2.getFieldValues(\"user_screen_name\"));\n          assertEquals(doc.getFieldValues(\"text\"), doc2.getFieldValues(\"text\"));\n          \n          // perform delete\n          cloudClient.deleteById((String)doc.getFirstValue(\"id\"));\n      }\n      cloudClient.commit();\n      \n      // verify deletes\n      assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").size());\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n    }\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n    params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n    params.set(CoreAdminParams.DELETE_INDEX, true);\n    params.set(\"name\", replicatedCollection);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(replicatedCollection)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    if (TEST_NIGHTLY) {\n      createCollection(replicatedCollection, 11, 3, 11);\n    } else {\n      createCollection(replicatedCollection, 2, 3, 2);\n    }\n    \n    waitForRecoveriesToFinish(replicatedCollection, false);\n    printLayout();\n    assertEquals(0, executeSolrQuery(cloudClient, \"*:*\").getNumFound());\n    \n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    tool = new MapReduceIndexerTool();\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    assertTrue(tool.job.isComplete());\n    assertTrue(tool.job.isSuccessful());\n    \n    checkConsistency(replicatedCollection);\n    \n    assertEquals(RECORD_COUNT, executeSolrQuery(cloudClient, \"*:*\").size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"0d29d666b8eaa2fe8efe1e0d42fc8e32876d7294":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["a475a1c165e02515bff766b3b7be0c42edf1870c"],"d1806dbd0764a9ee0958f91e02dc8629b7a0ac02":["42d384b06aa87eae925b668b65f3246154f0b0fa"],"abb23fcc2461782ab204e61213240feb77d355aa":["bafca15d8e408346a67f4282ad1143b88023893b"],"a475a1c165e02515bff766b3b7be0c42edf1870c":["2b7f6e3ddc2af1fbb25d2161324367db5e977c9f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","d1806dbd0764a9ee0958f91e02dc8629b7a0ac02"],"bafca15d8e408346a67f4282ad1143b88023893b":["0d29d666b8eaa2fe8efe1e0d42fc8e32876d7294"],"2b7f6e3ddc2af1fbb25d2161324367db5e977c9f":["d1806dbd0764a9ee0958f91e02dc8629b7a0ac02"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["abb23fcc2461782ab204e61213240feb77d355aa"],"42d384b06aa87eae925b668b65f3246154f0b0fa":["70f91c8322fbffe3a3a897ef20ea19119cac10cd"]},"commit2Childs":{"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["42d384b06aa87eae925b668b65f3246154f0b0fa"],"0d29d666b8eaa2fe8efe1e0d42fc8e32876d7294":["bafca15d8e408346a67f4282ad1143b88023893b"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["0d29d666b8eaa2fe8efe1e0d42fc8e32876d7294"],"d1806dbd0764a9ee0958f91e02dc8629b7a0ac02":["74f45af4339b0daf7a95c820ab88c1aea74fbce0","2b7f6e3ddc2af1fbb25d2161324367db5e977c9f"],"abb23fcc2461782ab204e61213240feb77d355aa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a475a1c165e02515bff766b3b7be0c42edf1870c":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["70f91c8322fbffe3a3a897ef20ea19119cac10cd","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":[],"bafca15d8e408346a67f4282ad1143b88023893b":["abb23fcc2461782ab204e61213240feb77d355aa"],"2b7f6e3ddc2af1fbb25d2161324367db5e977c9f":["a475a1c165e02515bff766b3b7be0c42edf1870c"],"42d384b06aa87eae925b668b65f3246154f0b0fa":["d1806dbd0764a9ee0958f91e02dc8629b7a0ac02"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["74f45af4339b0daf7a95c820ab88c1aea74fbce0","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}