{"path":"lucene/core/src/test/org/apache/lucene/document/TestDocument#testTransitionAPI().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/document/TestDocument#testTransitionAPI().mjava","pathOld":"lucene/src/test/org/apache/lucene/document/TestDocument#testTransitionAPI().mjava","sourceNew":"  // LUCENE-3682\n  public void testTransitionAPI() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir);\n\n    Document doc = new Document();\n    doc.add(new Field(\"stored\", \"abc\", Field.Store.YES, Field.Index.NO));\n    doc.add(new Field(\"stored_indexed\", \"abc xyz\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"stored_tokenized\", \"abc xyz\", Field.Store.YES, Field.Index.ANALYZED));\n    doc.add(new Field(\"indexed\", \"abc xyz\", Field.Store.NO, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"tokenized\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED));\n    doc.add(new Field(\"tokenized_reader\", new StringReader(\"abc xyz\")));\n    doc.add(new Field(\"tokenized_tokenstream\", w.w.getAnalyzer().tokenStream(\"tokenized_tokenstream\", new StringReader(\"abc xyz\"))));\n    doc.add(new Field(\"binary\", new byte[10]));\n    doc.add(new Field(\"tv\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"tv_pos\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"tv_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n    doc.add(new Field(\"tv_pos_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    w.addDocument(doc);\n    IndexReader r = w.getReader();\n    w.close();\n\n    doc = r.document(0);\n    // 4 stored fields\n    assertEquals(4, doc.getFields().size());\n    assertEquals(\"abc\", doc.get(\"stored\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_indexed\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_tokenized\"));\n    final BytesRef br = doc.getBinaryValue(\"binary\");\n    assertNotNull(br);\n    assertEquals(10, br.length);\n\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"xyz\")), 1).totalHits);\n\n    for(String field : new String[] {\"tv\", \"tv_pos\", \"tv_off\", \"tv_pos_off\"}) {\n      Fields tvFields = r.getTermVectors(0);\n      Terms tvs = tvFields.terms(field);\n      assertNotNull(tvs);\n      assertEquals(2, tvs.getUniqueTermCount());\n      TermsEnum tvsEnum = tvs.iterator(null);\n      assertEquals(new BytesRef(\"abc\"), tvsEnum.next());\n      final DocsAndPositionsEnum dpEnum = tvsEnum.docsAndPositions(null, null, false);\n      if (field.equals(\"tv\")) {\n        assertNull(dpEnum);\n      } else {\n        assertNotNull(dpEnum);\n      }\n      assertEquals(new BytesRef(\"xyz\"), tvsEnum.next());\n      assertNull(tvsEnum.next());\n    }\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-3682\n  public void testTransitionAPI() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir);\n\n    Document doc = new Document();\n    doc.add(new Field(\"stored\", \"abc\", Field.Store.YES, Field.Index.NO));\n    doc.add(new Field(\"stored_indexed\", \"abc xyz\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"stored_tokenized\", \"abc xyz\", Field.Store.YES, Field.Index.ANALYZED));\n    doc.add(new Field(\"indexed\", \"abc xyz\", Field.Store.NO, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"tokenized\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED));\n    doc.add(new Field(\"tokenized_reader\", new StringReader(\"abc xyz\")));\n    doc.add(new Field(\"tokenized_tokenstream\", w.w.getAnalyzer().tokenStream(\"tokenized_tokenstream\", new StringReader(\"abc xyz\"))));\n    doc.add(new Field(\"binary\", new byte[10]));\n    doc.add(new Field(\"tv\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"tv_pos\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"tv_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n    doc.add(new Field(\"tv_pos_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    w.addDocument(doc);\n    IndexReader r = w.getReader();\n    w.close();\n\n    doc = r.document(0);\n    // 4 stored fields\n    assertEquals(4, doc.getFields().size());\n    assertEquals(\"abc\", doc.get(\"stored\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_indexed\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_tokenized\"));\n    final BytesRef br = doc.getBinaryValue(\"binary\");\n    assertNotNull(br);\n    assertEquals(10, br.length);\n\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"xyz\")), 1).totalHits);\n\n    for(String field : new String[] {\"tv\", \"tv_pos\", \"tv_off\", \"tv_pos_off\"}) {\n      Fields tvFields = r.getTermVectors(0);\n      Terms tvs = tvFields.terms(field);\n      assertNotNull(tvs);\n      assertEquals(2, tvs.getUniqueTermCount());\n      TermsEnum tvsEnum = tvs.iterator(null);\n      assertEquals(new BytesRef(\"abc\"), tvsEnum.next());\n      final DocsAndPositionsEnum dpEnum = tvsEnum.docsAndPositions(null, null, false);\n      if (field.equals(\"tv\")) {\n        assertNull(dpEnum);\n      } else {\n        assertNotNull(dpEnum);\n      }\n      assertEquals(new BytesRef(\"xyz\"), tvsEnum.next());\n      assertNull(tvsEnum.next());\n    }\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d08eba3d52b63561ebf936481ce73e6b6a14aa03","date":1333879759,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/document/TestDocument#testTransitionAPI().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/document/TestDocument#testTransitionAPI().mjava","sourceNew":"  // LUCENE-3682\n  public void testTransitionAPI() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir);\n\n    Document doc = new Document();\n    doc.add(new Field(\"stored\", \"abc\", Field.Store.YES, Field.Index.NO));\n    doc.add(new Field(\"stored_indexed\", \"abc xyz\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"stored_tokenized\", \"abc xyz\", Field.Store.YES, Field.Index.ANALYZED));\n    doc.add(new Field(\"indexed\", \"abc xyz\", Field.Store.NO, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"tokenized\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED));\n    doc.add(new Field(\"tokenized_reader\", new StringReader(\"abc xyz\")));\n    doc.add(new Field(\"tokenized_tokenstream\", w.w.getAnalyzer().tokenStream(\"tokenized_tokenstream\", new StringReader(\"abc xyz\"))));\n    doc.add(new Field(\"binary\", new byte[10]));\n    doc.add(new Field(\"tv\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"tv_pos\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"tv_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n    doc.add(new Field(\"tv_pos_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    w.addDocument(doc);\n    IndexReader r = w.getReader();\n    w.close();\n\n    doc = r.document(0);\n    // 4 stored fields\n    assertEquals(4, doc.getFields().size());\n    assertEquals(\"abc\", doc.get(\"stored\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_indexed\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_tokenized\"));\n    final BytesRef br = doc.getBinaryValue(\"binary\");\n    assertNotNull(br);\n    assertEquals(10, br.length);\n\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"xyz\")), 1).totalHits);\n\n    for(String field : new String[] {\"tv\", \"tv_pos\", \"tv_off\", \"tv_pos_off\"}) {\n      InvertedFields tvFields = r.getTermVectors(0);\n      Terms tvs = tvFields.terms(field);\n      assertNotNull(tvs);\n      assertEquals(2, tvs.getUniqueTermCount());\n      TermsEnum tvsEnum = tvs.iterator(null);\n      assertEquals(new BytesRef(\"abc\"), tvsEnum.next());\n      final DocsAndPositionsEnum dpEnum = tvsEnum.docsAndPositions(null, null, false);\n      if (field.equals(\"tv\")) {\n        assertNull(dpEnum);\n      } else {\n        assertNotNull(dpEnum);\n      }\n      assertEquals(new BytesRef(\"xyz\"), tvsEnum.next());\n      assertNull(tvsEnum.next());\n    }\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-3682\n  public void testTransitionAPI() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir);\n\n    Document doc = new Document();\n    doc.add(new Field(\"stored\", \"abc\", Field.Store.YES, Field.Index.NO));\n    doc.add(new Field(\"stored_indexed\", \"abc xyz\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"stored_tokenized\", \"abc xyz\", Field.Store.YES, Field.Index.ANALYZED));\n    doc.add(new Field(\"indexed\", \"abc xyz\", Field.Store.NO, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"tokenized\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED));\n    doc.add(new Field(\"tokenized_reader\", new StringReader(\"abc xyz\")));\n    doc.add(new Field(\"tokenized_tokenstream\", w.w.getAnalyzer().tokenStream(\"tokenized_tokenstream\", new StringReader(\"abc xyz\"))));\n    doc.add(new Field(\"binary\", new byte[10]));\n    doc.add(new Field(\"tv\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"tv_pos\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"tv_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n    doc.add(new Field(\"tv_pos_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    w.addDocument(doc);\n    IndexReader r = w.getReader();\n    w.close();\n\n    doc = r.document(0);\n    // 4 stored fields\n    assertEquals(4, doc.getFields().size());\n    assertEquals(\"abc\", doc.get(\"stored\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_indexed\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_tokenized\"));\n    final BytesRef br = doc.getBinaryValue(\"binary\");\n    assertNotNull(br);\n    assertEquals(10, br.length);\n\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"xyz\")), 1).totalHits);\n\n    for(String field : new String[] {\"tv\", \"tv_pos\", \"tv_off\", \"tv_pos_off\"}) {\n      Fields tvFields = r.getTermVectors(0);\n      Terms tvs = tvFields.terms(field);\n      assertNotNull(tvs);\n      assertEquals(2, tvs.getUniqueTermCount());\n      TermsEnum tvsEnum = tvs.iterator(null);\n      assertEquals(new BytesRef(\"abc\"), tvsEnum.next());\n      final DocsAndPositionsEnum dpEnum = tvsEnum.docsAndPositions(null, null, false);\n      if (field.equals(\"tv\")) {\n        assertNull(dpEnum);\n      } else {\n        assertNotNull(dpEnum);\n      }\n      assertEquals(new BytesRef(\"xyz\"), tvsEnum.next());\n      assertNull(tvsEnum.next());\n    }\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf","date":1333892281,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/document/TestDocument#testTransitionAPI().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/document/TestDocument#testTransitionAPI().mjava","sourceNew":"  // LUCENE-3682\n  public void testTransitionAPI() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir);\n\n    Document doc = new Document();\n    doc.add(new Field(\"stored\", \"abc\", Field.Store.YES, Field.Index.NO));\n    doc.add(new Field(\"stored_indexed\", \"abc xyz\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"stored_tokenized\", \"abc xyz\", Field.Store.YES, Field.Index.ANALYZED));\n    doc.add(new Field(\"indexed\", \"abc xyz\", Field.Store.NO, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"tokenized\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED));\n    doc.add(new Field(\"tokenized_reader\", new StringReader(\"abc xyz\")));\n    doc.add(new Field(\"tokenized_tokenstream\", w.w.getAnalyzer().tokenStream(\"tokenized_tokenstream\", new StringReader(\"abc xyz\"))));\n    doc.add(new Field(\"binary\", new byte[10]));\n    doc.add(new Field(\"tv\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"tv_pos\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"tv_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n    doc.add(new Field(\"tv_pos_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    w.addDocument(doc);\n    IndexReader r = w.getReader();\n    w.close();\n\n    doc = r.document(0);\n    // 4 stored fields\n    assertEquals(4, doc.getFields().size());\n    assertEquals(\"abc\", doc.get(\"stored\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_indexed\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_tokenized\"));\n    final BytesRef br = doc.getBinaryValue(\"binary\");\n    assertNotNull(br);\n    assertEquals(10, br.length);\n\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"xyz\")), 1).totalHits);\n\n    for(String field : new String[] {\"tv\", \"tv_pos\", \"tv_off\", \"tv_pos_off\"}) {\n      Fields tvFields = r.getTermVectors(0);\n      Terms tvs = tvFields.terms(field);\n      assertNotNull(tvs);\n      assertEquals(2, tvs.getUniqueTermCount());\n      TermsEnum tvsEnum = tvs.iterator(null);\n      assertEquals(new BytesRef(\"abc\"), tvsEnum.next());\n      final DocsAndPositionsEnum dpEnum = tvsEnum.docsAndPositions(null, null, false);\n      if (field.equals(\"tv\")) {\n        assertNull(dpEnum);\n      } else {\n        assertNotNull(dpEnum);\n      }\n      assertEquals(new BytesRef(\"xyz\"), tvsEnum.next());\n      assertNull(tvsEnum.next());\n    }\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-3682\n  public void testTransitionAPI() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir);\n\n    Document doc = new Document();\n    doc.add(new Field(\"stored\", \"abc\", Field.Store.YES, Field.Index.NO));\n    doc.add(new Field(\"stored_indexed\", \"abc xyz\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"stored_tokenized\", \"abc xyz\", Field.Store.YES, Field.Index.ANALYZED));\n    doc.add(new Field(\"indexed\", \"abc xyz\", Field.Store.NO, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"tokenized\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED));\n    doc.add(new Field(\"tokenized_reader\", new StringReader(\"abc xyz\")));\n    doc.add(new Field(\"tokenized_tokenstream\", w.w.getAnalyzer().tokenStream(\"tokenized_tokenstream\", new StringReader(\"abc xyz\"))));\n    doc.add(new Field(\"binary\", new byte[10]));\n    doc.add(new Field(\"tv\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"tv_pos\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"tv_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n    doc.add(new Field(\"tv_pos_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    w.addDocument(doc);\n    IndexReader r = w.getReader();\n    w.close();\n\n    doc = r.document(0);\n    // 4 stored fields\n    assertEquals(4, doc.getFields().size());\n    assertEquals(\"abc\", doc.get(\"stored\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_indexed\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_tokenized\"));\n    final BytesRef br = doc.getBinaryValue(\"binary\");\n    assertNotNull(br);\n    assertEquals(10, br.length);\n\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"xyz\")), 1).totalHits);\n\n    for(String field : new String[] {\"tv\", \"tv_pos\", \"tv_off\", \"tv_pos_off\"}) {\n      InvertedFields tvFields = r.getTermVectors(0);\n      Terms tvs = tvFields.terms(field);\n      assertNotNull(tvs);\n      assertEquals(2, tvs.getUniqueTermCount());\n      TermsEnum tvsEnum = tvs.iterator(null);\n      assertEquals(new BytesRef(\"abc\"), tvsEnum.next());\n      final DocsAndPositionsEnum dpEnum = tvsEnum.docsAndPositions(null, null, false);\n      if (field.equals(\"tv\")) {\n        assertNull(dpEnum);\n      } else {\n        assertNotNull(dpEnum);\n      }\n      assertEquals(new BytesRef(\"xyz\"), tvsEnum.next());\n      assertNull(tvsEnum.next());\n    }\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bdb5e42b0cecd8dfb27767a02ada71899bf17917","date":1334100099,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/document/TestDocument#testTransitionAPI().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/document/TestDocument#testTransitionAPI().mjava","sourceNew":"  // LUCENE-3682\n  public void testTransitionAPI() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir);\n\n    Document doc = new Document();\n    doc.add(new Field(\"stored\", \"abc\", Field.Store.YES, Field.Index.NO));\n    doc.add(new Field(\"stored_indexed\", \"abc xyz\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"stored_tokenized\", \"abc xyz\", Field.Store.YES, Field.Index.ANALYZED));\n    doc.add(new Field(\"indexed\", \"abc xyz\", Field.Store.NO, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"tokenized\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED));\n    doc.add(new Field(\"tokenized_reader\", new StringReader(\"abc xyz\")));\n    doc.add(new Field(\"tokenized_tokenstream\", w.w.getAnalyzer().tokenStream(\"tokenized_tokenstream\", new StringReader(\"abc xyz\"))));\n    doc.add(new Field(\"binary\", new byte[10]));\n    doc.add(new Field(\"tv\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"tv_pos\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"tv_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n    doc.add(new Field(\"tv_pos_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    w.addDocument(doc);\n    IndexReader r = w.getReader();\n    w.close();\n\n    doc = r.document(0);\n    // 4 stored fields\n    assertEquals(4, doc.getFields().size());\n    assertEquals(\"abc\", doc.get(\"stored\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_indexed\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_tokenized\"));\n    final BytesRef br = doc.getBinaryValue(\"binary\");\n    assertNotNull(br);\n    assertEquals(10, br.length);\n\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"xyz\")), 1).totalHits);\n\n    for(String field : new String[] {\"tv\", \"tv_pos\", \"tv_off\", \"tv_pos_off\"}) {\n      Fields tvFields = r.getTermVectors(0);\n      Terms tvs = tvFields.terms(field);\n      assertNotNull(tvs);\n      assertEquals(2, tvs.size());\n      TermsEnum tvsEnum = tvs.iterator(null);\n      assertEquals(new BytesRef(\"abc\"), tvsEnum.next());\n      final DocsAndPositionsEnum dpEnum = tvsEnum.docsAndPositions(null, null, false);\n      if (field.equals(\"tv\")) {\n        assertNull(dpEnum);\n      } else {\n        assertNotNull(dpEnum);\n      }\n      assertEquals(new BytesRef(\"xyz\"), tvsEnum.next());\n      assertNull(tvsEnum.next());\n    }\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-3682\n  public void testTransitionAPI() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir);\n\n    Document doc = new Document();\n    doc.add(new Field(\"stored\", \"abc\", Field.Store.YES, Field.Index.NO));\n    doc.add(new Field(\"stored_indexed\", \"abc xyz\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"stored_tokenized\", \"abc xyz\", Field.Store.YES, Field.Index.ANALYZED));\n    doc.add(new Field(\"indexed\", \"abc xyz\", Field.Store.NO, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"tokenized\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED));\n    doc.add(new Field(\"tokenized_reader\", new StringReader(\"abc xyz\")));\n    doc.add(new Field(\"tokenized_tokenstream\", w.w.getAnalyzer().tokenStream(\"tokenized_tokenstream\", new StringReader(\"abc xyz\"))));\n    doc.add(new Field(\"binary\", new byte[10]));\n    doc.add(new Field(\"tv\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"tv_pos\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"tv_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n    doc.add(new Field(\"tv_pos_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    w.addDocument(doc);\n    IndexReader r = w.getReader();\n    w.close();\n\n    doc = r.document(0);\n    // 4 stored fields\n    assertEquals(4, doc.getFields().size());\n    assertEquals(\"abc\", doc.get(\"stored\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_indexed\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_tokenized\"));\n    final BytesRef br = doc.getBinaryValue(\"binary\");\n    assertNotNull(br);\n    assertEquals(10, br.length);\n\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"xyz\")), 1).totalHits);\n\n    for(String field : new String[] {\"tv\", \"tv_pos\", \"tv_off\", \"tv_pos_off\"}) {\n      Fields tvFields = r.getTermVectors(0);\n      Terms tvs = tvFields.terms(field);\n      assertNotNull(tvs);\n      assertEquals(2, tvs.getUniqueTermCount());\n      TermsEnum tvsEnum = tvs.iterator(null);\n      assertEquals(new BytesRef(\"abc\"), tvsEnum.next());\n      final DocsAndPositionsEnum dpEnum = tvsEnum.docsAndPositions(null, null, false);\n      if (field.equals(\"tv\")) {\n        assertNull(dpEnum);\n      } else {\n        assertNotNull(dpEnum);\n      }\n      assertEquals(new BytesRef(\"xyz\"), tvsEnum.next());\n      assertNull(tvsEnum.next());\n    }\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5a238fc456663f685a9db1ed8d680e348bb45171","date":1334173266,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/document/TestDocument#testTransitionAPI().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/document/TestDocument#testTransitionAPI().mjava","sourceNew":"  // LUCENE-3682\n  public void testTransitionAPI() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir);\n\n    Document doc = new Document();\n    doc.add(new Field(\"stored\", \"abc\", Field.Store.YES, Field.Index.NO));\n    doc.add(new Field(\"stored_indexed\", \"abc xyz\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"stored_tokenized\", \"abc xyz\", Field.Store.YES, Field.Index.ANALYZED));\n    doc.add(new Field(\"indexed\", \"abc xyz\", Field.Store.NO, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"tokenized\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED));\n    doc.add(new Field(\"tokenized_reader\", new StringReader(\"abc xyz\")));\n    doc.add(new Field(\"tokenized_tokenstream\", w.w.getAnalyzer().tokenStream(\"tokenized_tokenstream\", new StringReader(\"abc xyz\"))));\n    doc.add(new Field(\"binary\", new byte[10]));\n    doc.add(new Field(\"tv\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"tv_pos\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"tv_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n    doc.add(new Field(\"tv_pos_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    w.addDocument(doc);\n    IndexReader r = w.getReader();\n    w.close();\n\n    doc = r.document(0);\n    // 4 stored fields\n    assertEquals(4, doc.getFields().size());\n    assertEquals(\"abc\", doc.get(\"stored\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_indexed\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_tokenized\"));\n    final BytesRef br = doc.getBinaryValue(\"binary\");\n    assertNotNull(br);\n    assertEquals(10, br.length);\n\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"xyz\")), 1).totalHits);\n\n    for(String field : new String[] {\"tv\", \"tv_pos\", \"tv_off\", \"tv_pos_off\"}) {\n      Fields tvFields = r.getTermVectors(0);\n      Terms tvs = tvFields.terms(field);\n      assertNotNull(tvs);\n      assertEquals(2, tvs.size());\n      TermsEnum tvsEnum = tvs.iterator(null);\n      assertEquals(new BytesRef(\"abc\"), tvsEnum.next());\n      final DocsAndPositionsEnum dpEnum = tvsEnum.docsAndPositions(null, null, false);\n      if (field.equals(\"tv\")) {\n        assertNull(dpEnum);\n      } else {\n        assertNotNull(dpEnum);\n      }\n      assertEquals(new BytesRef(\"xyz\"), tvsEnum.next());\n      assertNull(tvsEnum.next());\n    }\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-3682\n  public void testTransitionAPI() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir);\n\n    Document doc = new Document();\n    doc.add(new Field(\"stored\", \"abc\", Field.Store.YES, Field.Index.NO));\n    doc.add(new Field(\"stored_indexed\", \"abc xyz\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"stored_tokenized\", \"abc xyz\", Field.Store.YES, Field.Index.ANALYZED));\n    doc.add(new Field(\"indexed\", \"abc xyz\", Field.Store.NO, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"tokenized\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED));\n    doc.add(new Field(\"tokenized_reader\", new StringReader(\"abc xyz\")));\n    doc.add(new Field(\"tokenized_tokenstream\", w.w.getAnalyzer().tokenStream(\"tokenized_tokenstream\", new StringReader(\"abc xyz\"))));\n    doc.add(new Field(\"binary\", new byte[10]));\n    doc.add(new Field(\"tv\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"tv_pos\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"tv_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n    doc.add(new Field(\"tv_pos_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    w.addDocument(doc);\n    IndexReader r = w.getReader();\n    w.close();\n\n    doc = r.document(0);\n    // 4 stored fields\n    assertEquals(4, doc.getFields().size());\n    assertEquals(\"abc\", doc.get(\"stored\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_indexed\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_tokenized\"));\n    final BytesRef br = doc.getBinaryValue(\"binary\");\n    assertNotNull(br);\n    assertEquals(10, br.length);\n\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"xyz\")), 1).totalHits);\n\n    for(String field : new String[] {\"tv\", \"tv_pos\", \"tv_off\", \"tv_pos_off\"}) {\n      Fields tvFields = r.getTermVectors(0);\n      Terms tvs = tvFields.terms(field);\n      assertNotNull(tvs);\n      assertEquals(2, tvs.getUniqueTermCount());\n      TermsEnum tvsEnum = tvs.iterator(null);\n      assertEquals(new BytesRef(\"abc\"), tvsEnum.next());\n      final DocsAndPositionsEnum dpEnum = tvsEnum.docsAndPositions(null, null, false);\n      if (field.equals(\"tv\")) {\n        assertNull(dpEnum);\n      } else {\n        assertNotNull(dpEnum);\n      }\n      assertEquals(new BytesRef(\"xyz\"), tvsEnum.next());\n      assertNull(tvsEnum.next());\n    }\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/document/TestDocument#testTransitionAPI().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/document/TestDocument#testTransitionAPI().mjava","sourceNew":"  // LUCENE-3682\n  public void testTransitionAPI() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n\n    Document doc = new Document();\n    doc.add(new Field(\"stored\", \"abc\", Field.Store.YES, Field.Index.NO));\n    doc.add(new Field(\"stored_indexed\", \"abc xyz\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"stored_tokenized\", \"abc xyz\", Field.Store.YES, Field.Index.ANALYZED));\n    doc.add(new Field(\"indexed\", \"abc xyz\", Field.Store.NO, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"tokenized\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED));\n    doc.add(new Field(\"tokenized_reader\", new StringReader(\"abc xyz\")));\n    doc.add(new Field(\"tokenized_tokenstream\", w.w.getAnalyzer().tokenStream(\"tokenized_tokenstream\", new StringReader(\"abc xyz\"))));\n    doc.add(new Field(\"binary\", new byte[10]));\n    doc.add(new Field(\"tv\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"tv_pos\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"tv_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n    doc.add(new Field(\"tv_pos_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    w.addDocument(doc);\n    IndexReader r = w.getReader();\n    w.close();\n\n    doc = r.document(0);\n    // 4 stored fields\n    assertEquals(4, doc.getFields().size());\n    assertEquals(\"abc\", doc.get(\"stored\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_indexed\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_tokenized\"));\n    final BytesRef br = doc.getBinaryValue(\"binary\");\n    assertNotNull(br);\n    assertEquals(10, br.length);\n\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"xyz\")), 1).totalHits);\n\n    for(String field : new String[] {\"tv\", \"tv_pos\", \"tv_off\", \"tv_pos_off\"}) {\n      Fields tvFields = r.getTermVectors(0);\n      Terms tvs = tvFields.terms(field);\n      assertNotNull(tvs);\n      assertEquals(2, tvs.size());\n      TermsEnum tvsEnum = tvs.iterator(null);\n      assertEquals(new BytesRef(\"abc\"), tvsEnum.next());\n      final DocsAndPositionsEnum dpEnum = tvsEnum.docsAndPositions(null, null, false);\n      if (field.equals(\"tv\")) {\n        assertNull(dpEnum);\n      } else {\n        assertNotNull(dpEnum);\n      }\n      assertEquals(new BytesRef(\"xyz\"), tvsEnum.next());\n      assertNull(tvsEnum.next());\n    }\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-3682\n  public void testTransitionAPI() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir);\n\n    Document doc = new Document();\n    doc.add(new Field(\"stored\", \"abc\", Field.Store.YES, Field.Index.NO));\n    doc.add(new Field(\"stored_indexed\", \"abc xyz\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"stored_tokenized\", \"abc xyz\", Field.Store.YES, Field.Index.ANALYZED));\n    doc.add(new Field(\"indexed\", \"abc xyz\", Field.Store.NO, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"tokenized\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED));\n    doc.add(new Field(\"tokenized_reader\", new StringReader(\"abc xyz\")));\n    doc.add(new Field(\"tokenized_tokenstream\", w.w.getAnalyzer().tokenStream(\"tokenized_tokenstream\", new StringReader(\"abc xyz\"))));\n    doc.add(new Field(\"binary\", new byte[10]));\n    doc.add(new Field(\"tv\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"tv_pos\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"tv_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n    doc.add(new Field(\"tv_pos_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    w.addDocument(doc);\n    IndexReader r = w.getReader();\n    w.close();\n\n    doc = r.document(0);\n    // 4 stored fields\n    assertEquals(4, doc.getFields().size());\n    assertEquals(\"abc\", doc.get(\"stored\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_indexed\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_tokenized\"));\n    final BytesRef br = doc.getBinaryValue(\"binary\");\n    assertNotNull(br);\n    assertEquals(10, br.length);\n\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"xyz\")), 1).totalHits);\n\n    for(String field : new String[] {\"tv\", \"tv_pos\", \"tv_off\", \"tv_pos_off\"}) {\n      Fields tvFields = r.getTermVectors(0);\n      Terms tvs = tvFields.terms(field);\n      assertNotNull(tvs);\n      assertEquals(2, tvs.size());\n      TermsEnum tvsEnum = tvs.iterator(null);\n      assertEquals(new BytesRef(\"abc\"), tvsEnum.next());\n      final DocsAndPositionsEnum dpEnum = tvsEnum.docsAndPositions(null, null, false);\n      if (field.equals(\"tv\")) {\n        assertNull(dpEnum);\n      } else {\n        assertNotNull(dpEnum);\n      }\n      assertEquals(new BytesRef(\"xyz\"), tvsEnum.next());\n      assertNull(tvsEnum.next());\n    }\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","date":1338430031,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/test/org/apache/lucene/document/TestDocument#testTransitionAPI().mjava","sourceNew":null,"sourceOld":"  // LUCENE-3682\n  public void testTransitionAPI() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n\n    Document doc = new Document();\n    doc.add(new Field(\"stored\", \"abc\", Field.Store.YES, Field.Index.NO));\n    doc.add(new Field(\"stored_indexed\", \"abc xyz\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"stored_tokenized\", \"abc xyz\", Field.Store.YES, Field.Index.ANALYZED));\n    doc.add(new Field(\"indexed\", \"abc xyz\", Field.Store.NO, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"tokenized\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED));\n    doc.add(new Field(\"tokenized_reader\", new StringReader(\"abc xyz\")));\n    doc.add(new Field(\"tokenized_tokenstream\", w.w.getAnalyzer().tokenStream(\"tokenized_tokenstream\", new StringReader(\"abc xyz\"))));\n    doc.add(new Field(\"binary\", new byte[10]));\n    doc.add(new Field(\"tv\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"tv_pos\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"tv_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n    doc.add(new Field(\"tv_pos_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    w.addDocument(doc);\n    IndexReader r = w.getReader();\n    w.close();\n\n    doc = r.document(0);\n    // 4 stored fields\n    assertEquals(4, doc.getFields().size());\n    assertEquals(\"abc\", doc.get(\"stored\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_indexed\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_tokenized\"));\n    final BytesRef br = doc.getBinaryValue(\"binary\");\n    assertNotNull(br);\n    assertEquals(10, br.length);\n\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"xyz\")), 1).totalHits);\n\n    for(String field : new String[] {\"tv\", \"tv_pos\", \"tv_off\", \"tv_pos_off\"}) {\n      Fields tvFields = r.getTermVectors(0);\n      Terms tvs = tvFields.terms(field);\n      assertNotNull(tvs);\n      assertEquals(2, tvs.size());\n      TermsEnum tvsEnum = tvs.iterator(null);\n      assertEquals(new BytesRef(\"abc\"), tvsEnum.next());\n      final DocsAndPositionsEnum dpEnum = tvsEnum.docsAndPositions(null, null, false);\n      if (field.equals(\"tv\")) {\n        assertNull(dpEnum);\n      } else {\n        assertNotNull(dpEnum);\n      }\n      assertEquals(new BytesRef(\"xyz\"), tvsEnum.next());\n      assertNull(tvsEnum.next());\n    }\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"bdb5e42b0cecd8dfb27767a02ada71899bf17917":["e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf"],"5a238fc456663f685a9db1ed8d680e348bb45171":["e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf","bdb5e42b0cecd8dfb27767a02ada71899bf17917"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf":["d08eba3d52b63561ebf936481ce73e6b6a14aa03"],"d08eba3d52b63561ebf936481ce73e6b6a14aa03":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["bdb5e42b0cecd8dfb27767a02ada71899bf17917"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"]},"commit2Childs":{"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"bdb5e42b0cecd8dfb27767a02ada71899bf17917":["5a238fc456663f685a9db1ed8d680e348bb45171","629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"5a238fc456663f685a9db1ed8d680e348bb45171":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["d08eba3d52b63561ebf936481ce73e6b6a14aa03"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf":["bdb5e42b0cecd8dfb27767a02ada71899bf17917","5a238fc456663f685a9db1ed8d680e348bb45171"],"d08eba3d52b63561ebf936481ce73e6b6a14aa03":["e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5a238fc456663f685a9db1ed8d680e348bb45171","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}