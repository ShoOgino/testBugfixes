{"path":"src/java/org/apache/solr/analysis/SynonymFilter#next().mjava","commits":[{"id":"bd72c170d0045d244f19bc40f18b517f56fbf9c9","date":1138916232,"type":1,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/analysis/SynonymFilter#next().mjava","pathOld":"src/lucene_extras/org/apache/lucene/analysis/SynonymFilter#next().mjava","sourceNew":"  public Token next() throws IOException {\n    while (true) {\n      // if there are any generated tokens, return them... don't try any\n      // matches against them, as we specifically don't want recursion.\n      if (replacement!=null && replacement.hasNext()) {\n        return (Token)replacement.next();\n      }\n\n      // common case fast-path of first token not matching anything\n      Token firstTok = nextTok();\n      if (firstTok ==null) return null;\n      String str = ignoreCase ? firstTok.termText().toLowerCase() : firstTok.termText();\n      Object o = map.submap!=null ? map.submap.get(str) : null;\n      if (o == null) return firstTok;\n\n      // OK, we matched a token, so find the longest match.\n\n      // since matched is only used for matches >= 2, defer creation until now\n      if (matched==null) matched=new LinkedList();\n\n      SynonymMap result = match((SynonymMap)o);\n\n      if (result==null) {\n        // no match, simply return the first token read.\n        return firstTok;\n      }\n\n      // reuse, or create new one each time?\n      ArrayList generated = new ArrayList(result.synonyms.length + matched.size() + 1);\n\n      //\n      // there was a match... let's generate the new tokens, merging\n      // in the matched tokens (position increments need adjusting)\n      //\n      Token lastTok = matched.isEmpty() ? firstTok : (Token)matched.getLast();\n      boolean includeOrig = result.includeOrig();\n\n      Token origTok = includeOrig ? firstTok : null;\n      int origPos = firstTok.getPositionIncrement();  // position of origTok in the original stream\n      int repPos=0; // curr position in replacement token stream\n      int pos=0;  // current position in merged token stream\n\n      for (int i=0; i<result.synonyms.length; i++) {\n        Token repTok = result.synonyms[i];\n        Token newTok = new Token(repTok.termText(), firstTok.startOffset(), lastTok.endOffset(), firstTok.type());\n        repPos += repTok.getPositionIncrement();\n        if (i==0) repPos=origPos;  // make position of first token equal to original\n\n        // if necessary, insert original tokens and adjust position increment\n        while (origTok != null && origPos <= repPos) {\n          origTok.setPositionIncrement(origPos-pos);\n          generated.add(origTok);\n          pos += origTok.getPositionIncrement();\n          origTok = matched.isEmpty() ? null : (Token)matched.removeFirst();\n          if (origTok != null) origPos += origTok.getPositionIncrement();\n        }\n\n        newTok.setPositionIncrement(repPos - pos);\n        generated.add(newTok);\n        pos += newTok.getPositionIncrement();\n      }\n\n      // finish up any leftover original tokens\n      while (origTok!=null) {\n        origTok.setPositionIncrement(origPos-pos);\n        generated.add(origTok);\n        pos += origTok.getPositionIncrement();\n        origTok = matched.isEmpty() ? null : (Token)matched.removeFirst();\n        if (origTok != null) origPos += origTok.getPositionIncrement();\n      }\n\n      // what if we replaced a longer sequence with a shorter one?\n      // a/0 b/5 =>  foo/0\n      // should I re-create the gap on the next buffered token?\n\n      replacement = generated.iterator();\n      // Now return to the top of the loop to read and return the first\n      // generated token.. The reason this is done is that we may have generated\n      // nothing at all, and may need to continue with more matching logic.\n    }\n  }\n\n","sourceOld":"  public Token next() throws IOException {\n    while (true) {\n      // if there are any generated tokens, return them... don't try any\n      // matches against them, as we specifically don't want recursion.\n      if (replacement!=null && replacement.hasNext()) {\n        return (Token)replacement.next();\n      }\n\n      // common case fast-path of first token not matching anything\n      Token firstTok = nextTok();\n      if (firstTok ==null) return null;\n      String str = ignoreCase ? firstTok.termText.toLowerCase() : firstTok.termText;\n      Object o = map.submap!=null ? map.submap.get(str) : null;\n      if (o == null) return firstTok;\n\n      // OK, we matched a token, so find the longest match.\n\n      // since matched is only used for matches >= 2, defer creation until now\n      if (matched==null) matched=new LinkedList();\n\n      SynonymMap result = match((SynonymMap)o);\n\n      if (result==null) {\n        // no match, simply return the first token read.\n        return firstTok;\n      }\n\n      // reuse, or create new one each time?\n      ArrayList generated = new ArrayList(result.synonyms.length + matched.size() + 1);\n\n      //\n      // there was a match... let's generate the new tokens, merging\n      // in the matched tokens (position increments need adjusting)\n      //\n      Token lastTok = matched.isEmpty() ? firstTok : (Token)matched.getLast();\n      boolean includeOrig = result.includeOrig();\n\n      Token origTok = includeOrig ? firstTok : null;\n      int origPos = firstTok.getPositionIncrement();  // position of origTok in the original stream\n      int repPos=0; // curr position in replacement token stream\n      int pos=0;  // current position in merged token stream\n\n      for (int i=0; i<result.synonyms.length; i++) {\n        Token repTok = result.synonyms[i];\n        Token newTok = new Token(repTok.termText, firstTok.startOffset, lastTok.endOffset, firstTok.type);\n        repPos += repTok.getPositionIncrement();\n        if (i==0) repPos=origPos;  // make position of first token equal to original\n\n        // if necessary, insert original tokens and adjust position increment\n        while (origTok != null && origPos <= repPos) {\n          origTok.setPositionIncrement(origPos-pos);\n          generated.add(origTok);\n          pos += origTok.getPositionIncrement();\n          origTok = matched.isEmpty() ? null : (Token)matched.removeFirst();\n          if (origTok != null) origPos += origTok.getPositionIncrement();\n        }\n\n        newTok.setPositionIncrement(repPos - pos);\n        generated.add(newTok);\n        pos += newTok.getPositionIncrement();\n      }\n\n      // finish up any leftover original tokens\n      while (origTok!=null) {\n        origTok.setPositionIncrement(origPos-pos);\n        generated.add(origTok);\n        pos += origTok.getPositionIncrement();\n        origTok = matched.isEmpty() ? null : (Token)matched.removeFirst();\n        if (origTok != null) origPos += origTok.getPositionIncrement();\n      }\n\n      // what if we replaced a longer sequence with a shorter one?\n      // a/0 b/5 =>  foo/0\n      // should I re-create the gap on the next buffered token?\n\n      replacement = generated.iterator();\n      // Now return to the top of the loop to read and return the first\n      // generated token.. The reason this is done is that we may have generated\n      // nothing at all, and may need to continue with more matching logic.\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"751bd9c20e69c4ab60e9b7c5f01ad1a3ff353226","date":1172784996,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/analysis/SynonymFilter#next().mjava","pathOld":"src/java/org/apache/solr/analysis/SynonymFilter#next().mjava","sourceNew":"  public Token next() throws IOException {\n    while (true) {\n      // if there are any generated tokens, return them... don't try any\n      // matches against them, as we specifically don't want recursion.\n      if (replacement!=null && replacement.hasNext()) {\n        return (Token)replacement.next();\n      }\n\n      // common case fast-path of first token not matching anything\n      Token firstTok = nextTok();\n      if (firstTok ==null) return null;\n      String str = ignoreCase ? firstTok.termText().toLowerCase() : firstTok.termText();\n      Object o = map.submap!=null ? map.submap.get(str) : null;\n      if (o == null) return firstTok;\n\n      // OK, we matched a token, so find the longest match.\n\n      matched = new LinkedList();\n\n      SynonymMap result = match((SynonymMap)o);\n\n      if (result==null) {\n        // no match, simply return the first token read.\n        return firstTok;\n      }\n\n      // reuse, or create new one each time?\n      ArrayList generated = new ArrayList(result.synonyms.length + matched.size() + 1);\n\n      //\n      // there was a match... let's generate the new tokens, merging\n      // in the matched tokens (position increments need adjusting)\n      //\n      Token lastTok = matched.isEmpty() ? firstTok : (Token)matched.getLast();\n      boolean includeOrig = result.includeOrig();\n\n      Token origTok = includeOrig ? firstTok : null;\n      int origPos = firstTok.getPositionIncrement();  // position of origTok in the original stream\n      int repPos=0; // curr position in replacement token stream\n      int pos=0;  // current position in merged token stream\n\n      for (int i=0; i<result.synonyms.length; i++) {\n        Token repTok = result.synonyms[i];\n        Token newTok = new Token(repTok.termText(), firstTok.startOffset(), lastTok.endOffset(), firstTok.type());\n        repPos += repTok.getPositionIncrement();\n        if (i==0) repPos=origPos;  // make position of first token equal to original\n\n        // if necessary, insert original tokens and adjust position increment\n        while (origTok != null && origPos <= repPos) {\n          origTok.setPositionIncrement(origPos-pos);\n          generated.add(origTok);\n          pos += origTok.getPositionIncrement();\n          origTok = matched.isEmpty() ? null : (Token)matched.removeFirst();\n          if (origTok != null) origPos += origTok.getPositionIncrement();\n        }\n\n        newTok.setPositionIncrement(repPos - pos);\n        generated.add(newTok);\n        pos += newTok.getPositionIncrement();\n      }\n\n      // finish up any leftover original tokens\n      while (origTok!=null) {\n        origTok.setPositionIncrement(origPos-pos);\n        generated.add(origTok);\n        pos += origTok.getPositionIncrement();\n        origTok = matched.isEmpty() ? null : (Token)matched.removeFirst();\n        if (origTok != null) origPos += origTok.getPositionIncrement();\n      }\n\n      // what if we replaced a longer sequence with a shorter one?\n      // a/0 b/5 =>  foo/0\n      // should I re-create the gap on the next buffered token?\n\n      replacement = generated.iterator();\n      // Now return to the top of the loop to read and return the first\n      // generated token.. The reason this is done is that we may have generated\n      // nothing at all, and may need to continue with more matching logic.\n    }\n  }\n\n","sourceOld":"  public Token next() throws IOException {\n    while (true) {\n      // if there are any generated tokens, return them... don't try any\n      // matches against them, as we specifically don't want recursion.\n      if (replacement!=null && replacement.hasNext()) {\n        return (Token)replacement.next();\n      }\n\n      // common case fast-path of first token not matching anything\n      Token firstTok = nextTok();\n      if (firstTok ==null) return null;\n      String str = ignoreCase ? firstTok.termText().toLowerCase() : firstTok.termText();\n      Object o = map.submap!=null ? map.submap.get(str) : null;\n      if (o == null) return firstTok;\n\n      // OK, we matched a token, so find the longest match.\n\n      // since matched is only used for matches >= 2, defer creation until now\n      if (matched==null) matched=new LinkedList();\n\n      SynonymMap result = match((SynonymMap)o);\n\n      if (result==null) {\n        // no match, simply return the first token read.\n        return firstTok;\n      }\n\n      // reuse, or create new one each time?\n      ArrayList generated = new ArrayList(result.synonyms.length + matched.size() + 1);\n\n      //\n      // there was a match... let's generate the new tokens, merging\n      // in the matched tokens (position increments need adjusting)\n      //\n      Token lastTok = matched.isEmpty() ? firstTok : (Token)matched.getLast();\n      boolean includeOrig = result.includeOrig();\n\n      Token origTok = includeOrig ? firstTok : null;\n      int origPos = firstTok.getPositionIncrement();  // position of origTok in the original stream\n      int repPos=0; // curr position in replacement token stream\n      int pos=0;  // current position in merged token stream\n\n      for (int i=0; i<result.synonyms.length; i++) {\n        Token repTok = result.synonyms[i];\n        Token newTok = new Token(repTok.termText(), firstTok.startOffset(), lastTok.endOffset(), firstTok.type());\n        repPos += repTok.getPositionIncrement();\n        if (i==0) repPos=origPos;  // make position of first token equal to original\n\n        // if necessary, insert original tokens and adjust position increment\n        while (origTok != null && origPos <= repPos) {\n          origTok.setPositionIncrement(origPos-pos);\n          generated.add(origTok);\n          pos += origTok.getPositionIncrement();\n          origTok = matched.isEmpty() ? null : (Token)matched.removeFirst();\n          if (origTok != null) origPos += origTok.getPositionIncrement();\n        }\n\n        newTok.setPositionIncrement(repPos - pos);\n        generated.add(newTok);\n        pos += newTok.getPositionIncrement();\n      }\n\n      // finish up any leftover original tokens\n      while (origTok!=null) {\n        origTok.setPositionIncrement(origPos-pos);\n        generated.add(origTok);\n        pos += origTok.getPositionIncrement();\n        origTok = matched.isEmpty() ? null : (Token)matched.removeFirst();\n        if (origTok != null) origPos += origTok.getPositionIncrement();\n      }\n\n      // what if we replaced a longer sequence with a shorter one?\n      // a/0 b/5 =>  foo/0\n      // should I re-create the gap on the next buffered token?\n\n      replacement = generated.iterator();\n      // Now return to the top of the loop to read and return the first\n      // generated token.. The reason this is done is that we may have generated\n      // nothing at all, and may need to continue with more matching logic.\n    }\n  }\n\n","bugFix":["0c3e228bf650e96f3002a8fb73dd0c13d55af077"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5016c465288a8c5a7b3e4bf088167b26480f6913","date":1172858328,"type":3,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"src/java/org/apache/solr/analysis/SynonymFilter#next().mjava","pathOld":"src/java/org/apache/solr/analysis/SynonymFilter#next().mjava","sourceNew":"  /*\n   * Need to worry about multiple scenarios:\n   *  - need to go for the longest match\n   *    a b => foo      #shouldn't match if \"a b\" is followed by \"c d\"\n   *    a b c d => bar\n   *  - need to backtrack - retry matches for tokens already read\n   *     a b c d => foo\n   *       b c => bar\n   *     If the input stream is \"a b c x\", one will consume \"a b c d\"\n   *     trying to match the first rule... all but \"a\" should be\n   *     pushed back so a match may be made on \"b c\".\n   *  - don't try and match generated tokens (thus need separate queue)\n   *    matching is not recursive.\n   *  - handle optional generation of original tokens in all these cases,\n   *    merging token streams to preserve token positions.\n   *  - preserve original positionIncrement of first matched token\n   */\n  public Token next() throws IOException {\n    while (true) {\n      // if there are any generated tokens, return them... don't try any\n      // matches against them, as we specifically don't want recursion.\n      if (replacement!=null && replacement.hasNext()) {\n        return (Token)replacement.next();\n      }\n\n      // common case fast-path of first token not matching anything\n      Token firstTok = nextTok();\n      if (firstTok ==null) return null;\n      String str = ignoreCase ? firstTok.termText().toLowerCase() : firstTok.termText();\n      Object o = map.submap!=null ? map.submap.get(str) : null;\n      if (o == null) return firstTok;\n\n      // OK, we matched a token, so find the longest match.\n\n      matched = new LinkedList();\n\n      SynonymMap result = match((SynonymMap)o);\n\n      if (result==null) {\n        // no match, simply return the first token read.\n        return firstTok;\n      }\n\n      // reuse, or create new one each time?\n      ArrayList generated = new ArrayList(result.synonyms.length + matched.size() + 1);\n\n      //\n      // there was a match... let's generate the new tokens, merging\n      // in the matched tokens (position increments need adjusting)\n      //\n      Token lastTok = matched.isEmpty() ? firstTok : (Token)matched.getLast();\n      boolean includeOrig = result.includeOrig();\n\n      Token origTok = includeOrig ? firstTok : null;\n      int origPos = firstTok.getPositionIncrement();  // position of origTok in the original stream\n      int repPos=0; // curr position in replacement token stream\n      int pos=0;  // current position in merged token stream\n\n      for (int i=0; i<result.synonyms.length; i++) {\n        Token repTok = result.synonyms[i];\n        Token newTok = new Token(repTok.termText(), firstTok.startOffset(), lastTok.endOffset(), firstTok.type());\n        repPos += repTok.getPositionIncrement();\n        if (i==0) repPos=origPos;  // make position of first token equal to original\n\n        // if necessary, insert original tokens and adjust position increment\n        while (origTok != null && origPos <= repPos) {\n          origTok.setPositionIncrement(origPos-pos);\n          generated.add(origTok);\n          pos += origTok.getPositionIncrement();\n          origTok = matched.isEmpty() ? null : (Token)matched.removeFirst();\n          if (origTok != null) origPos += origTok.getPositionIncrement();\n        }\n\n        newTok.setPositionIncrement(repPos - pos);\n        generated.add(newTok);\n        pos += newTok.getPositionIncrement();\n      }\n\n      // finish up any leftover original tokens\n      while (origTok!=null) {\n        origTok.setPositionIncrement(origPos-pos);\n        generated.add(origTok);\n        pos += origTok.getPositionIncrement();\n        origTok = matched.isEmpty() ? null : (Token)matched.removeFirst();\n        if (origTok != null) origPos += origTok.getPositionIncrement();\n      }\n\n      // what if we replaced a longer sequence with a shorter one?\n      // a/0 b/5 =>  foo/0\n      // should I re-create the gap on the next buffered token?\n\n      replacement = generated.iterator();\n      // Now return to the top of the loop to read and return the first\n      // generated token.. The reason this is done is that we may have generated\n      // nothing at all, and may need to continue with more matching logic.\n    }\n  }\n\n","sourceOld":"  public Token next() throws IOException {\n    while (true) {\n      // if there are any generated tokens, return them... don't try any\n      // matches against them, as we specifically don't want recursion.\n      if (replacement!=null && replacement.hasNext()) {\n        return (Token)replacement.next();\n      }\n\n      // common case fast-path of first token not matching anything\n      Token firstTok = nextTok();\n      if (firstTok ==null) return null;\n      String str = ignoreCase ? firstTok.termText().toLowerCase() : firstTok.termText();\n      Object o = map.submap!=null ? map.submap.get(str) : null;\n      if (o == null) return firstTok;\n\n      // OK, we matched a token, so find the longest match.\n\n      matched = new LinkedList();\n\n      SynonymMap result = match((SynonymMap)o);\n\n      if (result==null) {\n        // no match, simply return the first token read.\n        return firstTok;\n      }\n\n      // reuse, or create new one each time?\n      ArrayList generated = new ArrayList(result.synonyms.length + matched.size() + 1);\n\n      //\n      // there was a match... let's generate the new tokens, merging\n      // in the matched tokens (position increments need adjusting)\n      //\n      Token lastTok = matched.isEmpty() ? firstTok : (Token)matched.getLast();\n      boolean includeOrig = result.includeOrig();\n\n      Token origTok = includeOrig ? firstTok : null;\n      int origPos = firstTok.getPositionIncrement();  // position of origTok in the original stream\n      int repPos=0; // curr position in replacement token stream\n      int pos=0;  // current position in merged token stream\n\n      for (int i=0; i<result.synonyms.length; i++) {\n        Token repTok = result.synonyms[i];\n        Token newTok = new Token(repTok.termText(), firstTok.startOffset(), lastTok.endOffset(), firstTok.type());\n        repPos += repTok.getPositionIncrement();\n        if (i==0) repPos=origPos;  // make position of first token equal to original\n\n        // if necessary, insert original tokens and adjust position increment\n        while (origTok != null && origPos <= repPos) {\n          origTok.setPositionIncrement(origPos-pos);\n          generated.add(origTok);\n          pos += origTok.getPositionIncrement();\n          origTok = matched.isEmpty() ? null : (Token)matched.removeFirst();\n          if (origTok != null) origPos += origTok.getPositionIncrement();\n        }\n\n        newTok.setPositionIncrement(repPos - pos);\n        generated.add(newTok);\n        pos += newTok.getPositionIncrement();\n      }\n\n      // finish up any leftover original tokens\n      while (origTok!=null) {\n        origTok.setPositionIncrement(origPos-pos);\n        generated.add(origTok);\n        pos += origTok.getPositionIncrement();\n        origTok = matched.isEmpty() ? null : (Token)matched.removeFirst();\n        if (origTok != null) origPos += origTok.getPositionIncrement();\n      }\n\n      // what if we replaced a longer sequence with a shorter one?\n      // a/0 b/5 =>  foo/0\n      // should I re-create the gap on the next buffered token?\n\n      replacement = generated.iterator();\n      // Now return to the top of the loop to read and return the first\n      // generated token.. The reason this is done is that we may have generated\n      // nothing at all, and may need to continue with more matching logic.\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"92335bcfcb7158f0d7417ef5b9ac3a156f5a72ac","date":1201149698,"type":5,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/analysis/SynonymFilter#next(Token).mjava","pathOld":"src/java/org/apache/solr/analysis/SynonymFilter#next().mjava","sourceNew":"  /*\n   * Need to worry about multiple scenarios:\n   *  - need to go for the longest match\n   *    a b => foo      #shouldn't match if \"a b\" is followed by \"c d\"\n   *    a b c d => bar\n   *  - need to backtrack - retry matches for tokens already read\n   *     a b c d => foo\n   *       b c => bar\n   *     If the input stream is \"a b c x\", one will consume \"a b c d\"\n   *     trying to match the first rule... all but \"a\" should be\n   *     pushed back so a match may be made on \"b c\".\n   *  - don't try and match generated tokens (thus need separate queue)\n   *    matching is not recursive.\n   *  - handle optional generation of original tokens in all these cases,\n   *    merging token streams to preserve token positions.\n   *  - preserve original positionIncrement of first matched token\n   */\n  @Override\n  public Token next(Token target) throws IOException {\n    while (true) {\n      // if there are any generated tokens, return them... don't try any\n      // matches against them, as we specifically don't want recursion.\n      if (replacement!=null && replacement.hasNext()) {\n        return replacement.next();\n      }\n\n      // common case fast-path of first token not matching anything\n      Token firstTok = nextTok(target);\n      if (firstTok == null) return null;\n      SynonymMap result = map.submap!=null ? map.submap.get(firstTok.termBuffer(), 0, firstTok.termLength()) : null;\n      if (result == null) return firstTok;\n\n      // OK, we matched a token, so find the longest match.\n\n      matched = new LinkedList<Token>();\n\n      result = match(result);\n\n      if (result==null) {\n        // no match, simply return the first token read.\n        return firstTok;\n      }\n\n      // reuse, or create new one each time?\n      ArrayList<Token> generated = new ArrayList<Token>(result.synonyms.length + matched.size() + 1);\n\n      //\n      // there was a match... let's generate the new tokens, merging\n      // in the matched tokens (position increments need adjusting)\n      //\n      Token lastTok = matched.isEmpty() ? firstTok : matched.getLast();\n      boolean includeOrig = result.includeOrig();\n\n      Token origTok = includeOrig ? firstTok : null;\n      int origPos = firstTok.getPositionIncrement();  // position of origTok in the original stream\n      int repPos=0; // curr position in replacement token stream\n      int pos=0;  // current position in merged token stream\n\n      for (int i=0; i<result.synonyms.length; i++) {\n        Token repTok = result.synonyms[i];\n        Token newTok = new Token(firstTok.startOffset(), lastTok.endOffset(), firstTok.type());\n        newTok.setTermBuffer(repTok.termBuffer(), 0, repTok.termLength());\n        repPos += repTok.getPositionIncrement();\n        if (i==0) repPos=origPos;  // make position of first token equal to original\n\n        // if necessary, insert original tokens and adjust position increment\n        while (origTok != null && origPos <= repPos) {\n          origTok.setPositionIncrement(origPos-pos);\n          generated.add(origTok);\n          pos += origTok.getPositionIncrement();\n          origTok = matched.isEmpty() ? null : matched.removeFirst();\n          if (origTok != null) origPos += origTok.getPositionIncrement();\n        }\n\n        newTok.setPositionIncrement(repPos - pos);\n        generated.add(newTok);\n        pos += newTok.getPositionIncrement();\n      }\n\n      // finish up any leftover original tokens\n      while (origTok!=null) {\n        origTok.setPositionIncrement(origPos-pos);\n        generated.add(origTok);\n        pos += origTok.getPositionIncrement();\n        origTok = matched.isEmpty() ? null : matched.removeFirst();\n        if (origTok != null) origPos += origTok.getPositionIncrement();\n      }\n\n      // what if we replaced a longer sequence with a shorter one?\n      // a/0 b/5 =>  foo/0\n      // should I re-create the gap on the next buffered token?\n\n      replacement = generated.iterator();\n      // Now return to the top of the loop to read and return the first\n      // generated token.. The reason this is done is that we may have generated\n      // nothing at all, and may need to continue with more matching logic.\n    }\n  }\n\n","sourceOld":"  /*\n   * Need to worry about multiple scenarios:\n   *  - need to go for the longest match\n   *    a b => foo      #shouldn't match if \"a b\" is followed by \"c d\"\n   *    a b c d => bar\n   *  - need to backtrack - retry matches for tokens already read\n   *     a b c d => foo\n   *       b c => bar\n   *     If the input stream is \"a b c x\", one will consume \"a b c d\"\n   *     trying to match the first rule... all but \"a\" should be\n   *     pushed back so a match may be made on \"b c\".\n   *  - don't try and match generated tokens (thus need separate queue)\n   *    matching is not recursive.\n   *  - handle optional generation of original tokens in all these cases,\n   *    merging token streams to preserve token positions.\n   *  - preserve original positionIncrement of first matched token\n   */\n  public Token next() throws IOException {\n    while (true) {\n      // if there are any generated tokens, return them... don't try any\n      // matches against them, as we specifically don't want recursion.\n      if (replacement!=null && replacement.hasNext()) {\n        return (Token)replacement.next();\n      }\n\n      // common case fast-path of first token not matching anything\n      Token firstTok = nextTok();\n      if (firstTok ==null) return null;\n      String str = ignoreCase ? firstTok.termText().toLowerCase() : firstTok.termText();\n      Object o = map.submap!=null ? map.submap.get(str) : null;\n      if (o == null) return firstTok;\n\n      // OK, we matched a token, so find the longest match.\n\n      matched = new LinkedList();\n\n      SynonymMap result = match((SynonymMap)o);\n\n      if (result==null) {\n        // no match, simply return the first token read.\n        return firstTok;\n      }\n\n      // reuse, or create new one each time?\n      ArrayList generated = new ArrayList(result.synonyms.length + matched.size() + 1);\n\n      //\n      // there was a match... let's generate the new tokens, merging\n      // in the matched tokens (position increments need adjusting)\n      //\n      Token lastTok = matched.isEmpty() ? firstTok : (Token)matched.getLast();\n      boolean includeOrig = result.includeOrig();\n\n      Token origTok = includeOrig ? firstTok : null;\n      int origPos = firstTok.getPositionIncrement();  // position of origTok in the original stream\n      int repPos=0; // curr position in replacement token stream\n      int pos=0;  // current position in merged token stream\n\n      for (int i=0; i<result.synonyms.length; i++) {\n        Token repTok = result.synonyms[i];\n        Token newTok = new Token(repTok.termText(), firstTok.startOffset(), lastTok.endOffset(), firstTok.type());\n        repPos += repTok.getPositionIncrement();\n        if (i==0) repPos=origPos;  // make position of first token equal to original\n\n        // if necessary, insert original tokens and adjust position increment\n        while (origTok != null && origPos <= repPos) {\n          origTok.setPositionIncrement(origPos-pos);\n          generated.add(origTok);\n          pos += origTok.getPositionIncrement();\n          origTok = matched.isEmpty() ? null : (Token)matched.removeFirst();\n          if (origTok != null) origPos += origTok.getPositionIncrement();\n        }\n\n        newTok.setPositionIncrement(repPos - pos);\n        generated.add(newTok);\n        pos += newTok.getPositionIncrement();\n      }\n\n      // finish up any leftover original tokens\n      while (origTok!=null) {\n        origTok.setPositionIncrement(origPos-pos);\n        generated.add(origTok);\n        pos += origTok.getPositionIncrement();\n        origTok = matched.isEmpty() ? null : (Token)matched.removeFirst();\n        if (origTok != null) origPos += origTok.getPositionIncrement();\n      }\n\n      // what if we replaced a longer sequence with a shorter one?\n      // a/0 b/5 =>  foo/0\n      // should I re-create the gap on the next buffered token?\n\n      replacement = generated.iterator();\n      // Now return to the top of the loop to read and return the first\n      // generated token.. The reason this is done is that we may have generated\n      // nothing at all, and may need to continue with more matching logic.\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"bd72c170d0045d244f19bc40f18b517f56fbf9c9":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"92335bcfcb7158f0d7417ef5b9ac3a156f5a72ac":["5016c465288a8c5a7b3e4bf088167b26480f6913"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5016c465288a8c5a7b3e4bf088167b26480f6913":["751bd9c20e69c4ab60e9b7c5f01ad1a3ff353226"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"751bd9c20e69c4ab60e9b7c5f01ad1a3ff353226":["bd72c170d0045d244f19bc40f18b517f56fbf9c9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"bd72c170d0045d244f19bc40f18b517f56fbf9c9":["751bd9c20e69c4ab60e9b7c5f01ad1a3ff353226"],"92335bcfcb7158f0d7417ef5b9ac3a156f5a72ac":[],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["bd72c170d0045d244f19bc40f18b517f56fbf9c9"],"5016c465288a8c5a7b3e4bf088167b26480f6913":["92335bcfcb7158f0d7417ef5b9ac3a156f5a72ac"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"751bd9c20e69c4ab60e9b7c5f01ad1a3ff353226":["5016c465288a8c5a7b3e4bf088167b26480f6913"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["92335bcfcb7158f0d7417ef5b9ac3a156f5a72ac","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}