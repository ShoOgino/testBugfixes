{"path":"lucene/core/src/test/org/apache/lucene/search/TestTopFieldCollectorEarlyTermination#createRandomIndex(boolean).mjava","commits":[{"id":"404d1ab7f6f396235047017c88d545fec15dafb7","date":1511975378,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTopFieldCollectorEarlyTermination#createRandomIndex(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","sourceNew":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(FORCE_MERGE_MAX_SEGMENT_COUNT);\n    }\n    reader = iw.getReader();\n  }\n\n","sourceOld":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(FORCE_MERGE_MAX_SEGMENT_COUNT);\n    }\n    reader = iw.getReader();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bb2ca551506eb8f7577cee251cd2a0cf55b0f020","date":1511981526,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTopFieldCollectorEarlyTermination#createRandomIndex(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTopFieldCollectorEarlyTermination#createRandomIndex(boolean).mjava","sourceNew":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(FORCE_MERGE_MAX_SEGMENT_COUNT);\n    }\n    reader = iw.getReader();\n    if (reader.maxDoc() == 0) {\n      iw.addDocument(new Document());\n      reader.close();\n      reader = iw.getReader();\n    }\n  }\n\n","sourceOld":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(FORCE_MERGE_MAX_SEGMENT_COUNT);\n    }\n    reader = iw.getReader();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1aad05eeff7818b0833c02ac6b743aa72054963b","date":1512093122,"type":1,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTopFieldCollectorEarlyTermination#createRandomIndex(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","sourceNew":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(FORCE_MERGE_MAX_SEGMENT_COUNT);\n    }\n    reader = iw.getReader();\n    if (reader.maxDoc() == 0) {\n      iw.addDocument(new Document());\n      reader.close();\n      reader = iw.getReader();\n    }\n  }\n\n","sourceOld":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(FORCE_MERGE_MAX_SEGMENT_COUNT);\n    }\n    reader = iw.getReader();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7f8a23446329cc7f1063e9f995ba8d32a45e82f8","date":1512658048,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTopFieldCollectorEarlyTermination#createRandomIndex(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTopFieldCollectorEarlyTermination#createRandomIndex(boolean).mjava","sourceNew":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(FORCE_MERGE_MAX_SEGMENT_COUNT);\n    }\n    reader = iw.getReader();\n    if (reader.numDocs() == 0) {\n      iw.addDocument(new Document());\n      reader.close();\n      reader = iw.getReader();\n    }\n  }\n\n","sourceOld":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(FORCE_MERGE_MAX_SEGMENT_COUNT);\n    }\n    reader = iw.getReader();\n    if (reader.maxDoc() == 0) {\n      iw.addDocument(new Document());\n      reader.close();\n      reader = iw.getReader();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"417142ff08fda9cf0b72d5133e63097a166c6458","date":1512729693,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTopFieldCollectorEarlyTermination#createRandomIndex(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTopFieldCollectorEarlyTermination#createRandomIndex(boolean).mjava","sourceNew":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(FORCE_MERGE_MAX_SEGMENT_COUNT);\n    }\n    reader = iw.getReader();\n    if (reader.numDocs() == 0) {\n      iw.addDocument(new Document());\n      reader.close();\n      reader = iw.getReader();\n    }\n  }\n\n","sourceOld":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(FORCE_MERGE_MAX_SEGMENT_COUNT);\n    }\n    reader = iw.getReader();\n    if (reader.maxDoc() == 0) {\n      iw.addDocument(new Document());\n      reader.close();\n      reader = iw.getReader();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"1aad05eeff7818b0833c02ac6b743aa72054963b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","bb2ca551506eb8f7577cee251cd2a0cf55b0f020"],"404d1ab7f6f396235047017c88d545fec15dafb7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"7f8a23446329cc7f1063e9f995ba8d32a45e82f8":["1aad05eeff7818b0833c02ac6b743aa72054963b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"bb2ca551506eb8f7577cee251cd2a0cf55b0f020":["404d1ab7f6f396235047017c88d545fec15dafb7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["417142ff08fda9cf0b72d5133e63097a166c6458"],"417142ff08fda9cf0b72d5133e63097a166c6458":["1aad05eeff7818b0833c02ac6b743aa72054963b","7f8a23446329cc7f1063e9f995ba8d32a45e82f8"]},"commit2Childs":{"1aad05eeff7818b0833c02ac6b743aa72054963b":["7f8a23446329cc7f1063e9f995ba8d32a45e82f8","417142ff08fda9cf0b72d5133e63097a166c6458"],"404d1ab7f6f396235047017c88d545fec15dafb7":["bb2ca551506eb8f7577cee251cd2a0cf55b0f020"],"7f8a23446329cc7f1063e9f995ba8d32a45e82f8":["417142ff08fda9cf0b72d5133e63097a166c6458"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1aad05eeff7818b0833c02ac6b743aa72054963b","404d1ab7f6f396235047017c88d545fec15dafb7"],"bb2ca551506eb8f7577cee251cd2a0cf55b0f020":["1aad05eeff7818b0833c02ac6b743aa72054963b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"417142ff08fda9cf0b72d5133e63097a166c6458":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}