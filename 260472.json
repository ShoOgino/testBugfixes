{"path":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#keywordTokenStream(Collection).mjava","commits":[{"id":"96fd67f40714014f6acbe3b8161a2fc30c59875f","date":1115166257,"type":0,"author":"Erik Hatcher","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#keywordTokenStream(Collection).mjava","pathOld":"/dev/null","sourceNew":"  /**\n  * Convenience method; Creates and returns a token stream that generates a\n  * token for each keyword in the given collection, \"as is\", without any\n  * transforming text analysis. The resulting token stream can be fed into\n  * {@link #addField(String, TokenStream)}, perhaps wrapped into another\n  * {@link org.apache.lucene.analysis.TokenFilter}, as desired.\n  *\n  * @param keywords\n  *            the keywords to generate tokens for\n  * @return the corresponding token stream\n  */\n  public TokenStream keywordTokenStream(final Collection keywords) {\n    if (keywords == null)\n      throw new IllegalArgumentException(\"keywords must not be null\");\n\n    return new TokenStream() {\n      Iterator iter = keywords.iterator();\n      int pos = 0;\n      int start = 0;\n      public Token next() {\n        if (!iter.hasNext()) return null;\n\n        Object obj = iter.next();\n        if (obj == null)\n          throw new IllegalArgumentException(\"keyword must not be null\");\n\n        String term = obj.toString();\n        Token token = new Token(term, start, start + term.length());\n        start += term.length() + 1; // separate words by 1 (blank) character\n        pos++;\n        return token;\n      }\n    };\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c8f14489323057ef6de92ba5ea2d0cfe6e34755f","date":1120167605,"type":3,"author":"Mark Harwood","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#keywordTokenStream(Collection).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#keywordTokenStream(Collection).mjava","sourceNew":"\t/**\r\n\t * Convenience method; Creates and returns a token stream that generates a\r\n\t * token for each keyword in the given collection, \"as is\", without any\r\n\t * transforming text analysis. The resulting token stream can be fed into\r\n\t * {@link #addField(String, TokenStream)}, perhaps wrapped into another\r\n\t * {@link org.apache.lucene.analysis.TokenFilter}, as desired.\r\n\t * \r\n\t * @param keywords\r\n\t *            the keywords to generate tokens for\r\n\t * @return the corresponding token stream\r\n\t */\r\n\tpublic TokenStream keywordTokenStream(final Collection keywords) {\r\n\t\tif (keywords == null)\r\n\t\t\tthrow new IllegalArgumentException(\"keywords must not be null\");\r\n\t\t\r\n\t\treturn new TokenStream() {\r\n\t\t\tprivate Iterator iter = keywords.iterator();\r\n\t\t\tprivate int start = 0;\r\n\t\t\tpublic Token next() {\r\n\t\t\t\tif (!iter.hasNext()) return null;\r\n\t\t\t\t\r\n\t\t\t\tObject obj = iter.next();\r\n\t\t\t\tif (obj == null) \r\n\t\t\t\t\tthrow new IllegalArgumentException(\"keyword must not be null\");\r\n\t\t\t\t\r\n\t\t\t\tString term = obj.toString();\r\n\t\t\t\tToken token = new Token(term, start, start + term.length());\r\n\t\t\t\tstart += term.length() + 1; // separate words by 1 (blank) character\r\n\t\t\t\treturn token;\r\n\t\t\t}\r\n\t\t};\r\n\t}\r\n\n","sourceOld":"  /**\n  * Convenience method; Creates and returns a token stream that generates a\n  * token for each keyword in the given collection, \"as is\", without any\n  * transforming text analysis. The resulting token stream can be fed into\n  * {@link #addField(String, TokenStream)}, perhaps wrapped into another\n  * {@link org.apache.lucene.analysis.TokenFilter}, as desired.\n  *\n  * @param keywords\n  *            the keywords to generate tokens for\n  * @return the corresponding token stream\n  */\n  public TokenStream keywordTokenStream(final Collection keywords) {\n    if (keywords == null)\n      throw new IllegalArgumentException(\"keywords must not be null\");\n\n    return new TokenStream() {\n      Iterator iter = keywords.iterator();\n      int pos = 0;\n      int start = 0;\n      public Token next() {\n        if (!iter.hasNext()) return null;\n\n        Object obj = iter.next();\n        if (obj == null)\n          throw new IllegalArgumentException(\"keyword must not be null\");\n\n        String term = obj.toString();\n        Token token = new Token(term, start, start + term.length());\n        start += term.length() + 1; // separate words by 1 (blank) character\n        pos++;\n        return token;\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"35dd40ede4dd66fa47506858c4a073d295c5a76e","date":1133587328,"type":4,"author":"Wolfgang Hoschek","isMerge":false,"pathNew":"/dev/null","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#keywordTokenStream(Collection).mjava","sourceNew":null,"sourceOld":"\t/**\r\n\t * Convenience method; Creates and returns a token stream that generates a\r\n\t * token for each keyword in the given collection, \"as is\", without any\r\n\t * transforming text analysis. The resulting token stream can be fed into\r\n\t * {@link #addField(String, TokenStream)}, perhaps wrapped into another\r\n\t * {@link org.apache.lucene.analysis.TokenFilter}, as desired.\r\n\t * \r\n\t * @param keywords\r\n\t *            the keywords to generate tokens for\r\n\t * @return the corresponding token stream\r\n\t */\r\n\tpublic TokenStream keywordTokenStream(final Collection keywords) {\r\n\t\tif (keywords == null)\r\n\t\t\tthrow new IllegalArgumentException(\"keywords must not be null\");\r\n\t\t\r\n\t\treturn new TokenStream() {\r\n\t\t\tprivate Iterator iter = keywords.iterator();\r\n\t\t\tprivate int start = 0;\r\n\t\t\tpublic Token next() {\r\n\t\t\t\tif (!iter.hasNext()) return null;\r\n\t\t\t\t\r\n\t\t\t\tObject obj = iter.next();\r\n\t\t\t\tif (obj == null) \r\n\t\t\t\t\tthrow new IllegalArgumentException(\"keyword must not be null\");\r\n\t\t\t\t\r\n\t\t\t\tString term = obj.toString();\r\n\t\t\t\tToken token = new Token(term, start, start + term.length());\r\n\t\t\t\tstart += term.length() + 1; // separate words by 1 (blank) character\r\n\t\t\t\treturn token;\r\n\t\t\t}\r\n\t\t};\r\n\t}\r\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a93e1e4a21be8ebb98e53e6933412a363931faa1","date":1133587471,"type":0,"author":"Wolfgang Hoschek","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#keywordTokenStream(Collection).mjava","pathOld":"/dev/null","sourceNew":"\t/**\n\t * Convenience method; Creates and returns a token stream that generates a\n\t * token for each keyword in the given collection, \"as is\", without any\n\t * transforming text analysis. The resulting token stream can be fed into\n\t * {@link #addField(String, TokenStream)}, perhaps wrapped into another\n\t * {@link org.apache.lucene.analysis.TokenFilter}, as desired.\n\t * \n\t * @param keywords\n\t *            the keywords to generate tokens for\n\t * @return the corresponding token stream\n\t */\n\tpublic TokenStream keywordTokenStream(final Collection keywords) {\n\t\t// TODO: deprecate & move this method into AnalyzerUtil?\n\t\tif (keywords == null)\n\t\t\tthrow new IllegalArgumentException(\"keywords must not be null\");\n\t\t\n\t\treturn new TokenStream() {\n\t\t\tprivate Iterator iter = keywords.iterator();\n\t\t\tprivate int start = 0;\n\t\t\tpublic Token next() {\n\t\t\t\tif (!iter.hasNext()) return null;\n\t\t\t\t\n\t\t\t\tObject obj = iter.next();\n\t\t\t\tif (obj == null) \n\t\t\t\t\tthrow new IllegalArgumentException(\"keyword must not be null\");\n\t\t\t\t\n\t\t\t\tString term = obj.toString();\n\t\t\t\tToken token = new Token(term, start, start + term.length());\n\t\t\t\tstart += term.length() + 1; // separate words by 1 (blank) character\n\t\t\t\treturn token;\n\t\t\t}\n\t\t};\n\t}\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7f68e24227d5556d33ee6d586fd9010cd9ff8bec","date":1150091176,"type":3,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#keywordTokenStream(Collection).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#keywordTokenStream(Collection).mjava","sourceNew":"  /**\n   * Convenience method; Creates and returns a token stream that generates a\n   * token for each keyword in the given collection, \"as is\", without any\n   * transforming text analysis. The resulting token stream can be fed into\n   * {@link #addField(String, TokenStream)}, perhaps wrapped into another\n   * {@link org.apache.lucene.analysis.TokenFilter}, as desired.\n   * \n   * @param keywords\n   *            the keywords to generate tokens for\n   * @return the corresponding token stream\n   */\n  public TokenStream keywordTokenStream(final Collection keywords) {\n    // TODO: deprecate & move this method into AnalyzerUtil?\n    if (keywords == null)\n      throw new IllegalArgumentException(\"keywords must not be null\");\n    \n    return new TokenStream() {\n      private Iterator iter = keywords.iterator();\n      private int start = 0;\n      public Token next() {\n        if (!iter.hasNext()) return null;\n        \n        Object obj = iter.next();\n        if (obj == null) \n          throw new IllegalArgumentException(\"keyword must not be null\");\n        \n        String term = obj.toString();\n        Token token = new Token(term, start, start + term.length());\n        start += term.length() + 1; // separate words by 1 (blank) character\n        return token;\n      }\n    };\n  }\n\n","sourceOld":"\t/**\n\t * Convenience method; Creates and returns a token stream that generates a\n\t * token for each keyword in the given collection, \"as is\", without any\n\t * transforming text analysis. The resulting token stream can be fed into\n\t * {@link #addField(String, TokenStream)}, perhaps wrapped into another\n\t * {@link org.apache.lucene.analysis.TokenFilter}, as desired.\n\t * \n\t * @param keywords\n\t *            the keywords to generate tokens for\n\t * @return the corresponding token stream\n\t */\n\tpublic TokenStream keywordTokenStream(final Collection keywords) {\n\t\t// TODO: deprecate & move this method into AnalyzerUtil?\n\t\tif (keywords == null)\n\t\t\tthrow new IllegalArgumentException(\"keywords must not be null\");\n\t\t\n\t\treturn new TokenStream() {\n\t\t\tprivate Iterator iter = keywords.iterator();\n\t\t\tprivate int start = 0;\n\t\t\tpublic Token next() {\n\t\t\t\tif (!iter.hasNext()) return null;\n\t\t\t\t\n\t\t\t\tObject obj = iter.next();\n\t\t\t\tif (obj == null) \n\t\t\t\t\tthrow new IllegalArgumentException(\"keyword must not be null\");\n\t\t\t\t\n\t\t\t\tString term = obj.toString();\n\t\t\t\tToken token = new Token(term, start, start + term.length());\n\t\t\t\tstart += term.length() + 1; // separate words by 1 (blank) character\n\t\t\t\treturn token;\n\t\t\t}\n\t\t};\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e2cb543b41c145f33390f460ee743d6693c9c6c","date":1219243087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#keywordTokenStream(Collection).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#keywordTokenStream(Collection).mjava","sourceNew":"  /**\n   * Convenience method; Creates and returns a token stream that generates a\n   * token for each keyword in the given collection, \"as is\", without any\n   * transforming text analysis. The resulting token stream can be fed into\n   * {@link #addField(String, TokenStream)}, perhaps wrapped into another\n   * {@link org.apache.lucene.analysis.TokenFilter}, as desired.\n   * \n   * @param keywords\n   *            the keywords to generate tokens for\n   * @return the corresponding token stream\n   */\n  public TokenStream keywordTokenStream(final Collection keywords) {\n    // TODO: deprecate & move this method into AnalyzerUtil?\n    if (keywords == null)\n      throw new IllegalArgumentException(\"keywords must not be null\");\n    \n    return new TokenStream() {\n      private Iterator iter = keywords.iterator();\n      private int start = 0;\n      public Token next(final Token reusableToken) {\n        assert reusableToken != null;\n        if (!iter.hasNext()) return null;\n        \n        Object obj = iter.next();\n        if (obj == null) \n          throw new IllegalArgumentException(\"keyword must not be null\");\n        \n        String term = obj.toString();\n        reusableToken.reinit(term, start, start+reusableToken.termLength());\n        start += term.length() + 1; // separate words by 1 (blank) character\n        return reusableToken;\n      }\n    };\n  }\n\n","sourceOld":"  /**\n   * Convenience method; Creates and returns a token stream that generates a\n   * token for each keyword in the given collection, \"as is\", without any\n   * transforming text analysis. The resulting token stream can be fed into\n   * {@link #addField(String, TokenStream)}, perhaps wrapped into another\n   * {@link org.apache.lucene.analysis.TokenFilter}, as desired.\n   * \n   * @param keywords\n   *            the keywords to generate tokens for\n   * @return the corresponding token stream\n   */\n  public TokenStream keywordTokenStream(final Collection keywords) {\n    // TODO: deprecate & move this method into AnalyzerUtil?\n    if (keywords == null)\n      throw new IllegalArgumentException(\"keywords must not be null\");\n    \n    return new TokenStream() {\n      private Iterator iter = keywords.iterator();\n      private int start = 0;\n      public Token next() {\n        if (!iter.hasNext()) return null;\n        \n        Object obj = iter.next();\n        if (obj == null) \n          throw new IllegalArgumentException(\"keyword must not be null\");\n        \n        String term = obj.toString();\n        Token token = new Token(term, start, start + term.length());\n        start += term.length() + 1; // separate words by 1 (blank) character\n        return token;\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9b5756469957918cac40a831acec9cf01c8c2bb3","date":1249167152,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#keywordTokenStream(Collection).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#keywordTokenStream(Collection).mjava","sourceNew":"  /**\n   * Convenience method; Creates and returns a token stream that generates a\n   * token for each keyword in the given collection, \"as is\", without any\n   * transforming text analysis. The resulting token stream can be fed into\n   * {@link #addField(String, TokenStream)}, perhaps wrapped into another\n   * {@link org.apache.lucene.analysis.TokenFilter}, as desired.\n   * \n   * @param keywords\n   *            the keywords to generate tokens for\n   * @return the corresponding token stream\n   */\n  public TokenStream keywordTokenStream(final Collection keywords) {\n    // TODO: deprecate & move this method into AnalyzerUtil?\n    if (keywords == null)\n      throw new IllegalArgumentException(\"keywords must not be null\");\n    \n    return new TokenStream() {\n      private Iterator iter = keywords.iterator();\n      private int start = 0;\n      private TermAttribute termAtt = (TermAttribute) addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = (OffsetAttribute) addAttribute(OffsetAttribute.class);\n      \n      public boolean incrementToken() {\n        if (!iter.hasNext()) return false;\n        \n        Object obj = iter.next();\n        if (obj == null) \n          throw new IllegalArgumentException(\"keyword must not be null\");\n        \n        String term = obj.toString();\n        termAtt.setTermBuffer(term);\n        offsetAtt.setOffset(start, start+termAtt.termLength());\n        start += term.length() + 1; // separate words by 1 (blank) character\n        return true;\n      }\n    };\n  }\n\n","sourceOld":"  /**\n   * Convenience method; Creates and returns a token stream that generates a\n   * token for each keyword in the given collection, \"as is\", without any\n   * transforming text analysis. The resulting token stream can be fed into\n   * {@link #addField(String, TokenStream)}, perhaps wrapped into another\n   * {@link org.apache.lucene.analysis.TokenFilter}, as desired.\n   * \n   * @param keywords\n   *            the keywords to generate tokens for\n   * @return the corresponding token stream\n   */\n  public TokenStream keywordTokenStream(final Collection keywords) {\n    // TODO: deprecate & move this method into AnalyzerUtil?\n    if (keywords == null)\n      throw new IllegalArgumentException(\"keywords must not be null\");\n    \n    return new TokenStream() {\n      private Iterator iter = keywords.iterator();\n      private int start = 0;\n      public Token next(final Token reusableToken) {\n        assert reusableToken != null;\n        if (!iter.hasNext()) return null;\n        \n        Object obj = iter.next();\n        if (obj == null) \n          throw new IllegalArgumentException(\"keyword must not be null\");\n        \n        String term = obj.toString();\n        reusableToken.reinit(term, start, start+reusableToken.termLength());\n        start += term.length() + 1; // separate words by 1 (blank) character\n        return reusableToken;\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8d78f014fded44fbde905f4f84cdc21907b371e8","date":1254383623,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#keywordTokenStream(Collection).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#keywordTokenStream(Collection).mjava","sourceNew":"  /**\n   * Convenience method; Creates and returns a token stream that generates a\n   * token for each keyword in the given collection, \"as is\", without any\n   * transforming text analysis. The resulting token stream can be fed into\n   * {@link #addField(String, TokenStream)}, perhaps wrapped into another\n   * {@link org.apache.lucene.analysis.TokenFilter}, as desired.\n   * \n   * @param keywords\n   *            the keywords to generate tokens for\n   * @return the corresponding token stream\n   */\n  public TokenStream keywordTokenStream(final Collection keywords) {\n    // TODO: deprecate & move this method into AnalyzerUtil?\n    if (keywords == null)\n      throw new IllegalArgumentException(\"keywords must not be null\");\n    \n    return new TokenStream() {\n      private Iterator iter = keywords.iterator();\n      private int start = 0;\n      private TermAttribute termAtt = addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      public boolean incrementToken() {\n        if (!iter.hasNext()) return false;\n        \n        Object obj = iter.next();\n        if (obj == null) \n          throw new IllegalArgumentException(\"keyword must not be null\");\n        \n        String term = obj.toString();\n        termAtt.setTermBuffer(term);\n        offsetAtt.setOffset(start, start+termAtt.termLength());\n        start += term.length() + 1; // separate words by 1 (blank) character\n        return true;\n      }\n    };\n  }\n\n","sourceOld":"  /**\n   * Convenience method; Creates and returns a token stream that generates a\n   * token for each keyword in the given collection, \"as is\", without any\n   * transforming text analysis. The resulting token stream can be fed into\n   * {@link #addField(String, TokenStream)}, perhaps wrapped into another\n   * {@link org.apache.lucene.analysis.TokenFilter}, as desired.\n   * \n   * @param keywords\n   *            the keywords to generate tokens for\n   * @return the corresponding token stream\n   */\n  public TokenStream keywordTokenStream(final Collection keywords) {\n    // TODO: deprecate & move this method into AnalyzerUtil?\n    if (keywords == null)\n      throw new IllegalArgumentException(\"keywords must not be null\");\n    \n    return new TokenStream() {\n      private Iterator iter = keywords.iterator();\n      private int start = 0;\n      private TermAttribute termAtt = (TermAttribute) addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = (OffsetAttribute) addAttribute(OffsetAttribute.class);\n      \n      public boolean incrementToken() {\n        if (!iter.hasNext()) return false;\n        \n        Object obj = iter.next();\n        if (obj == null) \n          throw new IllegalArgumentException(\"keyword must not be null\");\n        \n        String term = obj.toString();\n        termAtt.setTermBuffer(term);\n        offsetAtt.setOffset(start, start+termAtt.termLength());\n        start += term.length() + 1; // separate words by 1 (blank) character\n        return true;\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f11899016a0460a7ea2e4b008d002e1e75c7d867","date":1256772085,"type":5,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#[T]_keywordTokenStream(Collection[T]).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#keywordTokenStream(Collection).mjava","sourceNew":"  /**\n   * Convenience method; Creates and returns a token stream that generates a\n   * token for each keyword in the given collection, \"as is\", without any\n   * transforming text analysis. The resulting token stream can be fed into\n   * {@link #addField(String, TokenStream)}, perhaps wrapped into another\n   * {@link org.apache.lucene.analysis.TokenFilter}, as desired.\n   * \n   * @param keywords\n   *            the keywords to generate tokens for\n   * @return the corresponding token stream\n   */\n  public <T> TokenStream keywordTokenStream(final Collection<T> keywords) {\n    // TODO: deprecate & move this method into AnalyzerUtil?\n    if (keywords == null)\n      throw new IllegalArgumentException(\"keywords must not be null\");\n    \n    return new TokenStream() {\n      private Iterator<T> iter = keywords.iterator();\n      private int start = 0;\n      private TermAttribute termAtt = addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      public boolean incrementToken() {\n        if (!iter.hasNext()) return false;\n        \n        T obj = iter.next();\n        if (obj == null) \n          throw new IllegalArgumentException(\"keyword must not be null\");\n        \n        String term = obj.toString();\n        termAtt.setTermBuffer(term);\n        offsetAtt.setOffset(start, start+termAtt.termLength());\n        start += term.length() + 1; // separate words by 1 (blank) character\n        return true;\n      }\n    };\n  }\n\n","sourceOld":"  /**\n   * Convenience method; Creates and returns a token stream that generates a\n   * token for each keyword in the given collection, \"as is\", without any\n   * transforming text analysis. The resulting token stream can be fed into\n   * {@link #addField(String, TokenStream)}, perhaps wrapped into another\n   * {@link org.apache.lucene.analysis.TokenFilter}, as desired.\n   * \n   * @param keywords\n   *            the keywords to generate tokens for\n   * @return the corresponding token stream\n   */\n  public TokenStream keywordTokenStream(final Collection keywords) {\n    // TODO: deprecate & move this method into AnalyzerUtil?\n    if (keywords == null)\n      throw new IllegalArgumentException(\"keywords must not be null\");\n    \n    return new TokenStream() {\n      private Iterator iter = keywords.iterator();\n      private int start = 0;\n      private TermAttribute termAtt = addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      public boolean incrementToken() {\n        if (!iter.hasNext()) return false;\n        \n        Object obj = iter.next();\n        if (obj == null) \n          throw new IllegalArgumentException(\"keyword must not be null\");\n        \n        String term = obj.toString();\n        termAtt.setTermBuffer(term);\n        offsetAtt.setOffset(start, start+termAtt.termLength());\n        start += term.length() + 1; // separate words by 1 (blank) character\n        return true;\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7e2cb543b41c145f33390f460ee743d6693c9c6c":["7f68e24227d5556d33ee6d586fd9010cd9ff8bec"],"35dd40ede4dd66fa47506858c4a073d295c5a76e":["c8f14489323057ef6de92ba5ea2d0cfe6e34755f"],"96fd67f40714014f6acbe3b8161a2fc30c59875f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c8f14489323057ef6de92ba5ea2d0cfe6e34755f":["96fd67f40714014f6acbe3b8161a2fc30c59875f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9b5756469957918cac40a831acec9cf01c8c2bb3":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"7f68e24227d5556d33ee6d586fd9010cd9ff8bec":["a93e1e4a21be8ebb98e53e6933412a363931faa1"],"8d78f014fded44fbde905f4f84cdc21907b371e8":["9b5756469957918cac40a831acec9cf01c8c2bb3"],"a93e1e4a21be8ebb98e53e6933412a363931faa1":["35dd40ede4dd66fa47506858c4a073d295c5a76e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["f11899016a0460a7ea2e4b008d002e1e75c7d867"],"f11899016a0460a7ea2e4b008d002e1e75c7d867":["8d78f014fded44fbde905f4f84cdc21907b371e8"]},"commit2Childs":{"7e2cb543b41c145f33390f460ee743d6693c9c6c":["9b5756469957918cac40a831acec9cf01c8c2bb3"],"35dd40ede4dd66fa47506858c4a073d295c5a76e":["a93e1e4a21be8ebb98e53e6933412a363931faa1"],"96fd67f40714014f6acbe3b8161a2fc30c59875f":["c8f14489323057ef6de92ba5ea2d0cfe6e34755f"],"c8f14489323057ef6de92ba5ea2d0cfe6e34755f":["35dd40ede4dd66fa47506858c4a073d295c5a76e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["96fd67f40714014f6acbe3b8161a2fc30c59875f"],"7f68e24227d5556d33ee6d586fd9010cd9ff8bec":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"9b5756469957918cac40a831acec9cf01c8c2bb3":["8d78f014fded44fbde905f4f84cdc21907b371e8"],"8d78f014fded44fbde905f4f84cdc21907b371e8":["f11899016a0460a7ea2e4b008d002e1e75c7d867"],"a93e1e4a21be8ebb98e53e6933412a363931faa1":["7f68e24227d5556d33ee6d586fd9010cd9ff8bec"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"f11899016a0460a7ea2e4b008d002e1e75c7d867":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}