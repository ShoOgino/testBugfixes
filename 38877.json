{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/TestWikipediaTokenizerFactory#testTokenizerUntokenizedOnly().mjava","commits":[{"id":"a9ff0aa05c1ae214397beec2eae8ce7b214dd4c4","date":1496825484,"type":0,"author":"jpgilaberte","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/TestWikipediaTokenizerFactory#testTokenizerUntokenizedOnly().mjava","pathOld":"/dev/null","sourceNew":"    public void testTokenizerUntokenizedOnly() throws Exception {\n      String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n      Set<String> untoks = new HashSet<>();\n      untoks.add(WikipediaTokenizer.CATEGORY);\n      untoks.add(WikipediaTokenizer.ITALICS);\n      Tokenizer tf = tokenizerFactory(WIKIPEDIA, TOKEN_OUTPUT, new Integer(WikipediaTokenizer.UNTOKENIZED_ONLY).toString(), UNTOKENIZED_TYPES, WikipediaTokenizer.CATEGORY + \", \" + WikipediaTokenizer.ITALICS).create(newAttributeFactory());\n      tf.setReader(new StringReader(test));\n      assertTokenStreamContents(tf,\n                                new String[] { \"a b c d\", \"e f g\", \"link\", \"here\", \"link\",\n                                               \"there\", \"italics here\", \"something\", \"more italics\", \"h   i   j\" },\n                                new int[] { 11, 32, 42, 47, 56, 61, 71, 86, 98, 124 },\n                                new int[] { 18, 37, 46, 51, 60, 66, 83, 95, 110, 133 },\n                                new int[] { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 }\n      );\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f344bb33ca91f48e99c061980115b46fa84fc8f5","date":1496903283,"type":0,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/TestWikipediaTokenizerFactory#testTokenizerUntokenizedOnly().mjava","pathOld":"/dev/null","sourceNew":"    public void testTokenizerUntokenizedOnly() throws Exception {\n      String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n      Set<String> untoks = new HashSet<>();\n      untoks.add(WikipediaTokenizer.CATEGORY);\n      untoks.add(WikipediaTokenizer.ITALICS);\n      Tokenizer tf = tokenizerFactory(WIKIPEDIA, TOKEN_OUTPUT, new Integer(WikipediaTokenizer.UNTOKENIZED_ONLY).toString(), UNTOKENIZED_TYPES, WikipediaTokenizer.CATEGORY + \", \" + WikipediaTokenizer.ITALICS).create(newAttributeFactory());\n      tf.setReader(new StringReader(test));\n      assertTokenStreamContents(tf,\n                                new String[] { \"a b c d\", \"e f g\", \"link\", \"here\", \"link\",\n                                               \"there\", \"italics here\", \"something\", \"more italics\", \"h   i   j\" },\n                                new int[] { 11, 32, 42, 47, 56, 61, 71, 86, 98, 124 },\n                                new int[] { 18, 37, 46, 51, 60, 66, 83, 95, 110, 133 },\n                                new int[] { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 }\n      );\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","date":1498028748,"type":0,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/TestWikipediaTokenizerFactory#testTokenizerUntokenizedOnly().mjava","pathOld":"/dev/null","sourceNew":"    public void testTokenizerUntokenizedOnly() throws Exception {\n      String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n      Set<String> untoks = new HashSet<>();\n      untoks.add(WikipediaTokenizer.CATEGORY);\n      untoks.add(WikipediaTokenizer.ITALICS);\n      Tokenizer tf = tokenizerFactory(WIKIPEDIA, TOKEN_OUTPUT, new Integer(WikipediaTokenizer.UNTOKENIZED_ONLY).toString(), UNTOKENIZED_TYPES, WikipediaTokenizer.CATEGORY + \", \" + WikipediaTokenizer.ITALICS).create(newAttributeFactory());\n      tf.setReader(new StringReader(test));\n      assertTokenStreamContents(tf,\n                                new String[] { \"a b c d\", \"e f g\", \"link\", \"here\", \"link\",\n                                               \"there\", \"italics here\", \"something\", \"more italics\", \"h   i   j\" },\n                                new int[] { 11, 32, 42, 47, 56, 61, 71, 86, 98, 124 },\n                                new int[] { 18, 37, 46, 51, 60, 66, 83, 95, 110, 133 },\n                                new int[] { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 }\n      );\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/TestWikipediaTokenizerFactory#testTokenizerUntokenizedOnly().mjava","pathOld":"/dev/null","sourceNew":"    public void testTokenizerUntokenizedOnly() throws Exception {\n      String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n      Set<String> untoks = new HashSet<>();\n      untoks.add(WikipediaTokenizer.CATEGORY);\n      untoks.add(WikipediaTokenizer.ITALICS);\n      Tokenizer tf = tokenizerFactory(WIKIPEDIA, TOKEN_OUTPUT, new Integer(WikipediaTokenizer.UNTOKENIZED_ONLY).toString(), UNTOKENIZED_TYPES, WikipediaTokenizer.CATEGORY + \", \" + WikipediaTokenizer.ITALICS).create(newAttributeFactory());\n      tf.setReader(new StringReader(test));\n      assertTokenStreamContents(tf,\n                                new String[] { \"a b c d\", \"e f g\", \"link\", \"here\", \"link\",\n                                               \"there\", \"italics here\", \"something\", \"more italics\", \"h   i   j\" },\n                                new int[] { 11, 32, 42, 47, 56, 61, 71, 86, 98, 124 },\n                                new int[] { 18, 37, 46, 51, 60, 66, 83, 95, 110, 133 },\n                                new int[] { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 }\n      );\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b6a269c1ddba3f8c9fa9a40572ecc538eddda41a","date":1528054850,"type":3,"author":"Michael Braun","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/TestWikipediaTokenizerFactory#testTokenizerUntokenizedOnly().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/TestWikipediaTokenizerFactory#testTokenizerUntokenizedOnly().mjava","sourceNew":"    public void testTokenizerUntokenizedOnly() throws Exception {\n      String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n      Set<String> untoks = new HashSet<>();\n      untoks.add(WikipediaTokenizer.CATEGORY);\n      untoks.add(WikipediaTokenizer.ITALICS);\n      Tokenizer tf = tokenizerFactory(WIKIPEDIA, TOKEN_OUTPUT, Integer.toString(WikipediaTokenizer.UNTOKENIZED_ONLY), UNTOKENIZED_TYPES, WikipediaTokenizer.CATEGORY + \", \" + WikipediaTokenizer.ITALICS).create(newAttributeFactory());\n      tf.setReader(new StringReader(test));\n      assertTokenStreamContents(tf,\n                                new String[] { \"a b c d\", \"e f g\", \"link\", \"here\", \"link\",\n                                               \"there\", \"italics here\", \"something\", \"more italics\", \"h   i   j\" },\n                                new int[] { 11, 32, 42, 47, 56, 61, 71, 86, 98, 124 },\n                                new int[] { 18, 37, 46, 51, 60, 66, 83, 95, 110, 133 },\n                                new int[] { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 }\n      );\n    }\n\n","sourceOld":"    public void testTokenizerUntokenizedOnly() throws Exception {\n      String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n      Set<String> untoks = new HashSet<>();\n      untoks.add(WikipediaTokenizer.CATEGORY);\n      untoks.add(WikipediaTokenizer.ITALICS);\n      Tokenizer tf = tokenizerFactory(WIKIPEDIA, TOKEN_OUTPUT, new Integer(WikipediaTokenizer.UNTOKENIZED_ONLY).toString(), UNTOKENIZED_TYPES, WikipediaTokenizer.CATEGORY + \", \" + WikipediaTokenizer.ITALICS).create(newAttributeFactory());\n      tf.setReader(new StringReader(test));\n      assertTokenStreamContents(tf,\n                                new String[] { \"a b c d\", \"e f g\", \"link\", \"here\", \"link\",\n                                               \"there\", \"italics here\", \"something\", \"more italics\", \"h   i   j\" },\n                                new int[] { 11, 32, 42, 47, 56, 61, 71, 86, 98, 124 },\n                                new int[] { 18, 37, 46, 51, 60, 66, 83, 95, 110, 133 },\n                                new int[] { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 }\n      );\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"628903f37b6c442da0d390db1c6af9a0e74d41a7","date":1531736685,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/TestWikipediaTokenizerFactory#testTokenizerUntokenizedOnly().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/TestWikipediaTokenizerFactory#testTokenizerUntokenizedOnly().mjava","sourceNew":"    public void testTokenizerUntokenizedOnly() throws Exception {\n      String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n      Set<String> untoks = new HashSet<>();\n      untoks.add(WikipediaTokenizer.CATEGORY);\n      untoks.add(WikipediaTokenizer.ITALICS);\n      Tokenizer tf = tokenizerFactory(WIKIPEDIA, TOKEN_OUTPUT, Integer.toString(WikipediaTokenizer.UNTOKENIZED_ONLY), UNTOKENIZED_TYPES, WikipediaTokenizer.CATEGORY + \", \" + WikipediaTokenizer.ITALICS).create(newAttributeFactory());\n      tf.setReader(new StringReader(test));\n      assertTokenStreamContents(tf,\n                                new String[] { \"a b c d\", \"e f g\", \"link\", \"here\", \"link\",\n                                               \"there\", \"italics here\", \"something\", \"more italics\", \"h   i   j\" },\n                                new int[] { 11, 32, 42, 47, 56, 61, 71, 86, 98, 124 },\n                                new int[] { 18, 37, 46, 51, 60, 66, 83, 95, 110, 133 },\n                                new int[] { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 }\n      );\n    }\n\n","sourceOld":"    public void testTokenizerUntokenizedOnly() throws Exception {\n      String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n      Set<String> untoks = new HashSet<>();\n      untoks.add(WikipediaTokenizer.CATEGORY);\n      untoks.add(WikipediaTokenizer.ITALICS);\n      Tokenizer tf = tokenizerFactory(WIKIPEDIA, TOKEN_OUTPUT, new Integer(WikipediaTokenizer.UNTOKENIZED_ONLY).toString(), UNTOKENIZED_TYPES, WikipediaTokenizer.CATEGORY + \", \" + WikipediaTokenizer.ITALICS).create(newAttributeFactory());\n      tf.setReader(new StringReader(test));\n      assertTokenStreamContents(tf,\n                                new String[] { \"a b c d\", \"e f g\", \"link\", \"here\", \"link\",\n                                               \"there\", \"italics here\", \"something\", \"more italics\", \"h   i   j\" },\n                                new int[] { 11, 32, 42, 47, 56, 61, 71, 86, 98, 124 },\n                                new int[] { 18, 37, 46, 51, 60, 66, 83, 95, 110, 133 },\n                                new int[] { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 }\n      );\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","date":1531905561,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/TestWikipediaTokenizerFactory#testTokenizerUntokenizedOnly().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/TestWikipediaTokenizerFactory#testTokenizerUntokenizedOnly().mjava","sourceNew":"    public void testTokenizerUntokenizedOnly() throws Exception {\n      String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n      Set<String> untoks = new HashSet<>();\n      untoks.add(WikipediaTokenizer.CATEGORY);\n      untoks.add(WikipediaTokenizer.ITALICS);\n      Tokenizer tf = tokenizerFactory(WIKIPEDIA, TOKEN_OUTPUT, Integer.toString(WikipediaTokenizer.UNTOKENIZED_ONLY), UNTOKENIZED_TYPES, WikipediaTokenizer.CATEGORY + \", \" + WikipediaTokenizer.ITALICS).create(newAttributeFactory());\n      tf.setReader(new StringReader(test));\n      assertTokenStreamContents(tf,\n                                new String[] { \"a b c d\", \"e f g\", \"link\", \"here\", \"link\",\n                                               \"there\", \"italics here\", \"something\", \"more italics\", \"h   i   j\" },\n                                new int[] { 11, 32, 42, 47, 56, 61, 71, 86, 98, 124 },\n                                new int[] { 18, 37, 46, 51, 60, 66, 83, 95, 110, 133 },\n                                new int[] { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 }\n      );\n    }\n\n","sourceOld":"    public void testTokenizerUntokenizedOnly() throws Exception {\n      String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n      Set<String> untoks = new HashSet<>();\n      untoks.add(WikipediaTokenizer.CATEGORY);\n      untoks.add(WikipediaTokenizer.ITALICS);\n      Tokenizer tf = tokenizerFactory(WIKIPEDIA, TOKEN_OUTPUT, new Integer(WikipediaTokenizer.UNTOKENIZED_ONLY).toString(), UNTOKENIZED_TYPES, WikipediaTokenizer.CATEGORY + \", \" + WikipediaTokenizer.ITALICS).create(newAttributeFactory());\n      tf.setReader(new StringReader(test));\n      assertTokenStreamContents(tf,\n                                new String[] { \"a b c d\", \"e f g\", \"link\", \"here\", \"link\",\n                                               \"there\", \"italics here\", \"something\", \"more italics\", \"h   i   j\" },\n                                new int[] { 11, 32, 42, 47, 56, 61, 71, 86, 98, 124 },\n                                new int[] { 18, 37, 46, 51, 60, 66, 83, 95, 110, 133 },\n                                new int[] { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 }\n      );\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a9ff0aa05c1ae214397beec2eae8ce7b214dd4c4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"b6a269c1ddba3f8c9fa9a40572ecc538eddda41a":["28288370235ed02234a64753cdbf0c6ec096304a"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":["28288370235ed02234a64753cdbf0c6ec096304a","628903f37b6c442da0d390db1c6af9a0e74d41a7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"f344bb33ca91f48e99c061980115b46fa84fc8f5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","a9ff0aa05c1ae214397beec2eae8ce7b214dd4c4"],"28288370235ed02234a64753cdbf0c6ec096304a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f344bb33ca91f48e99c061980115b46fa84fc8f5"],"628903f37b6c442da0d390db1c6af9a0e74d41a7":["28288370235ed02234a64753cdbf0c6ec096304a","b6a269c1ddba3f8c9fa9a40572ecc538eddda41a"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f344bb33ca91f48e99c061980115b46fa84fc8f5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["628903f37b6c442da0d390db1c6af9a0e74d41a7"]},"commit2Childs":{"a9ff0aa05c1ae214397beec2eae8ce7b214dd4c4":["f344bb33ca91f48e99c061980115b46fa84fc8f5"],"b6a269c1ddba3f8c9fa9a40572ecc538eddda41a":["628903f37b6c442da0d390db1c6af9a0e74d41a7"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a9ff0aa05c1ae214397beec2eae8ce7b214dd4c4","f344bb33ca91f48e99c061980115b46fa84fc8f5","28288370235ed02234a64753cdbf0c6ec096304a","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"f344bb33ca91f48e99c061980115b46fa84fc8f5":["28288370235ed02234a64753cdbf0c6ec096304a","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"28288370235ed02234a64753cdbf0c6ec096304a":["b6a269c1ddba3f8c9fa9a40572ecc538eddda41a","0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","628903f37b6c442da0d390db1c6af9a0e74d41a7"],"628903f37b6c442da0d390db1c6af9a0e74d41a7":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}